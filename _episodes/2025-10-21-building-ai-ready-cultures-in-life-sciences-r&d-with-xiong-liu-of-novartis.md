---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: e. Again, that's emerj.com/expert-one. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2161
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2196
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2562
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2624
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5305
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7031
- category: unknown
  confidence: medium
  context: generative AI coming down the horizon right now. Generative AI seems to
    have just found its place, but it's also
  name: Generative AI
  position: 23966
- category: ai_research
  confidence: high
  context: The organization hosting the podcast and publishing content related to
    AI adoption and thought leadership.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company where the guest (Jung Liu) is the Director of Data Science
    in AI, focusing on applying generative AI in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Mentioned as an example of a company whose CIO has been featured as an
    executive thought leader on the podcast.
  name: Goldman Sachs
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Mentioned as an example of a company whose head of AI has been featured
    as an executive thought leader on the podcast.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    sense of complex information by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The company that developed the Notebook LM tool and is credited with proposing
    the transformer architecture.
  name: Google
  source: llm_enhanced
- category: consulting/research
  confidence: high
  context: Mentioned as the source of a recent survey regarding generative AI investment
    and scaling in the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced indirectly through the mention of 'GPT-like models' and the
    explosion of LLMs, which are often associated with OpenAI's GPT series.
  name: OpenAI (implied via GPT)
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Referenced indirectly through the mention of 'GPT-like models' and the
    explosion of LLMs, often associated with Google's models as well.
  name: Google (implied via GPT)
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Credited with proposing the transformer architecture, which underpins modern
    LLMs and foundation models.
  name: Google (implied via Transformer Architecture)
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The company behind the Notebook LM product.
  name: Google (implied via Notebook LM)
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as a predecessor language model to GPT-like models, used previously
    for information extraction in clinical trials.
  name: BERT (Model/Architecture)
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the architecture (transformer) being leveraged, particularly
    its decoding part, for generating new sequences/data.
  name: GPT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as being quite good at building foundation models and publishing
    results.
  name: Academics
  source: llm_enhanced
- category: general_tech
  confidence: medium
  context: Mentioned as sources of open public architectures that can be leveraged.
  name: Tech Companies
  source: llm_enhanced
- category: industry_user
  confidence: high
  context: Mentioned as entities that might need to build their own versions of models
    if leveraging published work is insufficient.
  name: Healthcare Companies
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 22:40:52 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 100
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs, foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 102
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models, but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 72
  relevance_score: 3
  text: So, we can use the data you have to train, to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: Clearly defines the paradigm shift from traditional, task-specific
    supervised learning to the use of large, pre-trained foundation models.
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs, foundation models.
  topic: technical
- impact_reason: 'Articulates the primary advantage of foundation models: enabling
    high performance on tasks with limited proprietary data through transfer learning.'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data.
  topic: technical
- impact_reason: 'Provides a concrete example of GenAI''s value in life sciences:
    leveraging public molecular data within foundation models to gain insights into
    specific disease pathways even with limited proprietary data.'
  relevance_score: 10
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So, for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data.
  topic: technical
- impact_reason: A powerful analogy suggesting that biological systems (genetics)
    can be treated as a language, making them amenable to LLM architectures for simulation
    and drug development acceleration.
  relevance_score: 10
  source: llm_enhanced
  text: Genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of much of across the drug development process.
  topic: predictions
- impact_reason: Directly links the current wave of Gen AI success (including domain-specific
    models) back to the foundational importance of the Transformer architecture.
  relevance_score: 10
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Provides a clear, technical distinction between BERT-like models
    (encoders, good for representation/feature extraction) and the next generation.
  relevance_score: 10
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they use the decoder
    part of the transformer. So you can think it's as a very good representation model.
    So many, it can capture the hidden context among the, in the text between words,
    etc. that where a human sees. So the outcome is some representations or embeddings
    of those text tokens, etc.
  topic: technical
- impact_reason: Explains the fundamental shift from representation learning (BERT)
    to generative capabilities (GPT) via the decoder stack, which is crucial for understanding
    modern LLMs.
  relevance_score: 10
  source: llm_enhanced
  text: but now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: Highlights the critical danger of hallucinations in high-stakes domains
    like drug discovery, where validation requires slow, expensive physical experiments
    rather than instant text/code checks.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: 'Proposes a necessary solution for model selection in specialized
    fields: creating hybrid evaluation metrics that combine AI performance assessment
    with deep domain expertise checks.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is key here,
    because there could be so many versions of found data models. Now, if we, you
    know, have those kind of domain plus AI-driven those evaluation metrics, so we
    can score across different models, right?
  topic: safety/strategy
- impact_reason: Highlights the significant gap between investment interest and successful,
    scaled deployment of GenAI in the life sciences sector, a key business challenge.
  relevance_score: 9
  source: llm_enhanced
  text: Flash forward to 2024, a recent Deloitte survey notes that while 73% of life
    sciences leaders are investing in generative AI initiatives, fewer than 20% have
    successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: 'Explains the core principle of foundation models: leveraging massive,
    domain-wide data (even seemingly irrelevant data) during pre-training to build
    general capabilities.'
  relevance_score: 9
  source: llm_enhanced
  text: The idea is that yes, you still care about the specific data you are working
    on, but at the same time, we want to collect all the available data, relevant
    data, or irrelevant data, but as long as they fall into the same domain, and feed
    those data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical
- impact_reason: Describes a strategic shift in data infrastructure—centralizing data
    first to enable broader, strategic algorithmic applications, moving beyond simple
    document processing.
  relevance_score: 9
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later... from that
    data repository we can drive more advanced algorithms to help with strategic tasks
    across the entire organization.
  topic: strategy
- impact_reason: Offers a precise technical explanation of fine-tuning, clarifying
    that it involves adjusting weights rather than full retraining, which is crucial
    for efficient domain adaptation.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models, but we can still take
    the data you have, right? So, we can use the data you have to train, to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems.
  topic: technical
- impact_reason: 'Highlights a cutting-edge application: using GenAI to synthesize
    biological data (cell types, gene expressions) for *in silico* testing, accelerating
    preclinical research.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, Gen AI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: 'Lists three critical practical challenges for AI adoption in R&D:
    benchmarking, cross-functional alignment, and measuring ROI.'
  relevance_score: 9
  source: llm_enhanced
  text: Building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: 'Highlights a key benefit of using foundation models in drug discovery:
    focusing research efforts on high-potential areas identified through in silico
    methods.'
  relevance_score: 9
  source: llm_enhanced
  text: So that way can allow us to better focus on discovering, you know, novel targets,
    novel gene pathways, etc.
  topic: predictions/business
- impact_reason: Details the power of generative models in molecular design, emphasizing
    the ability to incorporate complex, multi-objective constraints (like toxicity
    and solubility) during generation.
  relevance_score: 9
  source: llm_enhanced
  text: So now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So, for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: predictions/technical
- impact_reason: Stresses the indispensable role of domain experts in guiding data
    selection and, critically, in the benchmarking and validation process for specialized
    foundation models.
  relevance_score: 9
  source: llm_enhanced
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves... we still need the domain knowledge
    people, you know, people have those kind of expertise to guide the, you know,
    the selection of the data and also more importantly, then these models are, you
    build, now how do we benchmark with them?
  topic: strategy/business
- impact_reason: 'Provides a pragmatic strategy for AI adoption in specialized fields
    like healthcare: a hybrid approach combining leveraging public models with building
    proprietary, expert-guided versions, necessitating deep cross-discipline collaboration.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think it's a combination, it's a hybrid method, right? So we could leverage
    what's already been published. And also, if for the not working, you know, specifically,
    then we have to build our own versions, you know, then again, we have to have
    this AI, computer science, domain experts in house, we need to, you know, work
    with those, of course, disciplinary functional teams, disease areas, biologists,
    chemistries, etc. So, yeah, so it's basically a cross-discipline collaboration,
    whether in house or externally, yeah.
  topic: strategy/business
- impact_reason: 'Identifies a major bottleneck in advancing domain-specific AI: the
    speed and availability of experimental, real-world data collection often lags
    behind theoretical model development.'
  relevance_score: 9
  source: llm_enhanced
  text: we still rely on those traditional biotechnology methods to collect those
    data through experiments on cell lines, on human tissues, etc. So, there's, you
    know, where AI, the architecture, the concept there, but the, sometimes we are
    limited by the, you know, experimental side.
  topic: limitations/strategy
- impact_reason: Provides a concise definition and justification for the term 'foundation
    model' in the context of broad applicability.
  relevance_score: 8
  source: llm_enhanced
  text: So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Raises a critical business and ecosystem question regarding the build
    vs. buy decision for foundational models in specialized industries like healthcare.
  relevance_score: 8
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
- impact_reason: Reiterates that the underlying technological commonality across diverse
    foundation models (text, image, molecular) is the Transformer architecture.
  relevance_score: 8
  source: llm_enhanced
  text: So all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: 'A classic but crucial reminder: model sophistication does not negate
    the fundamental requirement for high-quality, domain-specific data, especially
    when moving beyond general public datasets.'
  relevance_score: 8
  source: llm_enhanced
  text: the first thing is about the data challenge, right? So, so the foundation
    models, although they take all kinds of available data, but still they require
    high-quality data.
  topic: business/technical
- impact_reason: Pinpoints organizational alignment and cross-functional communication
    (scientists, modelers, leadership) as a primary hurdle for successful AI implementation.
  relevance_score: 8
  source: llm_enhanced
  text: there's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
source: Unknown Source
summary: '## Comprehensive Summary: Building AI-Ready Cultures in Life Sciences R&D


  This 30-minute podcast episode, featuring **Zhong Liu, Director of Data Science
  in AI at Novartis**, focuses on the transformative shift in Life Sciences R&D driven
  by **Generative AI (GenAI)** and **Foundation Models (FMs)**, and the cultural and
  technical challenges associated with scaling these technologies beyond pilot projects.


  ### 1. Focus Area

  The discussion centers on the evolution of AI in life sciences, moving from traditional,
  task-specific Machine Learning (ML) (supervised, unsupervised, deep learning like
  GNNs) to the paradigm shift introduced by **Foundation Models (FMs) and Large Language
  Models (LLMs)**. Key application areas discussed include accelerating **molecule
  design, digital twin creation, protocol optimization, and target identification**
  for specific indications (e.g., lung cancer). A major theme is the necessity of
  building **AI-ready cultures** that bridge the gap between AI practitioners and
  domain scientists.


  ### 2. Key Technical Insights

  *   **Foundation Model Advantage:** FMs, built on massive, diverse datasets (including
  public domain data like cell atlases), capture broad, relevant information. This
  allows for effective downstream tasks even when proprietary data for a specific
  indication is limited, leveraging **transfer learning** principles.

  *   **Evolution of Language Models:** The shift from models like **BERT** (which
  provided powerful text representations/embeddings for traditional ML) to **GPT-like
  architectures** (which leverage the decoder part of the transformer) enables the
  **generation of new data**—such as novel molecular sequences (represented via SMILES
  strings or graphs) constrained by desired properties (e.g., toxicity, solubility).

  *   **Fine-Tuning vs. Retraining:** Domain-specific adaptation is achieved through
  **fine-tuning** the pre-trained FM by adjusting weights using proprietary data,
  rather than retraining the entire large model, ensuring personalization and improved
  prediction accuracy for specific R&D problems.


  ### 3. Business/Investment Angle

  *   **Scaling Challenge:** Despite high investment interest (73% of life sciences
  leaders investing in GenAI), fewer than 20% have successfully scaled beyond pilots,
  indicating significant hurdles in operationalizing the technology.

  *   **Data Dependency vs. Architectural Advancement:** While architectural breakthroughs
  (like transformers) are public, the bottleneck often remains the availability and
  quality of high-throughput, experiment-driven data (e.g., single-cell RNA-seq) needed
  to fully realize FM potential in specialized areas.

  *   **ROI and Evaluation:** Measuring the Return on Investment (ROI) for GenAI in
  R&D requires establishing **domain-driven evaluation standards** and **knowledge-checking
  benchmarks** to validate outputs (like generated molecules or pathways) that cannot
  be instantly verified, unlike text or code generation.


  ### 4. Notable Companies/People

  *   **Zhong Liu (Novartis):** The expert guest, providing insights from his role
  as Director of Data Science in AI, focusing on practical implementation and cultural
  alignment within a major pharmaceutical company.

  *   **Google (Transformer Architecture):** Mentioned as the source of the foundational
  transformer architecture that underpins modern FMs.

  *   **Deloitte:** Cited for a recent survey highlighting the gap between GenAI investment
  and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is moving toward a hybrid development model, leveraging publicly available
  FMs while simultaneously building proprietary, domain-specific versions tailored
  to complex biological systems. The future success hinges on **cross-disciplinary
  collaboration** between AI experts, computer scientists, and domain experts (biologists,
  chemists) to guide data selection, model development, and rigorous, domain-specific
  validation. The concept of **digital twins** is implicitly supported, where foundational
  models, informed by population genetics/biology, can be tailored to simulate specific
  patient responses.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Professionals** working in Life Sciences
  (Data Scientists, ML Engineers), **R&D Leadership** responsible for technology adoption
  and strategy, and **Business Executives** navigating AI investment decisions and
  seeking to understand the operational challenges of scaling GenAI in drug discovery
  and clinical workflows.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 121
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 11
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 22:40:52 UTC -->
