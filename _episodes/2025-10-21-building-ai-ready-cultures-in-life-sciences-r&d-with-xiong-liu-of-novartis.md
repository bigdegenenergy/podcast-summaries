---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2159
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2194
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2559
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2621
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5301
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7025
- category: unknown
  confidence: medium
  context: generative AI coming down the horizon right now. Generative AI seems to
    have just found its place, but it's also
  name: Generative AI
  position: 24001
- category: ai_media_and_research
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI and LLMs in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    indicating their involvement in enterprise AI implementation.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, indicating their involvement in enterprise AI implementation.
  name: Raytheon
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    connections by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: ai_infrastructure_research
  confidence: high
  context: Mentioned as the originator of the transformer architecture, which is foundational
    to modern LLMs and GenAI models.
  name: Google
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: The URL provided for the Notebook LM tool, confirming its association with
    Google.
  name: NotebookLM.google.com
  source: llm_enhanced
- category: consulting_research
  confidence: medium
  context: Mentioned in reference to a recent survey regarding generative AI investment
    and scaling in the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_application_model
  confidence: high
  context: Mentioned as a benchmark for the explosion of LLMs, contrasting with earlier
    models like BERT.
  name: ChatGPT
  source: llm_enhanced
- category: ai_research_model
  confidence: high
  context: Mentioned as an earlier language model architecture developed before GPT-style
    models, used for information extraction in clinical trials.
  name: BERT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a model leveraging the decoding part of the
    transformer architecture for generation.
  name: GPT
  source: llm_enhanced
- category: general_tech
  confidence: medium
  context: Referenced as a source of open public architectures that can be leveraged.
  name: tech companies
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced as being good at building foundation models and publishing results.
  name: academics
  source: llm_enhanced
- category: industry_user
  confidence: medium
  context: Mentioned in the context of building or leveraging foundation models, often
    requiring a hybrid approach.
  name: healthcare companies
  source: llm_enhanced
- category: workforce_role
  confidence: high
  context: Professionals involved in building pipelines and models quickly.
  name: AI modelers
  source: llm_enhanced
- category: workforce_role
  confidence: high
  context: Experts needed to validate models and provide domain knowledge.
  name: scientists / basic scientists
  source: llm_enhanced
- category: organizational
  confidence: low
  context: Mentioned in the context of AI adoption within organizations.
  name: leadership teams
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 23:40:44 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 102
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models, but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 71
  relevance_score: 3
  text: So, we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: 'This is a core definition of the shift: from task-specific, small-data
    training to leveraging massive, domain-wide pre-trained models (foundation models)
    that capture broad knowledge.'
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs foundation models. So
    the idea is that yes, you still care about the specific data you are working on,
    but at the same time, we want to collect all the available data, relevant data,
    or irrelevant data, as long as they fall into the same domain, and fill those
    data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical
- impact_reason: 'Highlights the primary business/technical advantage of foundation
    models: enabling performance on low-resource, domain-specific tasks through transfer
    learning/pre-training.'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: 'Provides a concrete, high-value example in life sciences: using
    pre-trained molecular foundation models (like those trained on cell atlases) to
    gain insights even with limited proprietary data for a specific indication.'
  relevance_score: 10
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So, for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data. So, for example, cell atlases, etc.
  topic: AI in life sciences
- impact_reason: 'Provides specific, cutting-edge applications of GenAI in R&D: synthetic
    data generation (new cell types/gene expressions) for in silico testing, accelerating
    target discovery.'
  relevance_score: 10
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: AI in life sciences
- impact_reason: Crucial insight into constrained generative modeling, showing how
    foundation models can optimize outputs based on multiple, complex real-world criteria
    simultaneously (e.g., toxicity and solubility).
  relevance_score: 10
  source: llm_enhanced
  text: So now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So, for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: Technical insights/Business advice
- impact_reason: 'A critical limitation: AI potential is currently constrained by
    the speed and feasibility of physical data collection (experimental bottleneck).'
  relevance_score: 10
  source: llm_enhanced
  text: I mean, that is potentially possible. But unfortunately, to get those, you
    know, single-cell RNA-seq data, all those kind of more, you know, experiment-driven
    data, probably we're not exactly there yet, because, you know, we still rely on
    those traditional biotechnology methods to collect those data through experiments
    on cell lines, on human tissues, etc. So, there's, you know, where AI, the architecture,
    the concept there, but the, sometimes we are limited by the, you know, experimental
    side.
  topic: Limitations/Challenges
- impact_reason: Highlights the extreme danger of hallucination in high-stakes domains
    like drug discovery, where outputs cannot be instantly verified like text or code.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: Safety/Limitations
- impact_reason: Clearly delineates the historical progression of ML/DL techniques
    leading up to the current foundational model paradigm shift, which is crucial
    context for the life sciences audience.
  relevance_score: 9
  source: llm_enhanced
  text: we move to the deep learning techniques like convolutional neural networks,
    RNNs, and graph neural networks. And more recently, we have a paradigm shift into
    those language models, foundation models.
  topic: technical
- impact_reason: Describes a strategic shift in data management—centralizing and leveraging
    massive, diverse data lakes to power broad, strategic AI applications, moving
    beyond simple task automation.
  relevance_score: 9
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization that need that kind
    of whole-scale approach.
  topic: strategy
- impact_reason: Offers a precise, accessible definition of fine-tuning versus full
    retraining, which is a critical concept for organizations adopting LLMs.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models, but we can still take
    the data you have, right? So, we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems.
  topic: technical
- impact_reason: A powerful analogy equating biological code (genetics) to language,
    justifying the application of LLMs/language model architectures to molecular and
    genetic data for drug discovery simulation.
  relevance_score: 9
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of much of across the drug development process.
  topic: AI in life sciences
- impact_reason: A critical statistic highlighting the massive gap between investment/interest
    and successful, scaled deployment in the life sciences sector.
  relevance_score: 9
  source: llm_enhanced
  text: 73% of life sciences leaders are investing in generative AI initiatives, fewer
    than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: 'Lists the key practical challenges facing adoption: evaluation,
    alignment, and creating relevant performance metrics (benchmarks) for specialized
    AI.'
  relevance_score: 9
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: Confirms the Transformer architecture as the foundational backbone
    for modern GenAI and domain-specific foundation models.
  relevance_score: 9
  source: llm_enhanced
  text: So all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: Technical insights
- impact_reason: 'Outlines the strategy for creating domain-specific AI: applying
    the general Transformer paradigm to specialized datasets (chemical, clinical data).'
  relevance_score: 9
  source: llm_enhanced
  text: So people in the healthcare have been thinking similarly, can we put all the
    available, you know, data, chemical data, clinical trial data, etc. into those,
    you know, transformer architecture? So that way we can also build our domain-specific
    foundation models to enable a lot of downstream tasks.
  topic: Strategy/Technical insights
- impact_reason: Distinguishes GPT-style models (decoder-based) from BERT-style models
    (encoder-based) by highlighting the generative capability enabled by the decoder
    stack.
  relevance_score: 9
  source: llm_enhanced
  text: But now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data. So for example, a
    new types of sequence that represent your language.
  topic: Technical insights
- impact_reason: Emphasizes the non-negotiable role of domain experts (SMEs) in data
    curation, model development, and, critically, benchmarking.
  relevance_score: 9
  source: llm_enhanced
  text: But, you know, we still need the domain knowledge people, you know, people
    have those kind of expertise to guide the, you know, the selection of the data
    and also more importantly, then these models are, you build, now how do we benchmark
    with them? So, we would definitely need those, the domain experts to be part of
    this model development process.
  topic: Strategy/Business advice
- impact_reason: 'Proposes the solution to the hallucination/validation problem: evaluation
    must be driven by a combination of domain expertise and AI metrics.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is key here,
    because there could be so many versions of found data models.
  topic: Safety/Evaluation
- impact_reason: 'Strategic advice for domain experts: they must understand the evolving
    AI paradigm, recognizing that AI adoption is hybrid, not a complete replacement
    of old methods.'
  relevance_score: 9
  source: llm_enhanced
  text: It behooves them [subject matter experts] to kind of have some awareness of
    the new and old paradigm that we were talking about before, how both are still
    present, but how we thought that AI would impact life sciences organization or
    that they'd just be this one school way of doing things is not the case. And that
    door remains open.
  topic: Strategy/Business advice
- impact_reason: 'Clarifies the architecture: Foundation Model (broad knowledge) ->
    Fine-tuning/Smaller Models (task specialization), all underpinned by a massive
    data infrastructure.'
  relevance_score: 8
  source: llm_enhanced
  text: We're also describing the relationship as you'll have LLMs a foundational
    model and then break off into smaller models for smaller tasks, but this larger
    data repository is going to be the method of driving efficiencies across the organization.
  topic: technical
- impact_reason: 'Describes the ideal state of personalized medicine enabled by foundation
    models: learning population norms and applying that knowledge to individual patient
    data (the digital twin concept).'
  relevance_score: 8
  source: llm_enhanced
  text: foundational models will be able to, you know, track what these normal human
    bodily functions are like across populations and then curtail that data to very
    personalized circumstances among patients.
  topic: predictions
- impact_reason: 'Highlights the dual focus in AI development: general public models
    and highly specialized, domain-specific foundation models (like in healthcare).'
  relevance_score: 8
  source: llm_enhanced
  text: So there's a lot of ongoing for the foundation models in the public and also
    the domain-specific scenarios like healthcare, drug discovery, development.
  topic: AI technology trends
- impact_reason: 'Explains the core mechanism driving the success of large models:
    leveraging massive, diverse public data via the Transformer architecture for broad
    applicability.'
  relevance_score: 8
  source: llm_enhanced
  text: So now the difference is that we see the benefits of putting a lot of the
    generic public data like text, images into those transformers so we can do, you
    know, so many downstream tasks.
  topic: Technical insights
- impact_reason: 'A clear explanation of the utility of encoder-only models like BERT:
    generating high-quality, abstract feature representations for traditional ML tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: The outcome is some representations or embeddings of those text tokens, etc.
    So those embeddings are token representations they have predictive power. So you
    can think them as more advanced abstracted features to your machine learning models.
  topic: Technical insights
- impact_reason: Highlights the necessity of combining public domain data with proprietary,
    internal data for building truly effective, domain-specific foundation models.
  relevance_score: 8
  source: llm_enhanced
  text: The next question is about the data, right? What are the data? ... you could
    also leverage public data and also your protocols, which may be from publicly,
    there's a lot of, like, clinical trials registry, but also your own company owned
    those kinds of proprietary information, etc. So, those things could potentially
    are building into those models.
  topic: Business advice/Strategy
- impact_reason: Identifies a structural advantage for language-heavy scientific domains
    (like genetics) in adopting early GenAI technologies compared to other life science
    areas.
  relevance_score: 8
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: Strategy/Predictions
- impact_reason: Reiterates that data quality remains the primary bottleneck, even
    when leveraging massive datasets for foundation models.
  relevance_score: 8
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: Challenges/Business advice
- impact_reason: Shifts focus from technical challenges to organizational/cultural
    hurdles, stressing the need for alignment across scientists, modelers, and leadership.
  relevance_score: 8
  source: llm_enhanced
  text: So, it's quite important that we have to do those benchmarking and select
    models accordingly. And then it goes to the execution, right? So, there's a lot
    of cultural organizational issue. I see the key that really we should align, you
    know, between different parties, like scientists, basic scientists, or AI modelers,
    AI professionals, and also leadership teams, AI adoption, etc.
  topic: Business advice/Strategy
- impact_reason: Reiterates that the underlying technical commonality across diverse
    foundation models (text, image, molecular) is the Transformer architecture.
  relevance_score: 7
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Provides historical context, noting that specialized NLP/LM applications
    in regulated fields preceded the public explosion of models like GPT.
  relevance_score: 7
  source: llm_enhanced
  text: We were one of the first to develop language models for clinical trials, information
    extraction and the protocol design. So at that time that was before ChatGPT, right?
    Yeah, before that, there's also another language model called BERT.
  topic: Technical insights/History
- impact_reason: 'Summarizes the organizational requirement for successful AI implementation:
    mandatory collaboration between AI experts and domain scientists.'
  relevance_score: 7
  source: llm_enhanced
  text: So, it's basically a cross-discipline collaboration, whether in house or externally,
    yeah.
  topic: Strategy
- impact_reason: Identifies effective communication and alignment as the central organizational
    key to successful GenAI adoption.
  relevance_score: 7
  source: llm_enhanced
  text: how do we ensure, you know, the effective communication alignment between
    those parties is key, right?
  topic: Strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Building AI-Ready Cultures in Life Sciences
  R&D - with Xiong Liu of Novartis


  This 30-minute episode features **Xiong Liu, Director of Data Science in AI at Novartis**,
  discussing the paradigm shift in Life Sciences R&D driven by **Generative AI (GenAI)**
  and **Foundation Models (FMs)**, and the cultural and technical challenges of scaling
  these technologies beyond pilot projects.


  ---


  ### 1. Focus Area

  The primary focus is the transition in Life Sciences R&D from traditional, task-specific
  Machine Learning (ML) to leveraging large-scale **Foundation Models** (like LLMs)
  for accelerating drug discovery, molecular design, and clinical protocol optimization.
  Key themes include the architectural shift in AI, the necessity of domain-specific
  fine-tuning, and the organizational alignment required for successful AI adoption.


  ### 2. Key Technical Insights

  *   **Paradigm Shift to Foundation Models:** The discussion contrasts older ML (requiring
  labeled, proprietary data for specific tasks) with FMs, which are pre-trained on
  massive, domain-relevant datasets (like molecular data or public text corpora).
  This allows smaller, specific datasets to benefit from generalized knowledge captured
  by the FM, enabling downstream tasks even with limited proprietary data.

  *   **BERT vs. GPT Architectures in Life Sciences:** The evolution from models like
  BERT (excellent for representation/embeddings) to GPT-like models (leveraging the
  decoder for data generation) is highlighted. This enables the generation of novel
  sequences, such as new molecular structures represented by SMILES strings or graphs,
  constrained by desired properties (e.g., toxicity, solubility).

  *   **Domain-Specific Fine-Tuning:** While FMs provide a powerful baseline, effective
  application in life sciences requires **fine-tuning**—adjusting the model’s weights
  using proprietary, high-quality domain data to personalize predictions for specific
  indications (e.g., lung cancer pathways).


  ### 3. Business/Investment Angle

  *   **Scaling Challenge:** Despite high investment interest (73% of leaders investing),
  few life sciences organizations (under 20%) have successfully scaled GenAI beyond
  pilots, indicating significant hurdles in implementation and ROI validation.

  *   **Data Dependency vs. Model Availability:** While foundational architectures
  are often open-source or publicly available, the bottleneck shifts to acquiring
  and integrating high-quality, domain-specific experimental data (like single-cell
  RNA-seq) needed for effective fine-tuning and validation.

  *   **ROI and Evaluation:** Establishing clear, **domain-driven evaluation metrics**
  is crucial for measuring the ROI of AI-driven research, especially given the risk
  of model **hallucination** in complex biological outputs that cannot be instantly
  verified like text or code.


  ### 4. Notable Companies/People

  *   **Xiong Liu (Novartis):** The guest, providing insights from his role as Director
  of Data Science in AI, emphasizing practical implementation and cultural readiness
  within a major pharmaceutical company.

  *   **Google/OpenAI (Implied):** Mentioned in the context of the foundational **Transformer
  architecture** breakthrough that underpins modern LLMs.

  *   **Deloitte:** Referenced for a recent survey highlighting the gap between GenAI
  investment and successful scaling in the life sciences sector.


  ### 5. Future Implications

  The industry is moving toward a hybrid model where public FMs are leveraged, but
  significant in-house expertise is needed to build, fine-tune, and validate domain-specific
  models. The future success of AI adoption hinges not just on technology but on **building
  AI-ready cultures** characterized by tight alignment and effective communication
  between AI professionals, basic scientists, and leadership. The concept of using
  FMs to model biological systems is closely linked to accelerating concepts like
  **digital twins** in drug development.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Leaders** in Pharma/Biotech, **R&D
  Directors**, **Data Science Managers**, and **Strategy Executives** navigating the
  practical adoption, cultural integration, and investment justification for advanced
  AI/ML technologies in the highly regulated life sciences domain.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 120
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 14
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 23:40:44 UTC -->
