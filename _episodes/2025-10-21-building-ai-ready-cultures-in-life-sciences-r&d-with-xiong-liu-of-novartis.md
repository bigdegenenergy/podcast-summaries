---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2159
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2194
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2559
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2621
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5298
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7016
- category: ai_media_and_analysis
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI and LLMs in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    indicating enterprise AI adoption.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, indicating enterprise AI adoption.
  name: Raytheon
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: An AI-first tool for organizing ideas and making connections, built by
    Steven Johnson, designed for sense-making of complex information.
  name: Notebook LM
  source: llm_enhanced
- category: ai_infrastructure_research
  confidence: high
  context: Mentioned as the proposer of the transformer architecture, which is the
    breakthrough underpinning modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: consulting_and_analysis
  confidence: high
  context: Mentioned for conducting a recent survey on generative AI investment in
    the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_model_application
  confidence: high
  context: Mentioned as a benchmark/reference point for the capabilities of modern
    generative models (GPT-like models), contrasting with earlier models like BERT.
  name: ChatGPT
  source: llm_enhanced
- category: ai_model_research
  confidence: high
  context: Mentioned as an earlier language model (pre-ChatGPT) whose architecture
    (decoder part of the transformer) was applied to clinical trial documents for
    information extraction.
  name: BERT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of models leveraging the transformer architecture
    for data generation, specifically sequence generation like language.
  name: GPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: General reference to Large Language Models, which are a core part of generative
    AI technology.
  name: LLMs
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: General reference to Generative AI, the technology paradigm being discussed.
  name: Gen AI
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 10:53:22 +0000
quotes:
- length: 241
  relevance_score: 6
  text: We have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 102
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models, but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 121
  relevance_score: 4
  text: So, you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning
  topics: []
- length: 86
  relevance_score: 4
  text: But now, the GPT, they also leverage the decoding part of the transformer
    architecture
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 167
  relevance_score: 4
  text: Although the goal of the foundation model is, it's tried to, you know, leverage
    all the available data, minimizing, you know, the specifics, you know, of specific
    data
  topics: []
- length: 126
  relevance_score: 4
  text: It's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right
  topics: []
- length: 71
  relevance_score: 3
  text: So, we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 106
  relevance_score: 3
  text: So, that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: Clearly defines the historical evolution of ML/DL and pinpoints the
    current 'paradigm shift' toward foundation models, which is crucial for understanding
    modern AI strategy.
  relevance_score: 10
  source: llm_enhanced
  text: we move to the deep learning techniques like convolutional neural networks,
    RNNs, and graph neural networks. And more recently, we have a paradigm shift into
    those language models, foundation models.
  topic: technical
- impact_reason: Contrasts the traditional supervised learning approach (task-specific
    data collection/labeling) with the foundation model approach, explaining the core
    difference in methodology.
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs foundation models.
  topic: technical
- impact_reason: 'This explains the primary value proposition of foundation models:
    knowledge transfer and performance improvement even with limited domain-specific
    data (low-data regimes).'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data.
  topic: technical
- impact_reason: Provides a concrete, high-impact example of how foundation models
    leverage massive public biological data (like cell atlases) to aid specific research
    (disease pathways) where proprietary data is scarce.
  relevance_score: 10
  source: llm_enhanced
  text: But now with foundation models, it has a lot of pre-trained models based on
    publicly available molecular data. So, for example, cell atlases, etc. So, there's
    a lot of different cell types that gene expression data already being built into
    those foundation models.
  topic: technical
- impact_reason: 'This is a powerful conceptual link: treating genetics/biological
    sequences as a ''language'' makes them directly amenable to LLM architectures,
    suggesting massive potential for in-silico drug testing.'
  relevance_score: 10
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process...
  topic: predictions
- impact_reason: 'Illustrates a powerful, practical application of generative foundation
    models in drug discovery: constrained, simultaneous optimization of molecular
    properties.'
  relevance_score: 10
  source: llm_enhanced
  text: So, for example, you can generate many different types of new molecules, but
    you can add constraints like have, you know, better toxicity, solubility, etc.
    It can allow you to, you know, do that simultaneously.
  topic: predictions/business
- impact_reason: 'Identifies a major bottleneck in advancing AI in life sciences:
    the speed and availability of high-quality, experimental, real-world data are
    lagging behind algorithmic potential.'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
    So, there's, you know, where AI, the architecture, the concept there, but the,
    sometimes we are limited by the, you know, experimental side.
  topic: limitations
- impact_reason: Directly addresses the hallucination problem and stresses that in
    high-stakes domains like biomedicine, validation must rely on external, domain-specific
    experimental verification, not just internal model confidence.
  relevance_score: 10
  source: llm_enhanced
  text: But a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... when it goes to the bio-medical, biochemistry, etc....
    we have to through experiments, through our benchmarking with known knowledge
    to, you know, evaluate these models.
  topic: safety/limitations
- impact_reason: This highlights a massive gap between investment interest and successful,
    scaled implementation in the life sciences sector, a critical business challenge
    for AI adoption.
  relevance_score: 9
  source: llm_enhanced
  text: while 73% of life sciences leaders are investing in generative AI initiatives,
    fewer than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: Provides a concise definition and rationale for the term 'foundation
    model' in the context of broad applicability.
  relevance_score: 9
  source: llm_enhanced
  text: So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Offers a clear, non-technical explanation of the fine-tuning process,
    emphasizing weight adjustment rather than full retraining, which is vital for
    efficient domain adaptation.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models, but we can still take
    the data you have, right? So, we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems...
  topic: technical
- impact_reason: 'Describes the ideal state of personalized medicine enabled by foundation
    models: establishing population baselines and then customizing predictions for
    individuals.'
  relevance_score: 9
  source: llm_enhanced
  text: foundational models will be able to, you know, track what these normal human
    bodily functions are like across populations and then curtail that data to very
    personalized circumstances among patients.
  topic: predictions
- impact_reason: 'Highlights a cutting-edge application: using GenAI to *generate*
    synthetic biological data (cell types, gene expressions) for simulation, accelerating
    preclinical testing.'
  relevance_score: 9
  source: llm_enhanced
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: Highlights the foundational role of the Transformer architecture
    (from Google) as the basis for modern foundation models, including those in specialized
    domains like healthcare.
  relevance_score: 9
  source: llm_enhanced
  text: So, that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago.
  topic: technical
- impact_reason: 'Explains the key shift with GPT-style models: leveraging the decoder
    allows for generative capabilities, moving beyond just representation.'
  relevance_score: 9
  source: llm_enhanced
  text: But now, the GPT, they also leverage the decoding part of the transformer
    architecture. So, meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: Emphasizes the critical necessity of domain expertise (e.g., biology,
    chemistry) to guide data selection and benchmarking, even with powerful general
    models.
  relevance_score: 9
  source: llm_enhanced
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves. Although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data. But, you know, we still need
    the domain knowledge people, you know, people have those kind of expertise to
    guide the, you know, the selection of the data and also more importantly, then
    these models are, you build, now how do we benchmark with them?
  topic: strategy
- impact_reason: 'Proposes a crucial methodology for the future: hybrid evaluation
    metrics that combine AI performance indicators with domain-specific knowledge
    checks to select the best models.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is key here,
    because there could be so many versions of found data models. Now, if we, you
    know, have those kind of domain plus AI-driven those evaluation metrics, so we
    can score across different models, right?
  topic: strategy/safety
- impact_reason: 'Introduces a key strategic distinction for enterprise AI implementation:
    whether to use general models or specialize them for vertical needs.'
  relevance_score: 8
  source: llm_enhanced
  text: We're talking about generic AI versus the domain-specific Gen AI.
  topic: strategy
- impact_reason: Reiterates that the underlying commonality across diverse foundation
    models (text, image, molecular) is the Transformer architecture, linking them
    all back to a core technical breakthrough.
  relevance_score: 8
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So, that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Provides a clear technical distinction between BERT-like models (encoder-only,
    focusing on representation/embeddings) and generative models.
  relevance_score: 8
  source: llm_enhanced
  text: So, the difference is that the previously BERT architecture, they use the
    decoder part of the transformer. So, you can think it's as a very good representation
    model.
  topic: technical
- impact_reason: 'A concise summary of the organizational requirement for successful
    AI implementation in specialized fields: mandatory cross-disciplinary teamwork.'
  relevance_score: 8
  source: llm_enhanced
  text: So, it's basically a cross-discipline collaboration, whether in house or externally,
    yeah.
  topic: business
- impact_reason: 'Reiterates the enduring challenge in AI: the quality of data trumps
    sheer quantity, especially when building domain-specific models.'
  relevance_score: 8
  source: llm_enhanced
  text: So, the first thing is about the data challenge, right? So, so the foundation
    models, although they take all kinds of available data, but still they require
    high-quality data.
  topic: technical/business
- impact_reason: Shifts focus from technical challenges to organizational hurdles,
    pinpointing alignment and communication between scientists, modelers, and leadership
    as the key to successful adoption.
  relevance_score: 8
  source: llm_enhanced
  text: So, it's quite important that we have to do those benchmarking and select
    models accordingly. And then it goes to the execution, right? So, there's a lot
    of cultural organizational issue. I see the key that really we should align, you
    know, between different parties, like scientists, basic scientists, or AI modelers,
    AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: Highlights the gap between theoretical AI capability (modeling all
    phenotypes) and current experimental data collection reality.
  relevance_score: 8
  source: llm_enhanced
  text: So, we could potentially have all the, you know, human cells for different,
    you know, phenotypes of, you know, building to those models. I mean, that is potentially
    possible. But unfortunately, to get those, you know, single-cell RNA-seq data...
    probably we're not exactly there yet...
  topic: limitations
- impact_reason: 'Offers a strategic observation: the success of NLP provides a head
    start for genetics/biology because biological data (DNA, proteins) can be effectively
    modeled as sequences/language.'
  relevance_score: 7
  source: llm_enhanced
  text: It's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: strategy
- impact_reason: Provides a concrete, early-stage success story (pre-ChatGPT) demonstrating
    the tangible efficiency and accuracy gains from applying language models (like
    BERT) to structured domain documents.
  relevance_score: 7
  source: llm_enhanced
  text: We applied those kind of models into a clinical trial documents so that we
    found a lot of efficiency gain, accuracy gain in extracting information.
  topic: business
- impact_reason: 'A crucial strategic takeaway: AI adoption in life sciences will
    be a hybrid approach, not a complete replacement of existing methods, leaving
    room for continuous evolution.'
  relevance_score: 7
  source: llm_enhanced
  text: how both are still present, but how we thought that AI would impact life sciences
    organization or that they'd just be this one school way of doing things is not
    the case. And that door remains open.
  topic: strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Building AI-Ready Cultures in Life Sciences
  R&D


  **Title:** Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis

  **Duration:** 30 minutes


  This episode features a deep dive with **Xiong Liu, Director of Data Science in
  AI at Novartis**, focusing on the transformative shift in Life Sciences R&D driven
  by Generative AI (GenAI) and Foundational Models (FMs). The core narrative addresses
  the gap between the high investment in GenAI (73% of leaders) and the low success
  rate in scaling beyond pilots (under 20%). The discussion centers on moving from
  traditional, task-specific machine learning to leveraging large-scale FMs for accelerated
  discovery, digital twin creation, and workflow optimization.


  ---


  ### 1. Focus Area

  The discussion centers on the **paradigm shift in AI methodology within Life Sciences
  R&D**, specifically comparing traditional supervised/deep learning (using proprietary,
  labeled data) to the use of **Foundational Models (FMs) and Large Language Models
  (LLMs)**. Key application areas explored include **accelerating molecule design,
  generating cell/gene expression data (in silico methods), protocol optimization,
  and clinical trial information extraction.** A significant portion is dedicated
  to the **cultural and technical challenges** of integrating these models into scientific
  workflows.


  ### 2. Key Technical Insights

  *   **Foundation Model Advantage:** FMs, built on massive, diverse datasets (like
  public molecular data or general text corpora), provide essential background knowledge
  and generalized representations (embeddings) that significantly benefit downstream
  tasks, even when proprietary, task-specific data is scarce (e.g., studying a rare
  indication).

  *   **Transformer Architecture Evolution:** The shift from models like BERT (focused
  on representation/embeddings via the decoder part) to GPT-like models (leveraging
  the decoder part) enables generative capabilities—producing novel sequences (like
  new molecules represented by SMILES strings or graphs) constrained by desired properties
  (e.g., toxicity, solubility).

  *   **Fine-Tuning vs. Retraining:** Domain-specific adaptation of FMs is achieved
  through **fine-tuning**, which adjusts model weights based on proprietary data to
  personalize performance, rather than retraining the entire massive foundation model
  from scratch.


  ### 3. Business/Investment Angle

  *   **Scaling Bottleneck:** The primary business hurdle is moving GenAI initiatives
  from successful pilots to scaled, meaningful application across R&D and clinical
  workflows.

  *   **Data Quality vs. Quantity:** While FMs thrive on massive data, the need for
  high-quality, domain-specific data (e.g., experimental single-cell RNA-seq data)
  remains a limiting factor, constrained by the speed of traditional biotechnology
  experimentation.

  *   **ROI Evaluation Challenge:** Measuring the Return on Investment (ROI) for AI-driven
  research is complex because outputs (like novel molecules) cannot be instantly validated,
  necessitating the development of **domain-driven evaluation benchmarks** to score
  and select optimal models.


  ### 4. Notable Companies/People

  *   **Xiong Liu (Novartis):** Guest expert, Director of Data Science in AI, providing
  practical insights from a major pharmaceutical company on adopting FMs.

  *   **Google/OpenAI (Implied):** Referenced as sources of foundational architectural
  breakthroughs (Transformer architecture) and general-purpose LLMs that inspire domain-specific
  models.

  *   **Deloitte:** Cited for a recent survey highlighting the industry-wide challenge
  of scaling GenAI adoption.


  ### 5. Future Implications

  The industry is heading toward a **hybrid development model**, leveraging publicly
  available FMs while building proprietary, domain-specific versions where necessary.
  Success hinges on **cross-discipline collaboration** between AI modelers, basic
  scientists, and leadership. The future requires robust, objective **domain-driven
  evaluation metrics** to manage the inherent risk of model hallucination in high-stakes
  biological contexts.


  ### 6. Target Audience

  This episode is highly valuable for **Life Sciences R&D Executives, Chief Data Officers,
  AI/ML Leaders in Pharma/Biotech, and Data Scientists** focused on drug discovery,
  clinical operations, and enterprise AI strategy. It is relevant for professionals
  navigating the technical implementation and cultural adoption of advanced AI paradigms.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 120
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 12
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 10:53:22 UTC -->
