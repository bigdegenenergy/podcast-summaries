---
companies:
- category: unknown
  confidence: medium
  context: This is episode number 905 with Dr. Sebastian Garman, head of Responsible
    AI at Bloomberg. Today's epi
  name: Sebastian Garman
  position: 36
- category: unknown
  confidence: medium
  context: ode number 905 with Dr. Sebastian Garman, head of Responsible AI at Bloomberg.
    Today's episode is brought to you b
  name: Responsible AI
  position: 62
- category: unknown
  confidence: medium
  context: the conversational analytics platform, and by the Dell AI Factory with
    Nvidia. Welcome to the Super Data Science Po
  name: Dell AI Factory
  position: 188
- category: tech
  confidence: high
  context: alytics platform, and by the Dell AI Factory with Nvidia. Welcome to the
    Super Data Science Podcast, the m
  name: Nvidia
  position: 209
- category: unknown
  confidence: medium
  context: y the Dell AI Factory with Nvidia. Welcome to the Super Data Science Podcast,
    the most listened-to podcast in the data science
  name: Super Data Science Podcast
  position: 232
- category: unknown
  confidence: medium
  context: sforming our world for the better. I'm your host, John Cron. Thanks for
    joining me today. And now, let's make
  name: John Cron
  position: 517
- category: unknown
  confidence: medium
  context: stian is head of Responsible AI at Bloomberg, the New York-based financial
    software data and media company t
  name: New York
  position: 816
- category: tech
  confidence: high
  context: Prior to Bloomberg, he was a senior researcher at Google where he worked
    on the development of LLMs, inclu
  name: Google
  position: 1130
- category: unknown
  confidence: medium
  context: m models. He holds a PhD in computer science from Harvard University. Today's
    episode skews slightly towards our more
  name: Harvard University
  position: 1273
- category: unknown
  confidence: medium
  context: 'rse we''ll link to in the show notes. It''s called "RAG LLMs Are Not Safer:
    A Safety Analysis of Retrieval Augmented Generat'
  name: RAG LLMs Are Not Safer
  position: 3145
- category: unknown
  confidence: medium
  context: 'show notes. It''s called "RAG LLMs Are Not Safer: A Safety Analysis of
    Retrieval Augmented Generation for Large Langu'
  name: A Safety Analysis
  position: 3169
- category: unknown
  confidence: medium
  context: 'led "RAG LLMs Are Not Safer: A Safety Analysis of Retrieval Augmented
    Generation for Large Language Models," and this finds counte'
  name: Retrieval Augmented Generation
  position: 3190
- category: unknown
  confidence: medium
  context: ty Analysis of Retrieval Augmented Generation for Large Language Models,"
    and this finds counterintuitively—and this is t
  name: Large Language Models
  position: 3225
- category: unknown
  confidence: medium
  context: model providers put into these models. I got you. Now I understand. And
    I guess we should also—it occurs
  name: Now I
  position: 5744
- category: unknown
  confidence: medium
  context: t into these models. I got you. Now I understand. And I guess we should
    also—it occurs to me as we're tal
  name: And I
  position: 5762
- category: unknown
  confidence: medium
  context: t the insights you need right when you need them. With AdVarity's AI-powered
    data conversations, marketers will f
  name: With AdVarity
  position: 15875
- category: unknown
  confidence: medium
  context: unding, it seems to lead to fewer hallucinations. So I guess in my mind,
    maybe I kind of—and so I'd also
  name: So I
  position: 16758
- category: tech
  confidence: high
  context: nds of safeguards that an LLM creator put in—say, Meta put into some Llama
    models that they release—thos
  name: Meta
  position: 17319
- category: tech
  confidence: high
  context: cally the three Hs." They were first developed by Anthropic; many companies
    are now adopting them. The applic
  name: Anthropic
  position: 17578
- category: unknown
  confidence: medium
  context: nd of document with some kind of structured data. If I say the price of
    Meta is so-and-so today, then I
  name: If I
  position: 19542
- category: unknown
  confidence: medium
  context: or C produced, it achieves a better score on the Large Language Model Arena
    or on the following benchmarks," and it's better
  name: Large Language Model Arena
  position: 21065
- category: unknown
  confidence: medium
  context: e also released a second paper in addition to our RAG LLM paper where we
    developed our own content risk tax
  name: RAG LLM
  position: 21581
- category: unknown
  confidence: medium
  context: found that if you apply these LLM guardrails, or Shield Llama, or NeMo
    Guardrails, as they're called—if you app
  name: Shield Llama
  position: 22710
- category: unknown
  confidence: medium
  context: u apply these LLM guardrails, or Shield Llama, or NeMo Guardrails, as they're
    called—if you apply them to our speci
  name: NeMo Guardrails
  position: 22727
- category: unknown
  confidence: medium
  context: time frame should I limit the search results to? Am I filtering to particular
    industry, sectors, compan
  name: Am I
  position: 26749
- category: unknown
  confidence: medium
  context: if they're artificial benchmarks, can influence. But I think no one is
    making the point that just becaus
  name: But I
  position: 31466
- category: unknown
  confidence: medium
  context: you want a snappy, direct answer. This episode of Super Data Science is
    brought to you by the Dell AI Factory with Nvi
  name: Super Data Science
  position: 33019
- category: unknown
  confidence: medium
  context: paves the way for AI to work seamlessly for you. Integrated Dell and Nvidia
    capabilities accelerate your AI power
  name: Integrated Dell
  position: 33411
- category: unknown
  confidence: medium
  context: tted to arXiv in April, called "Understanding and Mitigating Risks of Generative
    AI in Financial Services." So mostl
  name: Mitigating Risks
  position: 42466
- category: unknown
  confidence: medium
  context: il, called "Understanding and Mitigating Risks of Generative AI in Financial
    Services." So mostly so far in this
  name: Generative AI
  position: 42486
- category: unknown
  confidence: medium
  context: standing and Mitigating Risks of Generative AI in Financial Services."
    So mostly so far in this episode, we've been ta
  name: Financial Services
  position: 42503
- category: unknown
  confidence: medium
  context: you use them out of the box and say, "Look, I use Llama Guard, I use Shield
    Llama, I use NeMo Guardrails, I'm s
  name: Llama Guard
  position: 44145
- category: unknown
  confidence: medium
  context: to intuitively know exactly what I'm looking for. When I'm doing research
    for a podcast episode, for examp
  name: When I
  position: 45690
- category: unknown
  confidence: medium
  context: roblems? Sign up for Claude today and get 50% off Claude Pro when you use
    my link, claude.ai/superdata. That's
  name: Claude Pro
  position: 46215
- category: ai_application
  confidence: high
  context: The financial software, data, and media company where Dr. Sebastian Garman
    is the Head of Responsible AI. They use AI/ML extensively, particularly in their
    terminal products, and are building RAG systems for grounding responses in proprietary
    data.
  name: Bloomberg
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Sponsor of the podcast. They provide a conversational analytics platform
    with AI-powered data conversations, allowing users to query data in plain English.
  name: AdVarity
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in partnership with NVIDIA for the 'Dell AI Factory,' indicating
    involvement in providing AI infrastructure solutions.
  name: Dell
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in partnership with Dell for the 'Dell AI Factory,' indicating
    their role as a key provider of AI hardware/infrastructure (GPUs).
  name: Nvidia
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Dr. Garman was previously a senior researcher here, working on the development
    of LLMs, including the Bloom and Palm models.
  name: Google
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The institution where Dr. Garman obtained his PhD in computer science.
  name: Harvard University
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific Large Language Model (LLM) that Dr. Garman worked on the development
    of while at Google.
  name: Bloom
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific Large Language Model (LLM) that Dr. Garman worked on the development
    of while at Google.
  name: Palm
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as the originator of the 'three Hs' framework (helpful, honest,
    harmless) for LLM evaluation, which many companies are adopting.
  name: Anthropic
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in the context of their Llama models and the built-in safeguards
    they implement, which can break down in RAG setups.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The speaker's go-to AI for years, described as a collaborator that understands
    workflows and performs research/reasoning.
  name: Claude
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The website/domain used for signing up for Claude Pro.
  name: claude.ai
  source: llm_enhanced
date: 2025-07-15 11:00:00 +0000
duration: 58
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: also—it occurs to me as we're talking about RAG—and probably a lot of
    listeners out there are aware of what retrieval augmented generation is, but maybe
    we should spend just a couple of minutes explaining it as well
  text: we should also—it occurs to me as we're talking about RAG—and probably a lot
    of listeners out there are aware of what retrieval augmented generation is, but
    maybe we should spend just a couple of minutes explaining it as well.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD5498286974.mp3?updated=1752067464
processing_date: 2025-10-05 02:06:27 +0000
quotes:
- length: 138
  relevance_score: 5
  text: And what we found is that actually RAG can circumvent these built-in safety
    mechanisms that language model providers put into these models
  topics: []
- length: 127
  relevance_score: 5
  text: And generally, what we found is if a model is safer from the get-go, even
    without RAG, it tends to be more robust to adding RAG
  topics: []
- length: 151
  relevance_score: 4
  text: Prior to Bloomberg, he was a senior researcher at Google where he worked on
    the development of LLMs, including the groundbreaking Bloom and Palm models
  topics: []
- length: 237
  relevance_score: 4
  text: And I guess we should also—it occurs to me as we're talking about RAG—and
    probably a lot of listeners out there are aware of what retrieval augmented generation
    is, but maybe we should spend just a couple of minutes explaining it as well
  topics: []
- length: 151
  relevance_score: 4
  text: Another term that you've used in a recent podcast—you were emphasizing that
    RAG is essential for grounding GenAI products in actual trusted information
  topics: []
- length: 198
  relevance_score: 4
  text: So on the one hand, you know, we need RAG for grounding GenAI products in
    more recent or in actual trusted information, or maybe in confidential information
    that an enterprise or an organization has
  topics: []
- length: 237
  relevance_score: 4
  text: Because it sounds like basically it's the user of a RAG setup or the user
    of an LLM that has to be cautious in their particular circumstance, given that
    the LLM creator couldn't have anticipated, like you said, all the possible use
    cases
  topics: []
- length: 256
  relevance_score: 4
  text: '" In this case, we can rely on the large language model to block these kinds
    of queries, which really puts a lot of trust in this developer of the large language
    model, that often there is a different entity from the organization that builds
    the RAG system'
  topics: []
- length: 205
  relevance_score: 4
  text: So there are probably lots of listeners out there who are sold on RAG and
    they're like, "Great, I want to reduce harmfulness; I want to increase the honesty
    of my LLM," and so I'm going to use a RAG system
  topics: []
- length: 254
  relevance_score: 4
  text: So we also released a second paper in addition to our RAG LLM paper where
    we developed our own content risk taxonomy for financial services where we say,
    "Here are 12 categories of risks that we really, really should address with applications
    in our area
  topics: []
- length: 228
  relevance_score: 4
  text: In our paper, we found that if you apply these LLM guardrails, or Shield Llama,
    or NeMo Guardrails, as they're called—if you apply them to our specific risks,
    they're the classifiers to say, "Is this input or is this output safe
  topics: []
- length: 122
  relevance_score: 4
  text: You observed in your paper that LLMs are often optimized for short prompts
    but deployed in long-text environments like RAG
  topics: []
- length: 119
  relevance_score: 4
  text: There's a lot of investment from companies that build large language models
    into increasing the possible context length
  topics:
  - investment
- length: 205
  relevance_score: 4
  text: This is all something we need to evaluate in the end, and those are all design
    decisions that are all being evaluated all around the globe right now as people
    are building their own GenAI and RAG solutions
  topics: []
- length: 86
  relevance_score: 4
  text: So what are the limitations this creates for LLMs in general, but particularly
    for RAG
  topics: []
- length: 53
  relevance_score: 3
  text: Sebastian Garman, head of Responsible AI at Bloomberg
  topics: []
- length: 145
  relevance_score: 3
  text: Sebastian is head of Responsible AI at Bloomberg, the New York-based financial
    software data and media company that with 20,000 employees is huge
  topics: []
- length: 169
  relevance_score: 3
  text: Previously, as head of NLP at Bloomberg, he directed the development and adoption
    of language technology to bring the best AI-in-hands products to the Bloomberg
    terminal
  topics: []
- length: 124
  relevance_score: 3
  text: All right, so you are the head of Responsible AI at Bloomberg, a hugely well-known
    financial software data and media company
  topics: []
- length: 148
  relevance_score: 3
  text: Although again, it can change from time to time because what we found really
    was the guardrails were broken because of this increased context length
  topics: []
- impact_reason: This is the title of the core research being discussed, immediately
    signaling a counter-intuitive and critical finding about a ubiquitous AI technique
    (RAG).
  relevance_score: 10
  source: llm_enhanced
  text: 'RAG LLMs Are Not Safer: A Safety Analysis of Retrieval Augmented Generation
    for Large Language Models'
  topic: safety/technical
- impact_reason: This is the central, most surprising thesis of the entire discussion,
    directly challenging the industry consensus on RAG's safety benefits.
  relevance_score: 10
  source: llm_enhanced
  text: Sebastian details the shocking discovery that retrieval augmented generation,
    RAG, actually makes LLMs less safe despite the popular perception of the opposite
  topic: safety/technical
- impact_reason: This is the core experimental finding demonstrating *how* RAG undermines
    safety mechanisms (jailbreaking via context injection).
  relevance_score: 10
  source: llm_enhanced
  text: what our paper did was it coupled unsafe queries—so think of the worst thing
    that you might want to ask a language model, 'How do I do insider trading?'—we
    coupled that with RAG. So we retrieved completely innocuous documents from Wikipedia.
    While most language models that we looked at didn't respond to the original question,
    when coupled with these completely harmless documents from Wikipedia, the response
    was often then unsafe
  topic: technical/safety
- impact_reason: A direct statement on the vulnerability RAG introduces to foundational
    model safety guardrails.
  relevance_score: 10
  source: llm_enhanced
  text: RAG can circumvent these built-in safety mechanisms that language model providers
    put into these models.
  topic: safety
- impact_reason: 'This is a critical strategic takeaway for enterprises: model builders
    cannot guarantee safety for specific downstream applications; the integrator must
    own the risk assessment.'
  relevance_score: 10
  source: llm_enhanced
  text: The people who build models, especially if we use third-party models, they
    don't understand our risks. So we need to study this to really understand and,
    instead of having an unmeasurable, unanticipated risk surface, we want to have
    one where we very much understand what are the risks and how to mitigate them.
  topic: strategy/safety
- impact_reason: Highlights the critical gap in risk ownership when integrating third-party
    LLMs. The integrator, not the model creator, bears the ultimate responsibility
    for application-specific risk.
  relevance_score: 10
  source: llm_enhanced
  text: Only the organization developing the application that you integrate the large
    language model into understands their risks.
  topic: safety
- impact_reason: 'A major finding: RAG environments can nullify the safety alignment
    baked into base models, necessitating custom, application-layer guardrails.'
  relevance_score: 10
  source: llm_enhanced
  text: for the general-purpose dangerous queries that we looked at, actually RAG
    safety breaks down because large language models really are only secured for non-RAG
    setups, which means that we need to build guardrails around our applications that
    go beyond what the large language model builders are actually providing to us.
  topic: safety
- impact_reason: 'Crucial nuance: RAG is essential for grounding/reducing hallucination
    (helpfulness), but this doesn''t automatically solve safety/harmlessness risks,
    which must be addressed separately.'
  relevance_score: 10
  source: llm_enhanced
  text: That's why we are not saying that RAG is dangerous; we're just saying it is
    not necessarily safe. It is absolutely necessary, but the harmlessness angle is
    something that is completely separate.
  topic: strategy
- impact_reason: 'Presents a concrete, actionable architectural pattern for enhanced
    RAG safety: implementing custom input/output validation layers around the core
    RAG loop.'
  relevance_score: 10
  source: llm_enhanced
  text: Can we have a classifier on inputs and on outputs that identify violations
    of our rules that we set up ourselves? So now, instead of having a vanilla RAG
    system with retrieval, answer, we have guardrail, retrieval, answer, guardrail.
  topic: technical
- impact_reason: A significant finding regarding context length and safety alignment.
    It suggests that increasing context, even with innocuous data, can degrade inherent
    safety mechanisms, posing a major risk surface.
  relevance_score: 10
  source: llm_enhanced
  text: we found that especially for safety, there seems to be this effect where the
    more context you put in, the more likely the model is to forget the built-in safety
    guardrails for this alignment that people talk about.
  topic: safety
- impact_reason: Provides a necessary counterpoint to the benefits of long context,
    highlighting the immediate business and performance hurdles (latency and cost).
  relevance_score: 10
  source: llm_enhanced
  text: There's a trade-off here, though, where longer context usually comes at a
    cost of significantly increased latency and cost.
  topic: business
- impact_reason: Directly contrasts the computational efficiency of vector search
    (cheap, fast) versus feeding massive context directly into the LLM (expensive,
    slow), justifying the necessity of RAG.
  relevance_score: 10
  source: llm_enhanced
  text: It allows the RAG system to work in real time, even over, like you said, billions
    of documents. And yeah, in contrast, if all of that text, if our millions or billions
    of documents were in the context of an LLM... you would have to have really high-end
    GPUs running to comb across all of the meaning in that huge context.
  topic: technical
- impact_reason: Contrasts the efficiency of retrieval (RAG) with the massive GPU
    requirements needed if one tried to process billions of documents directly within
    an LLM's context window, emphasizing the cost/performance trade-off.
  relevance_score: 10
  source: llm_enhanced
  text: And yeah, in contrast, if all of that text, if our millions or billions of
    documents were in the context of an LLM, even if it all fits in, you would then
    have, instead of this computationally simple calculation, this fast calculation,
    you would have to have tons and tons of really high-end GPUs running to comb across
    all of the meaning in that huge context when they're—
  topic: technical
- impact_reason: A key assessment on the current limitations of pure LLM processing
    versus dedicated retrieval infrastructure in terms of computational efficiency.
  relevance_score: 10
  source: llm_enhanced
  text: And we're still a while away from language models being able to do anything
    even remotely as efficient as this step [retrieval].
  topic: predictions
- impact_reason: Explains why raw parameter count is becoming an outdated metric for
    comparing model capability, pointing toward Mixture of Experts (MoE) and active
    parameter usage.
  relevance_score: 10
  source: llm_enhanced
  text: model size is a little bit of a misnomer because we have so many models that
    rely on mixture of experts and that have architectural advantages, that even though
    on paper there are more parameters, they actually are using fewer of them when
    you actually run them live.
  topic: technical
- impact_reason: 'A strong cautionary statement: RAG systems, regardless of the underlying
    LLM size/capability, are fundamentally vulnerable to context-based attacks.'
  relevance_score: 10
  source: llm_enhanced
  text: we found that basically every system was breakable regardless of whether small
    or large.
  topic: safety
- impact_reason: Emphasizes that safety (harmlessness) must be balanced with utility
    (helpfulness); a model that refuses everything is useless, regardless of its safety
    score.
  relevance_score: 10
  source: llm_enhanced
  text: if in the end Gemma is also refusing to answer completely safe questions and
    is completely safe and correct in a RAG setup, it's not going to be helpful. So
    even though it is harmless, it still would not be able to be used.
  topic: safety
- impact_reason: Strong business advice advocating for using the smallest, cheapest
    model that meets the specific application requirements, rather than defaulting
    to the largest available model.
  relevance_score: 10
  source: llm_enhanced
  text: It could be that the fast, cheap, small model is completely up to the task.
    And in that case, why would I use this completely overblown model to do the same
    task, just because it is performing better on things that are completely not relevant
    to your particular application?
  topic: business
- impact_reason: Points out that out-of-the-box safety layers (like Llama Guard) are
    often trained on general safety taxonomies and may fail to address domain-specific
    risks (e.g., financial compliance).
  relevance_score: 10
  source: llm_enhanced
  text: Even safeguards that are in dedicated models or systems to provide these kind
    of first-pass judgments, like 'Is this safe? Is this unsafe?'—they're also not
    trained on financial services.
  topic: safety
- impact_reason: Warns against the false sense of security derived from using generic,
    off-the-shelf safety tools in high-stakes, regulated environments.
  relevance_score: 10
  source: llm_enhanced
  text: If you use them out of the box and say, 'Look, I use Llama Guard, I use Shield
    Llama, I use NeMo Guardrails, I'm safe now, right?' You are protected against
    a particular view of safety that is very much grounded in categories that are
    relevant to broad populations...
  topic: safety
- impact_reason: Highlights a crucial distinction in AI alignment and safety goals
    that often gets conflated in general discussions.
  relevance_score: 9
  source: llm_enhanced
  text: why the difference between helpful and harmless AI matters more than you think
  topic: safety
- impact_reason: Points to specific, novel security/risk vectors introduced by the
    combination of RAG and proprietary/sensitive data.
  relevance_score: 9
  source: llm_enhanced
  text: the hidden attack surfaces that emerge when you combine RAG with enterprise
    data
  topic: safety/technical
- impact_reason: The primary actionable advice derived from the research regarding
    responsible deployment.
  relevance_score: 9
  source: llm_enhanced
  text: you need to really evaluate your AI application in the context you want to
    deploy it in.
  topic: strategy/business
- impact_reason: 'Defines the core mission of Responsible AI in a regulated environment:
    proactive identification, blocking, and monitoring.'
  relevance_score: 9
  source: llm_enhanced
  text: we want to make sure that if our clients or our customers use our AI, that
    if they use it for something that they shouldn't, that we can identify this, we
    can block this, we can monitor this over time, which is incredibly important,
    especially for such heavily regulated industries
  topic: safety/business
- impact_reason: Critiques the over-reliance on third-party model safety assurances
    without context-specific validation.
  relevance_score: 9
  source: llm_enhanced
  text: we only have one provider who says, 'Yeah, actually, our model is safe.' There's
    no way that would be possible to really anticipate every single use case around
    the world, around it, in all the relevant regulation.
  topic: safety/strategy
- impact_reason: 'Clearly defines the research focus: testing the robustness of foundational
    model safety mechanisms (like refusal mechanisms) when augmented by Retrieval-Augmented
    Generation (RAG).'
  relevance_score: 9
  source: llm_enhanced
  text: Our paper is very much in that third realm here because we wanted to know
    the built-in defenses of the language model—how good are they actually—and how
    will this then hold up to a RAG setup?
  topic: technical
- impact_reason: Defines 'transparent attribution' as a core requirement for trust
    in RAG systems, moving beyond just accuracy to provable sourcing.
  relevance_score: 9
  source: llm_enhanced
  text: If I say the price of Meta is so-and-so today, then I want to be able to look
    at that and say, 'Where does that number actually come from? Is it hallucinated,
    or do I actually know the query that produced that number?'
  topic: technical
- impact_reason: Reiterates the core message against relying solely on general benchmarks
    (like the LLM Arena) and advocates for context-specific, adversarial testing.
  relevance_score: 9
  source: llm_enhanced
  text: it's actually really, really important to measure and to evaluate the system
    in the context it's deployed in, and that includes things like safety testing
    and specific guardrails.
  topic: strategy
- impact_reason: Provides a concrete example of building domain-specific safety tooling
    (a taxonomy) necessary for regulated industries, moving beyond generic safety
    categories.
  relevance_score: 9
  source: llm_enhanced
  text: we developed our own content risk taxonomy for financial services where we
    say, 'Here are 12 categories of risks that we really, really should address with
    applications in our area.'
  topic: safety
- impact_reason: Demonstrates the failure of generic, off-the-shelf safety tools (like
    NeMo Guardrails) against specialized, domain-specific adversarial inputs.
  relevance_score: 9
  source: llm_enhanced
  text: if you apply them [open-source guardrails] to our specific risks, they're
    the classifiers to say, 'Is this input or is this output safe? Yes or no?'—and
    they also fail in our domain because, similar to our RAG paper, it's just not
    a use case that people necessarily have thought about before.
  topic: safety
- impact_reason: Emphasizes that production-grade LLM systems are complex pipelines,
    not single model calls, requiring SME oversight across the entire chain for both
    helpfulness and harmlessness.
  relevance_score: 9
  source: llm_enhanced
  text: in practice, to prevent hallucination and to add attribution, real systems
    that are deployed to wide ranges of audiences, they have many more components,
    and it's really just the end-to-end application that should be evaluated, and
    where you need the subject matter expertise to also know is it helpful and is
    it harmless.
  topic: strategy
- impact_reason: Provides a concise, practical architectural pattern for implementing
    safe and grounded RAG systems, emphasizing iterative safety checks.
  relevance_score: 9
  source: llm_enhanced
  text: this flow of guardrail, retrieval, answer, and then another set of guardrails.
  topic: technical
- impact_reason: Details the multi-stage engineering reality of high-scale RAG systems,
    moving beyond the simplified view of 'search and retrieve.'
  relevance_score: 9
  source: llm_enhanced
  text: In practice, retrieval systems have multiple components too... you do a first-pass
    retrieval where you go from hundreds of millions of documents or even billions
    of documents down to just a handful. And then there's usually a re-ranking step
    that's much more computationally intensive where you really pick out which snippets
    in the documents are actually trying to answer it.
  topic: technical
- impact_reason: Articulates the core challenge shifting from LLM capability to retrieval
    precision when context windows grow large.
  relevance_score: 9
  source: llm_enhanced
  text: 'Seems pretty obvious, but that then becomes a retrieval problem: how do I
    find within the—you know, if I have 100 million documents, each 10 pages long—how
    do I find the two paragraphs that actually answer the question?'
  topic: strategy
- impact_reason: A strong cautionary statement emphasizing that RAG system design
    choices (retrieval, context handling) are inseparable from safety outcomes.
  relevance_score: 9
  source: llm_enhanced
  text: every single one of these design decisions I just talked about can influence
    the harmlessness or the harmfulness in this case of the entire system.
  topic: safety
- impact_reason: Draws a crucial distinction between foundational model builders (focused
    on raw benchmarks like needle-in-a-haystack) and application integrators (focused
    on real-world task performance).
  relevance_score: 9
  source: llm_enhanced
  text: I think this is a perfect example of the difference between developers of
    LLMs and integrators of such language models in actual applications.
  topic: business
- impact_reason: Details the real-time query processing in RAG, emphasizing the computational
    efficiency of vector similarity search.
  relevance_score: 9
  source: llm_enhanced
  text: we take the natural language query that a user makes to the RAG system, we
    can convert that into the same high-dimensional space, find its location, and
    then retrieve the documents... using a very fast mathematical operation, like
    a cosine similarity score, to find the closest documents in that space.
  topic: technical
- impact_reason: 'Highlights the pragmatic engineering reality: legacy, computationally
    cheap methods (keyword search) often persist in production systems due to cost/speed
    advantages, even when newer methods exist.'
  relevance_score: 9
  source: llm_enhanced
  text: often, even for commercial systems, it is still the case that you rely on
    keyword retrieval just because it's even cheaper, it's even faster. There are
    techniques from the early '90s, or even late '80s, that are still around just
    because they're so computationally efficient.
  topic: business
- impact_reason: Highlights the computational efficiency and real-time capability
    of vector-based retrieval (like cosine similarity in high-dimensional space) for
    RAG systems handling massive datasets.
  relevance_score: 9
  source: llm_enhanced
  text: And that's all computationally very inexpensive, very fast. It allows the
    RAG system to work in real time, even over, like you said, billions of documents.
  topic: technical
- impact_reason: Describes a practical, engineered hybrid retrieval strategy combining
    cheap keyword search for filtering, followed by more expensive semantic search
    for refinement.
  relevance_score: 9
  source: llm_enhanced
  text: often, you use semantic search for the more toned-down—whatever—you do a first-pass
    keyword retrieval, you do a second semantic search within those keyword-retrieved
    steps.
  topic: technical
- impact_reason: Suggests a correlation between a model's inherent safety alignment
    and its resilience against adversarial attacks or jailbreaks introduced via RAG
    context.
  relevance_score: 9
  source: llm_enhanced
  text: if a model is safer from the get-go, even without RAG, it tends to be more
    robust to adding RAG.
  topic: safety
- impact_reason: Identifies increased context length as a vector for breaking alignment/guardrails,
    and suggests future alignment training will specifically target these longer-context
    vulnerabilities.
  relevance_score: 9
  source: llm_enhanced
  text: we found really was the guardrails were broken because of this increased context
    length. It could be that once the next generation of this model comes out, that
    this is being prevented; that there's an active component of the post-training
    of the alignment step that looks at how can someone use this with a longer context...
  topic: safety
- impact_reason: 'Articulates the core challenge in evaluating model refusal: distinguishing
    between ignorance/inability and genuine safety alignment.'
  relevance_score: 9
  source: llm_enhanced
  text: If you just refuse to answer, it could be because you don't know, or it could
    be because you actively find the input to be unsafe. And if you can't distinguish
    between them, it's very hard to know whether your model is just bad or whether
    it's unsafe or safe in this case.
  topic: safety
- impact_reason: Identifies a major limitation of general-purpose LLMs when deployed
    in highly specialized, regulated industries like finance.
  relevance_score: 9
  source: llm_enhanced
  text: most foundation models are not trained on finance-specific corporate bodies
    of knowledge.
  topic: business
- impact_reason: Broadens the critique beyond finance, emphasizing that domain-specific
    regulation dictates unique safety and risk management requirements that general
    models cannot meet.
  relevance_score: 9
  source: llm_enhanced
  text: '...those are not the same risks that we are under in financial services.
    Those are not the same obligations that companies or organizations in healthcare
    are under, or law, or any other highly domain-specific, knowledge-intensive domain
    that has a lot of specific regulation...'
  topic: safety
- impact_reason: Highlights the necessity of RAG/grounding for enterprise applications
    dealing with large, dynamic datasets, setting up the safety trade-off.
  relevance_score: 8
  source: llm_enhanced
  text: The only way to do this is if you use RAG or some kind of other similar retrieval
    augmented technology or document-grounded technology.
  topic: business/strategy
- impact_reason: A clear, concise explanation of the fundamental limitation of static
    LLMs that RAG is designed to solve.
  relevance_score: 8
  source: llm_enhanced
  text: Once you're done training this model, it is stuck. You freeze the model and
    you say, 'Okay, this is the model; its knowledge cut-off is January 2021.' And
    then if you ask a question that requires knowledge from beyond 2021, the only
    way that you can get this into the model is by actually providing it in its context.
  topic: technical
- impact_reason: Contrasts traditional cybersecurity attack surfaces with the new
    'content risks' inherent in LLMs.
  relevance_score: 8
  source: llm_enhanced
  text: attack surfaces are everywhere where you have unsecured endpoints, or you
    have code that you can kind of inject into, you have databases that are just out
    in the open. Large language models are very different because they, unless you
    give them the information, they don't have that kind of an attack surface.
  topic: safety
- impact_reason: A fundamental principle of risk management applied to AI systems.
  relevance_score: 8
  source: llm_enhanced
  text: we don't understand the risk unless we measure it.
  topic: strategy
- impact_reason: Outlines a comprehensive, lifecycle approach to responsible AI implementation
    within a large organization.
  relevance_score: 8
  source: llm_enhanced
  text: This was part of our broader responsibility research theme that we have going
    where we want to make sure that if our clients or our customers use our AI, that
    if they use it for something that they shouldn't, that we can identify this, we
    can block this, we can monitor this over time
  topic: safety/strategy
- impact_reason: Suggests that the responsibility for risk mitigation lies primarily
    with the application builder who controls the retrieval corpus and the prompt
    context.
  relevance_score: 8
  source: llm_enhanced
  text: The people who build the RAG system, they usually know what they're building
    it for. They understand the data that goes into the database
  topic: safety/strategy
- impact_reason: Provides a clear conceptual framework linking honesty (truthfulness/non-hallucination)
    directly to helpfulness, reinforcing the need for accuracy.
  relevance_score: 8
  source: llm_enhanced
  text: hallucination very much goes into this honesty bucket, which often is also
    then combined with the helpful, because how can you be helpful if you're not honest?
  topic: technical
- impact_reason: Identifies a fundamental mismatch between model training optimization
    (short, focused prompts) and RAG deployment reality (long, complex context windows).
  relevance_score: 8
  source: llm_enhanced
  text: LLMs are often optimized for short prompts but deployed in long-text environments
    like RAG.
  topic: technical
- impact_reason: 'Critiques a common development philosophy: relying on brute-force
    context expansion instead of improving retrieval accuracy, suggesting a potential
    strategic misstep.'
  relevance_score: 8
  source: llm_enhanced
  text: I hope really has been in large language model development to just increase
    the context length and to rely less on more and more accurate retrieval, but rather
    let the language model figure it out.
  topic: strategy
- impact_reason: 'Explains the primary benefit of long context: enabling nuanced,
    meta-aware answers rather than just pinpointing the answer fact.'
  relevance_score: 8
  source: llm_enhanced
  text: Being able to handle longer context also allows you to give much more contextualized
    answers... If you have the entire 10-page document... it can still give you the
    context from page one, or tell you what is this document type, where is it from,
    what was in the intro...
  topic: predictions
- impact_reason: Offers a balanced, time-horizon view on the economics of long context,
    acknowledging future potential while grounding current deployment reality.
  relevance_score: 8
  source: llm_enhanced
  text: Over a long enough time scale, over many years, maybe many decades, compute
    costs over millions of tokens become trivial, but at least for the foreseeable
    future, it isn't.
  topic: predictions
- impact_reason: A clear, foundational explanation of the vectorization step in RAG
    setup.
  relevance_score: 8
  source: llm_enhanced
  text: what we would do in advance before running any RAG queries is we would map
    each of those million documents into a high-dimensional space called vector space.
  topic: technical
- impact_reason: 'Explains the core principle of semantic search: proximity equals
    semantic similarity in vector space.'
  relevance_score: 8
  source: llm_enhanced
  text: The closer the things are in this space, the more overlap and meaning there
    is between the documents.
  topic: technical
- impact_reason: 'Describes a common hybrid RAG engineering strategy: using fast keyword
    search to narrow the corpus before applying the more expensive semantic search.'
  relevance_score: 8
  source: llm_enhanced
  text: often you use semantic search for the more toned-down—whatever—you do a first-pass
    keyword retrieval, you do a second semantic search within those keyword-retrieved
    steps.
  topic: technical
- impact_reason: Highlights the critical business/UX trade-off between accuracy gains
    (often marginal) and user experience factors like latency/speed in GenAI product
    design.
  relevance_score: 8
  source: llm_enhanced
  text: it could be that, in terms of helpfulness, users actually prefer the one that's
    not being fast, rather than the one that's maybe five points more accurate in
    the end. This is all something we need to evaluate in the end, and those are all
    design decisions...
  topic: business
- impact_reason: Actionable advice stressing that LLM evaluation must be holistic,
    considering safety, helpfulness, latency, and cost simultaneously.
  relevance_score: 8
  source: llm_enhanced
  text: highlighting the need for having a multifaceted evaluation. You need to consider
    those similarly to how model sizes will also affect latency and cost of running
    a system.
  topic: strategy
- impact_reason: Provides a concrete, personal example of massive productivity gains
    (days to minutes) achieved through advanced LLM usage (Claude with web search/reasoning).
  relevance_score: 8
  source: llm_enhanced
  text: What would have taken me days is now done in minutes. It's changed how I prep
    for every single episode, enabling me to get more high-quality content to you
    in each one...
  topic: business
- impact_reason: Provides a useful, broad definition of RAG encompassing various retrieval
    augmentation techniques.
  relevance_score: 7
  source: llm_enhanced
  text: we use RAG as kind of the overarching term for anything where you ground a
    language model's response with some kind of data that you retrieve from somewhere.
  topic: technical
- impact_reason: Illustrates a high-stakes, regulated enterprise use case for LLMs
    where grounding and accuracy are paramount.
  relevance_score: 7
  source: llm_enhanced
  text: Our content is very much focused on helping people conduct analyses in the
    financial space, synthesize information, summarize information, ground all of
    this and attribute all of this to our just-in-time data that we have on our Bloomberg
    terminal.
  topic: business
- impact_reason: Points out that simply increasing the context window size (a hardware/architecture
    feature) is insufficient; the models themselves need architectural improvements
    to effectively utilize that long context.
  relevance_score: 7
  source: llm_enhanced
  text: increasing the possible context length also requires developing methods and
    developing the models to actually be able to handle such a co[ntext].
  topic: technical
- impact_reason: 'Summarizes the allure of massive context windows: simplifying the
    engineering pipeline by offloading complexity to the LLM.'
  relevance_score: 7
  source: llm_enhanced
  text: If I'm able to just paste in more documents or more of a context, I don't
    need to rely on as many tricks to really narrow down the context window. I can
    just rely on a large language model.
  topic: technical
- impact_reason: Provides the formal name for the technique being described, linking
    the vector space discussion to established search terminology.
  relevance_score: 7
  source: llm_enhanced
  text: What you just described here is commonly known also as semantic search because
    you can really search based on the meaning of a query.
  topic: technical
- impact_reason: A strong endorsement framing Claude as a high-level cognitive partner
    rather than a simple tool, emphasizing deep workflow integration.
  relevance_score: 7
  source: llm_enhanced
  text: Claude is the AI for minds that don't stop at 'good enough.' It's the collaborator
    that actually understands your entire workflow and thinks with you, not for you...
  topic: business
- impact_reason: Confirms the host's understanding, which serves as a good, accessible
    summary of RAG for the general audience.
  relevance_score: 6
  source: llm_enhanced
  text: What do you think about my RAG explanation there? Absolutely correct.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: 905: Why RAG Makes LLMs Less Safe (And How to Fix It),
  with Bloomberg’s Dr. Sebastian Gehrmann


  This episode features Dr. Sebastian Gehrmann, Head of Responsible AI at Bloomberg,
  discussing the counterintuitive finding that **Retrieval Augmented Generation (RAG)
  systems can actually decrease the safety and reliability of Large Language Models
  (LLMs)**, despite RAG being widely adopted as a necessary tool for grounding responses
  in factual data.


  ### 1. Focus Area

  The discussion centers on **Responsible AI, LLM safety, and the architectural risks
  introduced by RAG systems**, particularly within high-stakes, regulated environments
  like finance. Key themes include the separation of "Helpful" versus "Harmless" AI
  goals, the concept of context-specific attack surfaces, and the limitations of general-purpose
  LLM safety guardrails when integrated with proprietary data via RAG.


  ### 2. Key Technical Insights

  *   **RAG Circumvents Built-in Safety:** The research demonstrated that coupling
  unsafe queries (e.g., "How do I commit insider trading?") with retrieved, otherwise
  innocuous documents (like Wikipedia articles) caused models to bypass their inherent
  safety mechanisms and generate unsafe responses.

  *   **Safety vs. Helpfulness Dichotomy:** RAG significantly enhances the **Helpfulness/Honesty**
  aspect (reducing hallucinations via grounding and attribution), but it does not
  inherently guarantee **Harmlessness**. The safety alignment baked into base LLMs
  is often compromised in the RAG pipeline.

  *   **Context Window Overload Risks:** LLMs optimized for short prompts often struggle
  when deployed in RAG environments that feed them extensive context (e.g., dozens
  of retrieved documents), potentially leading to unpredictable behavior beyond their
  intended operational limits.


  ### 3. Business/Investment Angle

  *   **Contextual Risk Assessment is Mandatory:** Organizations integrating LLMs
  must move beyond relying on vendor benchmarks. The primary responsibility for safety
  evaluation lies with the application developer, as they understand the specific
  socio-technical context and regulatory risks (e.g., financial misconduct, unsolicited
  advice).

  *   **Need for Domain-Specific Guardrails:** General-purpose open-source guardrails
  (like those from Meta or Google) fail when tested against domain-specific risks
  (e.g., financial crime queries). This necessitates building custom content risk
  taxonomies and tailored input/output classifiers.

  *   **RAG is Still Essential:** Despite the safety risks, RAG remains a necessity
  for grounding LLMs in timely, proprietary, or factual data, especially in industries
  requiring transparent attribution.


  ### 4. Notable Companies/People

  *   **Dr. Sebastian Gehrmann (Bloomberg):** Head of Responsible AI, author of the
  paper "RAG LLMs Are Not Safer," and expert in grounding LLM applications in regulated
  industries.

  *   **Bloomberg:** The context for the research, highlighting the stringent safety
  requirements in financial data and software.

  *   **Anthropic:** Mentioned for originating the "Helpful, Honest, Harmless" (3H)
  framework for AI evaluation.


  ### 5. Future Implications

  The industry must shift its focus from simply measuring base model performance on
  public benchmarks to rigorously **evaluating the entire end-to-end RAG application**
  within its specific deployment context. Future safe LLM architectures will require
  layered defense mechanisms: **Guardrail $\rightarrow$ Retrieval $\rightarrow$ Answer
  $\rightarrow$ Guardrail**, ensuring safety checks occur both before and after grounding
  the response.


  ### 6. Target Audience

  This episode is highly valuable for **AI Engineers, Data Scientists, Responsible
  AI practitioners, and Technology Leaders** responsible for deploying LLM applications
  in enterprise or regulated settings where factual accuracy and compliance are paramount.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- nvidia
- google
- meta
- anthropic
title: '905: Why RAG Makes LLMs Less Safe (And How to Fix It), with Bloomberg’s Dr.
  Sebastian Gehrmann'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 153
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 15
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 5
  prominence: 0.5
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 2
  prominence: 0.2
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 02:06:27 UTC -->
