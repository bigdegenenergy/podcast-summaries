---
companies:
- category: unknown
  confidence: medium
  context: Any LLM that was trained on pre-1915 physics would never
  name: Any LLM
  position: 0
- category: unknown
  confidence: medium
  context: ms, new science, and that's by definition of AGI. Fischal Mischra was trying
    to fix a broken cricket stats page and
  name: Fischal Mischra
  position: 470
- category: unknown
  confidence: medium
  context: sode of the a16z podcast, I talk with Fischal and Martin Casaro at a16z
    about how that moment led to retrieval au
  name: Martin Casaro
  position: 660
- category: unknown
  confidence: medium
  context: w LLMs work is one that Fischal did at MIT, which Harry Baller Christian
    pointed me to, and I watched that. So he did that
  name: Harry Baller Christian
  position: 1972
- category: unknown
  confidence: medium
  context: ut it has some reflexes on how humans reason too. So I just think he's
    doing some of the more profound w
  name: So I
  position: 2201
- category: unknown
  confidence: medium
  context: educes because Martin is only going to take me to Michelin Star restaurants.
    I'm not going to go to McDonald's. Y
  name: Michelin Star
  position: 6341
- category: unknown
  confidence: medium
  context: o. So one of the examples that I often tell is... Suppose I ask you, "What
    is 769 times 1025?" You have no id
  name: Suppose I
  position: 7365
- category: unknown
  confidence: medium
  context: going to be the correct answer. But if you say, "Can I write it down and
    do it?" The way we have learned
  name: Can I
  position: 7776
- category: unknown
  confidence: medium
  context: do A, B, C, D," and then I arrive at the answer. What I bet is... Let's
    zoom back out. I want to go into
  name: What I
  position: 8810
- category: unknown
  confidence: medium
  context: et team or something? I'm a minority owner at the San Francisco Unicorns.
    That's right. Very proud to have you. So, but in
  name: San Francisco Unicorns
  position: 9332
- category: unknown
  confidence: medium
  context: e people who started this portal called Cricinfo. And Cricinfo, at one
    point, it was the most popular website in
  name: And Cricinfo
  position: 9479
- category: unknown
  confidence: medium
  context: e searchable stats engine based on cricket called Stats Guru. And this
    has been available on Cricinfo since 20
  name: Stats Guru
  position: 9793
- category: unknown
  confidence: medium
  context: s. So, I'm still friendly with the people who run ESPN Cricinfo. The editor-in-chief,
    whenever he comes to New Yo
  name: ESPN Cricinfo
  position: 10979
- category: unknown
  confidence: medium
  context: icinfo. The editor-in-chief, whenever he comes to New York, you know, we
    meet up, we go out for a drink. So,
  name: New York
  position: 11036
- category: unknown
  confidence: medium
  context: ary 2020, right before the pandemic, he was here. And I again said, "Why
    don't you do something about Sta
  name: And I
  position: 11231
- category: unknown
  confidence: medium
  context: alized that, you know, no, I cannot really do it. Because Stats Guru, the
    backend databases were so complex. And if yo
  name: Because Stats Guru
  position: 11901
- category: unknown
  confidence: medium
  context: lexities of that database in that context window. And GPT-3 also did not
    do instruction following at that t
  name: And GPT
  position: 12120
- category: unknown
  confidence: medium
  context: t I could make it do, what I couldn't make it do. But I never thought of
    it as, you know, what these LLMs
  name: But I
  position: 14195
- category: tech
  confidence: high
  context: just one company and one model. You look at what OpenAI is coming up with,
    or what Anthropic, Google, or
  name: Openai
  position: 15452
- category: tech
  confidence: high
  context: ou look at what OpenAI is coming up with, or what Anthropic, Google, or
    all these open source, Sam's model, o
  name: Anthropic
  position: 15486
- category: tech
  confidence: high
  context: what OpenAI is coming up with, or what Anthropic, Google, or all these
    open source, Sam's model, or Mistra
  name: Google
  position: 15497
- category: unknown
  confidence: medium
  context: which then I can translate into a SQL query or a REST API or whatever.
    But getting the DSL is important. No
  name: REST API
  position: 21091
- category: tech
  confidence: high
  context: wave-particle duality or this whole probabilistic notion, or that, you
    know, energy is not continuous but
  name: Notion
  position: 25955
- category: unknown
  confidence: medium
  context: received a lot of prestige, is these IMO results, International Math Olympiad.
    You know, whether it's a human solving it or the
  name: International Math Olympiad
  position: 26503
- category: unknown
  confidence: medium
  context: ms, new science, and that's by definition of AGI. Which I can... Do you
    think that based on the work you've
  name: Which I
  position: 28512
- category: unknown
  confidence: medium
  context: be smoother manifolds. So, it's like a map. Yeah. Because I mean, there's
    there's this view that people have,
  name: Because I
  position: 29307
- category: unknown
  confidence: medium
  context: es of humans developing languages to know what... That I've been recorded,
    right? Like, the Nicaraguan sig
  name: That I
  position: 33136
- category: investment_firm
  confidence: high
  context: Venture capital firm hosting the podcast and employing Martin Casaro, actively
    investing in AI/ML.
  name: a16z
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as a leading developer of LLMs (GPT-3, ChatGPT, GPT-4) whose
    progress is being analyzed.
  name: OpenAI
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned alongside OpenAI and Google as a major developer whose LLM capabilities
    are being compared.
  name: Anthropic
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a major player whose LLM progress is being analyzed.
  name: Google
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Mentioned as an open-source LLM developer whose capabilities are being
    compared to proprietary models.
  name: Mistral
  source: llm_enhanced
- category: ai_company
  confidence: medium
  context: Likely a reference to a model developed by Sam Altman (CEO of OpenAI) or
    a specific open-source model associated with him, mentioned alongside Mistral.
  name: Sam's model
  source: llm_enhanced
- category: ai_application_origin
  confidence: high
  context: A cricket statistics website where the speaker (Fischal) worked, which
    led to the accidental invention of RAG while trying to integrate natural language
    querying.
  name: Cricinfo
  source: llm_enhanced
- category: technology_user
  confidence: high
  context: Acquired Cricinfo in 2006.
  name: ESPN
  source: llm_enhanced
- category: research_institution
  confidence: medium
  context: Where Fischal did his PhD thesis and early networking work.
  name: Columbia
  source: llm_enhanced
- category: research_institution
  confidence: high
  context: Where Fischal gave a highly regarded talk on understanding LLMs.
  name: MIT
  source: llm_enhanced
- category: technology_user
  confidence: low
  context: Mentioned historically as a website that Cricinfo surpassed in hits.
  name: Yahoo
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced as an example of a first-generation LLM used to illustrate the
    massive scale of the theoretical prompt-response matrix.
  name: GPT-3
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: The DSL/system the speaker created, which the LLM learned via in-context
    learning, implying an interaction with an API provided by a major LLM developer
    (likely OpenAI, given the 2020 context).
  name: Stats Guru
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned as the platform where the speaker made a rough argument about
    recursive self-improvement.
  name: X (Twitter)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: low
  context: Used as an analogy for incremental hardware improvements that plateau,
    suggesting LLMs will similarly plateau without architectural leaps.
  name: iPhone 15, 16, 17
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an energy-based architecture being pushed by 'Yann' (likely
    Yann LeCun) that seems promising for future AI development beyond current LLMs.
  name: JAPA architecture
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Mentioned as a benchmark that might hint at progress in new architectures.
  name: ARC prize
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Mentioned in relation to pushing the JAPA architecture and energy-based
    architectures, suggesting a specific researcher or group associated with these
    concepts.
  name: Yann
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as having work that listeners, especially those with systems/networking
    backgrounds, should read for understanding the bounds of current AI systems.
  name: Fischal
  source: llm_enhanced
date: 2025-10-13 10:00:00 +0000
duration: 51
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: put this in the notes for this, but the single best talk I've ever seen
    on trying to understand how LLMs work is one that Fischal did at MIT, which Harry
    Baller Christian pointed me to, and I watched that
  text: we should put this in the notes for this, but the single best talk I've ever
    seen on trying to understand how LLMs work is one that Fischal did at MIT, which
    Harry Baller Christian pointed me to, and I watched that.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/3f86df7b-51c6-4101-88a2-550dba782de8/episodes/ba04cc09-9eb8-4f48-9b80-f88e229ba42d/audio/128/default.mp3?aid=rss_feed&awCollectionId=3f86df7b-51c6-4101-88a2-550dba782de8&awEpisodeId=ba04cc09-9eb8-4f48-9b80-f88e229ba42d&feed=JGE3yC0V
processing_date: 2025-10-13 10:30:36 +0000
quotes:
- length: 176
  relevance_score: 5
  text: You look at what OpenAI is coming up with, or what Anthropic, Google, or all
    these open source, Sam's model, or Mistral, the capabilities of LLMs have not
    fundamentally changed
  topics: []
- length: 54
  relevance_score: 5
  text: And for that, you have to go outside your training set
  topics: []
- length: 226
  relevance_score: 4
  text: On this episode of the a16z podcast, I talk with Fischal and Martin Casaro
    at a16z about how that moment led to retrieval augmented generation and how Fischal's
    formal models explain what large language models can and can't do
  topics: []
- length: 235
  relevance_score: 4
  text: So ultimately what all these LLMs are doing, whether the early LLMs or the
    LLMs that we have today with also the post training, RLHF, whatever you do, at
    the end of the day, what they do is they create a distribution for the next token
  topics: []
- length: 146
  relevance_score: 4
  text: Now, what happens because of the way we train these LLMs, the architecture
    of the transformers, and the loss function, the way you put it is right
  topics: []
- length: 153
  relevance_score: 4
  text: So, you know, another phrase that we have been using recently is, you know,
    the output of the LLM is the inductive closure of what it has been trained on
  topics: []
- length: 190
  relevance_score: 4
  text: Well, actually, you know what's kind of interesting is like often the most
    people agree that if you have one LLM and you just feed the output in the input,
    like it's not going to do anything
  topics: []
- length: 225
  relevance_score: 3
  text: Similarly, you know, any LLM that was trained on it would not have come up
    with quantum mechanics, that wave-particle duality or this whole probabilistic
    notion, or that, you know, energy is not continuous but it is quantized
  topics: []
- length: 47
  relevance_score: 3
  text: The problem is, is they're all anecdotal, right
  topics: []
- length: 58
  relevance_score: 3
  text: You have to wonder if it's just kind of sloppy observation
  topics: []
- impact_reason: This provides a powerful analogy for the current limitations of LLMs,
    framing true AGI as the ability to generate entirely new scientific paradigms,
    not just interpolate existing knowledge.
  relevance_score: 10
  source: llm_enhanced
  text: Any LLM that was trained on pre-1915 physics would never have come up with
    the theory of relativity. Einstein had to reject the Newtonian physics and come
    up with a space-time continuum. He completely rewrote the rules.
  topic: predictions/limitations
- impact_reason: Offers a clear, high-bar definition for Artificial General Intelligence
    (AGI) centered on genuine scientific discovery and paradigm shifts, moving beyond
    pattern matching.
  relevance_score: 10
  source: llm_enhanced
  text: AGI will be when we are able to create new science, new results, new math.
    When an AGI comes up with a theory of relativity, it has to go beyond what it
    has been trained on to come up with new paradigms, new science, and that's by
    definition of AGI.
  topic: predictions/definitions
- impact_reason: 'Offers a formal explanation for hallucination: deviating from the
    learned, coherent geometric manifold of probable sequences.'
  relevance_score: 10
  source: llm_enhanced
  text: It sort of reduces the world into these Bayesian manifolds. And as long as
    the LLM is going in, so the traversing through these manifolds, it is confident
    and it can produce something which makes sense. The moment it sort of veers away
    from the manifold, then it starts hallucinating and starts spitting nonsense,
    confident nonsense, but nonsense.
  topic: technical/limitations
- impact_reason: 'This analogy perfectly explains why Chain-of-Thought (CoT) prompting
    works: it forces the model to switch from high-entropy guessing to low-entropy,
    algorithmic execution.'
  relevance_score: 10
  source: llm_enhanced
  text: Suppose I ask you, 'What is 769 times 1025?' You have no idea. You can have
    some vague idea given the two numbers. And so in your mind, the next token distribution
    of the answer is going to be diffuse. ... But if you say, 'Can I write it down
    and do it?' ... at each stage of that process, your prediction entropy is very
    low. You know exactly what to do because you have been taught this algorithm.
  topic: technical/chain-of-thought
- impact_reason: Provides a formal, entropy-based justification for the effectiveness
    of Chain-of-Thought prompting.
  relevance_score: 10
  source: llm_enhanced
  text: And the LLM is pretty much the same way. That's why chain of thought works.
    What happens with chain of thought is you ask the LLM to do something chain of
    thought. It starts breaking the problem into small steps. These steps, it has
    seen in the past. It has been trained on... and then it's confident, 'Okay, now
    I need to do A, B, C, D,' and then I arrive at the answer.
  topic: technical/chain-of-thought
- impact_reason: A firsthand account of the accidental invention of Retrieval Augmented
    Generation (RAG) as a solution to context window limits and database complexity.
  relevance_score: 10
  source: llm_enhanced
  text: But then in trying to solve this problem, I accidentally invented what's now
    called RAG. Where based on the natural language query, I created a database of
    natural language queries and structured queries. I created a DSL, which then translated
    into a REST call to Stats Guru.
  topic: technical/history/RAG
- impact_reason: A strong, potentially controversial prediction that the current generation
    of LLMs (post-GPT-4) is entering a phase of diminishing fundamental returns, similar
    to mature smartphone hardware.
  relevance_score: 10
  source: llm_enhanced
  text: Yes, in some sense, progress is plateauing. It's like the iPhone, you know,
    with the iPhone came out, 'Wow, what is this thing?'... But the last, you know,
    seven, eight, nine years, it's maybe the camera got a little bit better... But
    there has been no fundamental advance in what it's capable of.
  topic: predictions/trends
- impact_reason: 'Offers a precise, non-reductive definition of LLM function: they
    are Bayesian reasoners operating over the compressed knowledge derived from their
    training data subset.'
  relevance_score: 10
  source: llm_enhanced
  text: It's basically so it's more than a stochastic parrot. It is sort of Bayesian
    on this on this subset of the matrix that it has been trained on.
  topic: technical/theory
- impact_reason: A powerful analogy demonstrating that true paradigm shifts (like
    relativity) require rejecting the existing knowledge base, something current LLMs,
    bound by their training data, cannot fundamentally do.
  relevance_score: 10
  source: llm_enhanced
  text: So, any model, any LLM that was trained on pre-1915 physics would never have
    come up with the theory of relativity. Einstein had to reject the Newtonian physics
    and come up with a space-time continuum.
  topic: safety/limitations
- impact_reason: 'Provides a clear boundary for current LLM capabilities: they can
    refine or unroll existing knowledge but cannot generate fundamentally new concepts.'
  relevance_score: 10
  source: llm_enhanced
  text: So, you can sort of self-improve up to a point, but beyond the point, these
    models can only sort of generate what they have been trained on.
  topic: limitations
- impact_reason: Uses the Theory of Relativity as a powerful, concrete example of
    true scientific discovery that requires rejecting the existing training set paradigm,
    which current LLMs cannot do.
  relevance_score: 10
  source: llm_enhanced
  text: So, any LLM that was trained on pre-1915 physics would never have come up
    with the theory of relativity. Einstein had to reject the Newtonian physics and
    come up with a space-time continuum. He completely rewrote the rules.
  topic: limitations
- impact_reason: 'A clear call to action: creating truly novel knowledge requires
    architectural innovation, not just scaling current models.'
  relevance_score: 10
  source: llm_enhanced
  text: But creating new dots, I think we need an architectural advance.
  topic: strategy
- impact_reason: 'The most precise distinction made: LLMs navigate existing knowledge
    spaces (manifolds); AGI must create new ones.'
  relevance_score: 10
  source: llm_enhanced
  text: So, the way I would say that LLMs currently navigate through this known Bayesian
    manifold, AGI will create new manifolds. So, right now these models navigate,
    they do not create.
  topic: AGI definition
- impact_reason: Argues that purely language-based models miss crucial embodied intelligence
    capabilities, like real-time physical simulation.
  relevance_score: 10
  source: llm_enhanced
  text: Language is great, but language is not the answer. You know, when I'm looking
    at catching a ball that is coming to me, I'm mentally doing that simulation in
    my head. I'm not translating it to language to figure out where it'll land.
  topic: limitations
- impact_reason: Highlights the serendipitous nature of major AI breakthroughs (RAG)
    originating from practical, domain-specific problems (fixing a stats page).
  relevance_score: 9
  source: llm_enhanced
  text: Fischal Mischra was trying to fix a broken cricket stats page and accidentally
    helped spark one of A.I.'s biggest breakthroughs. On this episode of the a16z
    podcast, I talk with Fischal and Martin Casaro at a16z about how that moment led
    to retrieval augmented generation...
  topic: business/strategy/history
- impact_reason: 'Provides a concise, technical explanation of LLM operation: mapping
    high-dimensional data onto a lower-dimensional geometric manifold (state space).'
  relevance_score: 9
  source: llm_enhanced
  text: You're trying to describe how LLMs work, and one thing that you found is that
    they reduce a very, very complex multi-dimensional space into basically a geometric
    manifold that's a reduced state space.
  topic: technical
- impact_reason: 'Reiterates the fundamental mechanism of all current LLMs: next-token
    prediction based on a learned probability distribution.'
  relevance_score: 9
  source: llm_enhanced
  text: Ultimately what all these LLMs are doing, whether the early LLMs or the LLMs
    that we have today with also the post training, RLHF, whatever you do, at the
    end of the day, what they do is they create a distribution for the next token.
  topic: technical
- impact_reason: Links prompt specificity (information richness) directly to reduced
    output uncertainty (prediction entropy), explaining why detailed prompts yield
    better results.
  relevance_score: 9
  source: llm_enhanced
  text: The moment you add more context, you make the prompt information rich, the
    prediction entropy reduces.
  topic: technical/strategy
- impact_reason: Highlights the critical early limitation of context window size,
    which directly necessitated the invention of RAG.
  relevance_score: 9
  source: llm_enhanced
  text: But soon I realized that, you know, no, I cannot really do it [use GPT-3 directly
    for Stats Guru]. Because Stats Guru, the backend databases were so complex. And
    if you remember, GPT-3 had only a 2048 token context window. There was no way
    in hell I could fit the complexities of that database in that context window.
  topic: technical/limitations/history
- impact_reason: Establishes a precise timeline, showing RAG was in production long
    before the public explosion of generative AI, positioning it as a foundational
    technique.
  relevance_score: 9
  source: llm_enhanced
  text: That had been running in production since September 2021, about 15 months
    before ChatGPT came. And the whole revolution in some sense started, and RAG became
    very popular.
  topic: history/RAG
- impact_reason: Highlights the fundamental limitation of early LLMs (small context
    windows) that necessitated external retrieval mechanisms.
  relevance_score: 9
  source: llm_enhanced
  text: There was no way in hell I could fit the complexities of that database in
    that context window. And GPT-3 also did not do instruction following at that time.
  topic: technical/limitations
- impact_reason: 'Provides a concrete, early description of the RAG mechanism: retrieval
    of relevant examples/context to guide the LLM''s completion.'
  relevance_score: 9
  source: llm_enhanced
  text: So, based on the new query, I would look through my set of natural language
    queries. I had about 1500 examples. And I would pick the six or seven most relevant
    ones. And then that and the structured query I would send as a prefix and the
    new query, that GPT-3 magically completed it.
  topic: technical/architecture
- impact_reason: 'Reinforces the plateau argument: improvements are incremental (better,
    faster, larger context) rather than revolutionary (new capabilities).'
  relevance_score: 9
  source: llm_enhanced
  text: The capabilities of LLMs have not fundamentally changed. They've become better.
    They've improved, but they have not crossed into a different realm.
  topic: predictions/trends
- impact_reason: Identifies the speaker's work as providing one of the first formal,
    mechanistic explanations for the emergent property of in-context learning.
  relevance_score: 9
  source: llm_enhanced
  text: And then you took in-context learning as an example, and you mapped it to
    Bayesian reasoning, which to me was incredibly powerful, because at the time,
    nobody knew why in-context learning worked.
  topic: technical/theory
- impact_reason: A staggering illustration of the scale of the problem LLMs solve
    implicitly, emphasizing why exact representation is impossible and compression/generalization
    is necessary.
  relevance_score: 9
  source: llm_enhanced
  text: So, the size of this matrix is, you know, if you just take just the old first-generation
    GPT-3 model, which had a context window of 2000 tokens, and a vocabulary of 50,000
    next tokens or 50,000 tokens, then the size of it, the number of rows in this
    matrix is more than the number of atoms across all galaxies that we know of.
  topic: technical/limitations
- impact_reason: 'Clearly defines the mapping: the prompt acts as new evidence updating
    the model''s prior knowledge (the trained matrix subset) into a posterior distribution.'
  relevance_score: 9
  source: llm_enhanced
  text: Right? So the context of the prompt impacts the posterior distribution. Exactly.
    Yeah. Right. And you mapped to Bayesian learning where the context is the new
    evidence, new evidence.
  topic: technical/theory
- impact_reason: 'Crucial insight: The mechanism for ''learning'' in a prompt (in-context
    learning) is fundamentally the same as the mechanism for simple continuation.'
  relevance_score: 9
  source: llm_enhanced
  text: But the process with which it's generating or doing this inference is exactly
    the same. And that's what I have been trying to model and come up with a formal
    model of. What I've found very impressive is you've used this basic model to show
    a number of things, right? To describe in-context learning, to map to Bayesian
    learning.
  topic: technical/theory
- impact_reason: Sets a hard limit on inherent self-improvement for current LLM architectures
    based on their training data closure.
  relevance_score: 9
  source: llm_enhanced
  text: You could sort of self-improve up to a point, but beyond the point, these
    models can only sort of generate what they have been trained on.
  topic: safety/predictions
- impact_reason: Directly addresses the common misconception that chaining multiple
    LLMs together (without external input) leads to emergent new knowledge, suggesting
    a fundamental limitation based on their training data.
  relevance_score: 9
  source: llm_enhanced
  text: But again, you even address this even in the case of like N number of LLMs
    using kind of the matrix model to show that like you just aren't getting any new
    information.
  topic: limitations
- impact_reason: Offers a high-bar definition of AGI centered on 'discovering something
    fundamental' rather than just computation or interpolation.
  relevance_score: 9
  source: llm_enhanced
  text: So, that is an example of, you know, AGI where you are generating or generating
    new knowledge. It's not simply unrolling what you have. It's not computing something.
    It's actually discovering something fundamental about the universe.
  topic: AGI definition
- impact_reason: A concise summary statement on the architectural inability to achieve
    true scientific breakthroughs.
  relevance_score: 9
  source: llm_enhanced
  text: So, those are examples where you're creating new science or fundamentally
    new results. That kind of self-improvement is not possible with these architectures.
  topic: limitations
- impact_reason: Offers a nuanced middle ground definition for current advanced LLMs,
    positioning them above 'stochastic parrots' but below AGI due to their reliance
    on Bayesian reasoning over existing data.
  relevance_score: 9
  source: llm_enhanced
  text: So, the way, you know, I think about it, the way we've tried to formulate
    in our papers, is it's beyond a stochastic parrot, but it's not AGI. It's doing
    Bayesian reasoning over what it has been trained on.
  topic: AGI definition
- impact_reason: Strong assertion that scaling data/compute only refines existing
    capabilities ('smoother manifolds') and will plateau without architectural change.
  relevance_score: 9
  source: llm_enhanced
  text: I personally think that we need a new architecture. The more data that we
    have, the more compute we have, we'll get maybe smoother manifolds.
  topic: strategy
- impact_reason: Reiterates the consensus that the path to advanced intelligence requires
    a fundamental architectural breakthrough, not just scaling.
  relevance_score: 9
  source: llm_enhanced
  text: I completely agree. There has to be a new sort of architectural leap that
    is needed to go from the current, you know, just throwing more data and more compute.
  topic: strategy
- impact_reason: Points out a critical gap between current transformer architectures
    and human learning efficiency (few-shot learning vs. massive data requirements).
  relevance_score: 9
  source: llm_enhanced
  text: You know, the way human brains learn with very few examples, that's not the
    way transformers learn.
  topic: technical
- impact_reason: Identifies approximate simulation capability as a key feature needed
    in future architectures.
  relevance_score: 9
  source: llm_enhanced
  text: So, a way, you know, one of the new architectures, architectural things, is
    how do we do, how do we get these models to do approximate simulations to test
    out that idea and whether to proceed or not.
  topic: technical
- impact_reason: 'Illustrates a cultural clash: the AI community''s heavy reliance
    on empirical results (''So what?'') over formal modeling and theoretical understanding.'
  relevance_score: 9
  source: llm_enhanced
  text: I've submitted one version of this work to one very famous machine learning
    or AI conference, and the reviewer said, 'Okay, this is a model. So what?'
  topic: safety/ethics
- impact_reason: 'A critical observation on the current state of ML research: heavy
    empiricism is a symptom of a lack of fundamental theoretical understanding.'
  relevance_score: 9
  source: llm_enhanced
  text: I honestly, I mean, I find there's so much empiricism in like the current,
    you know, AI community. Exactly because we don't understand the system.
  topic: strategy
- impact_reason: Endorses the value of formal mathematical modeling over purely empirical
    observation for understanding LLM behavior.
  relevance_score: 8
  source: llm_enhanced
  text: In my experience over the last three years, the ones that have most impacted
    my understanding, and I think have been the most predictive, are the ones that
    Fischal has come up with [formal models for LLMs].
  topic: technical/strategy
- impact_reason: Introduces the concept of using Shannon entropy on the next-token
    probability distribution as a quantifiable metric for LLM uncertainty or choice
    breadth.
  relevance_score: 8
  source: llm_enhanced
  text: You can measure the entropy of the distribution. You measure entropy. Yes,
    Shannon entropy. Not thermodynamic entropy.
  topic: technical
- impact_reason: Clearly defines the meaning of high vs. low entropy in the context
    of next-token prediction.
  relevance_score: 8
  source: llm_enhanced
  text: A high entropy distribution means that there are many different ways that
    the LLM can go with high enough probability for all those paths. Low entropy means
    that there are only a small set of choices for the next token.
  topic: technical
- impact_reason: Illustrates the gap between successful engineering (building RAG)
    and deep understanding (modeling why it works), driving the need for formal AI
    theory.
  relevance_score: 8
  source: llm_enhanced
  text: I didn't call it RAG. But this is something I accidentally did in trying to
    solve that problem for Cricinfo. Now, once I built it, I was trying to turn that
    this work, but I had no idea why it worked. I stayed at that, I stayed at that,
    that's from an architecture diagram. I read those papers, but I couldn't understand
    how or why it worked. So, then I started on this journey of developing a mathematical
    model, trying to understand how it worked.
  topic: strategy/technical
- impact_reason: Illustrates the gap between successful engineering/implementation
    and theoretical understanding in early LLM work, driving the need for formal modeling.
  relevance_score: 8
  source: llm_enhanced
  text: Now, once I built it, I was trying to turn that this work, but I had no idea
    why it worked. I stayed at that, I stayed at that, that's from an architecture
    diagram. I read those papers, but I couldn't understand how or why it worked.
    So, then I started on this journey of developing a mathematical model, trying
    to understand how it worked.
  topic: strategy/research
- impact_reason: 'Defines the current paradigm shift in human-computer interaction:
    treating LLMs as active, collaborative agents rather than simple tools.'
  relevance_score: 8
  source: llm_enhanced
  text: We treat these models as co-workers, almost like an intern, that, you know,
    you're constantly chatting with them, brainstorming, making them do all sorts
    of work, which we couldn't imagine, you know.
  topic: business/impact
- impact_reason: Highlights the value of rigorous, mathematical modeling over speculative
    rhetoric in understanding AI capabilities.
  relevance_score: 8
  source: llm_enhanced
  text: The thing that really struck me about your work is, you're like, 'No, let's
    figure out exactly what's going on. Let's come up with a formal model.'
  topic: strategy/research
- impact_reason: 'Provides the foundational concept for the speaker''s formal model:
    the Prompt-to-Vocabulary Probability Matrix.'
  relevance_score: 8
  source: llm_enhanced
  text: So, so yeah, let's start with that matrix abstraction. So the idea behind
    the matrix is you have this gigantic matrix where every row corresponds to a prompt.
    And then the number of columns of this matrix is the vocabulary of the LLM, the
    number of tokens it has that it can emit.
  topic: technical/theory
- impact_reason: Provides a real-world demonstration of in-context learning (few-shot
    learning) explained through the Bayesian/matrix framework.
  relevance_score: 8
  source: llm_enhanced
  text: Yet, after showing it on a few examples, it learned it right away. So, that's
    an example where it has seen DSLs or structures in the past. And now, using this
    evidence that I show, 'Okay, this is what my DSL looks like.' Now, a new natural
    language query, it is able to create the right posterior distribution for the
    tokens that map to the example that I've seen.
  topic: technical/in-context learning
- impact_reason: Reinforces the limitation by citing Quantum Mechanics as another
    example of necessary paradigm shifts beyond the training data.
  relevance_score: 8
  source: llm_enhanced
  text: Similarly, you know, any LLM that was trained on it would not have come up
    with quantum mechanics, that wave-particle duality or this whole probabilistic
    notion, or that, you know, energy is not continuous but it is quantized.
  topic: limitations
- impact_reason: Provides a technical description of how LLMs solve problems (like
    IMO) using low-entropy paths within their learned manifold, linking it to Bayesian
    reasoning.
  relevance_score: 8
  source: llm_enhanced
  text: Even the LLMs, what they're doing is they are exploring all sorts of solutions.
    In some of these solutions, they start going on this path where their next token
    entropy is low. So, that's where I say they are in that Bayesian manifold where
    you have this entropy collapse.
  topic: technical
- impact_reason: Highlights the diminishing returns of incremental data/compute when
    trying to achieve paradigm shifts, suggesting the training set is too vast to
    significantly evolve incrementally.
  relevance_score: 8
  source: llm_enhanced
  text: But the counterpoint that I've always just intuitively thought to that is
    the amount of data used to train these things is so large. How much can you actually
    evolve that manifold given an incremental... I mean, almost none at all, right?
  topic: limitations
- impact_reason: Positions LLMs as powerful tools, but not the final destination for
    AGI, requiring new architectures built upon them.
  relevance_score: 8
  source: llm_enhanced
  text: So, you're like a little bit famously says that LLMs are a distraction on
    the road to AGI. I don't think I'm not quite in that, but I think we need a new
    new architecture to sit on top of LLMs to reach AGI.
  topic: strategy
- impact_reason: Suggests that an information-theoretic perspective, often rooted
    in networking principles, is highly valuable for understanding and modeling LLMs.
  relevance_score: 7
  source: llm_enhanced
  text: We both come from networking. He's a much more accomplished networking guy
    than I am, but that's a high bar given you. We actually view the world in an information
    theoretic way.
  topic: strategy/technical
- impact_reason: Captures the astonishment at the rapid iteration speed from GPT-3
    to highly capable models like GPT-4.
  relevance_score: 7
  source: llm_enhanced
  text: So what is most surprised me, the pace of development. So, GPT-3 was, you
    know, it was a nice toy... the pace of development has really surprised me.
  topic: predictions/trends
- impact_reason: Critiques the polarized, often unhelpful rhetoric surrounding LLMs
    (either hyper-optimistic AGI hype or overly reductionist dismissal).
  relevance_score: 7
  source: llm_enhanced
  text: Everybody else was like, 'AGI, these things are going to, you know, recursively
    self-improve,' or, or, or, they'll say, 'Oh, these are just stochastic parrots,'
    which doesn't mean anything. So everybody had a rhetoric.
  topic: strategy/safety
- impact_reason: Mentions specific promising research directions (like energy-based
    models) that might offer the necessary architectural leap.
  relevance_score: 7
  source: llm_enhanced
  text: Yann has been pushing at this JAPA architecture, yeah, energy-based architectures.
    They seem promising.
  topic: technical
- impact_reason: 'Suggests a diagnostic approach to architectural design: understanding
    failure modes on hard tests can guide the creation of next-generation models.'
  relevance_score: 7
  source: llm_enhanced
  text: And if you understand why the LLMs are failing on this test [ARC prize], maybe
    you can sort of reverse-engineer a new architecture that will help you succeed.
  topic: strategy
- impact_reason: A strong endorsement of the speaker's formal, information-theoretic
    approach for understanding the bounds of current AI systems, appealing to systems
    engineers.
  relevance_score: 7
  source: llm_enhanced
  text: I really think that you should look up Fischal's work and read it. I just
    think it'll give you really, really, especially if the systems background, like
    a networking and systems background, it'll give you a really, really good understanding
    of kind of the bounds on these.
  topic: business/strategy
- impact_reason: A surprising historical data point illustrating the massive scale
    and early success of niche internet ventures, providing context for the speaker's
    entrepreneurial background.
  relevance_score: 6
  source: llm_enhanced
  text: I was one of the people who started this portal called Cricinfo. And Cricinfo,
    at one point, it was the most popular website in the world. It had more hits than
    Yahoo before India came online.
  topic: business/history
- impact_reason: Poses a fundamental, unresolved philosophical/evolutionary question
    about the relationship between language and intelligence.
  relevance_score: 6
  source: llm_enhanced
  text: Did we develop language because we were intelligent, or because we developed
    language, we accelerated in our intelligence?
  topic: strategy
source: Unknown Source
summary: '## Podcast Summary: Columbia CS Professor: Why LLMs Can’t Discover New Science


  This 50-minute podcast episode features a discussion with Columbia CS Professor
  **Fischal Mishra** (alongside a16z''s **Martin Casari**) about the fundamental limitations
  of Large Language Models (LLMs) in achieving true scientific discovery, contrasting
  their current capabilities with the requirements for Artificial General Intelligence
  (AGI). The conversation centers on Mishra''s formal mathematical models that explain
  *how* LLMs reason and why they are inherently bound by their training data.


  ---


  ### 1. Focus Area

  The primary focus is on **Formal Modeling of LLMs and Reasoning Limitations**. Specific
  topics include:

  *   The mathematical underpinnings of LLM inference (Transformer architecture, next-token
  prediction).

  *   The concept of **Bayesian Manifolds** (or reduced state spaces) that constrain
  LLM outputs.

  *   The mechanism behind **Retrieval Augmented Generation (RAG)**, which Mishra
  accidentally pioneered.

  *   The distinction between interpolation/pattern matching (what LLMs do) and true
  paradigm shifts (what AGI requires).

  *   The impossibility of **Recursive Self-Improvement** without external information.


  ### 2. Key Technical Insights

  *   **LLMs operate on Bayesian Manifolds:** LLMs reduce the complexity of the world
  into a geometric manifold of reduced degrees of freedom. They are confident and
  coherent only when traversing within this learned manifold. Veering off it leads
  to confident hallucination.

  *   **Entropy and Context:** Reasoning quality is tied to the entropy of the next-token
  distribution. Highly specific, information-rich prompts (low prediction entropy)
  force the model into a narrower, more confident path, explaining why Chain-of-Thought
  (CoT) prompting works—it forces the model to follow known algorithmic steps (low
  entropy paths).

  *   **The Matrix Abstraction:** LLMs implicitly represent a massive, sparse matrix
  where rows are prompts and columns are vocabulary tokens (next-token distributions).
  They do not store this matrix explicitly but interpolate across the subset of rows
  seen during training to generate a posterior distribution for new prompts.


  ### 3. Business/Investment Angle

  *   **RAG as an Accidental Breakthrough:** Mishra’s work on fixing the Cricinfo
  stats page led directly to the core mechanism now known as RAG, demonstrating that
  augmenting models with external, structured data retrieval is crucial for complex
  tasks.

  *   **Plateauing Capabilities:** The rapid, surprising pace of LLM development (GPT-3
  to GPT-4) is beginning to plateau. Current improvements are incremental (better
  camera on the iPhone analogy), suggesting that scaling alone will not lead to fundamentally
  new capabilities like scientific discovery.

  *   **Defining AGI:** True AGI requires the ability to generate *new science* (e.g.,
  inventing Relativity by rejecting Newtonian physics), which necessitates breaking
  out of the learned manifold—something current LLMs, bound by their inductive closure,
  cannot do.


  ### 4. Notable Companies/People

  *   **Fischal Mishra (Columbia CS Professor):** The central expert, known for developing
  formal, predictive models of LLM behavior, stemming from his background in networking
  and his accidental invention of RAG while working on the Cricinfo platform.

  *   **Martin Casari (a16z):** Host and interviewer, highlighting the predictive
  power of Mishra’s formal models compared to other industry rhetoric.

  *   **Cricinfo/ESPN:** The context for Mishra’s initial problem-solving, leading
  to the development of the RAG precursor in 2020/2021.


  ### 5. Future Implications

  The industry is hitting a wall where scaling current Transformer architectures will
  not yield true scientific breakthroughs. The path forward requires moving beyond
  the inductive closure of training data. The next fundamental advance will likely
  involve models capable of **rejecting established paradigms** and generating entirely
  new mathematical or scientific frameworks, which is the definition of AGI in this
  context.


  ### 6. Target Audience

  This episode is highly valuable for **AI Researchers, Machine Learning Engineers,
  and Technology Strategists/VCs** who need a rigorous, non-rhetorical understanding
  of the current theoretical limits of LLMs and what architectural shifts might be
  necessary for the next major leap in AI capability.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- startup
- openai
- anthropic
- google
title: 'Columbia CS Professor: Why LLMs Can’t Discover New Science'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 114
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 18
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 7
  prominence: 0.7
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-13 10:30:36 UTC -->
