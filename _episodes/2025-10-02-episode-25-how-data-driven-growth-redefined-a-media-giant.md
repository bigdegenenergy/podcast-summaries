---
companies:
- category: unknown
  confidence: medium
  context: 0 million people worldwide that want to watch it. When I first joined,
    the goal really was about data unif
  name: When I
  position: 95
- category: unknown
  confidence: medium
  context: We did that by building out what we now call the Televisa Univision Household
    Graph, the representation of our best guess of who we t
  name: Televisa Univision Household Graph
  position: 324
- category: unknown
  confidence: medium
  context: s interested in Spanish language media across the United States. We take
    what data is available and make either i
  name: United States
  position: 464
- category: unknown
  confidence: medium
  context: four years that we've been a part of it. That was Sergey Fogelson, VP of
    Data Science at Televisa Univision, the wo
  name: Sergey Fogelson
  position: 796
- category: unknown
  confidence: medium
  context: een a part of it. That was Sergey Fogelson, VP of Data Science at Televisa
    Univision, the world's largest Spanis
  name: Data Science
  position: 819
- category: unknown
  confidence: medium
  context: . That was Sergey Fogelson, VP of Data Science at Televisa Univision, the
    world's largest Spanish language media compa
  name: Televisa Univision
  position: 835
- category: unknown
  confidence: medium
  context: 300 million people worldwide. In this episode of High Signal, I talk with
    Sergey about what it means to build
  name: High Signal
  position: 1002
- category: unknown
  confidence: medium
  context: ks are in the show notes. Let's now check in with Duncan Gilchrist from
    Delphina before we jump into the interview.
  name: Duncan Gilchrist
  position: 1793
- category: unknown
  confidence: medium
  context: from Delphina before we jump into the interview. Hey Duncan. Hey Hugo,
    how are you? So before we jump into th
  name: Hey Duncan
  position: 1859
- category: unknown
  confidence: medium
  context: na before we jump into the interview. Hey Duncan. Hey Hugo, how are you?
    So before we jump into the conversa
  name: Hey Hugo
  position: 1871
- category: unknown
  confidence: medium
  context: re up to at Delphina and why we make High Signal. At Delphina, we're building
    AI agents for Data Science, and t
  name: At Delphina
  position: 2046
- category: unknown
  confidence: medium
  context: o much for having me, Hugo. A real pleasure, man. So Televisa Univision
    is the largest Spanish language media company in
  name: So Televisa Univision
  position: 2986
- category: unknown
  confidence: medium
  context: before, I went to a soccer match in Argentina, or Boca Juniors. And this
    was one of the wildest experiences of m
  name: Boca Juniors
  position: 5414
- category: unknown
  confidence: medium
  context: tches are definitely an interesting experience in South America. So, I
    am interested, you're VP of Data Science.
  name: South America
  position: 5789
- category: unknown
  confidence: medium
  context: . So, I am interested, you're VP of Data Science. And I'm wondering if
    you just tell us a bit about the d
  name: And I
  position: 5852
- category: unknown
  confidence: medium
  context: e, apps that were hyper-local. So, if you were in Los Angeles, there was
    a Los Angeles local Univision app for
  name: Los Angeles
  position: 6970
- category: unknown
  confidence: medium
  context: ications, etc. We built a CDP, an in-house CDP, a Customer Data Platform,
    that allows us to basically manage those relatio
  name: Customer Data Platform
  position: 11472
- category: unknown
  confidence: medium
  context: stem looked like, and really before the launch of Paramount Plus. Back
    then, that ecosystem was basically very sim
  name: Paramount Plus
  position: 12860
- category: tech
  confidence: high
  context: mbedding, whatever those entities are. And I know Anthropic has done some
    really fascinating work with doing
  name: Anthropic
  position: 20209
- category: unknown
  confidence: medium
  context: I'm wondering if we could just tie it back to the Household Graph. So,
    if you could remind us what this flagship as
  name: Household Graph
  position: 24499
- category: unknown
  confidence: medium
  context: thing that ties a specific person's PII, so their Personally Identified
    Information, to any component of our graph. And that's really
  name: Personally Identified Information
  position: 25468
- category: unknown
  confidence: medium
  context: for us, they lead to increases in revenue, right? The CPMs that you can
    get for delivering content on a smar
  name: The CPMs
  position: 30413
- category: unknown
  confidence: medium
  context: would say, it was like a hybrid system. We used a Factorization Machine
    that was a combination of what I would call colla
  name: Factorization Machine
  position: 32493
- category: unknown
  confidence: medium
  context: oing some interesting personalization work there. The Clips product is
    basically a combination of short-form
  name: The Clips
  position: 36901
- category: unknown
  confidence: medium
  context: different highlights of different goals from the Champions League or wherever.
    So, we're starting to do some more i
  name: Champions League
  position: 37830
- category: unknown
  confidence: medium
  context: e show notes link to an episode that we have with Roberto Mejri, who's
    a VP of Data at Instagram, who he launched
  name: Roberto Mejri
  position: 39416
- category: tech
  confidence: high
  context: els and how it was really existential for him and Meta as well, of course,
    because of TikTok, and how th
  name: Meta
  position: 39578
- category: unknown
  confidence: medium
  context: able to deliver using data and not generative AI. But I would have to find
    myself if we didn't talk about
  name: But I
  position: 39936
- category: unknown
  confidence: medium
  context: to be clear, a lot of the successes in generative AI I've seen, for example,
    in people building customer
  name: AI I
  position: 44489
- category: unknown
  confidence: medium
  context: ovider. So, we told it explicitly that we were on Cloud Provider A doing
    XYZ, and it basically gave us steps—so, it
  name: Cloud Provider A
  position: 47323
- category: unknown
  confidence: medium
  context: ng XYZ, and it basically gave us steps—so, it was Cloud Provider 1 for
    steps one through four, and then part of st
  name: Cloud Provider
  position: 47393
- category: ai_application
  confidence: high
  context: The company hosting the podcast ('High Signal') that is building AI agents
    for Data Science.
  name: Delphina
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as a legacy system where data (like true logs) was stored at
    ViacomCBS/Paramount, implying the use of their data products.
  name: Adobe
  source: llm_enhanced
- category: ai_algorithm_research
  confidence: high
  context: An ancient, effective algorithm used to generate embeddings from sequential
    data (like text or user histories).
  name: Word2Vec
  source: llm_enhanced
- category: ai_algorithm_research
  confidence: high
  context: An algorithm mentioned alongside Word2Vec for generating embeddings from
    text data.
  name: GloVe
  source: llm_enhanced
- category: ai_architecture
  confidence: high
  context: Mentioned as a larger neural network architecture where embedding components
    sit higher up than in Word2Vec/GloVe.
  name: BERT
  source: llm_enhanced
- category: big_tech_ai_model
  confidence: high
  context: Referenced as examples of incredibly huge, deep transformer architectures.
  name: Llama models
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned for doing fascinating work with text embeddings and using sparse
    autoencoders.
  name: Anthropic
  source: llm_enhanced
- category: ai_research_technique
  confidence: high
  context: A specific technique mentioned in the context of Anthropic's research on
    model interpretability.
  name: Sparse autoencoders
  source: llm_enhanced
- category: ai_infrastructure_data
  confidence: medium
  context: The speaker mentions their first gig was at an ad-tech company that specialized
    only in building household graphs for the entire United States.
  name: Ad-tech company (unnamed)
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company whose internal data science team built and evolved their recommendation
    system using various ML models (Factorization Machines, sequential architectures).
  name: VIX
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced as a leading short-form video platform whose high-frequency
    feedback loop drives superior personalization, setting a benchmark for the Clips
    product.
  name: TikTok
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in relation to its Reels product, which was developed in response
    to TikTok.
  name: Instagram
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as the parent company of Instagram, for whom the launch of Reels
    was existential.
  name: Meta
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: VP of Data at Instagram, mentioned in connection with launching Reels,
    linking him to Meta's AI/product efforts.
  name: Roberto Mejri
  source: llm_enhanced
date: 2025-10-02 04:00:00 +0000
duration: 56
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/000ae3bc3e8d4aca92a70c3adb8bba11/
processing_date: 2025-10-06 04:09:26 +0000
quotes:
- length: 287
  relevance_score: 10
  text: '" And the problem is, you know, and as I think everyone that''s actually
    a practitioner in the field knows, is you have to check almost everything as an
    output, especially when it comes to code generation, documentation, really any
    kind of output that you have there when it comes to text'
  topics: []
- length: 160
  relevance_score: 7
  text: We're using deep neural networks, which and we have to train them at scale,
    which means that you have to start using GPUs, which are not fun to work with
    at all
  topics: []
- length: 181
  relevance_score: 7
  text: I would just say more like you have to be—I am much more currently in the
    camp where LLMs just in general are really, really good at making their outputs
    appear incredibly confident
  topics: []
- length: 274
  relevance_score: 7
  text: The problem is—so, as a person that has been doing what I would call classic
    data science and machine learning for the vast majority of my career, and I'm
    sure you're the same thing with you, Hugo, we really like understanding the level
    of uncertainty with a piece of output
  topics: []
- length: 297
  relevance_score: 6
  text: And to be clear, a lot of the successes in generative AI I've seen, for example,
    in people building customer service chatbots, have been conversational AI customer
    service agents that the customer service human users to then get information to
    serve to a customer as opposed to directly serving it
  topics: []
- length: 210
  relevance_score: 5
  text: So, I think the biggest, the biggest opportunity I had when I was there was
    to get really into the weeds with what the now Paramount digital ecosystem looked
    like, and really before the launch of Paramount Plus
  topics:
  - opportunity
- length: 265
  relevance_score: 4
  text: We explore the role of data science in serving bilingual and multicultural
    communities, the trade-offs between internal data products and revenue-driving
    ones, and how to bring modern machine learning practices into a company where
    creativity and content still lead
  topics:
  - revenue
- length: 152
  relevance_score: 4
  text: And something I love about the work you've done—a lot of generative AI applications
    these days rest upon, you know, the supreme importance of embeddings
  topics: []
- length: 293
  relevance_score: 4
  text: We don't talk enough about embeddings in the space at the moment, and your
    work and your team links heavily on embeddings across projects, so maybe you could
    just give us a 30-second refresher on embeddings and then how they fit into your
    machine learning stack and what quick wins they enable
  topics: []
- length: 281
  relevance_score: 4
  text: At the end of the day, if there is a random sports league for whatever reason
    that has a really high affinity or a high embedding-based similarity from the
    perspective of content consumption behavior to a telenovela or to an original
    VIX movie, we would rather leverage that, right
  topics: []
- length: 234
  relevance_score: 4
  text: And we have—we have seen in the past couple of years a lot of excitement and
    a lot of possibility of LLMs and generative AI—possibility as opposed to capability,
    is how I frame it—and we have seen them fall short in a variety of cases
  topics: []
- length: 101
  relevance_score: 4
  text: And currently, there are no error bars that you can see when you have outputs
    from LLM infrastructure
  topics: []
- length: 227
  relevance_score: 4
  text: There's just—it's just you get some output, and there's no way to say that
    the likelihood that this output, whatever it is, especially when it comes to text
    or code or whatever, is in the LLM itself has some certainty around it
  topics: []
- length: 196
  relevance_score: 4
  text: And that is very, very—and it's not only from the perspective—I mean, it's
    from the perspective of the way that the LLM actually communicates the output
    is just incredibly like, "This is how it is
  topics: []
- length: 252
  relevance_score: 4
  text: '" Oh, so I would just say that at this point, I think the way to really—to
    really take LLMs to the next level of usability there has to be some way that
    for specific use cases, there''s some kind of a confidence assigned to the output
    that the LLM makes'
  topics: []
- length: 280
  relevance_score: 3
  text: It did outperform their model, which was I think validating for our data scientists,
    but also showed that we could actually in-house the intellectual property around
    the system, because in many cases, it could be the secret sauce for the continued
    growth of your streaming service
  topics:
  - growth
- length: 88
  relevance_score: 3
  text: They are really challenging, especially when you have to string couples of
    them together
  topics: []
- length: 205
  relevance_score: 3
  text: So, last time we spoke, you told me how you tested LLM-generated metadata
    against existing features, and I'm wondering if you could explain what the motivation
    was here and what lift you—what surprised you
  topics: []
- length: 282
  relevance_score: 3
  text: So, what we wound up doing is we wound up cleaning a little bit the metadata
    tags and then applying them and running an A/B test between our classic tagging
    solution that we had and then the generative AI basically passed solution, and
    we found actually some significant improvement
  topics: []
- impact_reason: Introduces a core, proprietary data asset (the Household Graph) crucial
    for targeted media serving and personalization in a specific demographic niche.
  relevance_score: 10
  source: llm_enhanced
  text: We did that by building out what we now call the Televisa Univision Household
    Graph, the representation of our best guess of who we think is interested in Spanish
    language media across the United States.
  topic: technical
- impact_reason: Quantifies the massive business success directly attributed to the
    data and digital strategy implemented.
  relevance_score: 10
  source: llm_enhanced
  text: At this point, the digital business has grown at least 10x in the four years
    that we've been a part of it.
  topic: business
- impact_reason: 'Offers crucial strategic advice for digital transformation in legacy
    companies: focus on incremental wins, resilience, problem definition, and avoiding
    premature adoption of complex tech like LLMs.'
  relevance_score: 10
  source: llm_enhanced
  text: Sergey's data modernization and innovation story at a massive traditional
    business, Televisa Univision, is chock full of real examples of practical wisdom,
    focusing on landing small, clear improvements, no single points of failure, making
    sure you know what problem to solve, and not just bringing an LLM hammer to every
    text nail.
  topic: strategy
- impact_reason: Directly addresses a critical, often under-discussed foundational
    component (embeddings) necessary for modern GenAI and deep learning applications.
  relevance_score: 10
  source: llm_enhanced
  text: a lot of generative AI applications these days rest upon, you know, the supreme
    importance of embeddings. We don't talk enough about embeddings in the space at
    the moment...
  topic: technical
- impact_reason: Shows a powerful application of sequence modeling (like NLP algorithms)
    to non-textual data (user behavior history), treating consumption sequences as
    'sentences'.
  relevance_score: 10
  source: llm_enhanced
  text: What's critical about them is that they be sequential, meaning that there's
    some meaning behind token or token being brought up. So, we've done for us is
    we treat our, if you will, sentences or our documents as user histories or really
    histories of—it could be histories of actions, it could be histories of consumption
    behavior...
  topic: technical
- impact_reason: 'A powerful strategic pivot: prioritizing behavioral similarity (derived
    from consumption embeddings) over traditional content-based similarity, suggesting
    a more accurate path to engagement.'
  relevance_score: 10
  source: llm_enhanced
  text: We care a lot about not necessarily what the content is about, but how it's
    being consumed. At the end of the day, if there is a random sports league for
    whatever reason that has a really high affinity or a high embedding-based similarity
    from the perspective of content consumption behavior to a telenovela or to an
    original VIX movie, we would rather leverage that, right?
  topic: strategy
- impact_reason: 'A sophisticated use case of graph context: enriching device embeddings
    with household context (like the presence of a CTV) to improve downstream modeling.'
  relevance_score: 10
  source: llm_enhanced
  text: you can also do, I would say, context-level expansion within the household
    to inform device-level embeddings. So, for example, you can take knowing that
    device A, device B, device C... you can, as part of your embedding process for
    those devices, you can say, 'Hey, this device belongs to a household with at least
    one smart-enabled television.'
  topic: AI/ML (Embeddings)
- impact_reason: Describes the migration to a modern, sequence-based architecture
    (likely RNN/Transformer-inspired) that solves the user entity cold-start problem
    by treating users as interaction streams.
  relevance_score: 10
  source: llm_enhanced
  text: We moved to a sequential recommendation architecture. So, these are full,
    reasonably new systems... sequential architectures where what they do is you can
    inject basically user and item features, but the actual user—there's no real entity
    that represents a specific user. They're really just streams of again, tokens
    with some added side information...
  topic: AI/ML (Model Architecture)
- impact_reason: 'Provides a core insight into why short-form video excels at personalization:
    high frequency and density of explicit/implicit user feedback signals.'
  relevance_score: 10
  source: llm_enhanced
  text: The reason that TikTok is such a boon for personalization or these other short-form
    video platforms are a boon for personalization is that the amount of interactivity
    and the amount of signal that you get is much, much higher, the frequency of the
    signal, right?
  topic: technical/predictions
- impact_reason: 'Crucial distinction for ML practitioners: short-form video provides
    strong, frequent negative signals (swiping away), which is vital for model training
    robustness.'
  relevance_score: 10
  source: llm_enhanced
  text: But with the short-form content, you have equivalent both positive and negative,
    right? A user swipes away a video means they didn't want it; they didn't like
    it. Or if they actually watch a video for an extended period of time... it's just
    about a much, much higher level signal for both positive and negative intent.
  topic: technical/strategy
- impact_reason: 'Details a specific, high-value application of LLMs/Multimodal models:
    automated, deep metadata extraction for legacy content improvement, driving recommendation
    lift.'
  relevance_score: 10
  source: llm_enhanced
  text: We made a bet that if we could extract useful metadata basically automatically
    by passing the content through single-modal, initially LLMs, now multimodal models—models
    that take both video and audio—and basically prompt them to generate high-level
    moods, themes, things of that nature, apply them to and tag our content with them,
    we would be able to see some incremental lift in the performance of our models.
  topic: technical/business
- impact_reason: Provides a concrete, quantifiable success metric (10-15% engagement
    lift) for using LLM-generated metadata in a backend recommendation system.
  relevance_score: 10
  source: llm_enhanced
  text: We found actually some significant improvement. It was on the order of 10
    to 15 percent increases in overall engagement against the original model...
  topic: business/technical
- impact_reason: 'Offers crucial strategic advice: Generative AI is safer and easier
    to deploy successfully when used for backend infrastructure enhancement (like
    tagging) rather than direct user-facing interaction (like search results).'
  relevance_score: 10
  source: llm_enhanced
  text: I would say in this case, it was this way, but a good place to use generative
    AI because it didn't require—there was nothing at the end of the day that was
    directly surfaced to the user. It was behind, I would say, it was part of the
    infrastructure for our algorithm as opposed to something that was just directly
    being surfaced to the user...
  topic: strategy/safety
- impact_reason: 'The single most critical critique of current LLMs for practitioners:
    their inherent overconfidence masks underlying uncertainty, which is antithetical
    to traditional ML rigor (error bars).'
  relevance_score: 10
  source: llm_enhanced
  text: LLMs just in general are really, really good at making their outputs appear
    incredibly confident. The problem is—so, as a person that has been doing what
    I would call classic data science and machine learning for the vast majority of
    my career... we really like understanding the level of uncertainty with a piece
    of output. I really like seeing error bars on things, right?
  topic: safety/technical
- impact_reason: 'Pinpoints the missing feature in current LLM APIs: the lack of inherent,
    visible confidence scores or probabilities for generated text/code.'
  relevance_score: 10
  source: llm_enhanced
  text: And currently, there are no error bars that you can see when you have outputs
    from LLM infrastructure. There's just—it's just you get some output, and there's
    no way to say that the likelihood that this output, whatever it is... is in the
    LLM itself has some certainty around it.
  topic: technical/limitations
- impact_reason: 'A direct call to action for AI developers: confidence scoring/probabilities
    are the necessary next step for LLM usability and safety in professional workflows.'
  relevance_score: 10
  source: llm_enhanced
  text: I would just say that at this point, I think the way to really—to really take
    LLMs to the next level of usability there has to be some way that for specific
    use cases, there's some kind of a confidence assigned to the output that the LLM
    makes. There has to be some way to provide probabilities against the outputs.
  topic: technical/predictions
- impact_reason: 'Explains the core paradox of current LLM adoption: without confidence
    metrics, the required verification overhead negates the promised efficiency gains.'
  relevance_score: 10
  source: llm_enhanced
  text: If you can't assign those [probabilities], you basically have to have your
    end users check everything, and that makes it so that it actually takes, in some
    cases, longer leveraging the outputs of the LLMs than it would have been for the
    user to just do the thing themselves, which frankly was the whole point of why
    everyone's so excited about generative AI, right? It's supposed to shrink our
    time to ship...
  topic: business/strategy
- impact_reason: 'Defines the foundational, non-glamorous first step for any large-scale
    data transformation: infrastructure and standardization.'
  relevance_score: 9
  source: llm_enhanced
  text: When I first joined, the goal really was about data unification, standardization,
    and then building out some core infrastructure that would allow us to serve our
    digital audience.
  topic: strategy
- impact_reason: 'Provides a clear framework for prioritizing data science work: internal
    efficiency, client enablement, or direct revenue generation.'
  relevance_score: 9
  source: llm_enhanced
  text: We take what data is available and make either infrastructural products or
    data products that will either make our own lives easier, our internal clients'
    lives easier, or make the company more money.
  topic: business
- impact_reason: Links rich digital signal directly to the ability to deliver personalization
    and deep consumer understanding.
  relevance_score: 9
  source: llm_enhanced
  text: So, when I joined, we were basically in the middle of a reimagining of what
    the data function could look like specifically anchored within the digital ecosystem,
    because we got lots of signal there. The signal was rich and it allowed us to
    be able to start creating personalized experiences for our consumers and also
    to just really understand our consumers really well.
  topic: technical
- impact_reason: Lists the concrete, early-stage data science deliverables required
    before advanced ML can be deployed.
  relevance_score: 9
  source: llm_enhanced
  text: Initially, like I said, it was all about data unification, building out data
    infrastructure, and building out core components. So, that's the graph, some ancillary
    components on top of the graph, things like audience profiling, look-alike modeling,
    and then some basic attribution analyses around ad delivery, things like that.
  topic: technical
- impact_reason: Identifies the launch of a major D2C product (VIX) as the catalyst
    for the second phase of data science maturity (deeper personalization).
  relevance_score: 9
  source: llm_enhanced
  text: But that really happened after the second, I would say, evolution, big evolution
    after I came, which was the launch of our fully owned, fully created Spanish language,
    direct-to-consumer streaming service called VIX.
  topic: business
- impact_reason: Describes the practical application of ML/data infrastructure across
    the entire customer lifecycle, including off-app messaging for retention.
  relevance_score: 9
  source: llm_enhanced
  text: And so, to that end, we've built some algorithms—the team has built some algorithms
    for analyzing the experience on VIX, as well as infrastructure to tailor the messaging
    that we send to people, either within the app or outside of the app, they're into
    their inboxes or in push notifications, etc.
  topic: technical
- impact_reason: 'Focuses on the current frontier: advanced personalization within
    the app and using external messaging to drive re-engagement (retention focus).'
  relevance_score: 9
  source: llm_enhanced
  text: And then on the flip side, we're also doing some new and more interesting
    work with deeper personalization on the application, as well as deeper and more
    interesting work around messaging, tailoring, personalized messaging experiences.
    So, yeah, I think that's a great way to use it when they're not on the app so
    we can bring them back.
  topic: technical
- impact_reason: 'Reiterates the universal challenge in large enterprises: schema
    harmonization and integrating data from highly varied, legacy sources (Adobe,
    ad servers).'
  relevance_score: 9
  source: llm_enhanced
  text: I really got to understand how to take disparate data sources and put them
    into one kind of general schema that we could then use, leverage, and ultimately
    [apply value].
  topic: technical
- impact_reason: 'Crucial business/organizational insight: Success in data/AI requires
    clear division of labor (Data Engineering for plumbing, Data Science for modeling/value
    extraction) and strong cross-functional alignment.'
  relevance_score: 9
  source: llm_enhanced
  text: We have a really incredible data engineering organization at Univision, and
    we have really great product folks that I've been working with. One set the overall
    vision, the other really got data to me and shipped data out for me—things that
    I didn't necessarily care about having to do. That allowed me and my team to really
    just focus on the stuff that we can do well, which is take what data is available
    and make either infrastructural products or data products that will either make
    our own lives easier, our internal clients' lives easier, or make the company
    more money.
  topic: business
- impact_reason: 'A concise, accessible definition of embeddings, emphasizing their
    core utility: enabling vector mathematics on abstract objects.'
  relevance_score: 9
  source: llm_enhanced
  text: an embedding is it can be a low or high-dimensional representation of some
    object. That object can really be anything in a mathematical space that allows
    you to do interesting things with them. Specifically, you can do vector math with
    those embeddings, right?
  topic: technical
- impact_reason: 'Frames the core technical challenge in applied ML: the transformation
    (encoding) process from raw data into the useful mathematical representation (the
    embedding).'
  relevance_score: 9
  source: llm_enhanced
  text: Now, the secret sauce is how do you get to an embedding? How do you go from
    data that exists, whether it is text on a web page or a picture or an audio file
    or even just tabular data... into one of these things.
  topic: technical
- impact_reason: Demystifies older embedding techniques (Word2Vec/GloVe) by classifying
    them correctly as foundational algorithms rather than monolithic infrastructure,
    making them accessible for reuse.
  relevance_score: 9
  source: llm_enhanced
  text: So, things like Word2Vec or GloVe or Word2Vec—these are people talk about
    that as infrastructure or architectural components, but they're really just algorithms
    that you can take and take, I would say, unstructured text or unstructured data
    or what you think is unstructured data and then create embeddings out of them,
    right?
  topic: technical
- impact_reason: Provides a clear architectural progression showing how embeddings
    fit into models of increasing complexity (from shallow models like Word2Vec to
    massive Transformers like Llama).
  relevance_score: 9
  source: llm_enhanced
  text: You can think of Word2Vec or GloVe as very, very small neural networks where
    the component just sits right in the middle, and then you have BERT, which is
    a slightly larger neural network architecture where those embedding components
    might sit slightly higher up, or then you have the Llama models where you have
    incredibly huge, incredibly deep architectures.
  topic: technical
- impact_reason: 'Illustrates a direct business benefit of embeddings: finding latent
    similarities across different entity types (users, content, geography) to enable
    targeted scaling and personalization.'
  relevance_score: 9
  source: llm_enhanced
  text: We find similar demographic groups, we find similar shows, we find similar
    geographic regions, meaning the regions behave similarly from the perspective
    of our content consumption, and that allows us to scale small audiences.
  topic: business
- impact_reason: A concrete example of using similarity search on user embeddings
    for high-value business outcomes (conversion/retention).
  relevance_score: 9
  source: llm_enhanced
  text: we take their basically the embeddings that represent those users and we find
    other users that have embeddings that are very similar to them, whether they're
    not yet subscribers, and we can then try to find them in the wider world and try
    to get them to come back to our platform, come to our platforms, and to subscribe.
  topic: business
- impact_reason: 'Lists the core, stable identifiers used for household linkage: IP,
    Device IDs, Mobile Ad IDs, and CTV identifiers—essential for cross-device identity
    resolution.'
  relevance_score: 9
  source: llm_enhanced
  text: Those components are IP addresses, which is basically usually tied—there's
    a single IP address that is associated with a single household, if you own the
    United States, and then digital identifiers that exist in the digital ecosystem.
    So, that is device IDs, advertising identifiers on mobile phones, and then CTV
    identifiers.
  topic: technical
- impact_reason: 'Directly links device context (screen size) to core business metrics:
    engagement, experience, and revenue (via higher CPMs).'
  relevance_score: 9
  source: llm_enhanced
  text: what we ultimately want to do in general, when you're at your house, what
    we always want people to do is to watch our content on the largest possible screen
    that we can find, right? Because ultimately, larger screens almost always lead
    to more engagement, and then ultimately they lead to better consumer experience,
    and for us, they lead to increases in revenue, right?
  topic: business/strategy
- impact_reason: 'A key business validation point: proving that in-house ML expertise
    can beat paid third-party solutions, securing valuable IP.'
  relevance_score: 9
  source: llm_enhanced
  text: our homegrown system that we built outperforming the third-party model that
    we were paying quite a bit of money for to support. Thankfully, it did, which
    was great. It did outperform their model, which was I think validating for our
    data scientists, but also showed that we could actually in-house the intellectual
    property around the system...
  topic: business
- impact_reason: 'Highlights the operational trade-off: modern, flexible models require
    significant investment in GPU infrastructure and increased MLOps complexity.'
  relevance_score: 9
  source: llm_enhanced
  text: Now the complexity here though is, okay, great, you can do this... But now
    we're using a much more modern architecture. We're using deep neural networks,
    which and we have to train them at scale, which means that you have to start using
    GPUs, which are not fun to work with at all.
  topic: AI/ML (Deployment/MLOps)
- impact_reason: A direct warning/insight on the increased MLOps burden associated
    with deep learning/sequential models compared to traditional matrix factorization.
  relevance_score: 9
  source: llm_enhanced
  text: There's just lots of, there's lots of, I would say, more like MLOps that's
    involved with these kinds of infrastructure than the simpler, I would say, more
    classic like factorization-based models.
  topic: AI/ML (MLOps)
- impact_reason: Highlights the significant MLOps complexity increase when moving
    from simpler recommendation models (like factorization) to large-scale GPU-dependent
    sequential models.
  relevance_score: 9
  source: llm_enhanced
  text: They are really challenging, especially when you have to string couples of
    them together. There's just lots of, there's lots of, I would say, more like MLOps
    that's involved with these kinds of infrastructure than the simpler, I would say,
    more classic like factorization-based models.
  topic: technical/strategy
- impact_reason: Contrasts the sparse, mostly positive feedback loop of traditional
    long-form content recommendation with the richer signals of short-form content.
  relevance_score: 9
  source: llm_enhanced
  text: From the perspective of the long-form content that we have—the show or the
    movies or whatever—all we get is positive feedback, right? What people clicked,
    the positive feedback is explicit. The negative feedback is more implicit.
  topic: technical/strategy
- impact_reason: 'A key limitation/caveat for using LLMs/Multimodal models for content
    tagging: performance degrades significantly on non-standard or stylized content
    (like animation), requiring human oversight/cleaning.'
  relevance_score: 9
  source: llm_enhanced
  text: We had to actually do some post-cleaning against that. For example, one thing
    we found was that the automated—this automated approach did really well for content
    with actual people in it, right? So, when it was non-animated, but it was actually
    really not great with animated content...
  topic: safety/limitations
- impact_reason: 'A concise framework for evaluating current LLM technology: focusing
    on ''possibility'' (future potential) rather than current ''capability'' (reliable
    performance).'
  relevance_score: 9
  source: llm_enhanced
  text: I frame it as possibility as opposed to capability, and we have seen them
    fall short in a variety of cases.
  topic: strategy/predictions
- impact_reason: Describes the dangerous conversational style of LLMs that reinforces
    false certainty, making verification harder for non-experts.
  relevance_score: 9
  source: llm_enhanced
  text: And so, what winds up happening is it's 100% certain about 100% of everything.
    And that is very, very—and it's not only from the perspective—I mean, it's from
    the perspective of the way that the LLM actually communicates the output is just
    incredibly like, 'This is how it is. This is what you do.'
  topic: safety/limitations
- impact_reason: A concrete, high-stakes example of LLM hallucination/confabulation,
    where it mixes training data from disparate sources, leading to unusable or actively
    incorrect instructions.
  relevance_score: 9
  source: llm_enhanced
  text: And you get cases where... it basically gave us documentation from a completely
    different cloud provider. So, we told it explicitly that we were on Cloud Provider
    A doing XYZ, and it basically gave us steps—so, it was Cloud Provider 1 for steps
    one through four, and then part of step five, and then for whatever reason, for
    steps five and the back half of step five and step six, it just took stuff verbatim
    from a completely different cloud provider and popped it in.
  topic: safety/limitations
- impact_reason: Highlights the massive scale of the audience served by Televisa Univision,
    setting the stage for the impact of their data and personalization efforts.
  relevance_score: 8
  source: llm_enhanced
  text: Our content is available for at least 250, 300 million people worldwide that
    want to watch it.
  topic: business
- impact_reason: Details the complexity of defining the Total Addressable Market (TAM)
    for culturally specific media, contrasting it with general market targeting.
  relevance_score: 8
  source: llm_enhanced
  text: We're really just looking for people that have engaged with that content,
    and that allows us to cast a slightly wider net. But at the same time, it makes
    our job a little bit harder, I think, than your standard broad-base, traditional
    media company where your TAM, your total addressable market, at least in the United
    States, is literally every person that lives in the United States.
  topic: strategy
- impact_reason: Highlights the strategic decision to build a Customer Data Platform
    (CDP) in-house to manage cross-channel messaging and customer relationships.
  relevance_score: 8
  source: llm_enhanced
  text: We built a CDP, an in-house CDP, a Customer Data Platform, that allows us
    to basically manage those relationships and to be able to message them effectively.
  topic: technical
- impact_reason: Details the successful hybrid monetization strategy (AVOD/SVOD) for
    the VIX streaming service.
  relevance_score: 8
  source: llm_enhanced
  text: We now have millions and millions, I think over 10 million streaming subscribers
    for the paid version of the service. We also have a free version of the service.
    We have a totally free tier where you can just watch whatever you want that's
    not behind the paywall, and then you get ads every 10, 15 minutes.
  topic: business
- impact_reason: Provides a powerful data point demonstrating the strength and unique
    appeal of specialized content (soccer) transcending language demographics.
  relevance_score: 8
  source: llm_enhanced
  text: We also get, we're regularly, at least in the United States, we actually out-deliver
    or have larger audiences for soccer matches than the English language channels,
    right? Even if you look at just the English language speaking demographic, they
    actually like to watch soccer on our channels because I think it's a very different
    experience.
  topic: business
- impact_reason: Provides a concrete, high-impact metric demonstrating the massive
    business value derived from a focused data/modeling effort in the media/streaming
    space.
  relevance_score: 8
  source: llm_enhanced
  text: in terms of revenue, I think at this point, the digital business has grown
    at least 10x in the four years that we've been a part of, and my team can take
    some amount of, I would say, credit for that success.
  topic: business
- impact_reason: Confirms the primary, highest-ROI application for learned embeddings
    in media/content platforms.
  relevance_score: 8
  source: llm_enhanced
  text: we find the most success ultimately for these embeddings is in our recommendation
    systems.
  topic: business
- impact_reason: Defines the strategic importance of their 'Household Graph'—it's
    not about PII, but about defining the addressable market based on media interaction
    likelihood.
  relevance_score: 8
  source: llm_enhanced
  text: The graph is, like I said, it's a representation of who we think households
    in the United States that we think are very likely to interact with Spanish language
    media broadly construed... We just want to know that they are because it means
    that they are in our, basically, I would say, our addressable universe, if you
    will...
  topic: strategy
- impact_reason: Highlights the value of finding non-obvious, latent connections (potentially
    discovered via embeddings or graph analysis) for hyper-personalization, driving
    engagement.
  relevance_score: 8
  source: llm_enhanced
  text: that thing that you would think is serendipitous between, for whatever reason,
    that sports league or maybe that specific team and this piece of content is really
    interesting, and we can take that, use that to our advantage to personalize the
    experience on our applications.
  topic: strategy
- impact_reason: Directly links the anonymized graph structure to mitigating current
    and future privacy regulatory challenges.
  relevance_score: 8
  source: llm_enhanced
  text: And that's really critical because the whole point of this is to make it so
    that we don't have to necessarily deal with a lot of the kind of the issues around
    privacy that exist in the United States at the moment.
  topic: safety/ethics
- impact_reason: 'Describes the core process of identity resolution: using a proprietary
    algorithm to aggregate sightings of stable identifiers into household units.'
  relevance_score: 8
  source: llm_enhanced
  text: What we do is we have a proprietary algorithm that takes sightings of these
    IP addresses and device identifiers from all corners of wherever we can basically
    get this data... and we household them.
  topic: technical
- impact_reason: Explains the utility of coarse, high-level household embeddings for
    demographic and geographic targeting.
  relevance_score: 8
  source: llm_enhanced
  text: We can generate household-level embeddings, which are going to be coarse,
    but they are going to give you some understanding of the geography and the household-level
    demography of that household, right?
  topic: AI/ML (Embeddings)
- impact_reason: Quantifies the direct financial incentive for driving users to CTV
    platforms over mobile devices.
  relevance_score: 8
  source: llm_enhanced
  text: The CPMs that you can get for delivering content on a smart TV are significantly
    higher than the CPMs... significantly higher than what you can get from a mobile
    device.
  topic: business
- impact_reason: 'Details the architecture of their initial, successful recommender
    system: a hybrid Factorization Machine approach, a common industry baseline.'
  relevance_score: 8
  source: llm_enhanced
  text: Our initial recommender system was really a, I would say, it was like a hybrid
    system. We used a Factorization Machine that was a combination of what I would
    call collaborative filtering and or with most people called both collaborative
    and content-based filtering, where we used user features, item features, but we
    also used basically matrix factorization, I would say, on steroids...
  topic: AI/ML (Model Architecture)
- impact_reason: Clearly articulates the classic cold-start problem in collaborative
    filtering models and its negative impact on new user experience.
  relevance_score: 8
  source: llm_enhanced
  text: we had this issue with more recent users basically getting default recommendations,
    where a default cold-start recommendation, you can think of it as like a trend
    that we've noticed over some period of time, and we're just recommending that
    all new users that trend.
  topic: AI/ML (Limitations)
- impact_reason: Serves as a reminder that significant business value can still be
    derived from traditional data science and ML techniques, balancing the current
    generative AI hype.
  relevance_score: 8
  source: llm_enhanced
  text: It's so wonderful how we've been able to talk about the amount of value you
    and your team and the organization have been able to deliver using data and not
    generative AI.
  topic: business/strategy
- impact_reason: 'Defines a low-risk success criterion for internal AI experiments:
    if the metrics improve, the risk of hallucination or error is mitigated by the
    positive business outcome.'
  relevance_score: 8
  source: llm_enhanced
  text: The barrier to success was significantly lower because basically it was like,
    'Hey, either metrics go up or they don't.' In our case, they went up. We kept
    at it.
  topic: business/strategy
- impact_reason: Explains the strategic rationale behind the merger (content creation
    vs. distribution synergy), relevant for understanding media consolidation.
  relevance_score: 7
  source: llm_enhanced
  text: Televisa is actually the company that creates the vast majority of the content.
    Univision would then license and put on all of its broadcast channels. So, it
    really didn't make sense, I think, economically for them both to be separate entities,
    right? One basically is the largest customer of the other.
  topic: business
- impact_reason: Illustrates the historical divide between legacy media measurement
    (Nielsen/linear TV) and modern digital analytics, which the data team had to bridge.
  relevance_score: 7
  source: llm_enhanced
  text: And there was also a legacy and still is a legacy research function, which
    basically means analyzing traditional over-the-top, linear television content
    that serves Spanish language media, right? So, it's basically we're talking about,
    at least now it's a little different, but back then it was basically just analyses
    of Nielsen ratings and who is watching what on TV.
  topic: strategy
- impact_reason: Shows an early, fragmented approach to digital delivery (many hyper-local
    apps) that lacked the scale needed for modern data science investment.
  relevance_score: 7
  source: llm_enhanced
  text: Univision back then had, for example, apps that were hyper-local. So, if you
    were in Los Angeles, there was a Los Angeles local Univision app for your mobile
    phone... all of those served a very small, kind of hyper-passionate core base
    of users. It didn't really have the scale of what Univision now has or Televisa
    now has from the perspective of digital content and delivery.
  topic: business
- impact_reason: 'Important note on data governance and privacy: the core asset (the
    graph) is built on anonymized linkage, mitigating PII risk while maintaining utility.'
  relevance_score: 7
  source: llm_enhanced
  text: The graph itself is actually fully anonymized. So, we have no—there's nothing
    that ties a specific person's PII, so their Personally Identified Informatio
  topic: safety
- impact_reason: Provides a clear, practical definition of 'persistence' and 'stability'
    in the context of identity resolution for graph construction.
  relevance_score: 7
  source: llm_enhanced
  text: the building blocks of the graph are just a few components that are reasonably
    persistent and stable, right? So, they're persistent in the sense that they exist
    from session A to session B to session C, and they're stable, meaning they usually
    refer to the same entity, in our case, either a household or a device over a long
    enough period of time, where long enough means usually weeks or months.
  topic: technical
- impact_reason: 'Crucial distinction for content strategy: they are dealing with
    professionally curated content, which simplifies moderation but requires different
    personalization approaches than UGC platforms.'
  relevance_score: 7
  source: llm_enhanced
  text: This content that we have, none of it is user-generated; it's all basically
    created in-house.
  topic: strategy
- impact_reason: 'Details the final steps of graph construction: applying filtering
    and thresholding to the clustered identifiers.'
  relevance_score: 6
  source: llm_enhanced
  text: We call that a household. We do some filtering on it; we do some thresholding
    on it, and we get what we call our graph.
  topic: technical
- impact_reason: Defines their entry into the short-form video space, benchmarking
    it against established social media formats (TikTok/Instagram).
  relevance_score: 6
  source: llm_enhanced
  text: The Clips product is basically a combination of short-form video content.
    It's a combination of news, sports, and entertainment, but all in a single basically
    swipeable feed that you'd be familiar with obviously if you consume any TikTok
    content or content on Instagram or any of these other large-scale video applications.
  topic: business/strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Episode 25: How Data-Driven Growth Redefined
  a Media Giant


  This episode features an in-depth conversation with **Sergey Fogelson, VP of Data
  Science at Televisa Univision**, the world''s largest Spanish-language media company,
  detailing their journey of data unification, infrastructure build-out, and leveraging
  advanced analytics to fuel massive digital growth.


  ---


  ### 1. Focus Area

  The discussion centers on **Data Science Transformation within a Legacy Media Conglomerate**.
  Key themes include data unification, building core data infrastructure (specifically
  the Household Graph), developing data products for internal clients and revenue
  generation, personalization strategies for the DTC streaming service (VIX), and
  the practical application of machine learning techniques like embeddings for audience
  understanding and content recommendation, all within the context of serving a massive,
  multicultural, Spanish-speaking audience.


  ### 2. Key Technical Insights

  *   **Televisa Univision Household Graph:** A flagship infrastructural product built
  to represent the company''s "best guess" of households interested in Spanish-language
  media across the US, moving beyond simple ethnicity tracking to focus on content
  engagement signals.

  *   **Embedding Applications Beyond Text:** The team successfully adapted algorithms
  traditionally used for text (like Word2Vec/GloVe) to create embeddings for sequential
  user behavior histories (e.g., sequence of shows watched). These embeddings represent
  entities like shows, actors, or geographic regions, enabling similarity matching
  for scaling niche audiences.

  *   **Consumption Behavior Over Content Similarity:** For recommendation systems,
  the team prioritizes embeddings derived from *how* content is consumed (consumption
  behavior similarity) over purely content-based similarity (e.g., genre matching),
  as behavioral affinity has proven more effective at driving engagement.


  ### 3. Business/Investment Angle

  *   **Massive Digital Growth:** The data modernization efforts directly correlated
  with significant business success; the digital business has grown at least **10x**
  in the four years since Sergey joined, demonstrating the ROI of data infrastructure
  investment.

  *   **Balancing Internal vs. Revenue Products:** A key strategic focus is creating
  data products that either simplify internal operations or directly drive company
  revenue, ensuring data science efforts are tied to tangible business outcomes.

  *   **DTC Success (VIX):** The launch and scaling of the VIX streaming service (free
  and paid tiers) necessitated rapid development of personalization and messaging
  infrastructure, validating the investment in a unified data foundation.


  ### 4. Notable Companies/People

  *   **Sergey Fogelson (VP of Data Science, Televisa Univision):** The central figure
  detailing the data transformation strategy.

  *   **Televisa Univision:** The combined entity of Televisa (content creation powerhouse)
  and Univision (US distribution leader), serving over 250-300 million people globally.

  *   **VIX:** The company''s successful Spanish-language direct-to-consumer streaming
  service.

  *   **Duncan Gilchrist (Delphina):** Host/Interviewer, highlighting the practical
  wisdom in Fogelson’s approach—focusing on clear, small improvements rather than
  immediately jumping to complex tools like LLMs.


  ### 5. Future Implications

  The conversation suggests that for large, established media companies, the next
  phase of data science involves moving beyond basic unification and modeling into
  **deeper, nuanced personalization** driven by behavioral embeddings. The focus will
  remain on leveraging proprietary first-party data to create unique audience representations
  (like the Household Graph) that cannot be easily replicated by competitors, ensuring
  cultural relevance and maximizing engagement across diverse linguistic and cultural
  segments.


  ### 6. Target Audience

  This episode is highly valuable for **Data Science Leaders, VPs/Directors of Data
  Engineering, Media/Entertainment Technology Strategists, and Professionals interested
  in scaling ML/AI within large, established enterprises.** It offers practical wisdom
  on infrastructure building and applied ML rather than purely theoretical AI research.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- anthropic
- meta
title: 'Episode 25: How Data-Driven Growth Redefined a Media Giant'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 87
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 10
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 7
  prominence: 0.7
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 04:09:26 UTC -->
