---
companies:
- category: unknown
  confidence: medium
  context: This is the Everyday AI Show, the everyday podcast where we simplify AI
    and br
  name: Everyday AI Show
  position: 12
- category: unknown
  confidence: medium
  context: ay's conversation. I hope you are too. Welcome to Everyday AI. Maybe it's
    your first time here. If so, where ha
  name: Everyday AI
  position: 1158
- category: unknown
  confidence: medium
  context: ee years? We do this every single day. My name is Jordan Wilson, and this
    is your daily livestream podcast and fr
  name: Jordan Wilson
  position: 1297
- category: unknown
  confidence: medium
  context: versation. I hope you are too. So we have with us Itzall Shulman, the CEO
    and co-founder of Cobias AI. Thank you s
  name: Itzall Shulman
  position: 2243
- category: unknown
  confidence: medium
  context: with us Itzall Shulman, the CEO and co-founder of Cobias AI. Thank you
    so much for joining the Everyday AI Sh
  name: Cobias AI
  position: 2285
- category: unknown
  confidence: medium
  context: s one. Livestream audience, thanks for tuning in. Big Bogey and Michelle
    and Marie and Jamie and Vincent, eve
  name: Big Bogey
  position: 2450
- category: unknown
  confidence: medium
  context: o make the changes necessary to make them better. So I want to kind of
    skip to the end here, and this is
  name: So I
  position: 4117
- category: unknown
  confidence: medium
  context: his is kind of how I started off the show, right? Because I think so many
    people just blindly either copy and
  name: Because I
  position: 4215
- category: tech
  confidence: high
  context: is like people. It was built by people, just like Google was built by people.
    Before we had the whole AI e
  name: Google
  position: 4541
- category: unknown
  confidence: medium
  context: ple gave, and that's just absolutely not correct. And I think one of the
    things that people discover in A
  name: And I
  position: 4758
- category: tech
  confidence: high
  context: ink, well, you know, obviously look, it's made by OpenAI, it's made by
    Microsoft, it's made by Google, it
  name: Openai
  position: 6493
- category: tech
  confidence: high
  context: obviously look, it's made by OpenAI, it's made by Microsoft, it's made
    by Google, it has to be good. But they
  name: Microsoft
  position: 6514
- category: unknown
  confidence: medium
  context: one of the psychologists in the crowd yell at me. But I'm just trying to
    make sure that it's something ea
  name: But I
  position: 7430
- category: unknown
  confidence: medium
  context: on of what is being said to us, shown to us, etc. So AI is going to respond
    to us in a specific way and b
  name: So AI
  position: 11940
- category: unknown
  confidence: medium
  context: veryone, David here, one of the product leads for Google Gemini. Check
    out V3, our state-of-the-art AI video gene
  name: Google Gemini
  position: 16356
- category: unknown
  confidence: medium
  context: ideos with native audio generation. Try it with a Google AI Pro plan or
    get the highest access with the Ultra pla
  name: Google AI Pro
  position: 16550
- category: tech
  confidence: high
  context: ery podcast. Companies like Adobe, Microsoft, and Nvidia have partnered
    with us because they trust our exp
  name: Nvidia
  position: 22056
- category: unknown
  confidence: medium
  context: a vast amount of variance that it's not possible. To Cecilia's question,
    you're dealing with, there's a great
  name: To Cecilia
  position: 24315
- category: unknown
  confidence: medium
  context: ion, you're dealing with, there's a great book by Daniel Kahneman called
    "Thinking, Fast and Slow," which kind of i
  name: Daniel Kahneman
  position: 24383
- category: tech
  confidence: high
  context: ys liken it to the idea, you know, people have an Apple Watch, and then
    people have a manual watch that s
  name: Apple
  position: 24827
- category: unknown
  confidence: medium
  context: ys liken it to the idea, you know, people have an Apple Watch, and then
    people have a manual watch that still h
  name: Apple Watch
  position: 24827
- category: ai_application
  confidence: high
  context: Mentioned as a widely used large language model (LLM) whose outputs users
    blindly copy and paste, potentially leading to bias issues.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a large language model (LLM) whose outputs users blindly copy
    and paste, potentially leading to bias issues. Later mentioned specifically as
    a Google product (Google Gemini).
  name: Gemini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a large language model (LLM) whose outputs users blindly copy
    and paste, potentially leading to bias issues.
  name: Copilot
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as a large language model (LLM) whose outputs users blindly copy
    and paste, potentially leading to bias issues.
  name: Quad
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The company of the guest, Itzall Shulman. They built a platform to attack
    and mitigate cognitive bias in communication, customer discovery questions, and
    agent conversations.
  name: Cobias AI
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in the context of its historical search engine dominance ('it
    must be true') and later as the supporter of the podcast and the developer of
    Gemini.
  name: Google
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as a major developer of LLMs, implying their models (like ChatGPT)
    are not infallible and can carry biases.
  name: OpenAI
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned alongside OpenAI and Google as a major developer of AI systems
    whose products are sometimes blindly trusted.
  name: Microsoft
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Specifically mentioned as the product line featuring V3, a state-of-the-art
    AI video generation model.
  name: Google Gemini
  source: llm_enhanced
- category: media/organization
  confidence: high
  context: The name of the podcast/show itself, which focuses on simplifying and leveraging
    AI.
  name: Everyday AI Show
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Mentioned in reference to a recent event regarding their training data
    size (5.6 million tokens/data points mentioned, though context suggests this was
    a specific, perhaps exaggerated, figure discussed).
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned alongside ChatGPT as a major LLM whose cognitive bias consistency
    was discussed.
  name: Claude
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a company that partners with the podcast host's organization
    for generative AI education and strategy.
  name: Adobe
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a company that partners with the podcast host's organization
    for generative AI education and strategy.
  name: Nvidia
  source: llm_enhanced
- category: research_institution
  confidence: medium
  context: Mentioned in relation to his book 'Thinking, Fast and Slow,' which informs
    the discussion on System 1 vs. System 2 thinking in AI.
  name: Daniel Kahneman
  source: llm_enhanced
- category: other
  confidence: low
  context: A user on YouTube asking a final question, not an AI company.
  name: Big Bogey
  source: llm_enhanced
date: 2025-06-13 13:00:00 +0000
duration: 32
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: AI, right? Like everything going, you know, to agentic or multi-agent
    environments, but obviously now we have these models that think, these models
    that reason, these models that take their time. So good question here from Cecilia
    asking, what do you do to detect cognitive bias and encourage maybe slow thinking
    versus the fast thinking when we want AI to be fast? Yeah, so I'll even add on
    to her question. Do reasoning models that take their time to think,
  text: the future of AI, right? Like everything going, you know, to agentic or multi-agent
    environments, but obviously now we have these models that think, these models
    that reason, these models that take their time. So good question here from Cecilia
    asking, what do you do to detect cognitive bias and encourage maybe slow thinking
    versus the fast thinking when we want AI to be fast? Yeah, so I'll even add on
    to her question. Do reasoning models that take their time to think, is that also
    a process that maybe we're going to see less bias? We may put logic early, but
    again, it still falls back under the developers, right? Fortunately, it's like
    a wheel.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/www.buzzsprout.com/2175779/episodes/17329468-ep-546-ai-bias-exposed-real-world-strategies-to-keep-llms-honest.mp3
processing_date: 2025-10-05 10:09:05 +0000
quotes:
- length: 278
  relevance_score: 6
  text: But I want to ask you, how does the conversation and the context of a large
    language model, when we're prompting it, whether it's Copilot or ChatGPT or whatever,
    how is that going to influence it, like actually how we're prompting it and what
    the outputs we get in terms of bias
  topics: []
- length: 220
  relevance_score: 5
  text: And that's the biggest thing to consider like during the training because
    we can say it was a training, you know, they do spend months and months essentially
    training models, but it's not like they're hand-labeling stuff
  topics: []
- length: 195
  relevance_score: 5
  text: So whether you're looking for ChatGPT training for thousands or just need
    help building your front-end AI strategy, you can partner with us just like some
    of the biggest companies in the world do
  topics: []
- length: 101
  relevance_score: 4
  text: Just blindly trusting what a large language model like ChatGPT or Gemini or
    Copilot or Quad spits out
  topics: []
- length: 135
  relevance_score: 4
  text: Maybe your company has been tinkering with large language models for a year
    or more, but can't really get traction to find ROI on GenAI
  topics: []
- length: 157
  relevance_score: 4
  text: Companies like Adobe, Microsoft, and Nvidia have partnered with us because
    they trust our expertise in educating the masses around generative AI to get ahead
  topics: []
- length: 92
  relevance_score: 3
  text: One of the biggest reasons, and I think a reason that sometimes AI fails,
    is because of bias
  topics: []
- length: 153
  relevance_score: 3
  text: So if you have that difficult email to write, say, you have to break some
    bad news or an angry mail, you know how to say, wait 24 hours to write an email
  topics: []
- length: 168
  relevance_score: 3
  text: And I think the issue is a lot of time people think, well, you know, obviously
    look, it's made by OpenAI, it's made by Microsoft, it's made by Google, it has
    to be good
  topics: []
- length: 180
  relevance_score: 3
  text: But the problem is, as the more things get sophisticated, the more complex
    they'll get from a perspective of how do we ask a question that it's perceived
    by the AI in the right way
  topics: []
- length: 90
  relevance_score: 3
  text: The problem is the sheer amount of data is just not possible right now to
    catch everything
  topics: []
- impact_reason: Directly links LLM failure to societal bias reflected in training
    data, a core concern in AI ethics and reliability.
  relevance_score: 10
  source: llm_enhanced
  text: One of the biggest reasons, and I think a reason that sometimes AI fails,
    is because of bias. Essentially, large language models are a reflection of the
    internet. They're a reflection of society, and there are a lot of things wrong,
    and sometimes these models aren't the absolute truth. Sometimes they're very flawed.
  topic: safety/ethics
- impact_reason: Provides a blunt, memorable re-framing of 'hallucination' as fabrication
    driven by the imperative to answer, citing external validation ('ChatGPT is BS'
    paper).
  relevance_score: 10
  source: llm_enhanced
  text: people are calling hallucinations, it's really more of a form of BS. There's
    actually a very popular paper called 'ChatGPT is BS,' and it was written simply
    from the perspective that, you know, AI is really more like that friend you all
    have one in your circle. They tell you all these incredible things, and most people
    like, well, they know everything, they expect to accept the respect. But what
    the AI has to do is they have to answer your question. So even if they can't find
    the answers, they'll make the answer up.
  topic: technical/safety
- impact_reason: Provides a concrete, high-stakes example (legal precedent fabrication)
    of LLM failure due to fabrication, underscoring the danger.
  relevance_score: 10
  source: llm_enhanced
  text: Most recently, a big role of firms in America had a scandal where they had
    these being used to do casework with, actually created precedent cases on their
    own. So it created a whole universe of the cases that never existed.
  topic: safety/predictions
- impact_reason: A critical statement on the current state of LLM 'understanding,'
    emphasizing that their comprehension is superficial, not deep semantic understanding.
  relevance_score: 10
  source: llm_enhanced
  text: The one thing that AI claims to do that it actually doesn't do is it doesn't
    really understand well. It understands in a very initially in a very bare-bones
    way.
  topic: limitations
- impact_reason: A powerful, forward-looking quote suggesting that despite current
    advancements, the technology is still nascent, implying massive future improvements.
  relevance_score: 10
  source: llm_enhanced
  text: AI is the worst that's ever going to be. And that's a true statement right
    now where they're very beginning at the very earliest stages.
  topic: predictions
- impact_reason: This is a crucial insight into why AI hallucinations are so dangerous—they
    are often plausible fictions built on factual kernels.
  relevance_score: 10
  source: llm_enhanced
  text: In every hallucination or BS point that AI makes, there are always elements
    of the truth in it, which makes it so convincing.
  topic: limitations
- impact_reason: A provocative statement suggesting that bias is inherent to all models
    and that the framing of the input (question) dictates the perceived 'harm' or
    direction of that bias.
  relevance_score: 10
  source: llm_enhanced
  text: Another good question here from Douglas asking, are there some models that
    have more inherent bias than others because of how they were trained? Honestly,
    that's very subjective. Unfortunately, all of them have bias, and there's no such
    thing as harmful bias. It's simply their perception of certain information points
    to how you ask questions, right?
  topic: safety/ethics
- impact_reason: A direct hypothesis linking model size (LLMs) to increased inherent
    bias due to greater data volume and more complex development pipelines.
  relevance_score: 10
  source: llm_enhanced
  text: for instance, you know, I think if you look at size of model, large language
    models have a lot more inherent biases than smaller models, and that's just by
    the sheer amount of data they consume and also the sheer amount of hands that
    touch the model.
  topic: technical/safety
- impact_reason: 'Raises a critical concern about the rapid release cycle of foundation
    models: whether updates prioritize genuine fixes/improvements or merely feature
    bloat.'
  relevance_score: 10
  source: llm_enhanced
  text: But the question I have, and I think everybody has, is with such a fast evolution
    cycle, we're talking sometimes 30 days between new releases, not the fast ones,
    obviously a lot of that stuff is maybe quick fixes, but you always have to wonder
    what's going on in the background. Are they actually fixing the main issues, or
    are they just adding to them by creating more features?
  topic: predictions/strategy
- impact_reason: 'Poses the central, unsolved dilemma in AI ethics: how to clean data
    when the bias is inextricably linked to the necessary information.'
  relevance_score: 10
  source: llm_enhanced
  text: How do you remove unwanted bias when that bias may be deeply entangled with
    essential data?
  topic: safety/ethics
- impact_reason: Highlights the common, dangerous practice of uncritical reliance
    on LLM outputs, setting the stage for the entire discussion on bias and verification.
  relevance_score: 9
  source: llm_enhanced
  text: Do you ever just blindly copy and paste what a large language model gives
    you? Right? I get it. We're all overworked, we're stressed. But that can actually
    be very dangerous.
  topic: safety/strategy
- impact_reason: Points to a future application of bias detection specifically targeting
    autonomous AI agents, a growing area of concern.
  relevance_score: 9
  source: llm_enhanced
  text: We're actually able to audit agent conversations and detect biases that they
    have and make reports for companies to make the changes necessary to make them
    better.
  topic: predictions/safety
- impact_reason: 'Reiterates the central thesis: inherent trust in LLMs is fundamentally
    flawed, especially for non-binary tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: people are just blindly either copy and paste what comes out of a large language
    model, or they just inherently trust it as being accurate and factual and, you
    know, bias-free. Why is that a mistake?
  topic: safety/strategy
- impact_reason: Draws a powerful analogy between misplaced trust in Google search
    results and misplaced trust in LLMs, framing it as a recurring human error in
    technology adoption.
  relevance_score: 9
  source: llm_enhanced
  text: AI is like people. It was built by people... Before we had the whole AI explosion
    and people went on Google, well, it's on Google, it must be true. Was a very constant
    refrain that people gave, and that's just absolutely not correct.
  topic: strategy/safety
- impact_reason: 'Offers the core actionable advice for AI adoption: ''Trust but verify.'''
  relevance_score: 9
  source: llm_enhanced
  text: What we end up with is, you know, people are so enthusiastic about this new
    leap in innovation that they forget that just like with anything else, you should
    have an attitude of trust but verify.
  topic: strategy
- impact_reason: 'Explains the root cause of bias: the human engineers and their inherent
    perspectives embedded in the model''s structure and data curation.'
  relevance_score: 9
  source: llm_enhanced
  text: Because they're built by engineers, they have the biases that those engineers
    have. So they have the same human extensions of our personalities. So the way
    they gather the information, the way they disseminate it, is reflective of humanity.
  topic: safety/ethics
- impact_reason: Explains the persistence of bias across generations of AI models—bias
    is inherited, not eliminated, through iteration.
  relevance_score: 9
  source: llm_enhanced
  text: The first thing is it happens because humans are the ones who make it. So
    even if AI makes another AI, it's based on the original programming of the human.
    So you're just going to have a new permutation of the same biases or an evolution
    of some new biases based on the old biases.
  topic: safety/technical
- impact_reason: Suggests that initial data selection (the 'first 500 sources') by
    engineers introduces subjective weighting/bias before the model even generates
    an answer.
  relevance_score: 9
  source: llm_enhanced
  text: When you're pulling from new sources or media sources, these are the first
    500 you're going to look at before you look at anything else to solve the problem.
    Is it because of their personal beliefs? Is it because of their perception of
    what's reputable versus what's not as a new source?
  topic: technical/ethics
- impact_reason: Highlights the mismatch between the binary nature of computation
    and the nuanced, strategic questions users often pose to LLMs.
  relevance_score: 9
  source: llm_enhanced
  text: Yeah, that's the worst thing you can do with a large language model, right?
    Is this like, oh, it's a computer, it has to be right. But so many of the things
    that we ask large language models, there's nuance, right? It's not binary.
  topic: strategy/safety
- impact_reason: 'Explains the cyclical nature of confirmation bias: the AI confirms
    the user''s premise, which reinforces the user''s belief, creating a feedback
    loop.'
  relevance_score: 9
  source: llm_enhanced
  text: Confirmation bias is a big one. It's confirming its own initial beliefs...
    AI is going to respond to us in a specific way and bias us in that way. So in
    some cases, we'll ask it a certain question, it'll respond back, and it'll trigger
    our confirmation bias because it's going to be confirming our facts.
  topic: safety/conceptual
- impact_reason: Provides a clear example of how user input framing dictates the LLM's
    biased output, emphasizing the model's lack of argumentative pushback.
  relevance_score: 9
  source: llm_enhanced
  text: 'Framing bias: we frame something in a specific way to get a specific answer
    back. So if we say, I''m just making it up, Mercedes-Benz is the fastest car in
    the West, car for the money based on the luxury blah blah blah, and then we''re
    going to ask questions about it. Now the AI is going to be responding back in
    the same way, just like humans would, because again, the AI is not here to argue
    with you.'
  topic: technical/conceptual
- impact_reason: Highlights two major cognitive biases (confirmation and framing)
    that directly influence LLM outputs based on user input, crucial for prompt engineering
    and understanding output reliability.
  relevance_score: 9
  source: llm_enhanced
  text: 'Confirmation is a big one. Framing bias: we frame something in a specific
    way to get a specific answer back.'
  topic: safety/ethics
- impact_reason: Explains the fundamental role of the system prompt and the inherent
    'helpfulness' directive, which often leads to plausible but potentially incorrect
    or 'halfway' answers.
  relevance_score: 9
  source: llm_enhanced
  text: All large language models have system prompts. And one thing you said there
    is most of them, they are designed to be a helpful assistant, right? So even if
    there's not an answer, they kind of want to be helpful.
  topic: technical
- impact_reason: Provides an excellent analogy for managing user expectations regarding
    current AI capabilities versus hype.
  relevance_score: 9
  source: llm_enhanced
  text: People have expectations of a flying ship when we're probably somewhere closer
    to a horse-drawn carriage by AI standards.
  topic: strategy
- impact_reason: 'Clearly articulates the source of LLM bias: the unfiltered data
    reflecting human society.'
  relevance_score: 9
  source: llm_enhanced
  text: Large language models are a reflection of the internet, and that's a reflection
    of humanity, and that's why there are sometimes stereotypes and biases to begin
    with.
  topic: safety/ethics
- impact_reason: Starkly illustrates the massive scale and inherent danger/noise in
    training data, explaining why perfect filtering is impossible.
  relevance_score: 9
  source: llm_enhanced
  text: It's very much the equivalent of drinking from a fire hose. I can't even imagine
    the sheer amount of pedophilia and God knows what other measurements we could
    apply of information that are constantly flowing through that it has to pass through
    to do to do linearly, whether or not the one specific minuscule thing that we're
    asking has the answer for it, and it still returns it in seconds.
  topic: safety/ethics
- impact_reason: Explains the shift from human labeling in smaller models to self-labeling
    (or automated labeling) in massive models, which is a key source of systemic bias.
  relevance_score: 9
  source: llm_enhanced
  text: In the case of a massive model with massive, massive amounts of data, it labels
    it itself. So it's only taught to label things.
  topic: technical
- impact_reason: A critique of recent AI development trends, suggesting that many
    'breakthroughs' are optimizations (speed, cost, conversation) rather than fundamental
    improvements in knowledge depth.
  relevance_score: 9
  source: llm_enhanced
  text: The consistent theme is the same. They're not actually creating better depth.
    They're preparing better response time, maybe lower latency, cheaper. They're
    sometimes making a better conversational piece to it, but the info being put out
    is still very much the same.
  topic: strategy
- impact_reason: Provides a quantitative metric (30-40% consistency) comparing LLM
    bias handling against established scientific models, highlighting a significant
    gap.
  relevance_score: 9
  source: llm_enhanced
  text: When we actually look at the idea of how it measures cognitive biases versus
    the scientific models we have, the consistency of ChatGPT and Claude and a couple
    of the others were around 30 to 40% versus what we do.
  topic: safety/ethics
- impact_reason: Draws a parallel between human System 1/System 2 thinking (Kahneman)
    and the potential need for different types of AI models—some fast/intuitive, others
    analytical/focused.
  relevance_score: 9
  source: llm_enhanced
  text: There's a great book by Daniel Kahneman called "Thinking, Fast and Slow,"
    which kind of is, I don't want to say he's the father of modern cognitive bias,
    behavioral, I can't be sure, but he is, right? And so the best way to find a considerate,
    there will be a space for these models that are more analytical and focused on
    things. It's the same thing as we have other things that do certain more complex
    things.
  topic: strategy
- impact_reason: Shifts the focus of bias from the model to the user's intent—how
    the framing of prompts influences audience perception, crucial for marketing and
    research ethics.
  relevance_score: 9
  source: llm_enhanced
  text: Sometimes they're fine with the questions, they just want to understand, how
    am I going to be biasing my audience because no matter what you ask, you always
    bias them some way. But the question becomes, am I biasing them just to answer
    the question truthfully if I'm doing marketing research, or if I'm doing sales
    and I want to prompt them to action, am I doing that effectively?
  topic: safety/ethics
- impact_reason: 'Offers a concrete mitigation strategy for bias: ensuring diverse
    human teams are involved in labeling and auditing model perception.'
  relevance_score: 9
  source: llm_enhanced
  text: At the same point, narrow AIs totally have a bias in them, and the key element
    that we try to do, I'm always been a big proponent of having a very diverse team
    look at the model and label it and look at everything else on how it perceives
    data.
  topic: safety/ethics
- impact_reason: Describes a method to hard-code responses based on established scientific
    definitions rather than subjective human perception, aiming for a higher standard
    of truth.
  relevance_score: 9
  source: llm_enhanced
  text: We can give the model the nod of these ill-scientific facts, assign those
    cognitive biases. That's how you're going to react to this based on the scientific
    definition, not our perceptions, but what science defines.
  topic: technical
- impact_reason: Introduces a specific commercial solution aimed at solving the abstract
    problem of cognitive bias in AI-assisted communication.
  relevance_score: 8
  source: llm_enhanced
  text: We built a platform that attacks and mitigates cognitive bias in communication.
  topic: business/technical
- impact_reason: 'Identifies the source of misplaced trust: brand authority (OpenAI,
    Google) overriding critical assessment.'
  relevance_score: 8
  source: llm_enhanced
  text: It was always a trust but verify attitude. And I think the issue is a lot
    of time people think, well, you know, obviously look, it's made by OpenAI, it's
    made by Microsoft, it's made by Google, it has to be good. But they're just like
    they're not infallible. They can make the same mistakes.
  topic: strategy/safety
- impact_reason: 'Crucial clarification: biases are inherent to human cognition, not
    necessarily malicious, which helps frame why they appear in AI.'
  relevance_score: 8
  source: llm_enhanced
  text: The key point of cognitive biases, they're not bad. They're just part of our
    humanity.
  topic: safety/ethics
- impact_reason: Clearly defines the Availability Heuristic in practical terms, linking
    it to how LLMs might prioritize initial search results or training data.
  relevance_score: 8
  source: llm_enhanced
  text: Sometimes availability heuristic is reaching for the first thing that is closest
    available to us to solve the problem that we have.
  topic: technical/conceptual
- impact_reason: Illustrates the danger of accepting LLM output (like survey questions)
    without critical analysis, especially when the output seems superficially 'good.'
  relevance_score: 8
  source: llm_enhanced
  text: Here's the questions, and you're like, well, they sound good to me. They're
    perfect. There's no breakdown, there's no analysis, there's no belief. So I want
    to break down two key words I heard you say there. You said irrational beliefs
    based on perceptions.
  topic: strategy/safety
- impact_reason: Sets a realistic expectation about current LLM behavior, contrasting
    popular media portrayals with the reality that models are designed to agree or
    fulfill the prompt rather than debate.
  relevance_score: 8
  source: llm_enhanced
  text: AI is not here to argue with you. I know we see those comical stories where
    AI starts arguing facts with you, but that's not really the reality of how it
    operates.
  topic: limitations
- impact_reason: Illustrates the ambiguity and subjectivity inherent in persona-based
    prompting, showing how vague instructions introduce uncontrolled bias into the
    output.
  relevance_score: 8
  source: llm_enhanced
  text: What is truly a 20-year engineer? How do you write nicer? What is nicer? You
    know, sheer definition elements and how the biases are perceived from then becomes
    a hot mess.
  topic: technical
- impact_reason: Reframes prompting not as a tool for improving model quality, but
    often as a tool for satisfying the user's immediate desire or bias.
  relevance_score: 8
  source: llm_enhanced
  text: Our prompting doesn't necessarily help it be better at its job. Our prompting
    just helps, again, confirmation bias helps us confirm that it helps to confirm
    that we want something nicer.
  topic: technical
- impact_reason: A prediction favoring the near-term success and proliferation of
    specialized (narrow) AI applications over immediate AGI.
  relevance_score: 8
  source: llm_enhanced
  text: The explosion of narrow AI that are going to be good in specific elements,
    and that's going to be their main motif.
  topic: predictions
- impact_reason: Poses a key question linking 'slow thinking' (System 2 processing)
    in AI to potential bias reduction.
  relevance_score: 8
  source: llm_enhanced
  text: Do reasoning models that take their time to think, is that also a process
    that maybe we're going to see less bias?
  topic: safety/ethics
- impact_reason: 'Philosophical take on bias: it''s an unavoidable byproduct of human
    involvement in the creation process.'
  relevance_score: 8
  source: llm_enhanced
  text: As soon as you insert humanity into the wheel, we have our biases. Always
    do that. That's not a bad thing, like I said, it's just unfortunately how we perceive
    information and other things will dictate some of the more harmful biases that
    pop up, such as stereotypes and stuff.
  topic: safety/ethics
- impact_reason: Suggests a future where specialized, analytical models (System 2
    thinkers) will coexist with general models, similar to specialized tools.
  relevance_score: 8
  source: llm_enhanced
  text: The best way to find a considerate, there will be a space for these models
    that are more analytical and focused on things. It's the same thing as we have
    other things that do certain more complex things.
  topic: predictions
- impact_reason: Highlights the inherent limitation and complexity in fully auditing
    or controlling massive AI models due to the sheer scale of data and variance.
  relevance_score: 8
  source: llm_enhanced
  text: it's just such a vast amount of information and such a vast amount of variance
    that it's not possible.
  topic: limitations
- impact_reason: A strong analogy illustrating the trade-off between modern convenience
    (AI/smart devices) and perceived reliability/simplicity (traditional methods),
    relevant for AI adoption strategy.
  relevance_score: 8
  source: llm_enhanced
  text: I always liken it to the idea, you know, people have an Apple Watch, and then
    people have a manual watch that still has gears in it, and they prefer that element
    because they feel it's more reliable because it doesn't run out of batteries.
    It just moves because you move.
  topic: strategy
- impact_reason: Reinforces the necessity of diversity in development/auditing teams
    to approach objectivity in data interpretation.
  relevance_score: 8
  source: llm_enhanced
  text: if you have a large variety of people look through it, you're not going to
    have the perspective on how to parse data from one or two individuals or 10 individuals.
    You're going to be able to do these things in a much more, you know, almost objective
    way, if that's the proper word.
  topic: strategy
- impact_reason: Highlights the business risk and wasted engineering effort when rapid
    model iteration invalidates previous workarounds built to address known model
    flaws.
  relevance_score: 8
  source: llm_enhanced
  text: sometimes you spend time circumventing some of those shortcomings, and then
    a new model comes out, and it's like, okay, what about all that work that we put
    in to build bridges around, over, or through some of these inherent problems?
  topic: business
- impact_reason: Suggests that when bias impacts critical business decisions, the
    solution lies in evaluating the *value* of the data/output, often necessitating
    external, unbiased review.
  relevance_score: 8
  source: llm_enhanced
  text: Well, the element has to be the value of the data and the value of the output
    of the data, right? Quite often, if you're looking at specific data where the
    bias may, for instance, affect the key decision the company is making, getting
    outside help will be essential...
  topic: business
- impact_reason: Provides a concise, accessible definition of cognitive bias for a
    general audience.
  relevance_score: 7
  source: llm_enhanced
  text: Cognitive bias... is honestly irrational beliefs based on our perception.
    That's the best way to kind of extremely simplistic mind you.
  topic: technical/conceptual
- impact_reason: Reinforces the concept that LLMs default to the most easily accessible
    (and potentially shallowest) information to satisfy the prompt.
  relevance_score: 7
  source: llm_enhanced
  text: Availability heuristic... is the lowest hanging fruit, basic way of saying
    that people reach for the first thing that's available to them that may seem like
    it solves the problem.
  topic: technical/conceptual
- impact_reason: Offers a simplified but useful mental model (if-then logic) for understanding
    how LLMs process information flow, despite the underlying complexity.
  relevance_score: 7
  source: llm_enhanced
  text: It's more or less if-then rules. A lot of those are applied, and I'm simplifying
    way too much. That's not really how it works, but for the basis of understanding,
    it's really how it kind of perceives information.
  topic: technical
- impact_reason: A provocative statement suggesting that bias is inherent to all models,
    though the speaker immediately qualifies that the *impact* of the bias is what
    matters (harmful vs. not harmful).
  relevance_score: 7
  source: llm_enhanced
  text: Honestly, that's very subjective. Unfortunately, all of them have bias, and
    there's no such thing as harmful bias.
  topic: safety/ethics
- impact_reason: Reveals a common real-world workflow where users leverage general
    LLMs (like ChatGPT) for initial ideation/question generation before refining or
    validating with specialized tools.
  relevance_score: 7
  source: llm_enhanced
  text: Quite often what we do is what we noticed with a lot of our users is they
    actually will generate stuff on like ChatGPT, dovetail name, name, name, name
    assistant that generates questions, and then they'll run them through our system,
    and then that will be the result that they use for their actual product.
  topic: business
- impact_reason: Provides a specific example of scaling training/testing data through
    structural extrapolation, a key technique in specialized AI development.
  relevance_score: 7
  source: llm_enhanced
  text: we started with 17,000 questions, and we look to their sentence structure,
    and from them, we're able to extrapolate to 450,000 questions, somewhere in that
    neighborhood, probably even more.
  topic: technical
- impact_reason: 'A philosophical anchor point: AI alignment should strive toward
    the current societal consensus of ''best thinking'' (science), acknowledging that
    this standard is dynamic.'
  relevance_score: 7
  source: llm_enhanced
  text: science is based on the best thinking we have in existence for our society.
    This is what it is, and as it evolves, we evolve.
  topic: strategy
source: Unknown Source
summary: '## Podcast Episode Summary: EP 546: AI Bias Exposed: Real-World Strategies
  to Keep LLMs Honest


  This episode of the Everyday AI Show, featuring **Itzall Shulman, CEO and co-founder
  of Cobias AI**, dives deep into the pervasive and dangerous issue of **cognitive
  bias embedded within Large Language Models (LLMs)**. The central narrative stresses
  that blindly trusting LLM outputs (like those from ChatGPT or Gemini) is a significant
  professional risk because these models are direct reflections of the flawed, biased
  data found on the internet and within their human creators.


  The discussion moves from the general danger of over-reliance to defining cognitive
  bias, explaining how it manifests in AI, and outlining practical mitigation strategies.


  ### 1. Focus Area

  The primary focus is **AI Bias and Mitigation in Communication and LLM Outputs**.
  Specific technologies discussed include Large Language Models (LLMs), AI Agents,
  and the application of cognitive science principles (like Daniel Kahneman''s work)
  to AI auditing.


  ### 2. Key Technical Insights

  *   **LLMs as BS Generators:** LLMs often prioritize providing an answer, even if
  they lack factual grounding, leading to "hallucinations" which are essentially sophisticated
  forms of BS, often containing elements of truth to make them convincing (as seen
  in the legal precedent scandal).

  *   **Bias in Training Data and Labeling:** Cognitive biases are inserted during
  the initial programming (system prompts defining structure) and, crucially, during
  the massive self-labeling process of training data. Because models rely on "if-then"
  rules and limited neural depth compared to humans, they cannot fully capture nuanced
  scientific models of bias, leading to inconsistencies (Cobias AI found LLMs only
  align with scientific bias models about 30-40% of the time).

  *   **Prompting Reinforces Bias:** User prompting, especially tone-setting (e.g.,
  "write it nicer" or "act like an expert"), doesn''t solve underlying bias; it merely
  wraps the biased output in a tone that confirms the user''s immediate perception
  or request, often masking the core problem.


  ### 3. Business/Investment Angle

  *   **Risk Management in AI Adoption:** The primary business risk highlighted is
  the failure to "trust but verify," leading to potentially damaging outputs in critical
  areas like customer discovery, marketing research, and legal casework.

  *   **Emerging Audit Market:** There is a clear commercial opportunity in tools
  and services dedicated to auditing AI communications and agent conversations for
  bias, as companies need external verification beyond the developer''s internal checks.

  *   **Focus on Narrow AI ROI:** The conversation suggests that while general AI
  is advancing, the immediate ROI is likely to come from specialized, narrow AI applications
  that excel in specific, auditable tasks, rather than relying on general models for
  complex strategy.


  ### 4. Notable Companies/People

  *   **Itzall Shulman (Cobias AI):** Guest expert, CEO and co-founder, whose company
  focuses on auditing and mitigating cognitive bias in communication (surveys, emails,
  agent conversations).

  *   **Daniel Kahneman:** Mentioned for his foundational work on cognitive bias,
  particularly the concepts explored in *Thinking, Fast and Slow*, which provides
  a framework for understanding AI''s limitations.

  *   **Major LLM Providers (OpenAI, Google, Microsoft):** Referenced as sources of
  models that are not infallible and carry inherent developer biases.


  ### 5. Future Implications

  The industry is moving toward more sophisticated, potentially analytical models
  (like reasoning models), but the fundamental problem of human-derived bias will
  persist until training methodologies fundamentally change. The future requires a
  shift from expecting perfect AI to implementing robust **verification and auditing
  layers** (like agent conversation auditing) to ensure outputs align with organizational
  objectives and ethical standards.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Professionals, Product Managers, Marketing
  Researchers, Sales Leaders, and Executives** who are integrating LLMs into core
  business processes and need actionable strategies to ensure output quality and mitigate
  reputational or operational risk associated with AI-generated content.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- startup
- google
- openai
- microsoft
- nvidia
title: 'EP 546: AI Bias Exposed: Real-World Strategies to Keep LLMs Honest'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 140
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 21
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 10
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 10:09:05 UTC -->
