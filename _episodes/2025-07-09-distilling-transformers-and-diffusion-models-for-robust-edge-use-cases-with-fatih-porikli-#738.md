---
companies:
- category: unknown
  confidence: medium
  context: inued support and sponsorship of today's episode. Qualcomm AI research
    is dedicated to advancing AI to make its
  name: Qualcomm AI
  position: 106
- category: unknown
  confidence: medium
  context: right everyone, welcome to another episode of the Twomol AI podcast. I
    am, of course, your host Sam Charringt
  name: Twomol AI
  position: 895
- category: unknown
  confidence: medium
  context: the Twomol AI podcast. I am, of course, your host Sam Charrington. Today
    I'm joined by Fatih Pratikli. Fatih is Sen
  name: Sam Charrington
  position: 941
- category: unknown
  confidence: medium
  context: cast. I am, of course, your host Sam Charrington. Today I'm joined by Fatih
    Pratikli. Fatih is Senior Direc
  name: Today I
  position: 958
- category: unknown
  confidence: medium
  context: e, your host Sam Charrington. Today I'm joined by Fatih Pratikli. Fatih
    is Senior Director of Technology at Qualco
  name: Fatih Pratikli
  position: 978
- category: unknown
  confidence: medium
  context: ton. Today I'm joined by Fatih Pratikli. Fatih is Senior Director of Technology
    at Qualcomm. Before we get going, b
  name: Senior Director
  position: 1003
- category: unknown
  confidence: medium
  context: my conversations with you and your colleagues at Qualcomm AI Research is
    the idea of efficiency and distillation. That'
  name: Qualcomm AI Research
  position: 1955
- category: unknown
  confidence: medium
  context: the DIMA paper. The full title of that paper is "Distilling Multi-Modal
    Large Language Models for Autonomous Drivin
  name: Distilling Multi
  position: 2136
- category: unknown
  confidence: medium
  context: The full title of that paper is "Distilling Multi-Modal Large Language
    Models for Autonomous Driving." Maybe let's start with y
  name: Modal Large Language Models
  position: 2153
- category: unknown
  confidence: medium
  context: '"Distilling Multi-Modal Large Language Models for Autonomous Driving."
    Maybe let''s start with your broad take on the s'
  name: Autonomous Driving
  position: 2185
- category: unknown
  confidence: medium
  context: f interpretability, and the KPIs are much better. So DIMA is an end-to-end
    solution. It establishes the new
  name: So DIMA
  position: 4383
- category: unknown
  confidence: medium
  context: ith the previous state of the art, including VAD, Vectorized Autonomous
    Driving, which is an amazing paper also. And this DIMA ca
  name: Vectorized Autonomous Driving
  position: 7533
- category: unknown
  confidence: medium
  context: 'odel, right?


    Yeah, use the model to distill it.


    When I think about the idea of using LLMs in an autonomo'
  name: When I
  position: 8973
- category: unknown
  confidence: medium
  context: rs, the token rates are, you know, not that high. Maybe I can say, depending
    on the model size of the model
  name: Maybe I
  position: 9580
- category: unknown
  confidence: medium
  context: 'ng to performance, this distillation constraint.


    And I should note that there''s a really good image or F'
  name: And I
  position: 16580
- category: unknown
  confidence: medium
  context: where the other things, agents, are going to be. So I mean this in training
    time how the surrounding ag
  name: So I
  position: 20576
- category: unknown
  confidence: medium
  context: 'what''s the role of incorporating a VQA component, Visual Question Answering?


    Yeah, we want this capability to the—this is an'
  name: Visual Question Answering
  position: 21865
- category: unknown
  confidence: medium
  context: 'ed—SharpDepth, and the full title is "SharpDepth: Sharpening Metric Depth
    Predictions Using Diffusion Distillation." So distillation again a theme here.
    My impressi'
  name: Sharpening Metric Depth Predictions Using Diffusion Distillation
  position: 25905
- category: unknown
  confidence: medium
  context: 'der why we want to do monocular depth estimation. Should I talk about
    this?


    Yeah, I''m imagining something l'
  name: Should I
  position: 29710
- category: unknown
  confidence: medium
  context: 'tion: how am I going to know at each one is each? Because I don''t know
    this thing. You know, one is blurry, t'
  name: Because I
  position: 33162
- category: unknown
  confidence: medium
  context: of diffusion models beyond kind of the, you know, Stable Diffusion image,
    you know, text-to-image, and they all seem
  name: Stable Diffusion
  position: 35343
- category: ai_infrastructure
  confidence: high
  context: Sponsor of the podcast; Fatih Pratikli is Senior Director of Technology
    there. Their AI research focuses on making core AI capabilities (perception, reasoning,
    action) ubiquitous across devices.
  name: Qualcomm
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The dedicated research division within Qualcomm focused on advancing AI
    capabilities for devices.
  name: Qualcomm AI research
  source: llm_enhanced
- category: media_platform
  confidence: high
  context: The podcast hosting the discussion (implied organization/platform related
    to AI content).
  name: Twomol AI podcast
  source: llm_enhanced
- category: ai_research_venue
  confidence: high
  context: The conference where Qualcomm presented their papers (Computer Vision and
    Pattern Recognition Conference), a major venue for AI/ML research.
  name: CVPR
  source: llm_enhanced
- category: ai_model_or_paper
  confidence: high
  context: 'A specific paper/model discussed: ''Distilling Multi-Modal Large Language
    Models for Autonomous Driving.'''
  name: DIMA
  source: llm_enhanced
- category: ai_model_or_paper
  confidence: high
  context: 'A specific paper discussed: ''SharpDepth paper, which looks at diffusion
    distillation for computing absolute depth maps.'''
  name: SharpDepth
  source: llm_enhanced
- category: ai_model_or_paper
  confidence: high
  context: Mentioned as a previous state-of-the-art end-to-end autonomous driving
    solution that DIMA was compared against ('Vectorized Autonomous Driving').
  name: VAD
  source: llm_enhanced
- category: ai_model_or_paper
  confidence: medium
  context: Mentioned alongside VAD as a previous end-to-end solution in autonomous
    driving that DIMA aims to surpass.
  name: Unity
  source: llm_enhanced
date: 2025-07-09 15:53:00 +0000
duration: 60
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN4056797871.mp3?updated=1752077099
processing_date: 2025-10-05 03:24:45 +0000
quotes:
- length: 285
  relevance_score: 4
  text: Very quick listeners to the podcast will know that this idea of physics-based
    versus model-based or modular versus end-to-end is kind of a battle that's been
    raging in the machine learning community and these various communities like autonomous
    driving and robotics for quite some time
  topics: []
- length: 180
  relevance_score: 4
  text: You know, we can run this AI planner, a transformer-based model, still leveraging
    the language model, or the language model is tough, you know, as the, you know,
    overall AI planner
  topics: []
- length: 92
  relevance_score: 4
  text: Inferences for LLMs is very expensive, and presumably that's where the distillation
    comes in
  topics: []
- length: 188
  relevance_score: 4
  text: LLMs at this point, of course, we are talking about the systems that we can
    run on the vehicle map, like on a very expensive H-200 GPU, that would be more
    expensive than the vehicle itself
  topics: []
- length: 112
  relevance_score: 4
  text: But running everything end-to-end using an LLM would be much more computing-intensive
    than the transformer model
  topics: []
- length: 231
  relevance_score: 4
  text: Decided to transform models, you are saying that if you are interested in
    efficiency, here is the transformer model, still leveraging the language model,
    but there is also a language model version of it, a multi-modal version of it
  topics: []
- length: 209
  relevance_score: 4
  text: And so, is that world knowledge access via traditional prompting, or are we
    talking about doing something like taking the embeddings of an LLM and somehow
    directly accessing them to unlock this world knowledge
  topics: []
- length: 61
  relevance_score: 4
  text: But they are not all the tokens then we are training this LLM
  topics: []
- length: 133
  relevance_score: 4
  text: In training time, we learn better tokens, and now we take it and we learn
    an adapter, you know, kind of for this planning transformer
  topics: []
- length: 226
  relevance_score: 4
  text: So next time in the past, meaning so you can get the tokens that come out
    of the scene encoder and give them to the planning transformer directly and get
    a prediction as opposed to going through all of the machinery of the LLM
  topics: []
- length: 235
  relevance_score: 4
  text: Now, the scene encoder again generates the raw tokens, it goes through this
    adapter, and then it goes to the planning transformer, and that is it's not an
    LLM anymore in that version, but, you know, kind of the better task for planning
  topics: []
- length: 168
  relevance_score: 4
  text: Some of them are about also best token prediction, future VQA token prediction
    to encourage the LLM to learn spatial-temporal cues useful for planning and scene
    editing
  topics: []
- length: 109
  relevance_score: 4
  text: The transformer version doesn't answer the questions, but the LLM version
    in front of can answer the question
  topics: []
- length: 45
  relevance_score: 4
  text: So very fine-tuning going on in training time
  topics: []
- length: 119
  relevance_score: 4
  text: So kind of in training time, we learn a better diffusion model, and can we
    just use it, you know, in the inference time
  topics: []
- impact_reason: 'This is a core insight into *why* LLMs are valuable in perception/planning
    tasks like autonomous driving: their world knowledge acts as a powerful regularizer,
    reducing the need to explicitly train for every rare, long-tail scenario.'
  relevance_score: 10
  source: llm_enhanced
  text: The virtual knowledge representation capability of LLMs, it acts as a regularizer
    or conditioner to generalize the solution. So we don't need to really go every
    time for each long-tail specific, very rare scenario and try to learn them, model
    them. We want to harness the LLM's knowledge with the efficiency of this vision-based
    low-level perception stack.
  topic: technical/strategy
- impact_reason: Contradicts the common assumption that modular systems are superior
    for interpretability, claiming end-to-end systems offer better KPIs *and* better
    interpretability.
  relevance_score: 10
  source: llm_enhanced
  text: There is a big reason for this because end-to-end systems not only offer better,
    more robust solutions for a wide variety of long-tail scenarios, but also they
    are better in terms of interpretability, and the KPIs are much better.
  topic: predictions/strategy
- impact_reason: 'Explains the mechanism for end-to-end interpretability: the LLM
    component provides high-level, semantic explanations for planning decisions, similar
    to tracing thought processes.'
  relevance_score: 10
  source: llm_enhanced
  text: end-to-end systems and DIMA, in particular, divide the language model into
    the overall system, provides explanations. Look, I'm slowing down because there
    is congestion ahead, or we are approaching a zebra crossing. Things so, analogous
    to explaining thought traces in a reasoning model, the model can, to some degree,
    explain what it's doing.
  topic: safety/technical
- impact_reason: Provides a concrete, massive performance metric (80% collision rate
    reduction) for the DIMA approach compared to current state-of-the-art, signaling
    a major breakthrough.
  relevance_score: 10
  source: llm_enhanced
  text: It can reduce the collision rate—it's a very important KPI for autonomous
    driving systems—by 80% with respect to the 2024 or latest 2025 baseline, a total
    solution.
  topic: predictions/breakthroughs
- impact_reason: 'Describes the crucial efficiency gain via model distillation: creating
    a smaller, specialized ''planning transformer'' (student model) that bypasses
    the large LLM for real-time inference, while maintaining performance.'
  relevance_score: 10
  source: llm_enhanced
  text: So next time in the past, meaning so you can get the tokens that come out
    of the scene encoder and give them to the planning transformer directly and get
    a prediction as opposed to going through all of the machinery of the LLM? Exactly.
    So that version is the transformer version, more efficient. Now, the scene encoder
    again generates the raw tokens, it goes through this adapter, and then it goes
    to the planning transformer, and that is it's not an LLM anymore in that version,
    but, you know, kind of the better task for planning.
  topic: technical
- impact_reason: 'Explains the necessity of VQA (Visual Question Answering) in the
    training loop: to prevent the model from becoming purely optimized for low-level
    prediction tasks and to maintain its high-level semantic understanding (''world
    knowledge'').'
  relevance_score: 10
  source: llm_enhanced
  text: We want this capability to the—this is an important part of so it is world
    knowledge to be a part of the overall kind of solution. We do not want to just
    drive steer LLM towards surrogate tests, you know, the future prediction, or,
    you know, generating better tokens for the planner, but we want it to be grounded
    also, so that is keeping it's real connected to the semantic information.
  topic: safety/technical
- impact_reason: 'A critical benchmark for real-world deployment: confirming that
    the distilled, efficient model achieves sub-real-time latency on specialized edge
    hardware (Qualcomm accelerators).'
  relevance_score: 10
  source: llm_enhanced
  text: This model we can run as of now on Qualcomm accelerators. It's faster than
    real-time.
  topic: business
- impact_reason: Predicts a hybrid, redundant architecture where the fast, distilled
    transformer handles real-time control, while the slower, powerful LLM runs in
    the background (lower frequency) to catch long-tail, unexpected events, enhancing
    overall system safety.
  relevance_score: 10
  source: llm_enhanced
  text: I see one trend of running both transformer and language model together. And
    each model is, you know, kind of as we talked about, is slow, compute-intensive,
    so maybe not that fast at this moment. So it is on a longer horizon, you know,
    in a lower frame rate, it is estimating the missing on the, you know, estimating
    the points and, you know, some useful planning decisions at the current AI planner
    using transformers also running on 3D or 2D perceptions, and then these are combined.
    It is—there is redundancy in the system. If there is something unexpected for
    the, you know, world-level perception, at least we have high-level, you know,
    understanding that will provide some safety mechanism for the long tail.
  topic: predictions
- impact_reason: 'Identifies the most significant future application of LLMs in AVs:
    enabling dynamic, semantic customization of driving policy based on local regulations,
    culture, and user preference, moving beyond static tuning.'
  relevance_score: 10
  source: llm_enhanced
  text: 'But more important is this one: using a language model in the vehicle to
    interact with the vehicle decision mechanism, change its behavior, customize for
    a geospatial location, like for a city or for a country, because rules are different,
    and driving dynamics are different, and preferences are different. Maybe you want
    more aggressive driving or more conservative.'
  topic: business/predictions
- impact_reason: This is a major insight into the future of autonomous vehicle customization,
    suggesting LLMs will move beyond simple interaction to dynamically adapting vehicle
    behavior based on local regulations and user preferences.
  relevance_score: 10
  source: llm_enhanced
  text: 'More important is this one: using a language model in the vehicle to interact
    with the vehicle decision mechanism, change its behavior, customize for a geospatial
    location, like for a city or for a country, because rules are different, and driving
    dynamics are different, and preferences are different.'
  topic: Predictions/Business advice for AI companies
- impact_reason: 'Excellent explanation of the fundamental trade-off in generative
    depth models: high visual fidelity (sharpness) at the cost of absolute metric
    scale.'
  relevance_score: 10
  source: llm_enhanced
  text: Generative models... provide very sharp depth monocular depth estimation because
    there's a lot they can use synthetic data... but there is, you know, scale. It
    is point, you know, it is not like inches or millimeters or anything like that.
    You don't know what that point, each pixel, how big it is. It could be one meter
    or one millimeter, you know, very different. There are relative within the reconstructed
    image, but it's not absolute, so you can measure with it. Yeah, it is relative.
  topic: Technical insights/Limitations
- impact_reason: 'States the core contribution of the research: solving the sharpness
    vs. metric scale trade-off.'
  relevance_score: 10
  source: llm_enhanced
  text: This paper, SharpDepth, bridges these two approaches, integrating metric accuracy
    with detailed boundary preservation of the generative methods.
  topic: Breakthroughs/Technical
- impact_reason: 'Explains the core mechanism for fusing the two modalities: using
    an adaptive difference map to weigh the reliability of the coarse metric data
    against the fine-grained generative output.'
  relevance_score: 10
  source: llm_enhanced
  text: What we did, our intuition is we take these two estimations and then compare
    the head is adaptive subtraction. A skill of a subtraction which generates a difference
    map. So the intuition is in this—this is a difference map—the regions with minimal
    differences are more reliable in terms of their metric depth estimation, and while
    and other areas with larger kind of differences build require maybe updates.
  topic: Technical breakthroughs
- impact_reason: A powerful claim regarding data efficiency. This suggests the diffusion
    distillation method drastically reduces the need for expensive, labeled metric
    datasets.
  relevance_score: 10
  source: llm_enhanced
  text: We also show in the paper that you don't really need a lot of it [metric data].
    We are using maybe 100, 150 times smaller than the amount of data used to train
    such discriminative models.
  topic: Business advice/Technical breakthroughs
- impact_reason: Highlights a strong focus on multi-modal AI (text/image/video) and,
    crucially, the ability to run these complex models 'on device,' signaling major
    progress in model efficiency and edge AI.
  relevance_score: 10
  source: llm_enhanced
  text: The first one is going to be a text-to-3D demo. The second one is going to
    be either video-to-video or image-to-video generation demo. All these are all
    multi-modal demos running on device.
  topic: technical/edge AI/multi-modal
- impact_reason: 'A stunning demonstration of generative AI capabilities: fast, on-device,
    multi-output (mesh + texture) 3D content creation from natural language input.
    This is a major industry milestone.'
  relevance_score: 10
  source: llm_enhanced
  text: Text-to-3D is a model. Literally, a user speaks through an audio interface
    or text prompt, and you describe an object, like a cactus sword, you know, kind
    of a hippo wearing a sweater, you know, anything you can imagine, and it will
    generate on-device a 3D mesh and also associated texture map, like color, everything,
    in less than 3 seconds.
  topic: predictions/breakthroughs/generative AI
- impact_reason: 'Highlights the critical application area for advanced AI in AV:
    handling the ''long-tail'' of rare, high-risk events where traditional systems
    often fail.'
  relevance_score: 9
  source: llm_enhanced
  text: DIMA is about autonomous driving, as you just mentioned, and specifically
    it aims to provide safe motion planning in long-tail scenarios, rare events. Those
    rare events are very critical because those represent accident scenarios as well.
  topic: safety/predictions
- impact_reason: 'Clearly articulates the fundamental weakness of modular AV stacks:
    local optimization does not guarantee global, safe performance, justifying the
    shift to end-to-end.'
  relevance_score: 9
  source: llm_enhanced
  text: The previous trend was mostly modular systems, sequentially and independently
    trained models, components. [...] But when we do that, it is easy to train such
    things quickly, but since we are focusing at each module on its own performance
    goals for us—better segmentation, better detection, better tracking—it doesn't
    mean that at the end, altogether, they will work in harmony, and our goal is to
    drive safe.
  topic: strategy/technical
- impact_reason: 'Defines the breakthrough of end-to-end systems: optimizing the entire
    pipeline against the final objective (safe driving) rather than component metrics.'
  relevance_score: 9
  source: llm_enhanced
  text: This is the shortcoming of such approaches, and end-to-end now is a breakthrough
    in a way that the training objective applies to all components at the same time.
    We are not really focusing on test-specifically trained modules on limited datasets,
    but now we are trying to optimize everything, all the processing, with the end
    goal in mind.
  topic: strategy/technical
- impact_reason: Demonstrates the generalization power (zero-shot capability) derived
    from incorporating LLM knowledge, a key advantage over purely data-driven modular
    systems.
  relevance_score: 9
  source: llm_enhanced
  text: DIMA can also do things that they are not in the training data. For instance,
    there was a zero-shot scenario about a three-point turn. We didn't have any such
    example for it, but then we gave it for testing such examples. It was capable
    of, you know, kind of doing the right planning.
  topic: technical/breakthroughs
- impact_reason: 'Summarizes the core engineering goal: marrying the high-level reasoning
    of LLMs with the high-speed, efficient processing of dedicated perception hardware/models.'
  relevance_score: 9
  source: llm_enhanced
  text: We want to take advantage of this world knowledge of the LLMs to do it automatically.
    So this is why we want to harness the LLM's knowledge with the efficiency of this
    vision-based low-level perception stack.
  topic: strategy/technical
- impact_reason: Justifies the inclusion of LLMs by emphasizing their vast, pre-existing
    world knowledge, which surpasses manually engineered rules, despite the known
    risk of hallucination.
  relevance_score: 9
  source: llm_enhanced
  text: The reason we want to use LLMs is that they represent world knowledge, many
    things. So they capture all the knowledge, most of the knowledge, yes, they hallucinate
    at the end when they are generating this possible, they are much better than the
    things that we manually crafted.
  topic: strategy/limitations
- impact_reason: Highlights the critical trend of unifying diverse sensor data (visual,
    trajectory, map) into a standardized 'token' format, which is fundamental to modern
    multimodal LLM architectures in robotics/AV.
  relevance_score: 9
  source: llm_enhanced
  text: So there is this thing called a single representation, and I will kind of
    we can also consider it as tokenizing. Tokens are a very common term for language
    models, ever since.
  topic: technical
- impact_reason: 'Articulates the core cognitive task of advanced driving AI: modeling
    intent and predicting future actions of other agents, mirroring human intuition.'
  relevance_score: 9
  source: llm_enhanced
  text: So you are trying to anticipate what other person is going to do, other vehicles
    going to—like a real person at a traffic stop, and then I'm looking at the other
    vehicle, I'm thinking it's going to go, so it's going to be for me to go ahead.
  topic: predictions
- impact_reason: 'Provides a specific architectural breakdown: using specialized ''Q-Formers''
    to generate tokens for different modalities (ego, agents, map) before feeding
    them into the central LLM.'
  relevance_score: 9
  source: llm_enhanced
  text: We have these ego vehicle tokens, you know, coming from its own Q-Former,
    and then other vehicles' tokens coming from their Q-Former and intents. And there
    is this bird's-eye view scene that we are tokenizing it also, and there's map
    also, there's tokens for the.
  topic: technical
- impact_reason: 'Explains the iterative refinement process within the transformer
    layers: attention mechanisms improve the quality and consistency of the input
    tokens based on the overall context and prompt.'
  relevance_score: 9
  source: llm_enhanced
  text: This self-attention revises those tokens. So in any layer, you can think that
    all these tokens are now changing in a way that they are more consistent and they
    are more useful and they are answering, for instance, this input prompt better.
    So at the end, we will have updated representations of those tokens. Some road
    tokens go in, and then better tokens come out.
  topic: technical
- impact_reason: 'Provides a clear, high-level definition of knowledge distillation
    in this context: training a smaller student model (transformer) to mimic the refined
    token outputs of the larger teacher model (LLM).'
  relevance_score: 9
  source: llm_enhanced
  text: And is that the core idea behind distillation in general? It's like you've
    got this, you know, surrogate model, in this case the transformer, or the student
    model I'm referring to in this case, that is, you know, it's a function approximator,
    and so the distillation process is kind of teaching it how to approximate, you
    know, the function. In this case here, the teacher model, its output tokens, given
    the raw tokens from the scene encoder, are the function that we're trying to approximate?
  topic: technical
- impact_reason: Details the specific auxiliary tasks (surrogate tasks) used during
    distillation—trajectory prediction and VQA token prediction—to inject necessary
    temporal and spatial reasoning into the model.
  relevance_score: 9
  source: llm_enhanced
  text: What are surrogate tasks in this context? For instance, they are mostly trajectory
    prediction. Some of them are about also best token prediction, future VQA token
    prediction to encourage the LLM to learn spatial-temporal cues useful for planning
    and scene editing.
  topic: technical
- impact_reason: Provides a concrete example of a safety-critical VQA prompt used
    for training, directly linking semantic understanding to safe, actionable driving
    decisions.
  relevance_score: 9
  source: llm_enhanced
  text: For instance, those questions could be, "Okay, in this scenario, what are
    the safe actions to take for the ego vehicle?" That is the literally the question,
    and the answer could be the action is to brake gently to a stop.
  topic: safety
- impact_reason: 'Clearly defines the critical distinction in depth estimation: relative
    scale (generative) versus absolute, measurable scale (metric).'
  relevance_score: 9
  source: llm_enhanced
  text: SharpDepth is about metric depth estimation. You know whether it's a tiny
    chair or a huge chair, or, you know, some reasonable chair it is.
  topic: Technical insights
- impact_reason: 'Identifies the data scarcity problem for metric depth estimation:
    real-world, accurately scaled data is rare compared to synthetic or relative data.'
  relevance_score: 9
  source: llm_enhanced
  text: Discriminative solutions like Metric3D and UniAD... do provide these estimations
    that the estimation sells, either in inches or, you know, millimeters, centimeters.
    However, they are trained with such data doesn't exist in quantity because usually,
    like our sensors, images and like the radar sensors for such like sensors are
    used.
  topic: Technical insights/Limitations
- impact_reason: 'Summarizes the dichotomy: Generative = Sharp but Unscaled; Discriminative
    = Scaled but Blurry/Low Resolution.'
  relevance_score: 9
  source: llm_enhanced
  text: When you use solutions, you get, you know, kind of correct depth estimation
    from the camera or 3D, inaccurate in terms of scale. However, they look very blurred...
    But this depth discriminative matters, they will give exact absolute distance.
    Accuracy is great, but then very low resolution. Maybe you will not even see the
    chair.
  topic: Technical insights/Limitations
- impact_reason: Emphasizes the necessity of metric depth for reliable robotics and
    interaction, contrasting it with relative pixel-based understanding.
  relevance_score: 9
  source: llm_enhanced
  text: Of course, people are using for robotic navigation because we know exactly
    how far away all objects and other objects in the scene are, and robots are interacting
    with the scene that it is reaching us something. It knows where it is exactly,
    not, you know, how many pixels, you know, it doesn't have a day is far away or,
    you're right.
  topic: Predictions/Industry Impact
- impact_reason: Highlights a critical, high-stakes application (medical imaging)
    where single-camera metric depth is essential due to physical constraints.
  relevance_score: 9
  source: llm_enhanced
  text: Surgical assistance, you know, kind of you can do minimally invasive procedures
    using a single camera, and because they need to be very tiny, right? You may not
    be able to put like two cameras with wide baseline into the very tiny blood vessels,
    but you can squeeze something like a camera, a single camera, but then you know
    how far away everything is in the sense of maybe I mean these are really applications
    that grow also.
  topic: Industry Impact/Predictions
- impact_reason: Shows how the diffusion process is adapted from standard text-to-image
    (conditioning on text) to depth estimation (conditioning on the metric reference
    map).
  relevance_score: 9
  source: llm_enhanced
  text: In this case, it is not in this case, it is this, let's say, pure Gaussian
    noise. It is changing depending on the reference map that we have. We are conditioning
    on this.
  topic: Technical insights
- impact_reason: 'Highlights the self-supervision aspect: leveraging the existing,
    imperfect metric model as the supervisory signal to train the high-fidelity model,
    reducing reliance on external labeling.'
  relevance_score: 9
  source: llm_enhanced
  text: You are ultimately this supervisor, totally self-supervised, because we have
    this metric depth model, even though the resolution, we can in the training time,
    we can just use to learn, you know, a better SharpNet.
  topic: Technical insights/Breakthroughs
- impact_reason: Highlights a key advantage of their new approach (likely related
    to SharpNet or the depth estimation model) in requiring less labeled data, which
    is a major bottleneck in deep learning.
  relevance_score: 9
  source: llm_enhanced
  text: But this thing can also, like I said, can use leverage much less supervised
    data. So we can do SFT, supervised fine-tuning.
  topic: technical/model efficiency
- impact_reason: Outlines clear future research directions beyond the current scope
    (monocular) into multi-view and video-based reconstruction techniques, which are
    critical for robotics and AR/VR.
  relevance_score: 9
  source: llm_enhanced
  text: Very cool. Are there additional areas you foresee in terms of future research
    along these lines? Of course, this is monocular depth estimation, but you don't
    do monocular only. If you have multiple cameras, or if you have video, right?
    You can do structure from motion if there's video, or multi-view stereo, or multi-view
    depth estimation.
  topic: predictions/future research
- impact_reason: Addresses the primary deployment challenge for LLMs in AVs (cost/latency)
    and sets the stage for why distillation (transformer model) is necessary for edge
    deployment.
  relevance_score: 8
  source: llm_enhanced
  text: LLMs at this point, of course, we are talking about the systems that we can
    run on the vehicle map, like on a very expensive H-200 GPU, that would be more
    expensive than the vehicle itself. So using these reasonably priced accelerators,
    the token rates are, you know, not that high.
  topic: business/technical
- impact_reason: 'Details the architectural flexibility: offering both a distilled,
    efficient transformer version and a full LLM version, allowing trade-offs between
    performance and latency.'
  relevance_score: 8
  source: llm_enhanced
  text: The transformer-based model, still leveraging the language model, but there
    is also a language model version of it, a multi-modal version of it.
  topic: technical
- impact_reason: A concise summary of the three main benefits achieved by integrating
    LLMs into the AV planner.
  relevance_score: 8
  source: llm_enhanced
  text: This improves generalizability, it improves robustness. The explanations are
    semantically grounded.
  topic: strategy
- impact_reason: Details the complex input structure for autonomous driving models,
    emphasizing the fusion of ego-state, agent states, and map data into a unified
    scene representation.
  relevance_score: 8
  source: llm_enhanced
  text: And for each camera, or better, we are taking them and low-level perceptions.
    They're projecting them into a bird's-eye view map. And then we have things that
    the ego vehicle does, and then we have things for other agents, other vehicles,
    and the things moving in the scene, like pedestrians. And also, there is a map.
    So all together.
  topic: technical
- impact_reason: Clarifies that tokens are semantic features, not raw pixels or views,
    emphasizing the role of feature extraction (like Q-Former) before tokenization
    for LLM consumption.
  relevance_score: 8
  source: llm_enhanced
  text: No, the tokens are their kind of not necessarily the views, but the useful
    information that they kind of provide into. So maybe localizing other things in
    the scene or something?
  topic: technical
- impact_reason: Indicates active development towards more robust, domain-adaptive
    AI systems, moving beyond generalized models to specialized, context-aware solutions.
  relevance_score: 8
  source: llm_enhanced
  text: We are actually in the process of having an even more capable version of DIMA,
    which is domain-adapting to some of the scenarios that I mentioned.
  topic: Technical/Business
- impact_reason: Provides concrete, high-value business applications for accurate
    metric depth estimation, spanning e-commerce, AR, and 3D modeling.
  relevance_score: 8
  source: llm_enhanced
  text: Virtual try-out room, planning object placement, 3D model reconstruction,
    you can use your, you know, point to create a real-size with metric, absolute
    scale models.
  topic: Business/Predictions
- impact_reason: 'Details the initial step of the fusion process: using the existing,
    albeit blurry, metric model as a baseline input for the refinement process.'
  relevance_score: 8
  source: llm_enhanced
  text: We run it [discriminative model], we get an estimation of everything in the
    scene. The estimations are metric estimation, but it is just not high-resolution
    or, you know, kind of sharp enough. Then we also generate the other one [generative
    model].
  topic: Technical insights
- impact_reason: Confirms the broader trend of diffusion models being repurposed for
    structured prediction tasks beyond pure image synthesis.
  relevance_score: 8
  source: llm_enhanced
  text: Recently, I've been seeing a lot of interesting applications of diffusion
    models beyond kind of the, you know, Stable Diffusion image, you know, text-to-image,
    and they all seem to revolve around creative ways to manipulate the noise to do
    interesting things.
  topic: AI technology trends
- impact_reason: Suggests the core noise-manipulation and fusion methodology developed
    for monocular depth is generalizable to more complex multi-view and video-based
    reconstruction tasks.
  relevance_score: 8
  source: llm_enhanced
  text: If you have multiple cameras, or if you have video, right? You can do structure
    from motion if there's video, or multi-view stereo, or multi-view depth estimation.
    So, but all ideas will be the same.
  topic: Future research directions
- impact_reason: 'Poses a crucial question in depth estimation: achieving both sharpness
    (detail) and metric accuracy (real-world scale), indicating a significant research
    goal.'
  relevance_score: 8
  source: llm_enhanced
  text: Both of them are still providing very sharp depth maps, monocular depth maps,
    but metric also accurate depth maps?
  topic: technical/computer vision
- impact_reason: Hints at a novel, mathematically complex mechanism involving 'noise
    maps' for combining different depth information sources, suggesting a sophisticated
    technical innovation.
  relevance_score: 8
  source: llm_enhanced
  text: At the mechanisms, like how these noise is a way of getting or using to different
    maps is explained in the paper. I didn't go into those details. Maybe a little
    bit math-heavy.
  topic: technical/model architecture
- impact_reason: Acknowledges the competitive landscape in 3D generation while implying
    their version offers a significant advantage, likely in efficiency or quality,
    given the 'less than 3 seconds' claim.
  relevance_score: 8
  source: llm_enhanced
  text: I mean, such models—we are not the first to come up with such a model in terms
    of the existing models' computational complexity. There
  topic: business/competitive landscape
- impact_reason: 'Provides a high-level overview of the multi-modal input processing
    pipeline: fusing multi-camera vision into a unified bird''s-eye view representation
    before tokenization/input to the LLM system.'
  relevance_score: 7
  source: llm_enhanced
  text: So what goes into those models, safely, this kind of thing them is tokens,
    and we may have visual data, and this visual data is coming from multiple cameras,
    six cameras, eight cameras. For each camera, or better, we are taking them and
    low-level perceptions. They're projecting them into a bird's-eye view map.
  topic: technical
- impact_reason: Confirms 'scene representation' as the key concept being tokenized,
    reinforcing the standardization trend across complex AI systems.
  relevance_score: 7
  source: llm_enhanced
  text: Exactly. That is what people call a scene representation, and tokens everything
    now kind of standardized in terms of tokens.
  topic: technical
- impact_reason: Introduces the concept of 'distillation constraint' as a mechanism
    to ensure the refined tokens lead to better performance, linking the LLM processing
    back to actionable outputs.
  relevance_score: 7
  source: llm_enhanced
  text: So that's why we are implying to performance, this distillation constraint.
  topic: technical
- impact_reason: Confirms the continued relevance of the U-Net architecture in modern
    generative visual models, even amidst trends like Diffusion Transformers.
  relevance_score: 7
  source: llm_enhanced
  text: U-Net is still one of the choices for a generative model for visual data,
    you're right. And there are lots of applications of this, Sam. Why you may wonder
    why we want to do monocular depth estimation.
  topic: Technical insights
- impact_reason: 'A good strategic takeaway: the goal of the research is not just
    to solve the problem, but to create a framework (like the fusion methodology)
    that others can build upon.'
  relevance_score: 7
  source: llm_enhanced
  text: The idea is same kind of I mean these people foster further research in that
    sense.
  topic: Strategy
- impact_reason: Emphasizes the open and extensible nature of their research framework,
    encouraging community iteration and improvement on their core ideas.
  relevance_score: 7
  source: llm_enhanced
  text: And the reason we showed both of them so people can go, you know, if they
    designed a wrong metric depth, they can also incorporate in this framework, leveraging
    the ideas that we talk about in the paper, and then they may come up with even
    a better version of, you know, kind of SharpDepth.
  topic: strategy/community engagement
- impact_reason: 'Reflects the reality of cutting-edge AI research: high variability
    in results, requiring continuous learning and refinement, even when the core idea
    is promising.'
  relevance_score: 7
  source: llm_enhanced
  text: Yeah, this is active research, you know, kind of writing. Sometimes they generate
    amazing results, sometimes, you know, kind of they are asked to learn. And, you
    know, we find the idea great.
  topic: strategy/research reality
- impact_reason: Indicates a high volume of research output (11 papers and 10+ demos)
    from the presenting group at a major conference (implied CVPR), showcasing significant
    R&D productivity.
  relevance_score: 6
  source: llm_enhanced
  text: In addition to 11 papers, you know, we had maybe more than 10 demos.
  topic: business/R&D output
source: Unknown Source
summary: '## Podcast Summary: Distilling Transformers and Diffusion Models for Robust
  Edge Use Cases with Fatih Porikli - #738


  This episode of the Twomol AI podcast features Fatih Porikli, Senior Director of
  Technology at Qualcomm, focusing on two key research papers from CVPR: **DIMA**
  (Distilling Multi-Modal LLMs for Autonomous Driving) and **SharpDepth** (diffusion
  distillation for depth estimation). The central theme unifying these discussions
  is the critical role of **efficiency and distillation** in deploying advanced AI
  models, particularly Large Language Models (LLMs), onto resource-constrained edge
  devices like autonomous vehicles.


  ### 1. Focus Area

  The primary focus is on advancing **Autonomous Driving (AD) systems** by integrating
  **Multi-Modal Large Language Models (LLMs)** into end-to-end planning architectures,
  specifically addressing the challenge of robustness in **long-tail, rare scenarios**.
  Secondary focus includes the application of **diffusion model distillation** for
  high-fidelity perception tasks like depth estimation.


  ### 2. Key Technical Insights

  *   **DIMA Architecture:** DIMA proposes an end-to-end AD solution that leverages
  the world knowledge and semantic reasoning capabilities of LLMs. It integrates vision-based
  low-level perception (projected onto a bird''s-eye view) and vehicle/agent state
  information, all tokenized and fed into the LLM-based planner.

  *   **LLM Role as Regularizer:** The LLM''s virtual knowledge representation acts
  as a powerful regularizer, allowing the system to generalize effectively to rare,
  long-tail scenarios without explicit training data for every event, significantly
  improving robustness over purely modular systems.

  *   **Distillation for Efficiency:** To overcome the latency issues of running full
  LLMs on edge hardware, DIMA employs distillation. A smaller, efficient **transformer-based
  model** (the student) is trained to approximate the outputs (updated tokens) generated
  by the larger LLM teacher, achieving significant speedups while retaining high performance.
  Surrogate tasks like trajectory prediction and Visual Question Answering (VQA) are
  used during distillation to ensure the student model captures necessary spatial-temporal
  dynamics and semantic grounding.


  ### 3. Business/Investment Angle

  *   **End-to-End Superiority:** End-to-end systems like DIMA are setting a new state-of-the-art,
  demonstrating massive KPI improvements (e.g., 80% reduction in collision rate compared
  to 2025 baselines) over traditional modular AD stacks.

  *   **Edge Deployment Viability:** The success of distillation proves that complex,
  knowledge-rich models (LLMs) can be made efficient enough to run faster than real-time
  on Qualcomm accelerators, making advanced AI practical for mass-market automotive
  deployment.

  *   **Semantic Interpretability as a Feature:** Unlike modular systems where interpretability
  is often limited to component outputs, DIMA''s LLM integration allows for *semantic
  interpretability*—the system can explain *why* it is braking or changing lanes (e.g.,
  "slowing down due to congestion"), which is crucial for trust and safety validation.


  ### 4. Notable Companies/People

  *   **Fatih Porikli (Qualcomm):** Senior Director of Technology, presenting Qualcomm''s
  latest research in efficient, robust edge AI for AD.

  *   **Qualcomm AI Research:** The organization driving this research, focused on
  making perception, reasoning, and action ubiquitous across devices.

  *   **VAD (Vectorized Autonomous Driving):** Mentioned as a previous state-of-the-art
  end-to-end system that DIMA surpasses.


  ### 5. Future Implications

  The industry is moving toward hybrid architectures where large, knowledge-rich models
  (LLMs) guide the reasoning process, but smaller, highly optimized models (Transformers)
  handle the real-time execution. Future research will likely focus on running both
  models concurrently, using the LLM for high-level safety checks or low-frequency
  reasoning, while the distilled transformer handles the primary, high-frequency planning
  loop. This trend suggests a future where semantic understanding is deeply embedded
  in real-time control systems.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Robotics Researchers, Autonomous
  Vehicle Developers, and Technology Strategists** interested in practical applications
  of foundation models (LLMs) in safety-critical, real-time edge computing environments.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
title: 'Distilling Transformers and Diffusion Models for Robust Edge Use Cases with
  Fatih Porikli - #738'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 116
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 15
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 1
  prominence: 0.1
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 03:24:45 UTC -->
