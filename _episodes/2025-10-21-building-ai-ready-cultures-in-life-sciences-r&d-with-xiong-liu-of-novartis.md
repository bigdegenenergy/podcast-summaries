---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: e. Again, that's emerj.com/expert-one. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2161
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2196
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2561
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2623
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5303
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7028
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 17998
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24128
- category: ai_media_and_research
  confidence: high
  context: The organization producing the 'AI and Business Podcast' and featuring
    executive thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_finance
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured as an executive
    thought leader on the podcast.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_defense
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured as an executive
    thought leader on the podcast.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    sense of complex information by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the originator of the transformer architecture that underpins
    modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The specific URL provided for the Notebook LM tool, linking it directly
    to Google.
  name: NotebookLM.google.com
  source: llm_enhanced
- category: consulting_ai_analysis
  confidence: high
  context: Mentioned for conducting a recent survey on generative AI investment in
    the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_company
  confidence: medium
  context: Referenced indirectly through the mention of 'GPT-like models' and the
    explosion of LLMs onto the market.
  name: OpenAI (implied by GPT)
  source: llm_enhanced
- category: ai_model_architecture
  confidence: high
  context: A specific language model architecture mentioned as being used prior to
    GPT-like models for information extraction in clinical trial documents.
  name: BERT
  source: llm_enhanced
- category: ai_model_architecture
  confidence: high
  context: Referenced as the architecture behind the current generation of generative
    models that can generate new types of data (as opposed to BERT, which was primarily
    a representation model).
  name: GPT
  source: llm_enhanced
- category: general_reference
  confidence: low
  context: Mentioned as sources of open public architectures that can be leveraged.
  name: tech companies
  source: llm_enhanced
- category: ai_research
  confidence: low
  context: Mentioned as being quite good at building foundation models and publishing
    results.
  name: academics
  source: llm_enhanced
- category: general_reference
  confidence: low
  context: Mentioned in the context of building and deploying models, often using
    a hybrid approach leveraging published work.
  name: healthcare companies
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 08:40:06 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 71
  relevance_score: 3
  text: So we can use the data you have to train, to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: This is a crucial distinction between traditional ML (task-specific,
    labeled data) and the foundation model paradigm (leveraging massive, broad domain
    data for large-scale pre-training).
  relevance_score: 10
  source: llm_enhanced
  text: The difference is that previously when we do machine learning, we just collect
    our own data, we label the data, and then apply algorithms to train models...
    But now people have been developing these new paradigms, which is along with LLMs
    foundation models. So the idea is that yes, you still care about the specific
    data you are working on, but at the same time, we want to collect all the available
    data, relevant data, or irrelevant data, but as long as they fall into the same
    domain, and feed those data into those transformer architectures to build a much
    large-scale pre-trained models.
  topic: technical
- impact_reason: Provides a concrete, high-value example in life sciences (disease
    pathways) demonstrating how pre-trained models on public biological data (like
    cell atlases) provide immediate background knowledge.
  relevance_score: 10
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data. So for example, cell atlases, etc.
  topic: technical
- impact_reason: 'A powerful conceptual link: treating genetics/biological code as
    a language that LLMs can process, directly linking GenAI to accelerating core
    drug development tasks.'
  relevance_score: 10
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process.
  topic: predictions
- impact_reason: Pinpoints the Transformer architecture as the fundamental, unifying
    breakthrough enabling modern foundation models across different domains.
  relevance_score: 10
  source: llm_enhanced
  text: essentially they have the same, I mean, by nature, they have the same setup.
    So that is based on the breakthrough of the transformer architecture that Google
    proposed a long time ago.
  topic: technical
- impact_reason: 'Explains the core difference enabling GPT-like models: leveraging
    the decoder for sequence generation, moving beyond just representation.'
  relevance_score: 10
  source: llm_enhanced
  text: But now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: 'Highlights a critical bottleneck: the pace of AI innovation is currently
    outpacing the ability of experimental science to generate necessary high-fidelity,
    experiment-driven data.'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
  topic: limitations/predictions
- impact_reason: Articulates the severe risk of hallucination in high-stakes scientific
    domains where validation requires time-consuming, expensive physical experiments,
    unlike text or code generation.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: Highlights the significant gap between investment interest and successful,
    scaled deployment of GenAI in the critical life sciences sector, pointing to a
    major industry challenge.
  relevance_score: 9
  source: llm_enhanced
  text: Flash forward to 2024, a recent Deloitte survey notes that while 73% of life
    sciences leaders are investing in generative AI initiatives, fewer than 20% have
    successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: 'Explains the core value proposition of foundation models: knowledge
    transfer and utility even with limited domain-specific data, enabling applications
    where data scarcity was previously a blocker.'
  relevance_score: 9
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Emphasizes the utility of foundation models for hypothesis generation
    and initial exploration, even when proprietary data is scarce, by leveraging generalized
    domain knowledge.
  relevance_score: 9
  source: llm_enhanced
  text: You can still get some hints about the disease pathways, etc. And then you
    can benchmark that with the known biological knowledge, etc. So certainly you
    can find that even if you have very limited data for the indication that you are
    studying, you still could benefit from the background information that you extracted
    from the foundation models.
  topic: predictions
- impact_reason: Offers a precise, non-technical explanation of fine-tuning—adjusting
    weights rather than full retraining—which is critical for practitioners adopting
    these models efficiently.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right? So we can use the data you have to train, to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems.
  topic: technical
- impact_reason: 'Describes the ideal application of population-level foundation models
    in healthcare: establishing a baseline understanding of ''normal'' to better inform
    personalized medicine.'
  relevance_score: 9
  source: llm_enhanced
  text: foundational models will be able to, you know, track what these normal human
    bodily functions are like across populations and then curtail that data to very
    personalized circumstances among patients.
  topic: predictions
- impact_reason: 'Provides specific, cutting-edge use cases in early discovery: generating
    synthetic biological data (cell types, gene expressions) for in silico testing.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: Highlights a key application and benefit of foundation models in
    accelerating early-stage drug discovery by focusing research efforts.
  relevance_score: 9
  source: llm_enhanced
  text: So that way can allow us to better focus on discovering, you know, novel targets,
    novel gene pathways, etc.
  topic: predictions/business
- impact_reason: Provides a clear technical distinction between BERT-style models
    (encoder-only, good for representation/embeddings) and generative models.
  relevance_score: 9
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they use the decoder
    part of the transformer. So you can think it's as a very good representation model.
  topic: technical
- impact_reason: Details the power of generative models in specialized domains (like
    molecules represented as graphs/sequences) combined with constraint-based generation.
  relevance_score: 9
  source: llm_enhanced
  text: So now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge.
  topic: technical/predictions
- impact_reason: Stresses the indispensable role of domain experts in data curation,
    model guidance, and subsequent benchmarking, even with powerful general models.
  relevance_score: 9
  source: llm_enhanced
  text: But, you know, we still need the domain knowledge people, you know, people
    have those kind of expertise to guide the, you know, the selection of the data
    and also more importantly, then these models are, you build, now how do we benchmark
    with them?
  topic: strategy/safety
- impact_reason: 'Provides a strategic insight: the inherent linguistic nature of
    genetics gives it an early advantage in leveraging NLP/GenAI advancements compared
    to other biological data types.'
  relevance_score: 9
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: Identifies high-quality data as the primary challenge in deploying
    domain-specific foundation models, even when vast amounts of data exist.
  relevance_score: 9
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: business/technical
- impact_reason: 'Proposes the essential solution for model selection in specialized
    fields: evaluation must be driven by a combination of domain expertise and AI
    metrics.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven those evaluation is a
    key here, because there could be so many versions of found data models.
  topic: safety/strategy
- impact_reason: Identifies organizational alignment and effective communication between
    technical builders (modelers) and domain experts (scientists/leadership) as a
    core execution challenge.
  relevance_score: 9
  source: llm_enhanced
  text: So, there's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: Clearly articulates the historical progression of ML/DL techniques
    leading up to the current paradigm shift towards foundation models, providing
    essential context for the audience.
  relevance_score: 8
  source: llm_enhanced
  text: we move to the deep learning techniques like convolutional neural networks,
    RNNs, and graph neural networks. And more recently, we have a paradigm shift into
    those language models, foundation models.
  topic: technical
- impact_reason: Describes a strategic shift towards centralized, comprehensive data
    repositories feeding advanced algorithms for broad organizational impact, moving
    beyond simple document processing.
  relevance_score: 8
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later... from that
    data repository we can drive more advanced algorithms to help with strategic tasks
    across the entire organization that need that kind of whole-scale approach.
  topic: strategy
- impact_reason: Emphasizes that data availability (like cell atlases) is the next
    critical step after establishing the architecture for domain-specific foundation
    models.
  relevance_score: 8
  source: llm_enhanced
  text: Now the next question is about the data, right? What are the data? I gave
    some example like all the cell atlases. So those things, they could be fed into
    those models.
  topic: business/strategy
- impact_reason: 'Defines the necessary organizational structure for successful AI
    implementation in specialized fields: mandatory cross-disciplinary teamwork.'
  relevance_score: 8
  source: llm_enhanced
  text: So, yeah, so it's basically a cross-discipline collaboration, whether in house
    or externally, yeah.
  topic: strategy
- impact_reason: Actionable advice emphasizing rigorous, domain-aware benchmarking
    as the necessary step before trusting AI outputs in critical applications.
  relevance_score: 8
  source: llm_enhanced
  text: So, it's quite important that we have to do those benchmarking and select
    models accordingly.
  topic: strategy
- impact_reason: Highlights the challenge of translating complex model capabilities
    into actionable scientific insights and the necessity of integrating scientists
    into the validation loop.
  relevance_score: 8
  source: llm_enhanced
  text: how do we digest the models, the capabilities? And again, we have to plug
    in, you know, the right scientists to help us together validate those.
  topic: business/strategy
- impact_reason: 'Strategic advice for subject matter experts: they must understand
    the evolving AI landscape, recognizing that AI adoption is hybrid, not a complete
    replacement of existing methods.'
  relevance_score: 8
  source: llm_enhanced
  text: It behooves them to kind of have some awareness of the new and old paradigm
    that we were talking about before, how both are still present, but how we thought
    that AI would impact life sciences organization or that they'd just be this one
    school way of doing things is not the case.
  topic: strategy
- impact_reason: 'Clarifies the architectural relationship: large foundation models
    serving as the base, which then inform or feed into smaller, specialized models
    for specific applications.'
  relevance_score: 7
  source: llm_enhanced
  text: We're also describing the relationship as you'll have LLMs, a foundational
    model, and then break off into smaller models for smaller tasks, but this larger
    data repository is going to be the method of driving efficiencies across the organization.
  topic: technical
- impact_reason: Reiterates the fundamental dependence of modern foundation models
    (regardless of domain—text, image, molecular) on the core Transformer architecture.
  relevance_score: 7
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
source: Unknown Source
summary: '## Comprehensive Summary: Building AI-Ready Cultures in Life Sciences R&D


  This 30-minute podcast episode, featuring **Zhong Liu, Director of Data Science
  in AI at Novartis**, focuses on the paradigm shift in Life Sciences R&D driven by
  **Generative AI (GenAI)** and **Foundation Models (FMs)**, and the cultural and
  technical challenges associated with scaling these technologies beyond pilot projects.


  ### 1. Focus Area

  The discussion centers on the transition from traditional, task-specific Machine
  Learning (ML) to leveraging large-scale Foundation Models in drug discovery, molecular
  design, clinical protocol optimization, and digital twin creation within Life Sciences
  R&D. Key themes include the architectural differences between older ML/DL models
  and FMs, the necessity of domain-specific fine-tuning, and the organizational hurdles
  in achieving meaningful AI adoption and ROI.


  ### 2. Key Technical Insights

  *   **Shift to Foundation Models:** The core technical evolution is moving from
  training models solely on proprietary, labeled datasets to leveraging massive, pre-trained
  FMs (like those based on the Transformer architecture) that capture broad domain
  knowledge (e.g., public molecular data, human language corpus). This allows for
  better performance even with small, specific downstream datasets via **fine-tuning**.

  *   **BERT vs. GPT Architectures:** The evolution from models like BERT (which excel
  at representation/embedding generation) to GPT-like models (which utilize the decoder
  part of the transformer) enables the **generation of new data types**, such as novel
  molecular sequences (represented as SMILES strings or graphs) constrained by desired
  properties (e.g., low toxicity).

  *   **Data Limitations vs. Model Potential:** While AI architectures are advancing
  rapidly, a significant bottleneck remains the availability of high-quality, experimental,
  domain-specific data (e.g., single-cell RNA-seq data) necessary to fully realize
  the potential of these models in areas like comprehensive biological pathway mapping.


  ### 3. Business/Investment Angle

  *   **Scaling Challenge:** Despite high investment interest (73% of life sciences
  leaders investing in GenAI), fewer than 20% have successfully scaled initiatives
  beyond pilots, highlighting a gap between experimentation and meaningful application.

  *   **Domain-Specific ROI:** To justify investment, organizations must move beyond
  generic LLM use cases to develop **domain-specific GenAI** that can provide actionable
  insights in areas like target identification and molecule design, requiring rigorous,
  domain-driven evaluation.

  *   **Hybrid Development Strategy:** Successful implementation often requires a
  hybrid approach: leveraging publicly available, published foundation models while
  simultaneously building proprietary, fine-tuned versions using internal, specialized
  data and expertise.


  ### 4. Notable Companies/People

  *   **Zhong Liu (Novartis):** The expert guest, providing insights from his role
  as Director of Data Science in AI, focusing on practical implementation and cultural
  alignment within a major pharmaceutical company.

  *   **Google (implied):** Mentioned in relation to the breakthrough **Transformer
  architecture** that underpins modern foundation models.

  *   **Deloitte:** Cited for a recent survey highlighting the industry''s struggle
  to scale GenAI adoption.


  ### 5. Future Implications

  The future of R&D hinges on successfully integrating domain expertise with advanced
  AI capabilities. This requires establishing **knowledge-checking benchmarks** and
  objective evaluation metrics to combat model **hallucination**, especially when
  models generate novel biological hypotheses or molecular structures that cannot
  be instantly validated. Furthermore, organizational alignment between data scientists,
  AI professionals, and bench scientists is crucial for effective model digestion
  and adoption.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Professionals** working in Life Sciences,
  **R&D Leadership**, **Data Science Directors**, and **Business Strategists** involved
  in guiding AI investments and transformation within pharmaceutical and biotech sectors.
  It requires a foundational understanding of both AI concepts (ML, LLMs) and the
  R&D lifecycle.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 121
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 12
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 08:40:06 UTC -->
