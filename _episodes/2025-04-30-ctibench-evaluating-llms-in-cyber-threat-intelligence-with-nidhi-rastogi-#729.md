---
companies:
- category: unknown
  confidence: medium
  context: right everyone, welcome to another episode of the Twomel AI podcast. I
    am your host, Sam Charrington. Today I
  name: Twomel AI
  position: 465
- category: unknown
  confidence: medium
  context: episode of the Twomel AI podcast. I am your host, Sam Charrington. Today
    I'm joined by Niddy Rastogi. Niddy is an a
  name: Sam Charrington
  position: 500
- category: unknown
  confidence: medium
  context: omel AI podcast. I am your host, Sam Charrington. Today I'm joined by Niddy
    Rastogi. Niddy is an assistant
  name: Today I
  position: 517
- category: unknown
  confidence: medium
  context: m your host, Sam Charrington. Today I'm joined by Niddy Rastogi. Niddy
    is an assistant professor at Rochester Ins
  name: Niddy Rastogi
  position: 537
- category: unknown
  confidence: medium
  context: Niddy Rastogi. Niddy is an assistant professor at Rochester Institute of
    Technology. Before we get going, be sure to hi
  name: Rochester Institute
  position: 587
- category: unknown
  confidence: medium
  context: I think we first connected many, many years ago. And I think it's a good
    time for us to talk because the
  name: And I
  position: 926
- category: unknown
  confidence: medium
  context: alking about the intersection of those two areas. But I'd love to have
    you share a little bit about your
  name: But I
  position: 1168
- category: unknown
  confidence: medium
  context: before I did my PhD, I was in industry working in Verizon Wireless and
    GE, had some internships at Yahoo and IBM. So
  name: Verizon Wireless
  position: 1731
- category: unknown
  confidence: medium
  context: also thinking of it from a practical perspective. So I decided to do a
    PhD in more like applied research
  name: So I
  position: 1970
- category: unknown
  confidence: medium
  context: 'y-style data?


    That''s an excellent question, Sam. So December 2020 is when we first figured
    out that LLMs are a'
  name: So December
  position: 11713
- category: unknown
  confidence: medium
  context: output that one gets from these language models. So Llama 7 wouldn't perform
    very well on slightly complex
  name: So Llama
  position: 14060
- category: unknown
  confidence: medium
  context: ch projects or one of your recent projects called CTI Bench, which is a
    benchmark for evaluating LLMs in cybe
  name: CTI Bench
  position: 14908
- category: tech
  confidence: high
  context: e called Sec-Gemini version one which came out of Google. That is the one
    that we are aware of; there may
  name: Google
  position: 15427
- category: unknown
  confidence: medium
  context: 'ke, what are the tasks associated with it?


    Sure. So CTI Bench is basically a benchmarking framework. You know w'
  name: So CTI Bench
  position: 15709
- category: unknown
  confidence: medium
  context: 'le, I give you a very simple example: What is the MITRE ATT&CK framework?
    If a cybersecurity analyst is asked'
  name: MITRE ATT
  position: 17184
- category: unknown
  confidence: medium
  context: ll it be able to know which pattern CVE, which is Common Vulnerability
    and Exposures, which is an ID to every kind of so
  name: Common Vulnerability
  position: 17782
- category: unknown
  confidence: medium
  context: CTI Bench, and each of those tasks had questions. The LLM was to respond
    to those questions, and then we wo
  name: The LLM
  position: 18231
- category: tech
  confidence: high
  context: y're primarily, I guess, I would think of them as meta questions—they're
    kind of facts and tidbits that
  name: Meta
  position: 18507
- category: unknown
  confidence: medium
  context: ', a textbook knowledge; I have my question paper. Now I want to test you,
    but on real-world knowledge. So'
  name: Now I
  position: 21251
- category: unknown
  confidence: medium
  context: estions, so knowledge-oriented questions were the CTI MCQ. Then there were
    practical tasks like vulnerabili
  name: CTI MCQ
  position: 23798
- category: unknown
  confidence: medium
  context: ormed exceedingly well on questions such as MCQs. And ChatGPT-4 was the
    latest and greatest back then, which is
  name: And ChatGPT
  position: 25462
- category: unknown
  confidence: medium
  context: encing to some threat actor, like APT28 refers to Fancy Bear. So instead
    of saying Fancy Bear, we'll just use
  name: Fancy Bear
  position: 33422
- category: unknown
  confidence: medium
  context: 'ah.


    What was your observation about that model?


    So Sec-Gemini was not available at the time when we publ'
  name: So Sec
  position: 39796
- category: ai_research_institution
  confidence: high
  context: Niddy Rastogi is an assistant professor here, working at the intersection
    of AI and cybersecurity.
  name: Rochester Institute of Technology
  source: llm_enhanced
- category: ai_research_institution
  confidence: high
  context: Niddy Rastogi started his PhD here, focusing on machine learning and applied
    research in cybersecurity.
  name: RPI
  source: llm_enhanced
- category: industry_user
  confidence: high
  context: Niddy Rastogi worked here in industry before his PhD, shaping his practical
    research mindset.
  name: Verizon Wireless
  source: llm_enhanced
- category: industry_user
  confidence: high
  context: Niddy Rastogi worked here in industry before his PhD, shaping his practical
    research mindset.
  name: GE
  source: llm_enhanced
- category: industry_user
  confidence: high
  context: Niddy Rastogi had an internship here.
  name: Yahoo
  source: llm_enhanced
- category: industry_user
  confidence: high
  context: Niddy Rastogi had an internship here.
  name: IBM
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a high-performing, proprietary LLM example, contrasted with
    open-source models.
  name: ChatGPT
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: Mentioned as an example of an open-source LLM used for initial exploration
    in log comprehension.
  name: Llama 7 billion parameters
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: Mentioned as an example of an open-source LLM whose performance depends
    on its training cutoff date.
  name: Llama 70 billion parameters
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: Mentioned again as an open-source model used for benchmarking and testing
    syntax comprehension.
  name: Llama 7
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: Mentioned as one of the models benchmarked using CTI Bench.
  name: Llama 8
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: Mentioned as one of the models benchmarked using CTI Bench.
  name: Llama 3.5
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: Mentioned as one of the models benchmarked using CTI Bench.
  name: Gemini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as performing exceedingly well on cyber threat intelligence tasks
    during benchmarking.
  name: ChatGPT-4
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: A cybersecurity-specific LLM that came out of Google, benchmarked using
    CTI Bench.
  name: Sec-Gemini version one
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The origin of the Sec-Gemini version one model.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Open-source family of LLMs (Llama 70B, Llama 7, Llama 8) used for benchmarking
    against CTI Bench.
  name: Llama
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced as the source of the MITRE ATT&CK framework, a foundational
    source for cybersecurity knowledge used in the benchmark.
  name: MITRE
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced as a source of standards used to build the CTI Bench knowledge
    base.
  name: NIST
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced as a source of security and privacy regulation information used
    to build the CTI Bench knowledge base.
  name: GDPR
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The conference where the paper detailing CTI Bench was submitted.
  name: NeurIPS
  source: llm_enhanced
- category: ai_tooling
  confidence: high
  context: A benchmarking framework designed specifically for evaluating LLMs on cybersecurity
    tasks (knowledge, reasoning, attribution, CVSS scoring).
  name: CTI Bench
  source: llm_enhanced
- category: big_tech/ai_model
  confidence: high
  context: A specific AI model published by Google, which the researchers used their
    benchmark to evaluate shortly after its release.
  name: Sec-Gemini
  source: llm_enhanced
date: 2025-04-30 07:21:00 +0000
duration: 56
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: these trained models
  text: the problem with these trained models is that there is always a cutoff date
    when it was trained on.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN6726544546.mp3?updated=1746051537
processing_date: 2025-10-05 21:09:29 +0000
quotes:
- length: 113
  relevance_score: 6
  text: After means what LLMs are doing, and before is what purely machine learning
    and deep learning is capable of doing
  topics: []
- length: 122
  relevance_score: 4
  text: And that's what machine learning and deep learning were capable of doing,
    and exactly where my PhD was focusing on earlier
  topics: []
- length: 283
  relevance_score: 4
  text: And today, what the output, which is the after, is what the output we are
    able to see is we are ingesting small information, you know, some kind of a query
    into these LLMs as a prompt, and these LLMs are able to gather from the context
    and, you know, associate that with the response
  topics: []
- length: 191
  relevance_score: 4
  text: And as I mentioned earlier, the more robust and large the corpus, the training
    corpus is, it is kind of a very big factor in the quality of the output that one
    gets from these language models
  topics: []
- length: 240
  relevance_score: 4
  text: So you were right about it; we are more curious in CTI Bench about attribution,
    curious about knowledge, curious about how well is the LLM able to understand
    information and able to produce an output which is helpful, and how accurate it
    is
  topics: []
- length: 155
  relevance_score: 4
  text: So that is another avenue of research that we are pursuing for not just LLMs
    but also for pure AI, like machine learning models trained on a certain corpus
  topics: []
- length: 44
  relevance_score: 4
  text: Can the LLM be on the machine learning model
  topics: []
- length: 130
  relevance_score: 4
  text: We did end up spending a lot of training time and evaluation time, and everything
    did cost us a lot of money because we were using
  topics:
  - valuation
- length: 261
  relevance_score: 3
  text: So December 2020 is when we first figured out that LLMs are able to comprehend
    log data because LLMs were just coming about, and I had a master's student visiting
    us from Germany, and she was interested in LLMs and knowledge graphs and cyber
    threat intelligence
  topics: []
- length: 183
  relevance_score: 3
  text: And so we've started talking about one of your research projects or one of
    your recent projects called CTI Bench, which is a benchmark for evaluating LLMs
    in cyber threat intelligence
  topics: []
- length: 199
  relevance_score: 3
  text: Certainly for mitigation, but then there is also another aspect of this entire
    cyber threat intelligence and LLMs is, aside from benchmarking, are there other
    ways of getting into the head of the LLM
  topics: []
- length: 148
  relevance_score: 3
  text: that was the biggest challenge, and probably also the reason why we haven't
    come out with many such benchmarks because it did cost us a lot of money
  topics: []
- impact_reason: 'Highlights a critical gap in the AI/ML landscape: the lack of standardized,
    domain-specific benchmarking for LLMs in high-stakes fields like cybersecurity.'
  relevance_score: 10
  source: llm_enhanced
  text: We've identified a space where there is no such existing benchmarking model,
    which can tell whether this specific language model is capable of giving good
    responses or accurate responses on cybersecurity-specific tasks.
  topic: safety/strategy
- impact_reason: Articulates the shift from simple pattern recognition (pre-LLM ML)
    to requiring contextual reasoning, necessitating multimodal training data (logs
    + reports).
  relevance_score: 10
  source: llm_enhanced
  text: The challenge became slightly more when understanding, comprehending what
    these models are deciding or decisions they are making. They required getting
    trained on not just simple network logs and files behaving maliciously or in a
    benign manner; they also needed to be trained on other kinds of modalities of
    information like threat intelligence reports so that the model is not only able
    to give you a decision but also context behind that decision.
  topic: technical/AI trends
- impact_reason: Directly positions Retrieval-Augmented Generation (RAG) as the necessary
    architectural solution to overcome the knowledge cutoff problem in dynamic environments.
  relevance_score: 10
  source: llm_enhanced
  text: That's the place where RAGs come in. We can fine-tune these models using,
    you know, some more recent information using a RAG approach, and then these models
    can be up to date with the more recent information.
  topic: technical/AI trends
- impact_reason: A strong warning about the high-stakes nature of LLM hallucinations
    in security, where incorrect advice can lead to real organizational harm.
  relevance_score: 10
  source: llm_enhanced
  text: There is obviously this challenge of hallucinations. If it doesn't know, then
    it might just give you a response which might not even apply. And in domains like
    cybersecurity, this can be, this can be, you know, kind of detrimental.
  topic: safety/limitations
- impact_reason: Highlights the critical need for domain-specific benchmarking frameworks
    (like CTI Bench) to accurately assess LLM performance in specialized fields where
    general benchmarks fail.
  relevance_score: 10
  source: llm_enhanced
  text: CTI Bench, which is a benchmark for evaluating LLMs in cyber threat intelligence.
  topic: technical/benchmarking
- impact_reason: Provides a concrete example of a high-value reasoning task (CVE mapping)
    that tests an LLM's ability to connect descriptive text to standardized industry
    identifiers.
  relevance_score: 10
  source: llm_enhanced
  text: reasoning questions, like if there is a description of a vulnerability of
    a software vulnerability, will the LLM be able to... will it be able to know which
    pattern CVE, which is Common Vulnerability and Exposures, which is an ID to every
    kind of software vulnerability out there, will it be able to identify the correct
    ID of that?
  topic: technical/applications
- impact_reason: Lists essential, authoritative sources (NIST, MITRE ATT&CK, GDPR)
    for building reliable cybersecurity AI tools, providing a roadmap for others in
    the field.
  relevance_score: 10
  source: llm_enhanced
  text: for the source material, for example, we relied on NIST standards, we relied
    on MITRE ATT&CK framework, we relied on GDPR because it has security and privacy
    regulation-related information.
  topic: practical_lessons/data_strategy
- impact_reason: Crucial finding demonstrating the competitive performance of large
    open-source models (Llama 70B) against closed-source leaders, suggesting reduced
    reliance on expensive proprietary APIs for core security tasks.
  relevance_score: 10
  source: llm_enhanced
  text: a very close second was an open-source model, which was Llama 70 billion parameters.
    It was able to perform very well, not as well as ChatGPT-4, but on most of the
    questions like MCQs and threat attribution and attack pattern identification and
    CVSS scoring, it was able to perform very well.
  topic: technical
- impact_reason: 'Identifies a critical limitation: LLMs struggle with complex questions
    requiring deep, nuanced domain knowledge, pointing to a gap between general training
    and specialized expertise.'
  relevance_score: 10
  source: llm_enhanced
  text: 'Very interestingly, pretty much every single model fared very poorly on some
    of the questions, and that was very interesting for us. Why? Because we kind of
    looked back at those questions: What is it about the questions? Is it about the
    training? And then we also had human evaluators who were also experts kind of
    checking if the question is complex or does it require a lot of detailed understanding
    or deep knowledge, which is what we determined was the case.'
  topic: limitations
- impact_reason: 'A strong philosophical statement on the purpose of benchmarking:
    it must reveal weaknesses (blind spots/edge cases) rather than just confirm strengths.'
  relevance_score: 10
  source: llm_enhanced
  text: benchmarks are kind of an approach to telling what the model is capable of
    doing and where it will fail, and identify those blind spots or those edge cases
    where it either needs better training data set or there's something else needs
    to be done. Turning a blind eye towards it is not going to help anybody.
  topic: strategy
- impact_reason: 'Actionable advice for AI adoption: Benchmarks should directly inform
    operational strategy, specifically dictating when human oversight (Human-in-the-Loop)
    is mandatory due to model uncertainty.'
  relevance_score: 10
  source: llm_enhanced
  text: It's better to be aware of that, and that's what the role of the benchmark
    is. It's not to hide any kind of these edge cases or corner cases but to reveal
    them so the analyst knows that my model will be able to respond 80% of the time
    for these types of questions, and for these other types of questions, we need
    more of a human in the loop or some kind of human intervention or an expert intervention
    so we're not making mistakes that might cost us.
  topic: safety/strategy
- impact_reason: Directly addresses the critical need for Explainable AI (XAI) in
    high-stakes domains like cybersecurity, linking model output to confidence scores
    to trigger human intervention.
  relevance_score: 10
  source: llm_enhanced
  text: Can the LLM explain itself? Can the LLM explain why it gave this response?
    That takes us to another side of my research, which is explainable AI. Can the
    machine learning model or AI model explain itself along with the confidence that
    it has in its response?
  topic: safety/technical
- impact_reason: 'A stark example of dangerous hallucination: models fabricating specific,
    authoritative-sounding details (location, organization) to support an incorrect
    conclusion, increasing user trust in false information.'
  relevance_score: 10
  source: llm_enhanced
  text: And at the same time, it would say it very, very convincingly, adding information,
    you know, like, 'This threat was found in this organization at this location on
    this in this time period in this country,' and so on. So it would be very, very
    convincing but incorrect.
  topic: safety/limitations
- impact_reason: 'A critical observation on the state of AI benchmarking: abundance
    of general benchmarks versus a severe lack of practical, domain-specific (cybersecurity)
    benchmarks.'
  relevance_score: 10
  source: llm_enhanced
  text: It was definitely the need, which was very surprising to us, that there are
    so many benchmarks getting built or designed every other day, but there's nothing
    for cybersecurity which can be all applied in practice.
  topic: strategy/business
- impact_reason: This describes a crucial methodology for creating relevant benchmarks—grounding
    evaluation metrics in real-world expert workflows rather than purely synthetic
    tests.
  relevance_score: 9
  source: llm_enhanced
  text: So we designed something based on what an actual threat analyst would experience
    in a given day.
  topic: strategy/technical
- impact_reason: 'Summarizes the core value proposition of LLMs in this domain: transforming
    simple queries into contextually rich, informed outputs.'
  relevance_score: 9
  source: llm_enhanced
  text: The output, which is the after, is what the output we are able to see is we
    are ingesting small information, you know, some kind of a query into these LLMs
    as a prompt, and these LLMs are able to gather from the context and, you know,
    associate that with the response. And now what we get is a lot more informed response
    from AI or LLMs, so to speak.
  topic: AI trends
- impact_reason: Clearly states the fundamental limitation of static LLM training
    data regarding rapidly evolving domains like cybersecurity.
  relevance_score: 9
  source: llm_enhanced
  text: The problem with these trained models is that there is always a cutoff date
    when it was trained on... if I query it today... it may miss out on the recent
    cybersecurity information.
  topic: limitations/technical
- impact_reason: Quantifies the massive potential productivity gain LLMs offer in
    CTI analysis—reducing hours of expert work to seconds.
  relevance_score: 9
  source: llm_enhanced
  text: What an analyst would have taken a couple of hours to assess, to analyze and
    to, you know, comprehend and then associate within a threat pattern they have
    identified in their network, LLMs are able to do that in a couple of seconds.
  topic: business/predictions
- impact_reason: Provides a specific, early anecdote demonstrating the surprising
    emergent capability of LLMs (even smaller ones) to process structured, non-prose
    data like system logs.
  relevance_score: 9
  source: llm_enhanced
  text: December 2020 is when we first figured out that LLMs are able to comprehend
    log data... We were very, very surprised. Not only were they able to comprehend
    that format, but they were also able to identify that there was a presence of
    a malicious activity in these logs.
  topic: technical/breakthroughs
- impact_reason: Emphasizes the surprising robustness and generalization capability
    of LLMs across highly diverse, technical data formats beyond standard text.
  relevance_score: 9
  source: llm_enhanced
  text: 'Language models are able to comprehend a vast variety of formats and syntax
    that we wouldn''t know it has been trained on, but just give it a try, and it
    will surprise you. We have used different kinds of languages: coding languages,
    scripts, JSON, log files, even hashes of malware, and it was able to associate
    that.'
  topic: technical/breakthroughs
- impact_reason: Demonstrates the surprising capability of even smaller, general-purpose
    LLMs (like Llama 7B) to understand complex, non-standard data formats (like logs)
    and perform basic security analysis (malicious activity detection) right out of
    the box.
  relevance_score: 9
  source: llm_enhanced
  text: Not only were they able to comprehend that format, but they were also able
    to identify that there was a presence of a malicious activity in these logs.
  topic: technical/breakthroughs
- impact_reason: Provides concrete examples of the diverse data modalities that LLMs
    can process, highlighting their utility beyond standard natural language text,
    especially in technical domains like security.
  relevance_score: 9
  source: llm_enhanced
  text: 'We have used different kinds of languages: coding languages, scripts, JSON,
    log files, even hashes of malware, and it was able to associate that.'
  topic: technical/applications
- impact_reason: 'Defines the core problem solved by domain-specific benchmarks: the
    lack of standardized, accurate evaluation metrics for specialized AI applications.'
  relevance_score: 9
  source: llm_enhanced
  text: CTI Bench is basically a benchmarking framework. You know what it does? It...
    we kind of identified a space where there is no such existing benchmarking model
    which can tell whether this specific language model is capable of giving good
    responses or accurate responses on cybersecurity-specific tasks.
  topic: strategy/business_advice
- impact_reason: Emphasizes that effective domain-specific evaluation must be grounded
    in real-world workflows and tasks performed by domain experts, not just abstract
    knowledge testing.
  relevance_score: 9
  source: llm_enhanced
  text: So it was very, very rooted in practical applications. So having said that,
    the questions that we, the tasks that we assigned for CTI Bench were... we kind
    of organized again based on the actions or the activities that the security analyst
    would be pursuing in a given day.
  topic: practical_lessons/benchmarking
- impact_reason: Offers a powerful analogy comparing domain benchmarks to professional
    certification exams, clarifying that they test foundational competency rather
    than complex, procedural problem-solving.
  relevance_score: 9
  source: llm_enhanced
  text: CTI Bench is analogous to when we benchmark the NLM against, like, the bar
    exam or the MCAT exam. It's like the background knowledge that someone working
    in this space might have, but not necessarily how to go and install particular
    problems that's worked that's ongoing.
  topic: strategy/benchmarking
- impact_reason: 'A crucial warning about using LLMs for synthetic data generation:
    the output requires rigorous human validation due to the risk of plausible but
    incorrect content.'
  relevance_score: 9
  source: llm_enhanced
  text: we used ChatGPT to create questions based on the sources that we pulled...
    However, it was not, you know, just like most LLMs, we could not completely rely
    on it. It sounded very convincing, the questions, responses, but that's where
    a lot of the effort went in. Is we kind of reviewed every single question and
    the responses...
  topic: safety/ethics/practical_lessons
- impact_reason: Identifies CVSS calculation as a critical, quantitative task for
    threat analysts that LLMs should be able to perform, linking AI capability directly
    to industry-standard risk assessment.
  relevance_score: 9
  source: llm_enhanced
  text: we also wanted to see if the LLM is able to compute severity scores from text
    information, and that is called CVSS, calculating vulnerability severity score.
  topic: technical/applications
- impact_reason: Provides a specific performance benchmark for the state-of-the-art
    proprietary model (GPT-4) on knowledge retrieval tasks at a specific point in
    time (mid-2024).
  relevance_score: 9
  source: llm_enhanced
  text: ChatGPT-4 performed exceedingly well on questions such as MCQs. And ChatGPT-4
    was the latest and greatest back then, which is mid-2024.
  topic: technical
- impact_reason: 'Direct business/strategic takeaway: Open-source models offer a viable,
    cost-effective alternative to proprietary models for specific enterprise tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: So that kind of gives us kind of hope, you know, that we do not need to rely
    completely on expensive closed-source models such as ChatGPT-4 or their latest
    versions.
  topic: business
- impact_reason: Provides a nuanced view of failure modes in LLMs, distinguishing
    between outright hallucination and failures due to knowledge cutoff, which is
    essential for debugging and deployment planning.
  relevance_score: 9
  source: llm_enhanced
  text: 'Some questions every single model tripped on; they were not able to respond.
    And there could be many reasons behind it. I wouldn''t say hallucination is the
    only reason. There is also the cutoff date: when was the model training cutoff?'
  topic: limitations
- impact_reason: 'The ultimate goal of XAI and confidence scoring in this context:
    creating a reliable trigger mechanism for escalating tasks to human experts.'
  relevance_score: 9
  source: llm_enhanced
  text: So we can determine when human in the loop should take place.
  topic: safety
- impact_reason: Illustrates conceptual confusion (mixing up distinct vulnerability
    types) presented with high confidence, showing that LLMs can blend related but
    incorrect concepts.
  relevance_score: 9
  source: llm_enhanced
  text: But sometimes it would just make up information, you know, like saying cross-site
    scripting is related to SQL injection, but in reality, for that specific example,
    it was just improper input validation. So some kind of mix-up we would see, but
    very convincingly.
  topic: limitations
- impact_reason: 'Describes a specific, valuable application of LLMs in cybersecurity:
    sequence prediction based on established frameworks (MITRE ATT&CK) for proactive
    threat analysis.'
  relevance_score: 9
  source: llm_enhanced
  text: When we have that kind of threat report, we pick an initial part of the threat
    report and provide it to the LLM, and we ask the LLM what are the rest of the
    threat patterns, and it provides us with those threat patterns along with the
    ID. Now, those IDs are very, very standard and are part of the MITRE ATT&CK framework...
  topic: business
- impact_reason: Connects XAI with model confidence scoring, which is crucial for
    operationalizing AI in high-stakes environments.
  relevance_score: 9
  source: llm_enhanced
  text: Can the machine learning model or AI model explain itself along with the confidence
    that it has in its response?
  topic: technical/safety
- impact_reason: Reinforces the reality that state-of-the-art models are not infallible
    and that identifying failure modes (blind spots) is paramount for real-world deployment.
  relevance_score: 9
  source: llm_enhanced
  text: But what was kind of not surprising, I will say, but we validated it, is that
    even the most well-trained models will have blind spots, and we need to identify
    them because again, we need to integrate these systems into our environments.
  topic: safety/technical
- impact_reason: Provides a positive validation for the utility and viability of large,
    open-source LLMs, countering potential skepticism.
  relevance_score: 9
  source: llm_enhanced
  text: And also, what was very interesting was that the large language models which
    are open source using high parameters, they are not a complete disappointment;
    they are actually very, very useful.
  topic: technical/business
- impact_reason: Specific warning about the high computational and cost demands of
    Retrieval-Augmented Generation (RAG) systems, making them prohibitive for resource-constrained
    entities.
  relevance_score: 9
  source: llm_enhanced
  text: And then we also explored RAGs, which we did not include in the paper. That
    was extremely demanding in terms of the number of token requests and the response.
    All of that was kind of prohibitive, especially for a research lab.
  topic: technical/business
- impact_reason: A broad statement confirming the transformative impact of AI on cybersecurity
    defense mechanisms.
  relevance_score: 8
  source: llm_enhanced
  text: AI has kind of transformed how threats are detected, how we defend organizations,
    large networks against threats using AI.
  topic: predictions/business
- impact_reason: Illustrates the extreme variability in data modality and length that
    CTI analysts must process, highlighting the need for LLMs to handle diverse inputs.
  relevance_score: 8
  source: llm_enhanced
  text: All of that is contained in these reports. These reports can go, you know,
    up to like 50, 60 pages long, or they could be present in the form of a very small
    tweet or blog reports.
  topic: technical/data
- impact_reason: Highlights that significant comprehension capabilities are present
    even in smaller, open-source models, suggesting accessibility for specialized
    tasks.
  relevance_score: 8
  source: llm_enhanced
  text: I'm talking the open-source Llama 7, 8, which are 7 billion parameters, 8
    billion parameters, and even Llama 7. We have trained it; we've explored all of
    these, and they've surprised us with how much variety of syntax it is able to
    comprehend.
  topic: technical/AI trends
- impact_reason: 'Reiterates a fundamental principle in LLM scaling: training data
    size and quality are primary drivers of model capability, often outweighing minor
    architectural differences in early explorations.'
  relevance_score: 8
  source: llm_enhanced
  text: the capability of the language model is very much determined by the size of
    the corpus, the data corpus that was used to train on it.
  topic: technical/training
- impact_reason: Identifies a specific, publicly known, domain-specialized LLM (Sec-Gemini),
    marking a trend toward verticalized AI models.
  relevance_score: 8
  source: llm_enhanced
  text: For the cyber threat intelligence space or cybersecurity overall, there is
    one called Sec-Gemini version one which came out of Google.
  topic: AI_technology_trends
- impact_reason: Clearly delineates the scope of the current benchmark (understanding/attribution)
    versus future research goals (action/remediation), showing the staged development
    of AI evaluation.
  relevance_score: 8
  source: llm_enhanced
  text: we are more curious in CTI Bench about attribution, curious about knowledge,
    curious about how well is the LLM able to understand information and able to produce
    an output which is helpful, and how accurate it is. But when it comes to mitigation
    or remediation... we do not do that in CTI Bench.
  topic: strategy/limitations
- impact_reason: Highlights the dependency of high-quality domain-specific AI development
    (both models and benchmarks) on curated, trustworthy foundational data.
  relevance_score: 8
  source: llm_enhanced
  text: CTI Bench, like most domain-specific language models, it requires a trustworthy
    source of data.
  topic: business_advice/data_strategy
- impact_reason: 'Shifts research focus from pure detection/identification benchmarking
    to the crucial next step: AI-assisted mitigation strategies.'
  relevance_score: 8
  source: llm_enhanced
  text: We haven't continued down that path yet, but what we are interested in is
    more of a mitigation effort, you know, how do we resolve or how do we find solutions
    to mitigating a threat once it has been identified?
  topic: strategy
- impact_reason: Details a rigorous methodology for testing deep inference capabilities—removing
    explicit clues to force the model to rely on abstract pattern recognition for
    attribution.
  relevance_score: 8
  source: llm_enhanced
  text: So what we did was provide a snippet of the threat, and then now the expectation
    from, like, two of the tasks was the snippet that we provided, we kind of very
    carefully hid any kind of clue where the model is able to conclusively determine
    that this snippet is coming from this CVE or this CWA or which threat actor is
    responsible or behind this kind of threat.
  topic: technical
- impact_reason: 'Focuses on the practical, near-term impact of LLMs: augmenting Security
    Operations Center (SOC) analysts by optimizing their workflow, rather than replacing
    them entirely.'
  relevance_score: 8
  source: llm_enhanced
  text: how SOC analysts can optimize their time when LLMs are treated more like an
    assistant. So how does that new environment look like?
  topic: predictions
- impact_reason: Highlights the inherent limitations in current mitigation techniques,
    directly fueling the need for ongoing research into AI safety and robustness.
  relevance_score: 8
  source: llm_enhanced
  text: So some of those aspects we definitely miss out on, which is what I think
    gives ideas for future research.
  topic: safety/research
- impact_reason: Offers a clear vision for the future operational role of LLMs in
    cybersecurity—as an assistant—and the need to optimize workflows around this.
  relevance_score: 8
  source: llm_enhanced
  text: So mitigation and how SOC analysts can optimize their time when LLMs are treated
    more like an assistant.
  topic: business/predictions
- impact_reason: Emphasizes the non-negotiable requirement for failure analysis in
    mission-critical AI applications.
  relevance_score: 8
  source: llm_enhanced
  text: For that, knowing exactly where it will perform poorly or where it will fail,
    especially in mission-critical use cases like cybersecurity, it is very, very
    important.
  topic: safety/strategy
- impact_reason: Highlights the significant, often underestimated, cost of large-scale
    LLM inference and evaluation, especially for research labs.
  relevance_score: 8
  source: llm_enhanced
  text: The challenge, I will say, was I think just training all of this was very,
    very cost-intensive. We did end up spending a lot of training time and evaluation
    time, and everything did cost us a lot of money because we were using... what
    specifically was trained? No, I mean to say requesting tokens from GPT. Oh, inference,
    yes.
  topic: business/technical
- impact_reason: Provides a concise, expert definition of Cyber Threat Intelligence
    (CTI), establishing the domain context.
  relevance_score: 7
  source: llm_enhanced
  text: Cyber threat intelligence is basically an aggregation of all of cybersecurity-related
    information available on the internet, aggregating it, analyzing it, and then
    using it to detect any kind of a cyber threat as well as defend from those cyber
    threats.
  topic: strategy
- impact_reason: 'Breaks down ''knowledge'' testing in CTI into foundational components:
    factual recall, pattern recognition, and source identification, which are key
    for junior analysts.'
  relevance_score: 7
  source: llm_enhanced
  text: Knowledge would mean how much do you know, like textbook knowledge? Do you
    know about a pattern? You know, do you know how to find the idea of this pattern?
    What is this, like, which sources should you be looking up if you need more information?
  topic: technical/applications
- impact_reason: Stresses the importance of grounding AI tasks in established, non-proprietary
    industry standards (like CVSS) to ensure relevance and adoption.
  relevance_score: 7
  source: llm_enhanced
  text: CVSS is an existing industry kind of formula, or all of this is based on industry
    standards what most of the organizations in the world use. So none of this was
    invented by us.
  topic: strategy
- impact_reason: Confirms the superior performance of state-of-the-art models (like
    GPT-4) on foundational knowledge retrieval tasks compared to smaller or older
    models.
  relevance_score: 7
  source: llm_enhanced
  text: what we learned, and very unsurprisingly, is that ChatGPT-4 performed exceedingly
    well on questions such as MCQs.
  topic: predictions/benchmarking
- impact_reason: Clearly defines the role of Multiple Choice Questions (MCQs) in testing
    foundational knowledge recall for security analysts, setting a baseline for LLM
    evaluation.
  relevance_score: 7
  source: llm_enhanced
  text: MCQ is basically knowledge retrieval, recall, remembering important security
    information.
  topic: technical
- impact_reason: A grounding statement against over-reliance on SOTA models, stressing
    humility in deployment.
  relevance_score: 7
  source: llm_enhanced
  text: So it was good to know that even the best of the best models are able to fail
    sometimes.
  topic: strategy
- impact_reason: Directly links high operational costs (inference/evaluation) to the
    scarcity of high-quality, practical benchmarks.
  relevance_score: 7
  source: llm_enhanced
  text: So that was the only... that was the biggest challenge, and probably also
    the reason why we haven't come out with many such benchmarks because it did cost
    us a lot of money.
  topic: business/strategy
source: Unknown Source
summary: '## Podcast Summary: CTIBench: Evaluating LLMs in Cyber Threat Intelligence
  with Nidhi Rastogi - #729


  This episode features Dr. Nidhi Rastogi from Rochester Institute of Technology discussing
  the critical intersection of Large Language Models (LLMs) and Cyber Threat Intelligence
  (CTI), focusing on the development and application of their novel benchmarking framework,
  **CTIBench**.


  ### 1. Focus Area

  The primary focus is the **application and rigorous evaluation of LLMs specifically
  within the domain of Cyber Threat Intelligence (CTI)**. Discussions covered the
  evolution from traditional ML/DL in security to the contextual understanding provided
  by LLMs, the role of Retrieval-Augmented Generation (RAG) in keeping models current,
  and the creation of a practical, analyst-centric benchmark (CTIBench) to measure
  performance on real-world CTI tasks.


  ### 2. Key Technical Insights

  *   **LLMs Comprehend Diverse Syntax:** Contrary to expectations, LLMs (even smaller
  open-source models like Llama) demonstrated a surprising ability to comprehend non-prose
  data formats common in security, including log files, code, JSON, and even malware
  hashes.

  *   **RAG for Temporal Relevance:** Due to training data cutoff dates, Retrieval-Augmented
  Generation (RAG) is essential in CTI to inject the most recent threat intelligence,
  ensuring models are not operating on outdated information, which is critical in
  a fast-moving threat landscape.

  *   **CTIBench Task Structure:** The benchmark is designed around the daily activities
  of a threat analyst, encompassing knowledge retrieval (e.g., defining MITRE ATT&CK),
  reasoning (e.g., CVE mapping), threat attribution, and severity scoring (CVSS calculation).


  ### 3. Business/Investment Angle

  *   **High Stakes of Hallucination:** In CTI, LLM hallucinations are "detrimental,"
  as convincing but false information can lead analysts to implement ineffective or
  harmful security measures. Accuracy is paramount.

  *   **Efficiency Gains in CTI Analysis:** LLMs can compress tasks that take human
  analysts hours (gathering, analyzing, and correlating threat reports) into seconds,
  offering massive productivity boosts for security teams.

  *   **Need for Domain-Specific Benchmarking:** The existence of CTIBench highlights
  a market gap: general LLM benchmarks are insufficient for specialized, high-stakes
  domains like cybersecurity, driving demand for domain-specific evaluation tools.


  ### 4. Notable Companies/People

  *   **Nidhi Rastogi (RIT):** The expert driving the research, focusing on applied
  AI/ML in cybersecurity and leading the development of CTIBench.

  *   **General LLM Providers (Llama, Gemini, ChatGPT/GPT-4):** These models were
  used as the subjects for benchmarking against CTIBench, showing performance scaling
  with model size and sophistication (GPT-4 performed exceedingly well).

  *   **Google (Sec-Gemini v1):** Mentioned as a developer of a specialized, security-focused
  LLM that has utilized CTIBench for its own evaluation.

  *   **Foundational Sources (NIST, MITRE ATT&CK, GDPR):** These standards bodies
  provided the trustworthy source material used to construct the CTIBench questions.


  ### 5. Future Implications

  The industry is moving toward **specialized, continuously updated LLMs** validated
  by domain-specific benchmarks. Future research (already underway in Rastogi''s lab)
  will likely focus on extending benchmarks to cover **remediation and mitigation
  strategies**, moving beyond knowledge retrieval to actionable defense recommendations.
  The success of CTIBench suggests a trend toward creating standardized, practical
  evaluation suites for all critical AI applications.


  ### 6. Target Audience

  This episode is highly valuable for **Cybersecurity Professionals (especially Threat
  Intelligence Analysts and SOC Managers), AI/ML Researchers focusing on applied domains,
  and Technology Investors** tracking the commercialization and validation of enterprise
  AI solutions.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- google
- meta
title: 'CTIBench: Evaluating LLMs in Cyber Threat Intelligence with Nidhi Rastogi
  - #729'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 149
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 23
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 7
  prominence: 0.7
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 21:09:29 UTC -->
