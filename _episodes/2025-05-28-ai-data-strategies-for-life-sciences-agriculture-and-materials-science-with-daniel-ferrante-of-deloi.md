---
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Dani
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Daniel Feronte, AI leader in R&
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Daniel Feronte, AI leader
    in R&D and Data Strategy at Deloitte.
  name: Daniel Feronte
  position: 134
- category: unknown
  confidence: medium
  context: y's guest is Daniel Feronte, AI leader in R&D and Data Strategy at Deloitte.
    Daniel joins Emerge CEO and head of
  name: Data Strategy
  position: 171
- category: unknown
  confidence: medium
  context: n R&D and Data Strategy at Deloitte. Daniel joins Emerge CEO and head of
    research Daniel Fajella on today's sh
  name: Emerge CEO
  position: 211
- category: unknown
  confidence: medium
  context: tte. Daniel joins Emerge CEO and head of research Daniel Fajella on today's
    show to delve into the intersection of
  name: Daniel Fajella
  position: 243
- category: unknown
  confidence: medium
  context: on driving data insights in R&D for the Food and Beverage Sector sponsored
    by Deloitte. And without further ado, h
  name: Beverage Sector
  position: 991
- category: unknown
  confidence: medium
  context: d without further ado, here's their conversation. So Dan, welcome back
    to the podcast with Tabir. Likewise
  name: So Dan
  position: 1082
- category: unknown
  confidence: medium
  context: g like, data is the new oil. You see these on the Wall Street Journal,
    you see these on the New York Times, you see the
  name: Wall Street Journal
  position: 1832
- category: unknown
  confidence: medium
  context: on the Wall Street Journal, you see these on the New York Times, you see
    these everywhere. But to my memory, whic
  name: New York Times
  position: 1874
- category: tech
  confidence: high
  context: ', don''t think the way we think. So what I call an apple, it may just
    call a little red ball with green sp'
  name: Apple
  position: 5999
- category: unknown
  confidence: medium
  context: rather than just say, look, read this paper from South South Science Nature,
    whatever, and tell me all the cancer biology tha
  name: South South Science Nature
  position: 10719
- category: unknown
  confidence: medium
  context: stion, so let me start trying to break that down. Hopefully I won't miss
    any. One of them is just the informati
  name: Hopefully I
  position: 12452
- category: unknown
  confidence: medium
  context: nderstand what you're saying? That is 100% right. And I'll give you maybe
    two notes for each one of the c
  name: And I
  position: 17649
- category: unknown
  confidence: medium
  context: the comments that you make on the ontology side. So I'll ask you the following
    question, which is not a
  name: So I
  position: 17748
- category: unknown
  confidence: medium
  context: mark on 0, on 1, on 2, 3, 4, all the way to 255. Each Hula hoop in each
    one of the bricks that you have has
  name: Each Hula
  position: 24699
- category: unknown
  confidence: medium
  context: physics in the end of the day. You can sum up the Standard Model as a bunch
    of different geometries put together.
  name: Standard Model
  position: 35204
- category: ai_research
  confidence: high
  context: The organization hosting the podcast, focused on AI research.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company where the guest works, focusing on AI application in enterprise
    R&D, and sponsoring the podcast series. They use a multimodal framework called
    'Atlas'.
  name: Deloitte
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Deloitte's multimodal framework used to bridge gaps between disparate data
    sets and ontologies.
  name: Atlas
  source: llm_enhanced
- category: media_reference
  confidence: medium
  context: Mentioned as a source where common AI/data phrases (like 'data is the new
    oil') are published, not as an AI company itself, but relevant to the discourse.
  name: Wall Street Journal
  source: llm_enhanced
- category: media_reference
  confidence: medium
  context: Mentioned as a source where common AI/data phrases (like 'data is the new
    oil') are published, not as an AI company itself, but relevant to the discourse.
  name: New York Times
  source: llm_enhanced
- category: historical_reference
  confidence: low
  context: Referenced historically (600 years ago) as a scientist whose work is analogous
    to the labeling process in AI/data science, not a modern AI company.
  name: Galileo
  source: llm_enhanced
date: 2025-05-28 07:00:00 +0000
duration: 40
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: be aware of the fact that we might naturally be extracting information
    from our data that points in opposite directions, that pointing perpendicular
    directions, right, that lead to different conclusions from the same principle
  text: we should be aware of the fact that we might naturally be extracting information
    from our data that points in opposite directions, that pointing perpendicular
    directions, right, that lead to different conclusions from the same principle.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: conceive of it
  text: we should conceive of it.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: that
  text: The problem with that is that if you're doing an ontology for something big,
    like a brain, you're gonna get the brain parts, the solutions you're gonna get
    this, you're gonna get that, you're gonna get all these other stuff, but you typically
    don't go all the way down to bundles of neurons, which are sort of a midway scale,
    and much less to the cell, to the neurons, to their components, to what's happening
    inside, to calcium pumps, and this and that.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_5.28.25_-_Daniel_Ferrante_FIN.mp3?dest-id=151434
processing_date: 2025-10-05 14:19:39 +0000
quotes:
- length: 117
  relevance_score: 4
  text: He outlines how AI models, including large language models, can be leveraged
    to label and map complex data landscapes
  topics: []
- length: 153
  relevance_score: 4
  text: There's a natural dictionary, a natural mapping, physics between statistics,
    which is deep learning, machine learning, statistical learning, and geometry
  topics: []
- impact_reason: 'Highlights the core challenge in R&D data strategy: the mismatch
    between the conceptual variables of a scientific problem and the actual structure/scale
    of the available data.'
  relevance_score: 10
  source: llm_enhanced
  text: We have data across multiple different models, that is multiple different
    scales. But we don't know when you ask a business question, when you ask a proper
    scientific R&D question, you don't know the fundamental variables of the problem,
    typically speaking. Or sometimes different fundamental variables of the problem
    that maybe you'll know from physics, biology, chemistry, or whatnot, don't directly
    or one-to-one map to the data that you actually have.
  topic: technical/strategy
- impact_reason: 'This is the central thesis: using LLMs (trained on vast public knowledge)
    as a contextual framework or ''map'' onto which proprietary data can be overlaid.'
  relevance_score: 10
  source: llm_enhanced
  text: Why don't we use what these large language models have allegedly learned,
    be in the field that you have them, and try to map that landscape of their knowledge
    and then put your data within that context.
  topic: technical/AI trends
- impact_reason: 'A profound redefinition of scientific work: shifting the focus from
    mere data generation to the crucial act of labeling/defining the underlying structure
    (the ''parabola label''). This connects directly to supervised learning concepts.'
  relevance_score: 10
  source: llm_enhanced
  text: As scientists we think of ourselves as generating data, but really what we're
    doing is generating labels. ... our job is to figure out the parabola label, the
    label that you're going to give to the experiment that you're doing, right? Not
    just generate more and more data per cent, but now that you have all that, maybe
    this alalan's maybe this journey, maybe the high at the writ large above high
    statistical learning can actually help you put labels in these things.
  topic: strategy/technical
- impact_reason: 'Articulates the primary business value proposition: offloading the
    massive cognitive load of data integration and wrangling (which is often 80% of
    data science work) to AI systems so scientists can focus on discovery.'
  relevance_score: 10
  source: llm_enhanced
  text: So the goal would be, look, there is this in tango amount of data, there's
    some 75 if not more openly available databases online that folks in drug discovery...
    use on a daily basis... So why not take the cognitive bandwidth that it takes
    to try and connect all of those different data sets, all of those different landscapes
    of different modalities of data and serve that, if you will, to this different?
    And then allow the scientist to do science as opposed to do wrangling of the data...
  topic: business/strategy
- impact_reason: Emphasizes the necessity of multi-modal and multi-dimensional data
    extraction (tables, plots, diverse concepts) in complex fields like drug discovery,
    pointing out the current inadequacy of standard RAG architectures for this task.
  relevance_score: 10
  source: llm_enhanced
  text: So you don't want just one dimension of that, just my target, just my drug,
    you want the target, you want the target, you want the target, you want the target,
    you want the drug, you want the adjacent biology, you want things that went well,
    things that failed, you want the tables, you want the plots, you want everything.
    So that multi-modal, multi-dimensional extraction is actually really hard for
    RAGs to do, at least as designed now, right?
  topic: technical
- impact_reason: A critical cautionary note regarding the quality of input data in
    scientific research, implying that AI systems must account for inherent data contradictions,
    which might be mistaken for hallucinations.
  relevance_score: 10
  source: llm_enhanced
  text: Nature has published a couple times, if I'm not mistaken, the first one in
    2012, the other one more recently, 17 or 18, that this matter studies around cancer
    that 80-85% of cancer studies cannot be reproducible.
  topic: safety/limitations
- impact_reason: 'Offers a profound perspective shift: conflicting results in the
    source data are normal in science, and the AI''s job is to surface these conflicts
    rather than force a single, potentially false, consensus.'
  relevance_score: 10
  source: llm_enhanced
  text: So we should be aware of the fact that we might naturally be extracting information
    from our data that points in opposite directions, that pointing perpendicular
    directions, right, that lead to different conclusions from the same principle.
    And that is okay. That might not be a hallucination.
  topic: safety/strategy
- impact_reason: 'Introduces the speaker''s core belief/hypothesis regarding deep
    learning: the manifold hypothesis, which is central to understanding how models
    organize data.'
  relevance_score: 10
  source: llm_enhanced
  text: There's a fundamental, there's sort of a fundamental, maybe not if not hypothesis,
    what do you call it? Belief, well, I'll say belief, and then I'll get stone for
    it. In deep learning, I call the manifold hypothesis.
  topic: technical
- impact_reason: 'Clear definition of the manifold hypothesis: data learned by a model
    is structured into a lower-dimensional, meaningful space (the manifold).'
  relevance_score: 10
  source: llm_enhanced
  text: And what that says is that as you learn from the data, the model is organizing
    that data in a space.
  topic: technical
- impact_reason: 'This is a profound strategic vision for AI in scientific discovery:
    mapping complex scientific domains (chemistry, biology) into learnable geometric
    spaces for unified analysis.'
  relevance_score: 10
  source: llm_enhanced
  text: What we want to actually do in the end of the day is to learn the geometry
    of chemistry, the geometry of protein language, the geometry of DNA, the DNA language,
    the RNA language, and what else have you, and then put them all together in some
    capacity.
  topic: predictions/strategy
- impact_reason: Emphasizes the critical need for 'contextualized embedding' across
    multimodal data as the next frontier beyond current siloed models.
  relevance_score: 10
  source: llm_enhanced
  text: like some of these problems require this huge contextual localization across
    different data types, different understandings and whatnot. And that's what we're
    trying to provide, bringing all your models and all your data and you're going
    to pump the new oil from this contextualized embedding across all these different,
    all these different spaces.
  topic: technical/strategy
- impact_reason: Highlights Tensor Networks as the unifying mathematical language
    connecting deep learning, quantum computing, and classical linear algebra, suggesting
    a convergence of these fields.
  relevance_score: 10
  source: llm_enhanced
  text: There's a framework that takes care of all of this stuff that's called tensor
    networks. Tensor networks are a fancy way to do linear algebra, they're a fancy
    way to do matrices that connects all these different topics, meaning statistical
    learning, the deep learning, quantum computing, quantum circuits, and just matrix
    multiplication like we learned in school.
  topic: technical
- impact_reason: 'The ultimate strategic goal: achieving ''big brother context'' by
    explicitly modeling the geometric relationships *between* scientific disciplines,
    which current single-domain models miss.'
  relevance_score: 10
  source: llm_enhanced
  text: This is basically the punchline of the story that we're telling here, which
    is, look, chemistry will have a geometry, proteins will have a geometry, DNA and
    a blah, blah, blah. The whole chemist, they are going to have their own. Now let's
    put them all together and see what that big and brother context will tell off
    your data because that connectivity across the relative information between these
    different disciplines and what that is not captured by any single model.
  topic: strategy
- impact_reason: 'This challenges the common cliché (''data is the new oil'') by pointing
    out the missing element: the ''power'' or mechanism (like a refinery) needed to
    extract value, setting the stage for the discussion on AI as that mechanism.'
  relevance_score: 9
  source: llm_enhanced
  text: Typically people will get to that point and say something like, data is the
    new oil. ... But to my memory, which might as well be failing, but nonetheless,
    typically something doesn't follow that statement. The reason why something doesn't
    follow that statement is because you need something powerful, right? It fits the
    new oil.
  topic: strategy
- impact_reason: Crucial warning about the semantic gap between human understanding
    and model representation. Success depends on learning to map the model's latent
    space semantics back to human concepts.
  relevance_score: 9
  source: llm_enhanced
  text: The caveat is that these models don't have the same semantics, don't think
    the way we think. So what I call an apple, it may just call a little red ball
    with green spikes and a little corner, but outside of that, once we learn to map
    that, that is resolved.
  topic: safety/technical
- impact_reason: 'Provides a concrete outcome of the proposed methodology: using the
    LLM context to cluster and visualize proprietary data based on scientific properties
    (toxicity, half-life), enabling holistic analysis.'
  relevance_score: 9
  source: llm_enhanced
  text: So my proposal, so to speak, is to do that cartography, to do that mapping
    and say, look, I now ingested all my oncology data. Now everybody that has the
    same toxicity profile, all their sitting here in the toxicity mountain, everybody
    that has the same half-life in vivo, they're all sitting here in this body...
  topic: technical/strategy
- impact_reason: Emphasizes the shift from narrow, linear analysis to asking complex,
    multi-scale, multimodal questions that traditional methods struggle with.
  relevance_score: 9
  source: llm_enhanced
  text: Let's step back from that, give the wrangling sort of to this alarm and then
    we go back and ask these questions that are multimodal in nature, that are long-range,
    short-range, medium-range correlation in question and whatnot, and see what the
    answer comes back, right?
  topic: AI trends/predictions
- impact_reason: Contrasts generic LLM knowledge retrieval with highly valuable, proprietary,
    contextualized data interrogation, showing the path to actionable R&D insights.
  relevance_score: 9
  source: llm_enhanced
  text: So rather than just say, look, read this paper from South South Science Nature,
    whatever, and tell me all the cancer biology that you learned about, why not look
    at your own data and say, look, tell me all the properties of this drug with this
    target with this micro environment, so on and so forth, and let's see what that
    fully contextualized data problem challenge looks like put together.
  topic: business/strategy
- impact_reason: 'A critical assessment of current LLM capabilities in specialized
    domains: standard language parsing is insufficient for high-stakes scientific
    reasoning that requires multi-step, interconnected inference.'
  relevance_score: 9
  source: llm_enhanced
  text: For science English, that is unfortunately now good enough, and it's not good
    enough for a couple of different reasons. One of them is just parsing through
    the language, like it's really low return, low accuracy, low whatever that you
    get back. One, and two, it's just the fact that you want to be able to connect
    that information on several different steps with several different points of what
    you had before, what you h[ad].
  topic: limitations/technical
- impact_reason: Highlights the current state and evolution of Retrieval-Augmented
    Generation (RAG) techniques, moving beyond naive implementations to more complex
    structures like graph RAGs, which is crucial for advanced information extraction.
  relevance_score: 9
  source: llm_enhanced
  text: So the very first step in the direction are RAGs of some form or not, from
    naive RAGs to graph RAGs, cash RAGs, everything in between, the chameleon, that's
    one way to extract information for what I call normal English, that's typically
    fine, so you know the New York Times or whatever, the Atlantic, and be like the
    classical type of thing.
  topic: technical
- impact_reason: Suggests that agentic frameworks (CoT, GoT) are superior to basic
    RAG for complex reasoning and multi-step information synthesis.
  relevance_score: 9
  source: llm_enhanced
  text: So an agentic approach, an agent approach like chain of thought or graph of
    thought, things like that are much more helpful.
  topic: technical
- impact_reason: 'Provides a dual strategy for hallucination control: parameter tuning
    (temperature) and grounding the agent''s reasoning against previously established,
    trusted knowledge (iterative verification).'
  relevance_score: 9
  source: llm_enhanced
  text: One is by controlling the parameters of the model, like temperature and whatnot,
    and that's sure, you can set them to be as low as possible for the benchmarks
    that you will be making. But the other side is to say, look, I already know this
    body that was previously extracted. If your information doesn't agree with that,
    maybe you take another turn and make sure that what you're doing is from the other
    side of that answer, right, so to speak, before you continue.
  topic: technical
- impact_reason: Provides empirical evidence suggesting that grounding models within
    a graph structure (likely a knowledge graph) significantly mitigates hallucination
    risks.
  relevance_score: 9
  source: llm_enhanced
  text: In fact, on the experiments we have done, on the work that we have done so
    far, we have not yet had an issue with hallucination, particularly when you're
    grounded on a graph type of framework.
  topic: technical
- impact_reason: Highlights the necessity of upfront 'scaffolding' (data preparation,
    standardization) in complex domains, specifically pointing to the challenge of
    inconsistent nomenclature (ontologies) in chemistry/biology.
  relevance_score: 9
  source: llm_enhanced
  text: As number one, there's some white glove work of kind of scaffolding here to
    make sure that we're grounding in a proper sense. And of course, in these very
    complex worlds, I mean, you know, in life sciences just as an example, it's like
    the ontologies of like the name for the same compound has 17 different names and
    representations chemically and otherwise.
  topic: business/strategy
- impact_reason: Poses a fundamental question about the relationship between specialized
    LLMs and traditional knowledge representation (ontologies), suggesting that well-trained
    models should implicitly encode this knowledge.
  relevance_score: 9
  source: llm_enhanced
  text: If you do, let's say you have a chemistry language model, a protein language
    model. If you trust, if you gave it enough data and you actually trust that your
    model learned chemistry language or protein language, a solution with protein
    language and whatnot, why would you need to know ontology to begin with? Shouldn't
    the ontology of the field that's encompassed by protein language models or chemistry
    language models be represented in some capacity or another inside those models?
  topic: technical
- impact_reason: Highlights the gap between domain expertise (focusing on molecules/biology)
    and the multimodal nature of real-world data (images, plots), which necessitates
    a broader AI approach.
  relevance_score: 9
  source: llm_enhanced
  text: Because these problems are interdisciplinary, they are multimodal, they involve
    a bunch of different, they involve images, they involve these, they involve that,
    and you may not be prepared up front for that fact, because as a scientist, I
    think of the molecules, I think of the biology, I don't think about the fact that
    the data come in images, I don't really care, because I'm going to look at the
    image and make a decision.
  topic: strategy
- impact_reason: Connects the ontology mapping problem to established computational
    complexity theory (NP-hard), explaining why manual, hierarchical approaches break
    down at scale.
  relevance_score: 9
  source: llm_enhanced
  text: If you're trying to map an object from a big ontology to a little ontology
    or vice versa, you form this cycle. And that's now classic database theory, or
    it's an NP-hard problem. It's sort of impossible to search if you're gonna be
    trapped in a loop.
  topic: technical
- impact_reason: 'Proposes a balanced approach: utilize ontologies as valuable labeling
    tools but avoid being constrained or trapped by their rigid structure, leveraging
    AI agents to facilitate this labeling.'
  relevance_score: 9
  source: llm_enhanced
  text: Let's not throw ontologies away. They're labels, they're organized labels.
    It's great, let's use all of them as much as we can by labeling all of our data
    using these ontologies. You can have agents to label ontology, it's great. That's
    not an issue, but let's not trap ourselves in ontology.
  topic: strategy
- impact_reason: 'Reiterates the strategy: use ontologies as structural aids (symmetries)
    but rely on models to bridge the gaps between them and across modalities.'
  relevance_score: 9
  source: llm_enhanced
  text: Sure, ontologies are your symmetries to some extent. Let's now use all of
    them, but not be pitied down by them. Let's use the models and connect across
    different ontologies, different modalities, different everything.
  topic: strategy
- impact_reason: Provides a highly detailed, visual, and technical analogy (using
    Hula hoops on a grid) to explain how data dimensions (like pixel values) are represented
    and constrained within the model's learned space.
  relevance_score: 9
  source: llm_enhanced
  text: Think of grayscale images. Grayscale images are collections of dots where
    the value of each dot goes from 0 to 255. When you get to 256, go back to 0. Goes
    back to 0. So you have a little, you know, you can think, we can think of it as
    an engineer, so a fan of engineers in the audience, you have like a square of
    wood with brick positions inside as if you were pouring concrete. In each brick
    position, we're going to put a value between 0 and 255. You can think of that
    as your Hula hoop. And your Hula hoop has a mark on 0, on 1, on 2, 3, 4, all the
    way to 255. Each Hula hoop in each one of the bricks that you have has one of
    such marks because it's one of the colors of the grayscale. If you had RGB, so
    if you had color images, you would have one circle for each one of the colored
    dimensions that you have.
  topic: technical
- impact_reason: Highlights the deep, underlying mathematical connection between statistical
    learning methods (like deep learning) and geometric principles, suggesting a unified
    theoretical framework.
  relevance_score: 9
  source: llm_enhanced
  text: There's a natural dictionary, a natural mapping, physics between statistics,
    which is deep learning, machine learning, statistical learning, and geometry.
  topic: technical
- impact_reason: Describes a concrete methodology for integrating knowledge across
    scientific domains using geometry, probing, and graph databases for knowledge
    retrieval.
  relevance_score: 9
  source: llm_enhanced
  text: The way we put them all together is by probing these different geometries
    with your data, asking the pertinent questions, which would be the labels that
    you have or the labels in different ontologies that are relevant and so on and
    so forth, and mapping that in a big graph database.
  topic: technical/strategy
- impact_reason: 'Identifies a major gap in current AI infrastructure: the lack of
    a unified, context-aware landscape for integrating diverse datasets.'
  relevance_score: 9
  source: llm_enhanced
  text: But what I don't think exists is this multi-modal, semantically, hand-spied,
    these other lamps, contextualized landscape where you put all your data and you
    get all these different views.
  topic: strategy
- impact_reason: A powerful statement on the unification of data structures, suggesting
    that complex relationships (edges in a graph) can be represented by matrices,
    linking graph theory directly to linear algebra.
  relevance_score: 9
  source: llm_enhanced
  text: once you figure out that it's matrices and factors and whatever, everything
    is a matrix and a vector. Your edges don't need to be just a little arrow. They
    can be matrices themselves.
  topic: technical
- impact_reason: 'A crucial cautionary note for R&D leaders: AI generation (even in
    science) is not a replacement for physical validation (the wet lab).'
  relevance_score: 9
  source: llm_enhanced
  text: there's always going to be a wet lab somewhere. Right? Like in some sense,
    generative AI... is using the hallucination to create new molecules, to create
    new drugs... and you need to test that in the lab in some capacity.
  topic: safety/strategy
- impact_reason: 'Reiterates the core thesis: contextualization, informed by fundamental
    science (like physics), is the key to unlocking the next level of AI performance
    beyond current large models.'
  relevance_score: 9
  source: llm_enhanced
  text: My strong opinion is that the way you do that is by actually contextualizing
    the information. Right? The reason I say this based on a bunch of different biases,
    part of in this case, part of my education, like that's literally what we're doing
    in physics.
  topic: strategy
- impact_reason: Illustrates the severe fragility of R&D knowledge when it is siloed
    in individual experts rather than embedded in contextualized data systems.
  relevance_score: 8
  source: llm_enhanced
  text: God forbid that person gets kidnapped by an alien, you lose all that institutional
    information, that historical information and whatnot, and this is for better or
    worse true of most if not all farmers [pharma companies].
  topic: business/strategy
- impact_reason: Positions LLMs as the next evolution of Knowledge Discovery and Data
    Mining (KDD), specifically noting that standard RAGs might be insufficient for
    complex scientific text.
  relevance_score: 8
  source: llm_enhanced
  text: One of them is just the information extraction piece, right? So there's a
    whole field of computer science called KDD, like knowledge discovery and data
    mining, dedicated to this, which is now being empowered and taken to the nuclear
    level with LLM, right? So the very first step in the direction are RAGs of some
    form or not...
  topic: technical/AI trends
- impact_reason: Directly addresses the major concern of hallucination in LLMs, framing
    it as a control problem.
  relevance_score: 8
  source: llm_enhanced
  text: The other side of that answer is just sheer hallucination, which I call you
    control.
  topic: safety
- impact_reason: Directly links high-quality domain-specific scaffolding/grounding
    to a measurable reduction in model errors (hallucinations).
  relevance_score: 8
  source: llm_enhanced
  text: If you do that well given the research domain you're focused on, you can significantly
    drop hallucinations.
  topic: business/technical
- impact_reason: Critiques the rigidity and inherent limitations of manually constructed
    ontologies when faced with the complexity and evolving nature of scientific problems.
  relevance_score: 8
  source: llm_enhanced
  text: Very quickly, when you make these Frankenstein ontologies, you bump into their
    boundaries, because that's just the nature of the beast, by definition, virtually,
    right?
  topic: strategy
- impact_reason: 'Illustrates the scale mismatch problem in ontologies: they often
    cover macro-levels well but fail to connect seamlessly to micro-levels, leading
    to mapping difficulties.'
  relevance_score: 8
  source: llm_enhanced
  text: The problem with that is that if you're doing an ontology for something big,
    like a brain, you're gonna get the brain parts... but you typically don't go all
    the way down to bundles of neurons, which are sort of a midway scale, and much
    less to the cell, to the neurons, to their components, to what's happening inside...
  topic: technical/limitations
- impact_reason: Introduces a powerful meta-strategy from physics—introducing complexity
    (like gauge symmetry) to unlock solvability—as an analogy for AI problem-solving.
  relevance_score: 8
  source: llm_enhanced
  text: There's a trick that we use in physics all the time, which is to make a problem
    worse to make it solvable, and we typically use that trick in physics when we're
    doing symmetries.
  topic: strategy
- impact_reason: This is a fundamental concept in machine learning, describing how
    models create latent representations of data, which is key to understanding model
    function.
  relevance_score: 8
  source: llm_enhanced
  text: the model is organizing that data in a space.
  topic: technical
- impact_reason: A concise summary of Topological Data Analysis (TDA) concepts applied
    to ML, emphasizing that data has an inherent geometric structure the model must
    learn.
  relevance_score: 8
  source: llm_enhanced
  text: This is the shape of your data, kind of thing.
  topic: technical
- impact_reason: Provides a concrete, high-impact application example of generative
    AI in drug discovery/life sciences.
  relevance_score: 8
  source: llm_enhanced
  text: we did, for example, a generation of novel peptides to carry drug molecules
    across different cell types and different targets and whatnot.
  topic: business/predictions
- impact_reason: A colloquial but strong assertion that the underlying mathematical
    structure (tensors/matrices) is universal across seemingly disparate fields like
    quantum computing and deep learning.
  relevance_score: 8
  source: llm_enhanced
  text: It's totals all the way down. Like put it on a test on that, or call it a
    day and then the crank.
  topic: technical
- impact_reason: 'A realistic assessment of the current state of multimodal AI: while
    promising, existing implementations are technically difficult and cumbersome.'
  relevance_score: 8
  source: llm_enhanced
  text: And so far today, multi-modal model, a model that will read an image and will
    read a piece of text or what now, which is all multi-modal we have so far, they're
    very hard. It's just it's hard to train. It's hard to use. Everything about them
    is hard.
  topic: technical
- impact_reason: Describes the common, often cumbersome, organizational process of
    creating 'Frankenstein ontologies' via committee consensus, contrasting it with
    the potential of learned representations.
  relevance_score: 7
  source: llm_enhanced
  text: What typically happens in these places, is that there are differences, strokes,
    and folks, but typically what happens is that you define some sort of master ontology
    or ontology for the one drug or problem biology that you're looking at by committee.
    So a bunch of different folks from the bio side, the chemistry side, isn't that,
    and whatnot, sit together, you have your white smoke come out, you come up with
    your Frankenstein ontology, and that's the thing that you use, right?
  topic: business/strategy
- impact_reason: 'Summarizes the goal of the proposed framework: to overcome the brittleness
    inherent in manually constructed, committee-driven knowledge systems.'
  relevance_score: 7
  source: llm_enhanced
  text: It's overcoming the downside of the brittleness of the Frankenstein stuff
    you already saw.
  topic: strategy
- impact_reason: Provides a surprisingly intuitive geometric analogy for understanding
    pixel data structure, moving beyond simple arrays to conceptualize data manifolds.
  relevance_score: 7
  source: llm_enhanced
  text: This is really what image grayscale looks like. It's a flat surface for the
    pixels and it's a circle at every pixel position with the color that you have.
  topic: technical
source: Unknown Source
summary: '## Podcast Episode Summary: AI Data Strategies for Life Sciences, Agriculture,
  and Materials Science (with Daniel Ferrante of Deloitte)


  This 40-minute episode features Daniel Ferrante, AI leader in R&D and Data Strategy
  at Deloitte, discussing the critical challenges and advanced strategies for leveraging
  data and AI—particularly Large Language Models (LLMs)—to drive efficiency and unlock
  value in highly complex R&D sectors like Life Sciences, Agriculture, and Materials
  Science.


  The core narrative revolves around moving beyond the simple acknowledgment that
  "data is the new oil" to establishing the necessary infrastructure and context to
  actually "pump" that value. Ferrante argues that the primary barrier in R&D is the
  disconnect between scientific variables (known from physics, biology, etc.) and
  the actual data collected, often due to poor data context and fragmentation across
  the R&D value chain.


  ### 1. Focus Area

  The discussion centers on **AI Data Strategy within Enterprise R&D**, specifically
  focusing on:

  *   **Contextualizing Disparate Data:** Bridging gaps between different data sets,
  modalities (images, text, numerical), and scientific ontologies.

  *   **LLM Application in Scientific Discovery:** Using LLMs to map knowledge landscapes,
  generate data labels, and provide context for proprietary data.

  *   **R&D Process Efficiency:** Reducing the "data wrangling" burden on scientists
  and enabling long-range, multimodal feedback loops across the R&D value chain (e.g.,
  from target identification to clinical trials).


  ### 2. Key Technical Insights

  *   **Contextual Mapping via LLMs:** The strategy involves using domain-specific
  LLMs (e.g., chemistry or protein language models) to create a "landscape" of learned
  knowledge. Proprietary data is then mapped onto this latent space, allowing scientists
  to see where their data clusters relative to established scientific principles.

  *   **Data as Labels, Not Just Points:** Ferrante emphasizes that the goal of R&D
  is not just generating data points, but generating meaningful **labels** (the "parabola"
  analogy for Galileo''s dots). LLMs can assist in generating these high-level scientific
  labels for experimental results.

  *   **Agentic Approaches over Naive RAGs:** For complex scientific data extraction,
  simple Retrieval-Augmented Generation (RAG) is insufficient due to the need for
  multi-step reasoning and multimodal data integration (tables, plots, text). Agentic
  approaches (like Chain of Thought or Graph of Thought) are necessary for robust,
  multi-dimensional information extraction.


  ### 3. Business/Investment Angle

  *   **Reducing Institutional Knowledge Loss:** A major business risk is the loss
  of critical, undocumented knowledge when key personnel leave, as R&D value chains
  often rely on single individuals tracking information across silos. AI contextualization
  mitigates this risk.

  *   **Shifting Scientist Focus:** The primary ROI is enabling scientists to focus
  on actual science and hypothesis testing rather than spending up to 80% of their
  time wrangling and connecting disparate data sources.

  *   **Ontology Management as a Bottleneck:** The traditional method of creating
  "Frankenstein ontologies" by committee is brittle and quickly hits boundaries when
  interdisciplinary data (like images alongside molecular data) is introduced.


  ### 4. Notable Companies/People

  *   **Daniel Ferrante (Deloitte):** The featured expert, leading AI data strategy
  for R&D.

  *   **Deloitte''s Atlas:** Mentioned as Deloitte''s multimodal framework used to
  bridge gaps between disparate data sets and ontologies.

  *   **Academic Reproducibility Crisis:** Referenced via studies suggesting 80-85%
  of cancer studies are irreproducible, highlighting that conflicting data findings
  may be inherent to the research landscape, not just AI hallucinations.


  ### 5. Future Implications

  The industry is moving toward a paradigm where AI acts as a contextualizing layer,
  allowing for holistic, multimodal exploration of the data landscape. This will facilitate
  the discovery of long-range correlations previously missed due to siloed data and
  linear process thinking. The future involves using LLMs to manage and connect complex,
  multi-scale ontologies without being trapped by their inherent brittleness, effectively
  using them as "symmetries" to solve harder problems.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Professionals, R&D Leadership, Data
  Strategists, and Executives** within the **Life Sciences, Pharmaceutical, Agriculture
  Technology (AgTech), and Advanced Materials** sectors who are responsible for data
  governance, AI implementation, and maximizing R&D productivity.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- apple
title: AI Data Strategies for Life Sciences Agriculture and Materials Science - with
  Daniel Ferrante of Deloitte
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 69
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 3
  prominence: 0.3
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 1
  prominence: 0.1
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 14:19:39 UTC -->
