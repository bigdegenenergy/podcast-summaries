---
companies:
- category: unknown
  confidence: medium
  context: Hi, I'm Steph Strickland with Geekwire Studios. Your host for Does Compute
  name: Steph Strickland
  position: 8
- category: unknown
  confidence: medium
  context: Hi, I'm Steph Strickland with Geekwire Studios. Your host for Does Compute,
    the podcast where we
  name: Geekwire Studios
  position: 30
- category: unknown
  confidence: medium
  context: h Strickland with Geekwire Studios. Your host for Does Compute, the podcast
    where we delve into the latest innov
  name: Does Compute
  position: 62
- category: unknown
  confidence: medium
  context: s in computer science with the help of experts at Carnegie Mellon University
    School of Computer Science. I'd like to introduce you ri
  name: Carnegie Mellon University School
  position: 179
- category: unknown
  confidence: medium
  context: f experts at Carnegie Mellon University School of Computer Science. I'd
    like to introduce you right now to Virginia
  name: Computer Science
  position: 216
- category: unknown
  confidence: medium
  context: r Science. I'd like to introduce you right now to Virginia Smith. She is
    the Leonardo Associate Professor of Machi
  name: Virginia Smith
  position: 273
- category: unknown
  confidence: medium
  context: oduce you right now to Virginia Smith. She is the Leonardo Associate Professor
    of Machine Learning at Carnegie Mellon University
  name: Leonardo Associate Professor
  position: 300
- category: unknown
  confidence: medium
  context: Smith. She is the Leonardo Associate Professor of Machine Learning at Carnegie
    Mellon University and at SILAP. Her r
  name: Machine Learning
  position: 332
- category: unknown
  confidence: medium
  context: onardo Associate Professor of Machine Learning at Carnegie Mellon University
    and at SILAP. Her research broadly encompasses ma
  name: Carnegie Mellon University
  position: 352
- category: unknown
  confidence: medium
  context: d some are more immediate concerns around safety. And I think for both
    of these concerns, the main issue
  name: And I
  position: 1304
- category: unknown
  confidence: medium
  context: And I started out by working in this space called Federated Learning. And
    Federated Learning, the goal is that you wan
  name: Federated Learning
  position: 8851
- category: unknown
  confidence: medium
  context: working in this space called Federated Learning. And Federated Learning,
    the goal is that you want to be able to integrat
  name: And Federated Learning
  position: 8871
- category: tech
  confidence: high
  context: cific organizations, I've worked with groups like Google and Apple and
    Meta that are deploying this across
  name: Google
  position: 9160
- category: tech
  confidence: high
  context: izations, I've worked with groups like Google and Apple and Meta that are
    deploying this across mobile ph
  name: Apple
  position: 9171
- category: tech
  confidence: high
  context: I've worked with groups like Google and Apple and Meta that are deploying
    this across mobile phones. So
  name: Meta
  position: 9181
- category: unknown
  confidence: medium
  context: that's actually a scenario that we're okay with. If I have my own model,
    I would like it to know inform
  name: If I
  position: 14751
- category: unknown
  confidence: medium
  context: marked. And you may wonder why this is a problem. But I think this is a
    real issue for incentivizing the
  name: But I
  position: 17934
- category: tech
  confidence: high
  context: you could say, "Hey, look at this new output from OpenAI's latest model.
    This is really terrible." And I h
  name: Openai
  position: 18258
- category: unknown
  confidence: medium
  context: make us much more efficient, much more effective. What I am particularly
    excited about is the potential fo
  name: What I
  position: 19553
- category: unknown
  confidence: medium
  context: ntial for the use of these AI systems in science. So I think if we're using
    them for the right reasons i
  name: So I
  position: 19651
- category: unknown
  confidence: medium
  context: y learn all the information about the book series Harry Potter. So you
    can sort of fine-tune the model so it's m
  name: Harry Potter
  position: 23088
- category: ai_research
  confidence: high
  context: The host's organization, where the expert (Virginia Smith) is affiliated.
  name: Carnegie Mellon University School of Computer Science
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The university where the expert is a professor.
  name: Carnegie Mellon University
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: An organization Virginia Smith is affiliated with, likely a research lab
    or institute focused on AI/ML.
  name: SILAP
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An organization working on child safety issues, collaborating with the
    researcher on the misuse of AI models (specifically regarding CSAM generation).
  name: Thorn
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a global company deploying Federated Learning across mobile
    phones for developing language models (e.g., next word prediction).
  name: Google
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a global company deploying Federated Learning across mobile
    phones for developing language models.
  name: Apple
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a global company deploying Federated Learning across mobile
    phones for developing language models.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a model developer whose reputation could be damaged if malicious
    actors use watermarking to falsely attribute terrible output to their latest model.
  name: OpenAI
  source: llm_enhanced
date: 2025-10-06 14:00:00 +0000
duration: 33
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: be thinking of before we just throw up our hands and go, "It's too much,
    it's too stressful, too big to get ahead of
  text: we should be thinking of before we just throw up our hands and go, "It's too
    much, it's too stressful, too big to get ahead of.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: your work? Yeah, I don't have to look too far into the future. I think
    that there are things that already concern me a bit with these systems. And for
    me, the turning point was really some collaborations I've had recently with this
    organization Thorn, which looks at issues of child safety. So we're already seeing,
    from my perspective, some major safety concerns that are happening now in the
    realm of child safety, where some of these models are being used to develop child
    sexual abuse material at scales we've never seen before. You can take images of
    children that are not sexualized in any way and you can turn them into sexualized
    content. You can use that to victimize children for things like extortion. And
    so a downside of this technology
  text: the future of your work? Yeah, I don't have to look too far into the future.
    I think that there are things that already concern me a bit with these systems.
    And for me, the turning point was really some collaborations I've had recently
    with this organization Thorn, which looks at issues of child safety. So we're
    already seeing, from my perspective, some major safety concerns that are happening
    now in the realm of child safety, where some of these models are being used to
    develop child sexual abuse material at scales we've never seen before. You can
    take images of children that are not sexualized in any way and you can turn them
    into sexualized content. You can use that to victimize children for things like
    extortion. And so a downside of this technology is it's making it very, very easy
    to do this and it's making it very, very difficult to find the perpetrators.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/042637dcfc9e42a090f2744eead5305d/
processing_date: 2025-10-06 19:23:43 +0000
quotes:
- length: 117
  relevance_score: 4
  text: And it also, you can watch the speed with which people in the wild will jailbreak
    a large language model or a chatbot
  topics: []
- length: 176
  relevance_score: 3
  text: So to give you some examples and to tie this to some specific organizations,
    I've worked with groups like Google and Apple and Meta that are deploying this
    across mobile phones
  topics: []
- length: 185
  relevance_score: 3
  text: And what we found is that there's a real discrepancy between what we think
    these technologies can do or what we hope that they'll be able to do and what
    they can actually do in practice
  topics: []
- length: 236
  relevance_score: 3
  text: Because then what you could do is you could take a bunch of text, you could,
    I don't know, include a bunch of unsafe material or just degrade the quality of
    it, and you could say, "Hey, look at this new output from OpenAI's latest model
  topics: []
- impact_reason: Highlights the immediate, scalable threat of generative AI in creating
    disinformation, a critical current societal challenge.
  relevance_score: 10
  source: llm_enhanced
  text: one set of concerns is around the terrific ability for these systems to generate
    misinformation and disinformation. This is something that obviously we've had
    the ability to do for a while, but these systems are making it much, much easier
    to generate this type of content at great scales.
  topic: safety
- impact_reason: Provides a stark, real-world example of immediate, catastrophic harm
    being caused by current AI capabilities (CSAM generation), moving beyond theoretical
    risks.
  relevance_score: 10
  source: llm_enhanced
  text: we're already seeing, from my perspective, some major safety concerns that
    are happening now in the realm of child safety, where some of these models are
    being used to develop child sexual abuse material at scales we've never seen before.
  topic: safety
- impact_reason: 'A crucial insight into the current state of AI: ease of deployment
    coupled with poor interpretability, fueling long-term risk concerns.'
  relevance_score: 10
  source: llm_enhanced
  text: these systems are actually quite simple to implement, and we understand a
    lot less about them than people think we do.
  topic: technical/safety
- impact_reason: Advocates for 'safety by design' and emphasizes data provenance/understanding
    as the essential first step for mitigating harm before deployment.
  relevance_score: 10
  source: llm_enhanced
  text: we need to develop systems that are safe by design. So before these models
    are released to the public, for example, this seems like it should be an obvious
    thing, but we need to have a great understanding of the training data that was
    used to create the models.
  topic: safety/strategy
- impact_reason: A concise summary of the multi-faceted benefits derived from personalized
    models in federated learning, touching on core ML challenges (accuracy, fairness,
    robustness, privacy).
  relevance_score: 10
  source: llm_enhanced
  text: So we can improve the overall accuracy of the models. It can make them more
    fair, less biased. It can improve robustness to issues of noisy data or adversaries.
    It can actually improve some of the privacy, utility guarantees that you have
    as well.
  topic: technical/federated learning
- impact_reason: Identifies the fundamental conflict between standard DP guarantees
    (which protect against inference about *any* individual) and the goal of personalized
    models (which *should* reflect the individual's data).
  relevance_score: 10
  source: llm_enhanced
  text: And when you're learning personalized models, you get into this weird scenario
    where you don't actually care about, if I have a model that's personalized just
    to my data, I don't care that my data is private to my model. So that's actually
    a scenario that we're okay with.
  topic: technical/privacy
- impact_reason: A critical warning about the gap between regulatory/public expectation
    and the practical capabilities of AI safety techniques like watermarking.
  relevance_score: 10
  source: llm_enhanced
  text: What we found is that there's a real discrepancy between what we think these
    technologies can do or what we hope that they'll be able to do and what they can
    actually do in practice. And one of these areas is watermarking.
  topic: safety/ethics
- impact_reason: 'Identifies a crucial, less-discussed problem: robust watermarks
    can be exploited to falsely attribute harmful content to model developers, creating
    a disincentive for adoption.'
  relevance_score: 10
  source: llm_enhanced
  text: Now, the flip side to this, the downside to these robust watermarks is that
    it can be very easy then to take an image or take text and to manipulate it in
    various ways and have it still appear watermarked. And you may wonder why this
    is a problem. But I think this is a real issue for incentivizing the model developers
    to actually produce these solutions.
  topic: safety/business
- impact_reason: 'Details the specific failure modes of current unlearning techniques:
    sensitivity to prompt perturbation or subsequent benign fine-tuning.'
  relevance_score: 10
  source: llm_enhanced
  text: 'But if you do any one of the following: if you perturb in various ways the
    questions that are being asked of the models, the prompts that are being given,
    or if you take the model and perturb that, so you maybe fine-tune it even on entirely
    benign knowledge, you can see that it can now reproduce that supposedly unlearned
    information.'
  topic: technical/safety
- impact_reason: A profound statement on the current limitations of interpretability
    and the illusion of unlearning—suppression vs. deletion of knowledge.
  relevance_score: 10
  source: llm_enhanced
  text: And so what this tells me is that we don't yet have a great understanding
    of how these models are actually working, and that in particular for unlearning,
    that a lot of the information is actually still retained in the network in various
    ways in the model. So it might sort of suppress that particular output, but it
    doesn't mean that we've actually unlearned that information.
  topic: technical/limitations
- impact_reason: Directly connects the failure of unlearning (demonstrated via Harry
    Potter example) to severe safety implications, suggesting that safety guardrails
    based on unlearning are currently brittle.
  relevance_score: 10
  source: llm_enhanced
  text: And I think this has real concerning ramifications in terms of actually using
    unlearning for something maybe less benign than learning about Harry Potter. But
    if we think we could use this to unlearn, you know, sort of unsafe behaviors,
    I think it actually could be quite easy to just interact with the model and get
    back that unsafe information.
  topic: safety/ethics
- impact_reason: This is a critical insight into the limitations of current 'machine
    unlearning' techniques, suggesting suppression is not true erasure, which has
    major implications for data privacy and compliance.
  relevance_score: 10
  source: llm_enhanced
  text: a lot of the information is actually still retained in the network in various
    ways in the model. So it might sort of suppress that particular output, but it
    doesn't mean that we've actually unlearned that information.
  topic: technical/safety
- impact_reason: 'Highlights a severe safety and security risk: if unlearning fails
    to remove underlying knowledge, models can still be prompted (jailbroken) to reveal
    sensitive or harmful information.'
  relevance_score: 10
  source: llm_enhanced
  text: if we think we could use this to unlearn, you know, sort of unsafe behaviors,
    I think it actually could be quite easy to just interact with the model and get
    back that unsafe information.
  topic: safety/limitations
- impact_reason: 'Clearly delineates the two major categories of AI safety concerns:
    immediate societal harm versus long-term existential risk, providing a framework
    for the subsequent discussion.'
  relevance_score: 9
  source: llm_enhanced
  text: I think that broadly, there are two big safety concerns that people have right
    now. Some are long-term, sort of existential risk concerns around safety in AI,
    and some are more immediate concerns around safety.
  topic: safety
- impact_reason: Pinpoints data memorization and reproduction of sensitive information
    as a core, immediate privacy risk in large-scale ML systems.
  relevance_score: 9
  source: llm_enhanced
  text: Another safety concern that I am quite worried about, which is also immediate,
    has to do with the privacy of the data that we are generating as users. So these
    systems rely on tons of data... the harm here comes from the ability for these
    systems to memorize this data very, very effectively and to be able to reproduce
    this sort of sensitive information.
  topic: safety
- impact_reason: Articulates the growing gap between widespread AI adoption and the
    industry's current inability to guarantee reliable, understood behavior.
  relevance_score: 9
  source: llm_enhanced
  text: we're going to see, you know, wider swathes of society interacting with and
    using these systems, and yet we're not at the point where we, you know, can really
    ensure that the behavior is going to be reliable and that we really understand
    what's happening under the hood with them.
  topic: predictions/strategy
- impact_reason: Directly links the practice of large-scale internet data scraping
    to safety failures, highlighting the challenge of data transparency.
  relevance_score: 9
  source: llm_enhanced
  text: One of the reasons that you have things like this issue with child sexual
    abuse material, CSAM material, is that many of these models are created by gathering
    vast amounts of data. A really great source of this data is the internet. And
    because they're gathering so much data, scraping it from the internet, it can
    be difficult to have a precise knowledge of every single data point that is used.
  topic: safety/technical
- impact_reason: 'Suggests a key future research direction: applying successful privacy
    techniques from FL to the development of LLMs.'
  relevance_score: 9
  source: llm_enhanced
  text: The dream would be to take some of these, you know, things that we've developed
    in the space of Federated Learning and maybe think about how we could use, you
    know, similar ideas in the space of developing these large language models.
  topic: predictions/technical
- impact_reason: 'Details a concrete technique for improving fairness and robustness
    in distributed learning: upweighting data from underperforming subgroups.'
  relevance_score: 9
  source: llm_enhanced
  text: So one thing that we have looked at in our work, we've actually combated this
    from a couple of perspectives. One is that we can use techniques from fair machine
    learning to try to ensure a better balance across this diversity of data. So one
    way to do this is to take users where the model is performing poorly and sort
    of upweigh the importance of their data.
  topic: technical/fairness
- impact_reason: Presents model personalization as a highly effective strategy to
    improve accuracy, fairness, robustness, and privacy in federated settings.
  relevance_score: 9
  source: llm_enhanced
  text: Another approach that we found to be very, very effective has been to sort
    of decompose the problem and to develop models that are more personalized to the
    unique behavior that's coming from each of the users.
  topic: technical/strategy
- impact_reason: This describes a practical technique (upweighting outlier data) within
    fair machine learning to improve model performance and fairness for underserved
    groups in a decentralized setting.
  relevance_score: 9
  source: llm_enhanced
  text: So one way to do this is to take users where the model is performing poorly
    and sort of upweigh the importance of their data. So the model is sort of nudged
    to be a better fit to that data that's happening at the outlier.
  topic: technical/fairness
- impact_reason: Strong strategic insight against the 'one-size-fits-all' approach
    in federated learning, advocating for tailored, personalized models.
  relevance_score: 9
  source: llm_enhanced
  text: So we've spent a lot of time thinking about this idea of model personalization,
    of not developing just one sort of monolithic model for all of the data that we're
    seeing in a federated network, but developing maybe multiple models, models that
    are personalized to some of the unique behavior or sometimes unique devices that
    are generating data in a network.
  topic: strategy/federated learning
- impact_reason: Explains the utility of Joint DP as a necessary technical relaxation
    to enable personalized FL while maintaining privacy integrity.
  relevance_score: 9
  source: llm_enhanced
  text: And this is where joint differential privacy comes in. It's basically a relaxation
    that allows you to handle this edge case to develop these personalized models—so
    models that are unique to the behavior from different users—but you still have
    a meaningful privacy guarantee.
  topic: technical/privacy
- impact_reason: 'Captures the central tension in AI safety research: the reactive,
    problem-solving nature that often leads to new, unforeseen issues.'
  relevance_score: 9
  source: llm_enhanced
  text: Overall, are you optimistic that this, that we can strike this balance across
    all of the concerns that you've elaborated? Because it feels like hearing you
    discuss it, a game of whack-a-mole where you can find a solution which then creates
    a new set of problems. And perhaps that's just the way it is.
  topic: strategy/safety
- impact_reason: Expresses strong optimism regarding AI's positive potential, specifically
    highlighting scientific application as a key area of excitement.
  relevance_score: 9
  source: llm_enhanced
  text: I do think that when I step back, I think that these AI systems can provide
    incredible transformation in society in a positive way. I think that it can make
    us much more efficient, much more effective. What I am particularly excited about
    is the potential for the use of these AI systems in science.
  topic: predictions/strategy
- impact_reason: Shifts focus from long-term existential risk to immediate, tangible
    harms like misinformation, grounding the safety discussion in current reality.
  relevance_score: 9
  source: llm_enhanced
  text: At the same time, I am concerned not even about these sort of long-term existential
    sort of risks from AI, but immediate things that are already happening. I think
    the whack-a-mole with something like watermarking, so obviously the dream of something
    like watermarking is that it can help to combat issues like misinformation or
    disinformation.
  topic: safety/ethics
- impact_reason: Uses the common phenomenon of jailbreaking as real-world evidence
    supporting the fragility of machine unlearning methods.
  relevance_score: 9
  source: llm_enhanced
  text: You can watch the speed with which people in the wild will jailbreak a large
    language model or a chatbot. It is fascinating and a great illustration of why
    machine unlearning is more fraught than maybe it seems at the surface level.
  topic: safety/limitations
- impact_reason: 'Defines the core value proposition and challenge of Federated Learning
    (FL): balancing utility and privacy, a key trend in decentralized AI.'
  relevance_score: 9
  source: llm_enhanced
  text: a lot of the work that we had done in the federated learning space, which
    is aiming to again strike this balance between sort of protecting sensitive information
    while still developing models that are useful...
  topic: technical/strategy
- impact_reason: Clearly states the fundamental data sensitivity challenge in healthcare
    AI, justifying the need for privacy-preserving techniques like FL.
  relevance_score: 9
  source: llm_enhanced
  text: The issue, of course, is that data at hospitals is highly sensitive, right?
    So it's coming from patients, it includes their medical history, and includes
    whether or not they, you know, have this disease, for example.
  topic: safety/business
- impact_reason: Demonstrates a tangible success story where advanced privacy technology
    (FL) was successfully applied to solve a critical, real-world problem, validating
    the technology.
  relevance_score: 9
  source: llm_enhanced
  text: what we worked on was it was sort of porting some of our technologies from
    federated learning to this particular application. We actually won this challenge.
  topic: business/technical
- impact_reason: Raises the stakes for unlearning research by contrasting trivial
    use cases (Harry Potter) with serious ones (removing unsafe knowledge), implying
    potential misuse or failure in critical areas.
  relevance_score: 9
  source: llm_enhanced
  text: I think this has real concerning ramifications in terms of actually using
    unlearning for something maybe less benign than learning about Harry Potter.
  topic: safety
- impact_reason: 'Provides the fundamental, unifying goal across all AI safety discussions:
    the principle of ''do no harm.'''
  relevance_score: 8
  source: llm_enhanced
  text: I think that for both of these concerns, the main issue is developing systems
    that do no harm, right? So they aren't harming society in various ways.
  topic: safety
- impact_reason: 'A concise definition of Federated Learning and its primary objective:
    balancing efficiency with privacy across distributed data silos.'
  relevance_score: 8
  source: llm_enhanced
  text: Federated Learning, the goal is that you want to be able to integrate data
    that's coming from many different data silos in a way that is efficient and ideally
    privacy-preserving.
  topic: technical
- impact_reason: Offers an optimistic view on the potential of privacy-preserving
    ML to achieve both utility and strong privacy guarantees at scale.
  relevance_score: 8
  source: llm_enhanced
  text: we can use technologies from privacy-preserving machine learning to strike
    actually a really nice balance here where we are able to preserve the privacy
    of individual users in the system, but we're still able to develop models that
    are effective for our end goal in machine learning.
  topic: technical/privacy
- impact_reason: Strong strategic advice against monolithic model development in favor
    of personalized, localized models for distributed systems.
  relevance_score: 8
  source: llm_enhanced
  text: we've spent a lot of time thinking about this idea of model personalization,
    of not developing just one sort of monolithic model for all of the data that we're
    seeing in a federated network, but developing maybe multiple models, models that
    are personalized to some of the unique behavior or sometimes unique devices that
    are generating data in a network.
  topic: strategy/technical
- impact_reason: Introduces a specific, advanced privacy technique tailored for the
    complexities introduced by personalized models in federated settings.
  relevance_score: 8
  source: llm_enhanced
  text: So joint differential privacy is something that we looked at that was specific
    to this idea of personalized federated learning.
  topic: technical/privacy
- impact_reason: A clear, high-level definition of the core trade-off in Differential
    Privacy (DP).
  relevance_score: 8
  source: llm_enhanced
  text: So differential privacy, first of all, the goal in differential privacy, very
    broadly, is to try to learn as much as possible about a group of data while learning
    as little as possible about the individuals that generated that data.
  topic: safety/privacy
- impact_reason: 'Clear explanation of the ideal goal of AI watermarking: imperceptible
    attribution.'
  relevance_score: 8
  source: llm_enhanced
  text: So in watermarking, the goal, just like with an image watermark, you can imagine
    the same thing for language. It's basically this imperceptible signal that would
    appear on the image or in the text that allows you to identify that that information
    is coming from a model... without degrading the quality at all.
  topic: safety/attribution
- impact_reason: 'Highlights the primary, well-known vulnerability of current watermarking
    techniques: susceptibility to removal.'
  relevance_score: 8
  source: llm_enhanced
  text: 'One, I think this is the first thing that people think of: it''s very easy
    to remove these watermarks. So you can manipulate the text in various ways...
    And then you can remove the watermark that exists.'
  topic: safety/limitations
- impact_reason: A balanced conclusion on AI's dual nature, emphasizing that the critical
    control points are data access and output generation mechanisms.
  relevance_score: 8
  source: llm_enhanced
  text: I think in some ways it can provide things that are quite exciting for society
    at large. But I also think that there are some things we're really going to have
    to reckon with in terms of how we are using these technologies. And I think especially
    what data we're giving them access to and what data the models are using to produce
    their outputs.
  topic: strategy/safety
- impact_reason: Articulates the strong appeal and intuitive logic behind Machine
    Unlearning for safety purposes.
  relevance_score: 8
  source: llm_enhanced
  text: I think it's a very enticing solution to the problem if we can get it to work.
    We spend a lot of time thinking about how to get these models to learn specific
    information. So why can't we get them to unlearn some information, right? Why
    can't we get them to just anything that is sort of unsafe? Like you said, knowledge
    of how to build a bomb.
  topic: safety/technical
- impact_reason: Provides a concrete, high-stakes real-world application (pandemic
    forecasting) driving AI research, showing government investment in AI for public
    health.
  relevance_score: 8
  source: llm_enhanced
  text: we had looked at this problem of pandemic forecasting across hospitals. This
    was actually, we started looking at this, this was a competition put on by the
    US and UK governments after the COVID pandemic.
  topic: predictions/strategy
- impact_reason: 'Articulates a major strategic goal for collaborative AI: pooling
    decentralized data for superior predictive modeling in critical domains.'
  relevance_score: 8
  source: llm_enhanced
  text: what if we could leverage information about pandemic risk across multiple
    organizations? This seems like it would be a very natural thing to want to do.
  topic: strategy
- impact_reason: Provides context on the speaker's background, linking privacy concerns
    directly to the development of privacy-preserving techniques like Federated Learning
    (FL).
  relevance_score: 7
  source: llm_enhanced
  text: My journey to getting involved with AI safety started from this privacy perspective.
    And I started out by working in this space called Federated Learning.
  topic: technical/strategy
- impact_reason: Indicates successful transition from academic/research concepts (like
    FL) into practical, deployable solutions.
  relevance_score: 7
  source: llm_enhanced
  text: we've seen some really nice translation of some of our work to real-world
    applications.
  topic: business/strategy
- impact_reason: Reinforces the theme of AI interpretability/opacity (the 'black box'
    problem) as a core challenge underpinning both unlearning and safety.
  relevance_score: 7
  source: llm_enhanced
  text: It makes perfect sense. You talked earlier about how much we don't really
    know about what's going on under the hood, and that is the perfect button to that
    conversation.
  topic: technical/limitations
- impact_reason: A prompt that steers the conversation toward concrete, high-impact
    applications of AI beyond typical consumer tech, focusing on societal benefit.
  relevance_score: 6
  source: llm_enhanced
  text: I think of pandemic forecasting or something like that. Give me some tangible
    examples.
  topic: strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Teaching Models to Forget Information


  This 32-minute episode of "Does Compute" features a discussion with **Virginia Smith**,
  Leonardo Associate Professor of Machine Learning at Carnegie Mellon University,
  focusing on the critical challenges of **safety, privacy, and reliability** in large-scale
  machine learning systems, particularly Large Language Models (LLMs). The central
  narrative revolves around the gap between the perceived capabilities of current
  AI safety techniques (like watermarking and unlearning) and their actual robustness
  in practice.


  ---


  ### 1. Focus Area

  The primary focus is **AI Safety and Machine Learning Robustness**, specifically
  addressing immediate societal harms caused by current models, including the generation
  of **misinformation/disinformation** and **privacy violations** (data memorization).
  The discussion heavily covers technical solutions being researched to mitigate these
  risks, such as Federated Learning, Differential Privacy, AI Watermarking, and Machine
  Unlearning.


  ### 2. Key Technical Insights

  *   **Federated Learning & Personalization for Privacy:** Federated Learning allows
  for privacy-preserving model training across decentralized data silos (like mobile
  phones). Effective implementation often requires **model personalization**—developing
  models tailored to unique user behavior—which improves fairness, robustness, and
  accuracy, rather than relying on a single monolithic model.

  *   **Joint Differential Privacy (JDP):** JDP is a necessary relaxation of traditional
  Differential Privacy (DP) when implementing personalized federated learning. It
  provides meaningful privacy guarantees against the *rest* of the network while acknowledging
  that a user''s personalized model *should* retain information about that user''s
  own data.

  *   **Fragility of AI Safety Techniques:** Current state-of-the-art techniques like
  **Watermarking** are easily defeated (either by removal or by malicious actors using
  robust watermarks to frame model developers). Similarly, **Machine Unlearning**
  (teaching models to forget specific data, like copyrighted or unsafe material) appears
  effective only on narrow benchmarks; slight perturbations (like fine-tuning on benign
  related data) can cause the model to perfectly reproduce the "unlearned" information,
  suggesting the knowledge remains latent in the network.


  ### 3. Business/Investment Angle

  *   **Regulatory Preemption:** Global companies (Google, Apple, Meta) are actively
  deploying privacy-preserving ML techniques like Federated Learning ahead of anticipated
  regulation, indicating a strategic move to establish robust internal frameworks.

  *   **Risk of Misattribution and Reputational Damage:** The fragility of watermarking
  presents a business risk where malicious actors could easily generate degraded or
  unsafe content, falsely attribute it to a major model developer via the watermark,
  and damage their reputation.

  *   **Safety by Design Imperative:** There is a critical need for better understanding
  and auditing of training data *before* model release, as current data scraping practices
  (especially from the open internet) introduce significant safety liabilities (e.g.,
  exposure to CSAM).


  ### 4. Notable Companies/People

  *   **Virginia Smith (CMU):** The expert guest, whose research focuses on safety,
  optimization, and distributed systems, particularly in privacy-preserving ML.

  *   **Thorn:** An organization mentioned that collaborates with researchers on child
  safety issues, highlighting the immediate, real-world harm caused by generative
  models being used to create child sexual abuse material (CSAM).

  *   **Google, Apple, Meta:** Mentioned as major players deploying Federated Learning
  at scale on mobile devices for applications like next-word prediction.


  ### 5. Future Implications

  The industry is heading toward a realization that current "patch" solutions for
  safety (watermarking, unlearning) are insufficient because they do not address the
  fundamental lack of understanding regarding how information is stored and retrieved
  within large models. Future efforts must focus on **"safe by design"** principles,
  starting with rigorous auditing of training data and developing fundamentally more
  transparent and reliable methods for controlling model knowledge. The immediate
  future suggests an escalation of misuse, particularly in areas like child safety
  and disinformation, outpacing the effectiveness of current countermeasures.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Researchers, Data Scientists, AI Ethics
  and Policy Professionals, and Technology Executives** concerned with the governance,
  risk management, and long-term viability of deploying large-scale generative models.'
tags:
- artificial-intelligence
- ai-infrastructure
- google
- apple
- meta
- openai
title: Teaching Models to Forget Information
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 43
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 2
  prominence: 0.2
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 19:23:43 UTC -->
