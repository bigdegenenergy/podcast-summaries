---
companies:
- category: unknown
  confidence: medium
  context: Hey folks, Stephen Johnson here, co-founder of Notebook LM. As an author,
    I'
  name: Stephen Johnson
  position: 11
- category: unknown
  confidence: medium
  context: Hey folks, Stephen Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 47
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. If I'm an
    application developer, if five, 10
  name: Google
  position: 413
- category: unknown
  confidence: medium
  context: you brainstorm. Try it at notebooklm.google.com. If I'm an application
    developer, if five, 10 years ago
  name: If I
  position: 425
- category: unknown
  confidence: medium
  context: s a gold mine, it's becoming an actual liability. In Europe, for example,
    there's GDPR and data privacy. We h
  name: In Europe
  position: 537
- category: unknown
  confidence: medium
  context: nd data privacy. We have California data privacy. In China, you actually
    need to pay a data tax if you're us
  name: In China
  position: 625
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the Twombo AI podcast. I
    am your host, Sam Charrington. Today I
  name: Twombo AI
  position: 998
- category: unknown
  confidence: medium
  context: episode of the Twombo AI podcast. I am your host, Sam Charrington. Today
    I'm joined by Ilia Polosuchen. Ilia is a c
  name: Sam Charrington
  position: 1033
- category: unknown
  confidence: medium
  context: ombo AI podcast. I am your host, Sam Charrington. Today I'm joined by Ilia
    Polosuchen. Ilia is a co-founder
  name: Today I
  position: 1050
- category: unknown
  confidence: medium
  context: m your host, Sam Charrington. Today I'm joined by Ilia Polosuchen. Ilia
    is a co-founder of Nira AI, but is perhaps
  name: Ilia Polosuchen
  position: 1070
- category: unknown
  confidence: medium
  context: oined by Ilia Polosuchen. Ilia is a co-founder of Nira AI, but is perhaps
    best known as a co-author of the
  name: Nira AI
  position: 1111
- category: unknown
  confidence: medium
  context: haps best known as a co-author of the now famous "Attention Is All You
    Need" paper, which introduced the transformer. Before
  name: Attention Is All You Need
  position: 1180
- category: unknown
  confidence: medium
  context: uch in machine learning and AI research. I joined Google Research because
    I saw the cat neuron paper, if folks reme
  name: Google Research
  position: 1833
- category: unknown
  confidence: medium
  context: e. And so in 2017, I left, and with my co-founder Alex Kudanaf, we started
    Nira AI. Was the idea that how do we
  name: Alex Kudanaf
  position: 2763
- category: unknown
  confidence: medium
  context: aced was the students were in China, they were in Eastern Europe, they
    were in South Estonia. In all of these coun
  name: Eastern Europe
  position: 4133
- category: unknown
  confidence: medium
  context: China, they were in Eastern Europe, they were in South Estonia. In all
    of these countries, there's some kind of
  name: South Estonia
  position: 4162
- category: unknown
  confidence: medium
  context: China, people don't have bank accounts, they have WeChat Pay. In Ukraine,
    for example, you need to sell half o
  name: WeChat Pay
  position: 4306
- category: unknown
  confidence: medium
  context: e don't have bank accounts, they have WeChat Pay. In Ukraine, for example,
    you need to sell half of your dolla
  name: In Ukraine
  position: 4318
- category: unknown
  confidence: medium
  context: already in the pocket." And so that's kind of how Near Protocol was born.
    We launched it in 2020. It's one of the
  name: Near Protocol
  position: 5378
- category: tech
  confidence: high
  context: e interesting thing happened is the hardware. So, Nvidia hardware and Intel,
    both kind of at a similar tim
  name: Nvidia
  position: 8902
- category: tech
  confidence: high
  context: happened is the hardware. So, Nvidia hardware and Intel, both kind of at
    a similar time, enabled this mod
  name: Intel
  position: 8922
- category: unknown
  confidence: medium
  context: assembly, like C list, like special instructions. And Intel, in 2024, mid-2024
    released on the new fifth-gene
  name: And Intel
  position: 9855
- category: unknown
  confidence: medium
  context: ', but also private to the same level as local AI. Am I parsing that correctly?
    I mean, I run some of the'
  name: Am I
  position: 12814
- category: unknown
  confidence: medium
  context: So what's a specific example of that happening? So Mistral gave its weights
    to Hugging Face and it didn't do
  name: So Mistral
  position: 16084
- category: tech
  confidence: high
  context: of that happening? So Mistral gave its weights to Hugging Face and it didn't
    doubt one for chat. Oh, well, I had
  name: Hugging Face
  position: 16115
- category: tech
  confidence: high
  context: nd then joined in network. But yeah, I mean, like Amazon, you know, data
    center can repurpose itself to be
  name: Amazon
  position: 18427
- category: unknown
  confidence: medium
  context: ngs that you may not even have on the local host. Because I mean, local
    host, you still can access the hard d
  name: Because I
  position: 18802
- category: tech
  confidence: high
  context: uropean right now, I'm in Lisbon right now. I use OpenAI. OpenAI, I train
    on my data and then I go and I i
  name: Openai
  position: 20617
- category: tech
  confidence: high
  context: y their fine, like I mean, similar how Google and Facebook have paid, you
    know, billions of dollars in fines
  name: Facebook
  position: 21127
- category: unknown
  confidence: medium
  context: let's go back to like Google, right, a Facebook. Like Google and Facebook
    learned from user behavior, like dir
  name: Like Google
  position: 23799
- category: unknown
  confidence: medium
  context: 'then dealing with all the repercussions of that.


    So I had asked about the, you know, this like creating'
  name: So I
  position: 25888
- category: unknown
  confidence: medium
  context: t's payable. Like, for example, for payable data, New York Times can contribute
    their data fully privately into se
  name: New York Times
  position: 28917
- category: unknown
  confidence: medium
  context: a more interesting kind of ecosystem of results. Like I think, you know,
    there are people that do that ki
  name: Like I
  position: 32549
- category: unknown
  confidence: medium
  context: . It is relatively small. It is relatively small. But I think the idea
    here and kind of I think everybody
  name: But I
  position: 33546
- category: unknown
  confidence: medium
  context: like links you like effectively told it relevant Docker GitHubs and other
    things you need to know if you want to
  name: Docker GitHubs
  position: 39856
- category: tech
  confidence: high
  context: built? Like you're, you clearly have at least the notion of a user interface
    of not actual user interface.
  name: Notion
  position: 40188
- category: unknown
  confidence: medium
  context: s inertia that we kind of need to address, right? And I mean, not to, I
    mean, I work at Google, so there'
  name: And I
  position: 44726
- category: ai_application
  confidence: high
  context: An AI-first tool for organizing ideas and making connections by uploading
    documents.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned via Notebook LM (notebooklm.google.com) and Google Research where
    the speaker previously worked.
  name: Google
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The speaker's former workplace where they worked on question answering
    and machine translation, leading to the Transformer architecture.
  name: Google Research
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The company co-founded by Ilia Polosuchen, focused on private AI and teaching
    machines to code.
  name: Nira AI
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A blockchain project born out of Nira AI's need for global, cheap microtransactions,
    now used for payments and AI workloads like data labeling.
  name: Near Protocol
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned for enabling confidential computing via their new fifth-generation
    Xeons, which supports secure enclaves.
  name: Intel
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned for enabling a specific mode in their hardware that works with
    secure enclaves for confidential computing.
  name: Nvidia
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a third party that received model weights from Mistral, which
    subsequently leaked.
  name: Hugging Face
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a model developer that gave its weights to Hugging Face, leading
    to a leak.
  name: Mistral
  source: llm_enhanced
- category: other
  confidence: medium
  context: Used as an analogy to describe a potential future where a single entity
    (like a few large AI companies) monopolizes access to fundamental technology (intelligence).
  name: AOL
  source: llm_enhanced
- category: ai_company
  confidence: low
  context: Implied as one of the 'largest labs' or 'closed-source, profit-driven companies'
    dominating the AI space, though not explicitly named.
  name: OpenAI
  source: llm_enhanced
- category: ai_company
  confidence: low
  context: Implied as one of the 'largest labs' or 'closed-source, profit-driven companies'
    dominating the AI space, though not explicitly named.
  name: Anthropic
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a model developer whose first model was state-of-the-art
    among open-rate models and did not use explicit user queries for training.
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned in comparison to OpenAI regarding the difference in 'vibe' or
    output characteristics, implying it is a model developed by Anthropic (though
    Anthropic is not explicitly named).
  name: Claude
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned alongside Google as having paid billions of dollars in fines
    related to data handling.
  name: Facebook
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a content provider that could contribute data
    privately into secure enclaves for retrieval time use and be paid via tokens.
  name: New York Times
  source: llm_enhanced
- category: unknown
  confidence: low
  context: Mentioned via a post where he discussed the high cost of building models.
  name: Dario
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in the context of its data centers potentially repurposing themselves
    to become members of the decentralized cloud discussed.
  name: Amazon
  source: llm_enhanced
- category: big_tech
  confidence: low
  context: Not explicitly mentioned, but often grouped with Google/Meta/Amazon in
    discussions about Big Tech AI divisions, though no specific division was named.
  name: Microsoft
  source: llm_enhanced
- category: big_tech
  confidence: low
  context: Mentioned as 'Facebook' earlier in the transcript, but Meta AI division
    was not explicitly named.
  name: Meta
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the serving engine (or a customized version thereof) that
    the speaker's system runs, used for model inference.
  name: VLLM
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of data leaks where chat logs became visible and
    indexed.
  name: Grok
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the underlying technology whose nodes form a multi-party computation
    (MPC) network used for key management within the secure enclave.
  name: Near blockchain
  source: llm_enhanced
date: 2025-09-30 16:22:00 +0000
duration: 65
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: do that, but for text and really figure out how to learn
  text: we should do that, but for text and really figure out how to learn.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: solve this problem, right? We have this, you know, we talked to other
    people, other people have this as well
  text: we should solve this problem, right? We have this, you know, we talked to
    other people, other people have this as well.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/d2c5cdae21d34cd2be093f0ce7ec7765/
processing_date: 2025-10-06 05:04:25 +0000
quotes:
- length: 277
  relevance_score: 4
  text: And would you say that that is because, you know, we're just learning how
    to manipulate, you know, the vibe or or output characteristics of a model based
    on more kind of curated, you know, training data or feedback, or is there are
    there techniques that are enabling this shift
  topics: []
- length: 159
  relevance_score: 4
  text: And so you can also do that, or you can say, "Hey, I want it to be used at
    inference time as part of your search like retrieval index, but not not for training
  topics: []
- length: 81
  relevance_score: 4
  text: And then, I mean, the fine-tuning and kind of training, that's coming a bit
    later
  topics: []
- length: 48
  relevance_score: 3
  text: And the reality is back then, it wasn't possible
  topics: []
- length: 131
  relevance_score: 3
  text: And so, so the reality is actually like, if before this was like really valuable,
    for many use cases, now it's actually a liability
  topics: []
- length: 162
  relevance_score: 3
  text: I mean, and you know, maybe OpenAI has the money to pay their fine, like I
    mean, similar how Google and Facebook have paid, you know, billions of dollars
    in fines
  topics: []
- length: 191
  relevance_score: 3
  text: Like it brings along its own costs, you know, relative to, you know, the per-token
    inference costs that, you know, someone might see, you know, whether it's OpenAI
    or open router or something
  topics: []
- impact_reason: This is a crucial origin story for the Transformer architecture,
    linking its invention directly to the practical need for low-latency, context-aware
    processing in real-world applications like search/translation.
  relevance_score: 10
  source: llm_enhanced
  text: As part of that, due to even actual requirements and latency for Google.com,
    we were trying to figure out how to actually build a deep learning model that
    can consume lots of context and really reason about it without taking time and
    time to process that. And so that's where the Transformer architecture comes.
  topic: technical/history
- impact_reason: A critical observation on the current shift in the AI ecosystem from
    open research collaboration to proprietary, closed development, signaling a potential
    bottleneck for innovation.
  relevance_score: 10
  source: llm_enhanced
  text: It went from, you know, this was open research, everybody was contributing,
    the papers published, you know, transformer code was out, you know, for everybody
    to build on top to like kind of everybody was keeping secrets. Things are starting
    to like close up.
  topic: safety/strategy
- impact_reason: 'Articulates why AI centralization is more dangerous than previous
    tech monopolies: it controls not just information access, but the *processing*
    and *decision-making* layer of society.'
  relevance_score: 10
  source: llm_enhanced
  text: But in case of AI, because it's such a fundamental technology, right? And
    it's intelligence as a technology, it's so much more dangerous, right? Because
    the like, I mean, the internet is information, but this is actually like the processing,
    the decision-making.
  topic: safety/ethics
- impact_reason: A strong cautionary statement linking centralized, profit-driven
    AI control to dystopian societal outcomes (1984 analogy), emphasizing the philosophical
    stakes.
  relevance_score: 10
  source: llm_enhanced
  text: And so kind of the realization was that if we have only kind of a handful
    of kind of closed-source, like profit-driven companies dominating the space, we
    may end up in a 1984 type situation, right? Where you effectively have, you know,
    a company that, that, you know, can be very much, not intentionally, then effectively
    deciding how everybody thinks, right?
  topic: safety/ethics
- impact_reason: Identifies confidential computing (secure enclaves) enabled by recent
    hardware (Intel Xeon, Nvidia drivers) as the key technological breakthrough that
    lowers the overhead barrier for private ML.
  relevance_score: 10
  source: llm_enhanced
  text: So, so we've been doing a lot of research and actually the interesting thing
    happened is the hardware. So, Nvidia hardware and Intel, both kind of at a similar
    time, enabled this mode called confidential computing.
  topic: technical/breakthroughs
- impact_reason: 'This defines the core value proposition of the proposed secure computing
    environment: achieving local-level privacy and security even in a cloud/remote
    setting, which is a major breakthrough for sensitive AI workloads.'
  relevance_score: 10
  source: llm_enhanced
  text: And you can trust that there's no other single party, not, you know, developers,
    not operators of hardware, not model developers, etc., are able to access it.
    Right? So it's as if it was local and potentially even better than local because
    you have like additional security mechanisms.
  topic: technical/safety
- impact_reason: A significant shift in the perception of data—from an asset to a
    liability—driven by regulatory burdens and privacy concerns (GDPR, etc.).
  relevance_score: 10
  source: llm_enhanced
  text: The other thing is for developers, actually, like if I'm an application developer,
    if five, 10 years ago, data was, you know, a gold mine, it's becoming an actual
    liability.
  topic: business/safety
- impact_reason: Presents secure enclaves as the technical solution to protect model
    weights from leakage, even when running on rented or untrusted hardware.
  relevance_score: 10
  source: llm_enhanced
  text: So we also solved in that problem, interestingly, because, because of secure
    enclaves, you can actually encrypt the model weights and they only get decrypted
    inside the secure enclave.
  topic: technical/safety
- impact_reason: 'Clearly maps out the data flow and trust boundaries in this new
    architecture: User data stays private from the cloud operator and the model provider,
    while the model provider only interacts via the secure enclave.'
  relevance_score: 10
  source: llm_enhanced
  text: As a user, as an end user, like I'm contributing my data in some way because
    I want some processing on my data or to access intelligence. And the cloud can't
    access my data. And presumably the model provider can't access my data, but the
    model provider, like it's providing the model into this cloud and it can access
    my data and return some results back to me. Correct.
  topic: safety/technical
- impact_reason: Articulates the move towards 'Verifiable Results' training, emphasizing
    the need for high-quality, specific supervision over mass, noisy user interaction
    data.
  relevance_score: 10
  source: llm_enhanced
  text: Yeah, we're talking about like this transition from user feedback to verifiable
    results. It's a combination of data labeling, like indeed human, but like you
    actually want a very specific supervision and you want to control kind of what
    feedback you get.
  topic: technical/trends
- impact_reason: 'Defines the current frontier of AI innovation: moving beyond generic
    foundation models to integrating specialized expertise, verification, and complex
    synthesis pipelines.'
  relevance_score: 10
  source: llm_enhanced
  text: So part of it is we've collected enough data and we've kind of baked the generic
    stuff into the foundation model. So now where the innovation is happening is,
    you know, bringing in more subject matter expertise or specialized skills or indeed
    like a verifiable thing or combining like, you know, synthesizing.
  topic: AI technology trends
- impact_reason: 'Presents a radical business model innovation: tokenizing individual
    models to distribute revenue and reward contributors (data providers, developers)
    directly based on model success.'
  relevance_score: 10
  source: llm_enhanced
  text: So we actually do, I mean, we talked about this like last year, like where
    effectively every model gets its own token. So like a way to distribute the reward
    and value from the revenue, while also rewarding with the stock.
  topic: business model/monetization
- impact_reason: Details the mechanism for data ownership and long-term compensation,
    linking data contribution directly to the economic success of the resulting AI
    asset.
  relevance_score: 10
  source: llm_enhanced
  text: Whoever contributes data gets a token of this model that do the trained on
    their data. And then the revenue is distributed to the stockholders, right, as
    a model like for the model's lifetime.
  topic: business model/data rights
- impact_reason: 'Proposes a ''reverse'' model for open research: keeping data and
    weights encrypted/controlled while making the *process* of training transparent
    and auditable.'
  relevance_score: 10
  source: llm_enhanced
  text: So kind of the way we think about it is to reverse it where you can actually
    have an open process of training, right? So the training, I mean, the data either
    fully open or this like kind of encrypted data, right, that you can run over.
    So available but not necessarily transparent.
  topic: strategy/technical architecture
- impact_reason: 'Describes a paradigm shift in application deployment: moving computation
    to the user''s secure data store, creating powerful network effects where data
    utility drives application adoption.'
  relevance_score: 10
  source: llm_enhanced
  text: Or you can just say, "Actually, I'm going to build my app and deploy it into
    this cloud where it runs on it on your side, right?" You know, and saves context
    there as well in your data store. And so now your data store becomes even more
    useful because it has all the notes as well there, and your AI can now read over
    those notes.
  topic: strategy/network effects
- impact_reason: Details a novel cryptographic approach combining blockchain (on-chain
    hashing for integrity) and decentralized storage for model and data provenance.
  relevance_score: 10
  source: llm_enhanced
  text: what's happening is, you know, you checkpoint your, you know, model weights
    on-chain, so you know the hash, the hash of the model weights, and encrypted hash
    as well. You know, the encrypted data is uploaded kind of decentralized storage.
  topic: technical
- impact_reason: Explains the critical role of Multi-Party Computation (MPC) in conjunction
    with secure enclaves to manage decryption keys securely within the execution environment.
  relevance_score: 10
  source: llm_enhanced
  text: This multi-party computation network which allows inside the secure enclave
    effectively have its own private key to decrypt things.
  topic: technical
- impact_reason: Provides a crucial metric quantifying the performance cost of implementing
    advanced encryption and secure enclaves, suggesting it is minimal.
  relevance_score: 10
  source: llm_enhanced
  text: The overhead on the computation side is 1 to 5%.
  topic: technical
- impact_reason: Uses specific, recent examples (human review of chat logs) to substantiate
    the privacy risks inherent in current centralized LLM services, creating a fear-based
    incentive for adoption.
  relevance_score: 10
  source: llm_enhanced
  text: There is, I mean, we've seen this with OpenAI, right? There's news that the
    fact that scanning all the chat logs and then the ones that are flagged are sent
    to human for evaluation and then to police, right?
  topic: safety
- impact_reason: This is a deep dive into the architecture, showing how the blockchain
    network itself is leveraged to create the necessary cryptographic environment
    (MPC) for secure decryption within the enclave.
  relevance_score: 10
  source: llm_enhanced
  text: And then we also have this concept called multi-party computation. So the
    Near blockchain itself kind of right now, part of our nodes, form this multi-party
    computation network which allows inside the secure enclave effectively have its
    own private key to decrypt things.
  topic: technical
- impact_reason: This highlights a major shift in the perception and handling of data
    due to increasing privacy regulations (GDPR, CCPA) and costs, directly impacting
    how developers approach data strategy.
  relevance_score: 9
  source: llm_enhanced
  text: If I'm an application developer, if five, 10 years ago data was a gold mine,
    it's becoming an actual liability.
  topic: business/strategy
- impact_reason: Highlights the massive leap in feasibility for large-scale AI, emphasizing
    that the current capabilities are contingent on recent advancements in compute
    scale, which were not available when the Transformer was conceived.
  relevance_score: 9
  source: llm_enhanced
  text: Back then, nobody thought what's happening right now is possible, right? So
    it was like, whatever pitching was kind of somewhere between science fiction and
    fusion. The reality is back then, it wasn't possible. The compute wasn't there,
    right? Kind of the scale at which, you know, we know these models need, kind of
    wasn't there and wasn't studied very well.
  topic: technical/trends
- impact_reason: Uses a powerful historical analogy (AOL) to warn against the dangers
    of AI centralization and monopolization.
  relevance_score: 9
  source: llm_enhanced
  text: And in turn, becomes kind of the monopoly that we've seen before in other
    areas. And so the example that I use is AOL. Like imagine the internet was effectively
    run out of AOL...
  topic: safety/strategy
- impact_reason: Defines the core mission of 'user-owned AI' as a direct counter-movement
    to centralization, leveraging blockchain principles for decentralized participation.
  relevance_score: 9
  source: llm_enhanced
  text: And so that's kind of where this idea of user-owned AI was born, which was
    like, 'Hey, let's combine what's been building on the blockchain side, which is
    user ownership, kind of network effects of everybody contributing and participating
    in comparison to, you know, centralized kind of for-profit company and create
    the AI that actually is on the user side, on, you know, your side, not their side.'
  topic: strategy/business
- impact_reason: Quantifies the historical computational barrier (10k-100k overhead)
    that made advanced privacy techniques like ZK proofs and HE impractical for large-scale
    ML workloads.
  relevance_score: 9
  source: llm_enhanced
  text: All the methods that people use, for example, for privacy, for verifiability,
    are extremely expensive, right? There's like homomorphic encryption, there's ZK
    proof, etc. All of them have, you know, like 10,000 to 100,000 times overhead.
  topic: technical/limitations
- impact_reason: 'Provides a clear, concise definition of confidential computing:
    hardware-level isolation protecting computation even from the cloud provider/hardware
    owner.'
  relevance_score: 9
  source: llm_enhanced
  text: So this is inside the chips itself, there's a, like you can enable it in such
    a way that even the owner of the hardware of the compute is not able to access
    what computation is happening inside.
  topic: technical/concepts
- impact_reason: Directly addresses the trade-off between local (private, less powerful)
    and cloud (powerful, less private) AI, arguing that cloud capabilities are necessary
    for complex, continuous background agentic work.
  relevance_score: 9
  source: llm_enhanced
  text: Am I parsing that correctly? I mean, I run some of the models locally, but
    I mean, obviously they're not as intelligent as what you can have in the cloud.
    They know this fast. But importantly, also like, you know, even if we have like
    a smarter model, you still have a lot of things that are happening in the background
    that you want to, like, keep happening, right? You want, you know, set up an agent
    that runs and re-dolls and uses and summarizes it and processes it or workflows,
    etc.
  topic: predictions/strategy
- impact_reason: 'This summarizes the key technological goal: bridging the gap between
    the power of cloud AI and the privacy of local AI, a critical tension in current
    AI deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: But it sounds like what you're trying to do is more, like, create a system
    that would allow, like, remote and cloud-based, but also private to the same level
    as local AI.
  topic: technical/strategy
- impact_reason: 'Identifies the current fundamental gap in the market: the lack of
    a truly private cloud environment for computation and data handling.'
  relevance_score: 9
  source: llm_enhanced
  text: Right now, there is no really private cloud, right? There's like all these
    multiple companies, usually, who actually have access to the data and there's
    a computation.
  topic: business/safety
- impact_reason: Describes a new business model for developers where they can offer
    powerful, cloud-backed services without incurring the regulatory and operational
    burden of handling user data directly.
  relevance_score: 9
  source: llm_enhanced
  text: And so this, this actually creates a platform where you as a developer don't
    need to deal with the user data. You're actually pushing software to them. Again,
    similar how local works, right? You push the application, you know, to the user
    and then translate their device, you don't need to deal with whatever data.
  topic: business/strategy
- impact_reason: 'Highlights a major security concern for model developers: the risk
    of proprietary model weights being compromised when hosted on third-party infrastructure.'
  relevance_score: 9
  source: llm_enhanced
  text: And the challenge is actually this model developers don't trust third parties.
    Because they're afraid that their model will leak. And this has happened where
    the model weights have leaked from third parties.
  topic: safety/technical
- impact_reason: 'Summarizes the dual privacy benefit: protecting both the model IP
    (weights) and the user data simultaneously within the secure system.'
  relevance_score: 9
  source: llm_enhanced
  text: And then on the user side, the users don't have access to the model, but they
    also know their data is not going anywhere.
  topic: safety
- impact_reason: 'Defines the architecture: a ''decentralized, residential cloud''—a
    novel topology that combines the distribution of decentralized compute with the
    managed nature of a cloud.'
  relevance_score: 9
  source: llm_enhanced
  text: Topologically, this is this is compute hardware, let's say GPUs and CPUs that
    live in this decentralized, residential cloud.
  topic: technical
- impact_reason: Signals a major shift in model training methodology away from relying
    solely on noisy, direct user feedback (like RLHF) towards more controlled, verifiable
    data sources.
  relevance_score: 9
  source: llm_enhanced
  text: The piece number two is actually why I think the space is transitioning from
    user feedback is so let's use a DeepSeek example. So when DeepSeek releases their
    first model, right, which was, you know, at least at the time from the open-rate
    models was state-of-the-art. And like our, for example, for our one, it did not
    use the explicit like user queries, right?
  topic: technical/trends
- impact_reason: 'Explains the current frontier of AI innovation: moving beyond generic
    data collection to integrating deep subject matter expertise and verifiable reasoning
    methods into fine-tuning.'
  relevance_score: 9
  source: llm_enhanced
  text: Part of it is we've collected enough data and we've kind of baked the generic
    stuff into the foundation model. So now where the innovation is happening is,
    you know, bringing in more subject matter expertise or specialized skills or indeed
    like a verifiable thing or combining like, you know, synthesizing.
  topic: technical/trends
- impact_reason: 'Highlights the fundamental shift in data strategy for LLMs: moving
    from purely supervised/click data to massive unsupervised data augmented by targeted,
    specialized human labeling.'
  relevance_score: 9
  source: llm_enhanced
  text: I think what LLMs did is kind of transition that to like, "Hey, we actually
    just pass a lot of unsupervised data, right?" And not not like user click data.
    And then we add a little bit of a human, like a very specific human label data.
  topic: technical/data strategy
- impact_reason: A strong cautionary note on the liability and complexity associated
    with relying on passively collected user data versus proactively sourced, consented
    data.
  relevance_score: 9
  source: llm_enhanced
  text: And so like the amount of yeah, like I'm kind of that's what I mean, like
    the shift, like how much we can collect data and how how useful that is versus
    getting a bunch of user data and then dealing with all the repercussions of that.
  topic: safety/ethics/data strategy
- impact_reason: Proposes blockchain technology as the key enabler for creating sustainable,
    fair data contribution and compensation models, solving current structural limitations.
  relevance_score: 9
  source: llm_enhanced
  text: And the underlying blockchain has the mechanism to make that tenable in a
    way that it's not tenable today [contributing data for reward].
  topic: business model/technology integration
- impact_reason: Addresses the seemingly insurmountable problem of compensating content
    creators for internet-scale data scraping, suggesting tokenization provides a
    viable pathway.
  relevance_score: 9
  source: llm_enhanced
  text: How would you ever do that? Like, you know, you're crawling all of the internet,
    like, how would you even possibly begin to do that? But just you describing this
    token model, it's kind of like, "Oh, well, you maybe you could do that."
  topic: ethics/business model
- impact_reason: Describes granular, privacy-preserving data contribution controls,
    allowing users to specify usage rights (training vs. inference) even when data
    is sensitive.
  relevance_score: 9
  source: llm_enhanced
  text: You can contribute data privately. So you can say, "Hey, I want this data
    to be used in the model training, but I don't want anyone to see it," right? For
    example, right? And so you can also do that, or you can say, "Hey, I want it to
    be used at inference time as part of your search like retrieval index, but not
    not for training."
  topic: safety/privacy/technical control
- impact_reason: Introduces the concept of running encrypted weights within a secure
    environment (likely referencing Confidential Computing/DCML) to enable monetization
    while preserving security.
  relevance_score: 9
  source: llm_enhanced
  text: And then the resulting weights are actually also encrypted and and only run
    in this kind of DCML model. So you can actually monetize it, right?
  topic: technical architecture/monetization
- impact_reason: 'Defines the ultimate goal for private AI: managing a user''s entire
    digital life, starting by targeting the small, high-value cohort that prioritizes
    privacy.'
  relevance_score: 9
  source: llm_enhanced
  text: And so it's able to manage your whole life, not just like some aspects that
    you were willing to share. And so and kind of the first cohort that actually cares
    about privacy, that's, you know, that is our early adopters who we kind of target
    to really enable this.
  topic: product vision/strategy
- impact_reason: Connects the proposed monetization/open research framework directly
    to the creation of a diverse ecosystem of specialized, high-value fine-tuned models.
  relevance_score: 9
  source: llm_enhanced
  text: The other side of this is actually because of this open research process what
    we are aiming for is to have people again fine-tuning specialized models for specialized
    use cases, right? And again, this is where because they have a monetization embedded
    into this, right? They can actually, you know, invest effort and time and compute
    to actually build interesting specialized models.
  topic: business model/ecosystem growth
- impact_reason: 'Succinctly summarizes the desired outcome: a virtuous cycle where
    user data centralization (due to privacy/security) fuels application development
    and increases data value.'
  relevance_score: 9
  source: llm_enhanced
  text: The idea is like you have network effects of kind of more context, more data,
    more applications building around the user.
  topic: strategy/network effects
- impact_reason: Pinpoints the role of secure enclaves (Confidential Computing) in
    guaranteeing the integrity and privacy of the interaction between the model, weights,
    and user data during inference.
  relevance_score: 9
  source: llm_enhanced
  text: It's the secure enclave that ensures that there's no kind of man-in-the-middle
    attack between VLLM and like the model weights and the customer data.
  topic: technical architecture/security
- impact_reason: 'Clearly defines the trust boundary in their system: the integrity
    of the execution environment (the Docker container) is what is being cryptographically
    certified.'
  relevance_score: 9
  source: llm_enhanced
  text: The trust boundary is the container, and if the container is doing what you
    say it's doing, then the signature certifies that that was the container that
    was actually used.
  topic: safety
- impact_reason: Presents a powerful vision of data portability and identity ownership,
    where user context (memory, history) travels with them across different instances/forks
    of the application.
  relevance_score: 9
  source: llm_enhanced
  text: you can like launch your own version with custom like improvements, and then
    everybody who logs in will get all their messages, older history, older memory,
    all the ups with them. So it's also like kind of detaching your identity from
    a specific application but.
  topic: predictions
- impact_reason: Pinpoints 'inertia'—the established ecosystem and user trust in incumbents
    (like Google)—as the main barrier to adoption, rather than purely technical feasibility.
  relevance_score: 9
  source: llm_enhanced
  text: What do you say to the skeptic that says, you know, it all sounds too good
    to be true? ... I mean, the hard part right now is inertia, right?
  topic: business
- impact_reason: 'Presents a future vision for AI infrastructure: extreme decentralization
    where compute resources (GPUs) are hyper-localized to minimize network latency,
    potentially involving mobile data centers.'
  relevance_score: 9
  source: llm_enhanced
  text: ideally right now, yeah, you actually should be accessing the closest GPU
    ideally in your city. There's like, you I mean, I've talked to people who like,
    you know, they're going to be data centers everywhere, people build like mobile
    data centers, etc.
  topic: predictions
- impact_reason: This describes a potential future where developers leverage powerful,
    context-aware models without the direct burden of managing sensitive user data,
    enabled by new architectural paradigms (like private/confidential computing).
  relevance_score: 8
  source: llm_enhanced
  text: This actually creates a platform where you as a developer don't need to deal
    with the use of data. Pushing software to them, again, you can have higher intelligence
    models and you can access all of their context and memory as well in this.
  topic: business/strategy
- impact_reason: Reveals the early, specific product vision (code generation) that
    motivated the departure from Google research to found Nira AI, predating the current
    LLM boom.
  relevance_score: 8
  source: llm_enhanced
  text: I was excited to actually put it in production and build a product around
    this. And specifically, I was excited about the idea of machines writing code.
  topic: business/strategy
- impact_reason: Shows the convergence of blockchain expertise and the current AI
    wave, suggesting blockchain infrastructure will play a role in solving emerging
    AI challenges.
  relevance_score: 8
  source: llm_enhanced
  text: And so now, kind of in effectively in toy and toy industry as the, you know,
    as the resurgence of this and actually like figuring out the scale of the AI kind
    of came in, we started now with a renewed lens of the blockchain looking at it
    and actually see how can we contribute to it and how can we leverage it.
  topic: strategy/trends
- impact_reason: Pinpoints the recent convergence (circa 2023/2024) of hardware features
    that made decentralized confidential ML architecture feasible.
  relevance_score: 8
  source: llm_enhanced
  text: And so that kind of all came together, effectively like a year ago. And so
    since then, like, you know, okay, that enables us to, that gives us some components,
    but then now we still need the whole system, right? And so that's really what,
    you know, we're enabling is, we call it decentralized confidential machine learning.
  topic: technical/trends
- impact_reason: Positions confidential computing as a solution that eliminates the
    user's subjective 'trust threshold' by guaranteeing end-to-end encryption of data
    *during* processing.
  relevance_score: 8
  source: llm_enhanced
  text: So, there's always a threshold where you kind of get all like, 'Um, maybe
    I shouldn't do that, right?' And so what we offer is effectively removing that
    threshold and say, 'Hey, actually, it's so confidential, all enter and encrypted
    for you.'
  topic: business/solution
- impact_reason: Highlights the enduring necessity of cloud infrastructure for complex,
    persistent, and background AI agentic workflows, even as local models improve
    in capability.
  relevance_score: 8
  source: llm_enhanced
  text: I mean, obviously they're not as intelligent as what you can have in the cloud.
    They know this fast. But importantly, also like, you know, even if we have like
    a smarter model, you still have a lot of things that are happening in the background
    that you want to, like, keep happening, right? You want, you know, set up an agent
    that runs and re-dolls and uses and summarizes it and processes it or workflows,
    etc. So there's always going to be a need for background work and analysis and
    kind of surfacing it even as local models improve.
  topic: predictions/strategy
- impact_reason: Indicates that this decentralized cloud model is not limited to small,
    individual machines but can potentially incorporate large, existing data center
    resources, scaling the concept significantly.
  relevance_score: 8
  source: llm_enhanced
  text: Yeah, I mean, like Amazon, you know, data center can repurpose itself to become
    a member of this cloud. Yeah, so this is a cloud.
  topic: strategy/technical
- impact_reason: Raises a critical, unresolved legal/compliance risk for current centralized
    cloud AI providers regarding the 'right to be forgotten' under regulations like
    GDPR.
  relevance_score: 8
  source: llm_enhanced
  text: I use OpenAI. OpenAI, I train on my data and then I go and I invoke my GDPR
    law and say, "Hey, remove all my data." I'm assuming that that is not a resolved
    issue either because I can't believe it's because no one has asked yet.
  topic: safety/business
- impact_reason: Warns that data liability risks are not just financial fines for
    large players but existential threats for smaller AI companies.
  relevance_score: 8
  source: llm_enhanced
  text: But if you're a smaller model developer, that's why I was kind of using, you
    know, other examples. This effect, it can be like existential.
  topic: business/safety
- impact_reason: Provides a concrete, real-world scenario illustrating the regulatory
    nightmare (GDPR compliance) that centralized cloud AI faces regarding user data
    retention.
  relevance_score: 8
  source: llm_enhanced
  text: But first before we go there, why is this liability? So imagine I'm a European
    right now, I'm in Lisbon right now. I use OpenAI. OpenAI, I train on my data and
    then I go and I invoke my GDPR law and say, "Hey, remove all my data."
  topic: safety
- impact_reason: Connects model 'vibe' (personality/style) to the quality of feedback,
    suggesting that nuanced characteristics require expert human labeling rather than
    general user signals.
  relevance_score: 8
  source: llm_enhanced
  text: So there's like a lot of a lot of the, I would say, you know, even if we're
    talking about like the like shifting the vibe of the model, which I think something
    that usually credited to, you know, like Claude versus OpenAI, like the vibe is
    different and kind of how even that is, you probably want like more trained people
    to actually give feedback versus just rely on kind of very, very noisy signal
    that comes from users.
  topic: technical/strategy
- impact_reason: Describes advanced, multi-stage training pipelines involving model-on-model
    evaluation followed by targeted human labeling, indicating increased sophistication
    in data curation.
  relevance_score: 8
  source: llm_enhanced
  text: And then using another model to evaluate it and kind of then human labeling
    like all those kind of pipelines, right?
  topic: technical
- impact_reason: Reinforces the liability argument, specifically in multimodal domains
    (like audio), suggesting that relying on passively collected user data carries
    significant legal risk compared to paying for curated contributions.
  relevance_score: 8
  source: llm_enhanced
  text: It's great to have a bunch of audio from people that use your product. But
    then again, like you may get in trouble so much, right? We've seen that happening.
    It's better to just pay people to contribute their audio and like sig
  topic: safety/business
- impact_reason: Illustrates the necessity of high-quality, domain-specific human
    labeling data, even at scale, emphasizing that general data is insufficient for
    specialized tasks.
  relevance_score: 8
  source: llm_enhanced
  text: And so kind of like against for our example, we were finding computer science
    students because we needed people who code. And so just kind of the, you know,
    some maybe broader data wouldn't be that much that useful.
  topic: data strategy/product building
- impact_reason: Provides a specific, early prediction regarding the rapid acceleration
    and capability growth of open-source/open-weight models following the initial
    closed model dominance.
  relevance_score: 8
  source: llm_enhanced
  text: I was actually, I think I remember it was like February '23. I was talking
    about like, "Hey, open source is going to catch up."
  topic: predictions/AI trends
- impact_reason: 'Identifies a major inefficiency in the current open-source/open-weights
    landscape: the lack of transparency in training methodology leads to redundant
    research efforts.'
  relevance_score: 8
  source: llm_enhanced
  text: And so in result, we're actually wasting a lot of resources because everybody
    kind of redoing experiments because we actually don't know what were the things
    people did to get to these results.
  topic: strategy/research efficiency
- impact_reason: 'Frames the historical challenge of privacy adoption: it''s an economic/behavioral
    barrier, not purely a technical one.'
  relevance_score: 8
  source: llm_enhanced
  text: Historically, you know, there's always been this big barrier that's not at
    all technical, and that is, will people pay for privacy?
  topic: business/privacy adoption
- impact_reason: Acknowledges the niche nature of the privacy-first market but suggests
    that the value proposition (utility gained) must outweigh the perceived cost/inconvenience
    of sharing data.
  relevance_score: 8
  source: llm_enhanced
  text: It is relatively small [the audience that will pay for privacy]. It is relatively
    small. But I think the idea here and kind of I think everybody's on a threshold
    as a set of like what they feel they would give to the model.
  topic: business/privacy adoption
- impact_reason: Highlights the shift from manual integration (like Zapier) to AI
    natively handling context and data flow, emphasizing the value of network effects
    in AI ecosystems.
  relevance_score: 8
  source: llm_enhanced
  text: AI can now read over those notes. You don't need to like again merge the things
    with Zapier and do all this like that's the idea is like you have network effects
    of kind of more context, more data, more applications building around the user.
  topic: strategy
- impact_reason: Provides a relatable analogy (HTTPS shield) for users to verify the
    security and integrity of the confidential computation, making complex security
    tangible.
  relevance_score: 8
  source: llm_enhanced
  text: And in our UI, we have effectively like, you know, like a green shield that
    you can click, similar like like HTTPS works, to go there and gives you like it
    gives you like it's all correct.
  topic: business
- impact_reason: 'Reveals the dual market strategy: offering a B2D (developer) product
    for integration and a B2C (consumer) product (''private judge GPT'') to showcase
    the technology.'
  relevance_score: 8
  source: llm_enhanced
  text: It's it's both a developer product so you can, you know, buy credits and effectively
    use confidential friends in your own applications, as well as we have a kind of
    consumer product which is, you know, private judge. Yes, you know, private judge
    GPT effectively, which indeed provides you all the kind of certification and replication
    information if you want while using it.
  topic: business
- impact_reason: 'Articulates the core product strategy for adoption: privacy features
    must not degrade the user experience; they must be seamless or additive.'
  relevance_score: 8
  source: llm_enhanced
  text: The goal is to make it everything is like either the same or better, right?
    Like you either don't, I mean, it looks exactly, I mean, very similar experience,
    right?
  topic: strategy
- impact_reason: Captures the mindset of the average user who is comfortable with
    centralized providers, highlighting the challenge of selling privacy when convenience
    is already established.
  relevance_score: 8
  source: llm_enhanced
  text: But everything is already here. Google has my email and my calendar anyway,
    why do I care if it's private?
  topic: strategy
- impact_reason: Describes a paradigm shift where data is temporarily 'hydrated' (loaded)
    into the nearest secure compute node for inference, optimizing for locality.
  relevance_score: 8
  source: llm_enhanced
  text: And like you want to find that one, connect to it when you're compute on it,
    you know, hydrate your data there. Like that that's kind of where, you know, this
    infrastructure can move to.
  topic: strategy
- impact_reason: 'Highlights the trade-off: while custom models are supported, deviating
    from the platform''s audited container shifts the trust responsibility back to
    the model provider.'
  relevance_score: 8
  source: llm_enhanced
  text: If somebody builds a custom Docker container, you can do that, but then the
    user needs to trust your Docker container.
  topic: safety
- impact_reason: Provides historical context on the motivation behind early large-scale
    language model research, linking it directly to seminal computer vision work (the
    'cat neuron paper').
  relevance_score: 7
  source: llm_enhanced
  text: I joined Google Research because I saw the cat neuron paper, if folks remember
    that, and I was like, 'Okay, we should do that, but for text and really figure
    out how to learn.'
  topic: technical/history
- impact_reason: Details the unexpected pivot from an AI data sourcing problem to
    founding a major blockchain protocol (Near), illustrating how practical business
    constraints can drive foundational technology development.
  relevance_score: 7
  source: llm_enhanced
  text: And so we started looking at blockchain as like, 'Hey, this global payment
    network that people are talking about, you know, we can just use it right to pay
    people.' ... And so that's kind of how Near Protocol was born.
  topic: business/history
- impact_reason: 'Explains the non-legal/non-regulatory basis for needing privacy:
    fundamental human trust boundaries, using the ''hot mic'' analogy to illustrate
    data intrusion.'
  relevance_score: 7
  source: llm_enhanced
  text: Confidentiality is a combination of things, right? First of all, for the user...
    But there's important kind of interesting effects that there's still something
    that you're not going to trust. Like, you know, we don't normally walk around
    with like a hot mic that records everything we say...
  topic: safety/ethics
- impact_reason: Provides a specific, high-profile example (though potentially misremembered
    or slightly inaccurate in the context of the full story) illustrating the risk
    of model weight leakage from external hosts.
  relevance_score: 7
  source: llm_enhanced
  text: So Mistral gave its weights to Hugging Face and it didn't doubt one for chat.
    Oh, well, I hadn't heard that.
  topic: safety/business
- impact_reason: Contrasts the ability of tech giants to absorb massive regulatory
    fines versus the existential threat these fines pose to smaller competitors.
  relevance_score: 7
  source: llm_enhanced
  text: Yeah, so that's what I mean by liability, right? I mean, and you know, maybe
    OpenAI has the money to pay their fine, like I mean, similar how Google and Facebook
    have paid, you know, billions of dollars in fines.
  topic: business
- impact_reason: Discusses the philosophical difference between open source code and
    open models, suggesting that fine-tuning capability might be a sufficient proxy
    for 'changeability' in the model context.
  relevance_score: 7
  source: llm_enhanced
  text: I mean, a missing property is the ability to see it and change it. Maybe,
    you know, maybe that's more true for software than for a model. If you have the
    ability to to fine-tune on top of it, that's you can. Exactly. Yeah, you can change
    it.
  topic: strategy/open source philosophy
- impact_reason: 'Provides a concrete technical detail: existing serving infrastructure
    (like VLLM) can be adapted, lowering the barrier for model deployment in their
    secure environment.'
  relevance_score: 7
  source: llm_enhanced
  text: We actually run, you know, VLLM and a customized version of the LLM. So everything
    that you know normally served already works.
  topic: technical deployment
- impact_reason: 'Identifies the primary business hurdle: convincing users that the
    added privacy/security features are worth adopting, even if the direct cost is
    comparable to non-private alternatives.'
  relevance_score: 7
  source: llm_enhanced
  text: I think the kind of as we just discussed, right? Like are people willing to
    pay for it? Yeah, that's and again, I think but, you know, I well, I think what
    I heard you say is that like I'm not really paying like I'm paying, it's the same,
    right?
  topic: business
- impact_reason: Confirms that while hardware acceleration helps, the boundary operations
    (encryption/decryption) still introduce measurable latency, though manageable.
  relevance_score: 7
  source: llm_enhanced
  text: Is a lot of that stuff getting pushed into the hardware? Like, is it an issue
    for you? Not really. I mean, we have like it's a little bit a little bit higher
    latency again just because like when it goes to the boundaries is like some additional
    delay on kind of encryption.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: The Decentralized Future of Private AI with Illia Polosukhin
  - #749


  This episode of the Twombo AI podcast, hosted by Sam Charrington, features Illia
  Polosukhin, co-founder of Near AI and co-author of the seminal "Attention Is All
  You Need" paper, discussing his current focus on building a decentralized and private
  future for Artificial Intelligence. The conversation bridges Polosukhin''s foundational
  work on Transformers with his current efforts at Near Protocol and Nira AI to address
  centralization risks in the AI landscape.


  ### 1. Focus Area

  The primary focus is the **Decentralized Future of Private AI**. The discussion
  centers on mitigating the risks of centralized AI monopolies by leveraging blockchain
  principles (user ownership, self-sovereignty) combined with cutting-edge hardware
  capabilities (Confidential Computing) to enable private, scalable cloud-based AI
  services. Secondary themes include the evolution of model training data (shifting
  from raw user feedback to curated, verifiable supervision) and the regulatory burdens
  (like GDPR) making user data a liability for developers.


  ### 2. Key Technical Insights

  *   **Confidential Computing as the Enabler:** The convergence of recent hardware
  advancements (Intel''s 5th Gen Xeons and specific Nvidia modes) enabling **Confidential
  Computing** (secure enclaves) is the critical technical breakthrough. This allows
  computation to occur in an encrypted state, inaccessible even to the hardware owner,
  providing cloud-level intelligence with local-level privacy guarantees.

  *   **Decentralized Confidential Machine Learning (DCML):** This framework combines
  user data privacy (data remains encrypted) with model developer IP protection (model
  weights are encrypted and only decrypted within the secure enclave during inference).
  This solves the dual trust problem in the cloud.

  *   **Shift in Model Refinement:** The industry is moving away from relying solely
  on noisy, large-scale user feedback (like clicks) for fine-tuning towards more curated,
  verifiable supervision, synthetic data, and expert human labeling (e.g., using specialized
  CS students for code-related models).


  ### 3. Business/Investment Angle

  *   **Data as a Liability:** Regulatory pressures (GDPR, data taxes in China) are
  transforming consumer data from a "gold mine" into a significant legal and financial
  liability for centralized developers.

  *   **Decentralized Compute Market:** The DCML approach opens up the possibility
  of leveraging distributed, residential cloud hardware (GPUs/CPUs) for AI workloads
  without the model developer needing to manage data compliance or the user needing
  to trust a single cloud provider.

  *   **Model Leakage Risk:** Centralized GPU cloud providers pose an existential
  risk to model developers whose proprietary weights can leak (e.g., the mentioned
  Mistral weight leak via Hugging Face), driving the need for encrypted model deployment
  solutions.


  ### 4. Notable Companies/People

  *   **Illia Polosukhin:** Co-author of the Transformer paper, co-founder of Nira
  AI, and founder of **Near Protocol** (a major blockchain network with 50M MAUs,
  initially born out of solving global micropayment issues for data labeling).

  *   **Google Research:** Where Polosukhin worked on question answering and machine
  translation, leading to the Transformer architecture.

  *   **Intel & Nvidia:** Their recent hardware updates enabling secure enclave functionality
  were crucial for realizing the DCML vision.

  *   **OpenAI/Mistral:** Mentioned as examples of centralized entities whose data
  practices or security vulnerabilities highlight the need for decentralized alternatives.


  ### 5. Future Implications

  The industry is heading toward a model where **intelligence is accessible via the
  cloud but remains fundamentally user-owned and private.** This decentralized confidential
  cloud aims to offer the scale and intelligence of centralized services while eliminating
  the "1984" risk associated with a few profit-driven entities controlling fundamental
  decision-making technology. It suggests a future where developers push software,
  not data, to users, while still enabling powerful background processing.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Blockchain Developers, Product
  Managers in Tech, and Venture Capitalists** focused on infrastructure, privacy-preserving
  computation (PETs), and the long-term governance of foundational AI models.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- investment
- google
- nvidia
- openai
title: 'The Decentralized Future of Private AI with Illia Polosukhin - #749'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 118
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 16
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 9
  prominence: 0.9
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 3
  prominence: 0.3
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 05:04:25 UTC -->
