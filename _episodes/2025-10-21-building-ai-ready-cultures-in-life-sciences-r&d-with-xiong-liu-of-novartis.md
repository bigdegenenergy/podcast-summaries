---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: e. Again, that's emerj.com/expert-one. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2161
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2196
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2561
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2623
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5303
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7028
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 18003
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24123
- category: ai_media_analysis
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: high
  context: Jung Liu's employer, a life sciences company actively exploring and implementing
    generative AI and LLMs in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an example of an organization whose CIO has been featured
    on the podcast, indicating enterprise AI adoption.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an example of an organization whose head of AI has been featured
    on the podcast, indicating enterprise AI adoption.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    connections by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: 'Mentioned twice: once as the host of the Notebook LM product (notebooklm.google.com)
    and again as the originator of the transformer architecture.'
  name: Google
  source: llm_enhanced
- category: consulting_research
  confidence: high
  context: Conducted a recent survey noting that 73% of life sciences leaders are
    investing in generative AI initiatives.
  name: Deloitte
  source: llm_enhanced
- category: ai_company
  confidence: medium
  context: Referenced indirectly through mentions of 'GPT-like models' and the explosion
    of LLMs, which OpenAI pioneered with ChatGPT.
  name: OpenAI (Implied via GPT)
  source: llm_enhanced
- category: ai_research_model
  confidence: high
  context: A specific language model architecture mentioned as being used before GPT
    models for information extraction in clinical trial documents.
  name: BERT (Model/Architecture)
  source: llm_enhanced
- category: ai_model_architecture
  confidence: medium
  context: Referenced as a model architecture leveraging the transformer architecture
    for generating new sequences (language, molecules). Implies the technology pioneered
    by OpenAI.
  name: GPT
  source: llm_enhanced
- category: general_industry
  confidence: high
  context: Mentioned as sources of open public AI architectures that can be leveraged.
  name: Tech companies
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as being good at building foundation models and publishing results.
  name: Academics
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 06:36:35 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 70
  relevance_score: 3
  text: So we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: Clearly articulates the fundamental shift from traditional, task-specific
    ML (requiring bespoke data labeling) to the foundation model paradigm.
  relevance_score: 10
  source: llm_enhanced
  text: The difference is that previously when we do machine learning, we just collect
    our own data, we label the data, and then apply algorithms to train models...
    But now people have been developing these new paradigms, which is along with LLMs
    foundation models.
  topic: technical
- impact_reason: 'Explains the core concept of foundation models: leveraging massive,
    broad datasets (even seemingly irrelevant ones within a domain) for large-scale
    pre-training.'
  relevance_score: 10
  source: llm_enhanced
  text: The idea is that yes, you still care about the specific data you are working
    on, but at the same time, we want to collect all the available data, relevant
    data, or irrelevant data, but as long as they fall into the same domain, and fill
    those data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical
- impact_reason: 'This is the key value proposition of foundation models: enabling
    high performance on downstream tasks even with limited proprietary data via transfer
    learning.'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data.
  topic: technical
- impact_reason: Provides a concrete life sciences example (disease pathways) showing
    how pre-trained models on public biological data accelerate research where proprietary
    data is scarce.
  relevance_score: 10
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data.
  topic: predictions/technical
- impact_reason: Offers a precise, accessible definition of fine-tuning versus full
    retraining, crucial for understanding efficient model adaptation in specialized
    fields.
  relevance_score: 10
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right? So we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems...
  topic: technical
- impact_reason: 'Highlights a cutting-edge application: using GenAI to synthesize
    biological data (cell types, gene expression) for *in silico* testing, bypassing
    physical experimentation.'
  relevance_score: 10
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: predictions
- impact_reason: Illustrates the potential for applying the generative transformer
    paradigm (like GPT) to non-textual data structures like molecules (SMILES strings
    or graphs) to generate novel chemical entities.
  relevance_score: 10
  source: llm_enhanced
  text: So all those information publicly available data are from different databases.
    They could all fit into those kind of transformer architecture to let you learn
    the hidden embedding signatures of those molecules, atoms, etc. So now, certainly,
    it can allow you to generate new sequence, new graphs.
  topic: technical/predictions
- impact_reason: 'Crucial insight into controlled generation: foundation models can
    be guided by domain-specific constraints (e.g., toxicity, solubility) during the
    generation of novel outputs.'
  relevance_score: 10
  source: llm_enhanced
  text: And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/business
- impact_reason: 'A critical limitation: the pace of AI development (architecture/concept)
    is currently outpacing the ability of experimental science to generate the necessary
    high-fidelity, real-world data.'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
    So, there's, you know, where AI, the architecture, the concept there, but the,
    sometimes we are limited by the, you know, experimental side.
  topic: limitations/strategy
- impact_reason: Articulates the heightened danger of hallucination in scientific
    domains where outputs (new molecules, gene sets) cannot be instantly validated
    by human inspection, unlike text or code.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that, for example, if you let those AI models generate text,
    code, you probably can immediately judge, okay, whether this makes sense or not.
    Right, right. But when it goes to the bio-medical, biochemistry, etc. [...] Is
    this something that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: Highlights the significant gap between investment interest and successful,
    scaled deployment of GenAI in the life sciences sector, pointing to a major industry
    challenge.
  relevance_score: 9
  source: llm_enhanced
  text: Flash forward to 2024, a recent Deloitte survey notes that while 73% of life
    sciences leaders are investing in generative AI initiatives, fewer than 20% have
    successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: Describes a strategic shift in data management for AI—centralizing
    everything first, then applying advanced algorithms, moving beyond simple document
    processing.
  relevance_score: 9
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization...
  topic: strategy
- impact_reason: 'Clarifies the common architecture pattern: a large central foundation
    model feeding specialized, smaller models for specific use cases.'
  relevance_score: 9
  source: llm_enhanced
  text: We're also describing the relationship as you'll have LLMs, a foundational
    model, and then break off into smaller models for smaller tasks...
  topic: technical
- impact_reason: A powerful analogy linking genetics/biology to language modeling,
    suggesting LLMs can effectively simulate or test biological functions (like a
    digital twin).
  relevance_score: 9
  source: llm_enhanced
  text: Genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process...
  topic: predictions
- impact_reason: Connects the success of molecule design models directly back to the
    underlying, generalized innovation of the Transformer architecture.
  relevance_score: 9
  source: llm_enhanced
  text: And also then when it goes to the new molecule design, then also a lot of
    foundation models have been built. So all those models, you know, essentially
    they have the same, I mean, by nature, they have the same setup. So that is based
    on the breakthrough of the transformer architecture that Google proposed a long
    time ago.
  topic: technical
- impact_reason: 'Lists crucial practical challenges for AI adoption in R&D: evaluation,
    alignment, and benchmarking.'
  relevance_score: 9
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: Highlights the foundational role of the Transformer architecture
    across diverse foundation models, emphasizing architectural uniformity in modern
    AI breakthroughs.
  relevance_score: 9
  source: llm_enhanced
  text: So all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: 'Explains the key functional difference between BERT-like models
    and GPT-like models: the decoder enables generative capabilities, moving beyond
    pure representation.'
  relevance_score: 9
  source: llm_enhanced
  text: But now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data. So for example, a
    new types of sequence that represent your language.
  topic: technical
- impact_reason: Emphasizes the indispensable role of domain experts (e.g., biologists,
    chemists) in data curation, model guidance, and crucially, in the benchmarking
    and evaluation process.
  relevance_score: 9
  source: llm_enhanced
  text: But, you know, we still need the domain knowledge people, you know, people
    have those kind of expertise to guide the, you know, the selection of the data
    and also more importantly, then these models are, you build, now how do we benchmark
    with them? So we would definitely need those, the domain experts to be part of
    this model development process.
  topic: strategy/safety
- impact_reason: 'Proposes the essential solution for model selection in specialized
    fields: hybrid evaluation metrics that combine domain expertise with AI-driven
    scoring.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is key here,
    because there could be so many versions of found data models. Now, if we, you
    know, have those kind of domain plus AI-driven those evaluation metrics, so we
    can score across different models, right?
  topic: safety/strategy
- impact_reason: Defines the term 'foundation model' based on its utility across diverse
    applications.
  relevance_score: 8
  source: llm_enhanced
  text: So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Poses a critical business/strategy question about the build vs. buy
    decision for foundation models in specialized industries.
  relevance_score: 8
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
- impact_reason: Clearly distinguishes BERT's function (encoder-only, representation
    learning/embeddings) from generative models, offering a concise technical explanation
    of model types.
  relevance_score: 8
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they used the decoder
    part of the transformer. So you can think it's as a very good representation model.
    So many, it can capture the hidden context among the, in the text between words,
    etc. [...] So the outcome is some representations or embeddings of those text
    tokens, etc.
  topic: technical
- impact_reason: Identifies a strategic advantage for language-heavy domains (like
    genetics) in the current GenAI wave, as NLP techniques translate more directly.
  relevance_score: 8
  source: llm_enhanced
  text: that so much around model development can kind of reflect how biologists already
    do things. And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right? That
    builds in certain advantage to genetics, which is more language-based than anything
    else versus, you know, other domains of health and life sciences, the services
    involved.
  topic: strategy/predictions
- impact_reason: Highlights that even with massive data ingestion capabilities, data
    quality remains the primary bottleneck for domain-specific foundation models.
  relevance_score: 8
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: business/technical
- impact_reason: Identifies organizational alignment and communication between technical
    builders (AI modelers), domain experts (scientists), and decision-makers (leadership)
    as a key execution hurdle.
  relevance_score: 8
  source: llm_enhanced
  text: So, it's quite important that we have to do those benchmarking and select
    models accordingly. And then it goes to the execution, right? So, there's a lot
    of cultural organizational issue. I see the key that really we should align, you
    know, between different parties, like scientists, basic scientists, or AI modelers,
    AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: Describes the common organizational friction where speed of AI development
    outpaces the ability of domain experts to integrate and validate the results,
    necessitating structured validation pipelines.
  relevance_score: 8
  source: llm_enhanced
  text: sometimes AI modelers, they may be more advanced, they are, you know, first-handed,
    they can build pipelines, models, you know, very quickly, but how do we digest
    the models, the capabilities? And again, we have to plug in, you know, the right
    scientists to help us together validate those.
  topic: business/strategy
- impact_reason: 'A concise summary of the necessary organizational structure for
    successful AI implementation in specialized fields like healthcare: mandatory
    cross-disciplinary teamwork.'
  relevance_score: 7
  source: llm_enhanced
  text: So, it's basically a cross-discipline collaboration, whether in house or externally,
    yeah.
  topic: strategy
- impact_reason: 'Strategic advice for subject matter experts: they must understand
    the hybrid nature of AI adoption, recognizing that older methods persist alongside
    new AI paradigms.'
  relevance_score: 7
  source: llm_enhanced
  text: It behooves them to kind of have some awareness of the new and old paradigm
    that we were talking about before, how both are still present, but how we thought
    that AI would impact life sciences organization or that they'd just be this one
    school way of doing things is not the case. And that door remains open.
  topic: strategy
source: Unknown Source
summary: '## Comprehensive Summary of "Building AI-Ready Cultures in Life Sciences
  R&D" with Xiong Liu of Novartis


  This 30-minute podcast episode featuring **Xiong Liu, Director of Data Science in
  AI at Novartis**, focuses on the paradigm shift in Life Sciences R&D driven by **Generative
  AI (GenAI) and Foundation Models (FMs)**, and the cultural and technical challenges
  associated with scaling these technologies beyond pilot projects.


  ### 1. Focus Area

  The primary focus is the transition from traditional, task-specific Machine Learning
  (ML) to leveraging large-scale **Foundation Models** (like LLMs) in drug discovery,
  protocol design, and clinical workflows. Key themes include the architectural differences
  between older ML models (like BERT) and generative models (like GPT), the necessity
  of domain-specific fine-tuning, and the organizational alignment required to successfully
  integrate AI into R&D.


  ### 2. Key Technical Insights

  *   **Paradigm Shift to FMs:** The conversation highlights the move from training
  models solely on proprietary, labeled data to leveraging massive, pre-trained Foundation
  Models that capture general domain knowledge (e.g., public molecular data, human
  language corpus). This allows for better performance even with small, specific downstream
  datasets via **fine-tuning**.

  *   **Architectural Evolution:** The discussion contrasts older representation models
  (like BERT, using the decoder part of the transformer) with modern generative models
  (like GPT, leveraging the decoder part), enabling the creation of novel outputs
  such as new molecular sequences (represented as SMILES strings or graphs) constrained
  by desired properties (e.g., toxicity, solubility).

  *   **Digital Twins Analogy:** Foundation models, especially when applied to genetic
  code and population data, can act as a form of **digital twin** by modeling normal
  biological functions, allowing researchers to test hypotheses *in silico* against
  personalized or specific patient data.


  ### 3. Business/Investment Angle

  *   **Scaling Challenge:** Despite high investment (73% of leaders investing), scaling
  GenAI beyond pilots remains a major hurdle for life sciences organizations (fewer
  than 20% successful).

  *   **Data Dependency vs. Model Power:** While FMs reduce the reliance on massive
  proprietary datasets for initial training, the quality and availability of highly
  specific, experimental data (like single-cell RNA-seq) remain a bottleneck, limiting
  the immediate scope of model application.

  *   **ROI and Evaluation:** A critical business challenge is establishing **domain-driven
  evaluation standards** and knowledge-checking benchmarks to accurately measure the
  ROI and select the optimal models, especially given the risk of AI **hallucination**
  in complex biological outputs.


  ### 4. Notable Companies/People

  *   **Xiong Liu (Novartis):** Guest expert providing insights from a major pharmaceutical
  company on building AI-ready cultures and the technical evolution of AI in R&D.

  *   **Google (Transformer Architecture):** Mentioned as the originator of the foundational
  transformer architecture that underpins modern LLMs and domain-specific FMs.

  *   **Deloitte:** Cited for a recent survey highlighting the gap between GenAI investment
  and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is moving toward **domain-specific Foundation Models** tailored for
  chemistry, biology, and clinical trial optimization. Success hinges not just on
  technological advancement but on **cross-disciplinary collaboration**—ensuring tight
  alignment and communication between AI modelers, basic scientists (biologists, chemists),
  and leadership. The future requires robust, domain-aware evaluation frameworks to
  validate AI-generated hypotheses before costly experimental validation.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Leaders** within the Life Sciences
  sector, **R&D Executives**, **Data Science Directors**, and **Strategy Professionals**
  navigating the practical adoption, scaling, and cultural integration of advanced
  AI/ML technologies.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 121
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 12
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 06:36:35 UTC -->
