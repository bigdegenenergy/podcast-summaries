---
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew D'Mello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew D'Mello, Editorial
    Director here at Emerge AI Resea
  name: Matthew D
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew D'Mello, Editorial Director here
    at Emerge AI Research. Today's guest is Suni
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew D'Mello, Editorial Director here at Emerge AI Research. Today's
    guest is Suni Tharou, Special Vice Presi
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Suni Tharou, Special
    Vice President and General Manager for H
  name: Suni Tharou
  position: 134
- category: unknown
  confidence: medium
  context: Emerge AI Research. Today's guest is Suni Tharou, Special Vice President
    and General Manager for Hybrid Cloud Business at
  name: Special Vice President
  position: 147
- category: unknown
  confidence: medium
  context: guest is Suni Tharou, Special Vice President and General Manager for Hybrid
    Cloud Business at Hitachi Vantara. Hit
  name: General Manager
  position: 174
- category: unknown
  confidence: medium
  context: u, Special Vice President and General Manager for Hybrid Cloud Business
    at Hitachi Vantara. Hitachi Vantara is a wholly o
  name: Hybrid Cloud Business
  position: 194
- category: unknown
  confidence: medium
  context: and General Manager for Hybrid Cloud Business at Hitachi Vantara. Hitachi
    Vantara is a wholly owned subsidiary of
  name: Hitachi Vantara
  position: 219
- category: unknown
  confidence: medium
  context: . Hitachi Vantara is a wholly owned subsidiary of Hitachi Limited that
    provides data infrastructure foundations tha
  name: Hitachi Limited
  position: 284
- category: tech
  confidence: high
  context: dustries can modernize their data infrastructure, scale AI deployments,
    and align IT investments with their
  name: Scale Ai
  position: 701
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1468
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yoshua Bengio. With nearly
    a million annual listeners, AI and B
  name: Yoshua Bengio
  position: 1533
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit emerj.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1999
- category: unknown
  confidence: medium
  context: eedle on AI ROI, visit emerj.com and fill out our Thought Leader submission
    form. That's emerj.com and click on "B
  name: Thought Leader
  position: 2040
- category: unknown
  confidence: medium
  context: ng posed by explosive data growth for AI systems? So I think we should
    start with data itself as one of
  name: So I
  position: 3598
- category: unknown
  confidence: medium
  context: that's the second challenge. You have networking. As I said, data starts,
    how do you do the silo busters
  name: As I
  position: 4581
- category: unknown
  confidence: medium
  context: aid, data starts, how do you do the silo busters? When I say silo busters,
    AI clusters demand ultra-low la
  name: When I
  position: 4637
- category: unknown
  confidence: medium
  context: leads us more to data quality versus data volume. And I think that's the
    changing attitude across industr
  name: And I
  position: 6818
- category: tech
  confidence: high
  context: he noisy, duplicated data? How do you look at the gradient variance for
    these data sets? You need to have a
  name: Gradient
  position: 7951
- category: unknown
  confidence: medium
  context: of infrastructure than we're used to, even at the Gen AI explosion when
    everything first really—I call it
  name: Gen AI
  position: 9680
- category: unknown
  confidence: medium
  context: losion when everything first really—I call it the Ed Sullivan moment for
    LLMs—where everybody and their kid had
  name: Ed Sullivan
  position: 9740
- category: unknown
  confidence: medium
  context: itachi Vantara leads the pack in this order. It's Energy Star certified,
    and we have a huge amount of investmen
  name: Energy Star
  position: 16164
- category: unknown
  confidence: medium
  context: st, data quality matters more than data quantity. As Sunita put it, garbage
    in doesn't just mean garbage out;
  name: As Sunita
  position: 17462
- category: ai_media_research
  confidence: high
  context: The organization publishing the 'AI and Business Podcast' and featuring
    executive thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Sponsor of the podcast series, providing data infrastructure foundations
    to help manage and leverage data at scale for AI deployments.
  name: Hitachi Vantara
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Parent company of Hitachi Vantara.
  name: Hitachi Limited
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Mentioned as having a CIO who is an executive thought leader featured on
    the podcast, indicating enterprise AI adoption.
  name: Goldman Sachs
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Mentioned as having a head of AI who is an executive thought leader featured
    on the podcast, indicating enterprise AI adoption.
  name: Raytheon
  source: llm_enhanced
- category: ai_researcher
  confidence: high
  context: Referenced as an AI pioneer featured on the show, indicating a connection
    to foundational AI research.
  name: Yoshua Bengio
  source: llm_enhanced
- category: media_and_analysis
  confidence: high
  context: The organization producing the 'AI and Business Podcast' that features
    executive thought leaders on AI deployment and strategy.
  name: Emerge AI Research (Emerj AI Research)
  source: llm_enhanced
date: 2025-10-09 06:00:00 +0000
duration: 22
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: start with data itself as one of the key challenges because in the context
    of AI, there's a requirement for massive data inflow that's clean, accessible,
    and also it actually is scattered across, not just siloed in one location
  text: we should start with data itself as one of the key challenges because in the
    context of AI, there's a requirement for massive data inflow that's clean, accessible,
    and also it actually is scattered across, not just siloed in one location.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: look at not building bigger haystacks, but looking at how do you have
    better needles in the system? That's when you will improve the infrastructure
    degradation aspect of the data flows
  text: we should look at not building bigger haystacks, but looking at how do you
    have better needles in the system? That's when you will improve the infrastructure
    degradation aspect of the data flows.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business---10.9.25_-Sunitha_Rao.mp3?dest-id=151434
processing_date: 2025-10-09 08:37:13 +0000
quotes:
- length: 296
  relevance_score: 7
  text: The last one is you have to have leaders solve this challenge by moving into
    a unified framework where you need AI-ready platforms with elastic compute, storage
    that could auto-tier with an integrated MLOps framework to basically address all
    of these gaps that are kind of influenced at the start
  topics: []
- length: 246
  relevance_score: 5
  text: The most important thing is you need to have KPIs on a pretty regular basis
    so that the KPIs are the key performance indicators that basically help us to
    track the data freshness, it will help you to track the training-to-serving skew
    percentages
  topics: []
- length: 149
  relevance_score: 5
  text: The most important aspect is where you want the data to land, and that defines
    the investments, that defines the ROI, that defines the sustainability
  topics:
  - investment
- length: 167
  relevance_score: 4
  text: Through data storage, infrastructure systems, cloud management, and digital
    expertise, the company helps customers build the foundation for sustainable business
    growth
  topics:
  - growth
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 231
  relevance_score: 4
  text: How do you look at transformations in the training and serving workflows that
    could come up with a seamless path towards execution, and then batch and search
    controls and retrieve worlds and all the RAG pipelines with vector stores
  topics: []
- length: 141
  relevance_score: 4
  text: Are you driving AI transformation at your organization, or maybe you're guiding
    critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 194
  relevance_score: 3
  text: Suni Tharou joins us today to break down how enterprises across industries
    can modernize their data infrastructure, scale AI deployments, and align IT investments
    with their sustainability goals
  topics:
  - investment
- length: 135
  relevance_score: 3
  text: It actually can be fixable, provided you have to have earlier workflows that
    can best look at how do you really look at the error floor
  topics: []
- length: 94
  relevance_score: 3
  text: I think the most important piece that we forgot in the data quality is the
    bias towards safety
  topics: []
- length: 69
  relevance_score: 3
  text: If you have to take an example, let's take streaming ETL, for example
  topics: []
- length: 148
  relevance_score: 3
  text: The most important concept in all of these three different frameworks that
    I spoke about just now in the context of degradation, especially, is SLOs
  topics: []
- length: 226
  relevance_score: 3
  text: '" That''s the most important piece in the entire framework of building the
    ROI and sustainability, where you need to have a policy engine which basically
    looks at the defined outcomes for the data to be resident in its location'
  topics: []
- impact_reason: Elevates cost and sustainability (ESG) from an afterthought to the
    primary challenge in scaling AI infrastructure, linking it directly to ROI.
  relevance_score: 10
  source: llm_enhanced
  text: You need to really talk about cost and sustainability. And though it's coming
    at the last at the end, I would say that should be primary number one challenge
    that we need to solve for, especially bringing in the ESG and goals and also having
    a clearer ROI around all these things.
  topic: business/safety
- impact_reason: A concise, powerful rephrasing of the GIGO principle, emphasizing
    the high cost associated with poor data quality in large-scale AI.
  relevance_score: 10
  source: llm_enhanced
  text: Garbage in is an expensive garbage out.
  topic: business/technical
- impact_reason: Directly links poor data quality to amplified security risks and
    data leakage within iterative training loops.
  relevance_score: 10
  source: llm_enhanced
  text: Poor quality, skewed data will basically amplify the security risks that come
    out as the data gets transitioned from one face to another. So the leakage that
    comes because of this also gets injected back because you're doing a train-test,
    train-test, you know, you're kind of in the same loop.
  topic: safety
- impact_reason: Identifies Service Level Objectives (SLOs) as the critical concept
    for managing and preventing model degradation.
  relevance_score: 10
  source: llm_enhanced
  text: The most important concept in all of these three different frameworks that
    I spoke about just now in the context of degradation, especially, is SLOs.
  topic: technical/strategy
- impact_reason: Crucially links KPIs to tracking core MLOps health metrics like data
    freshness and training-serving skew, which are vital for model integrity.
  relevance_score: 10
  source: llm_enhanced
  text: The most important thing is you need to have KPIs on a pretty regular basis
    so that the KPIs are the key performance indicators that basically help us to
    track the data freshness, it will help you to track the training-to-serving skew
    percentages.
  topic: technical
- impact_reason: 'Offers a core strategic recommendation for managing AI infrastructure
    costs and performance: workload placement optimization.'
  relevance_score: 10
  source: llm_enhanced
  text: I would really recommend looking at mapping workloads to the right execution
    venue. That is the key.
  topic: business/strategy
- impact_reason: Establishes data residency/location as the foundational decision
    driving investment, financial return, and environmental impact for AI initiatives.
  relevance_score: 10
  source: llm_enhanced
  text: The most important aspect is where you want the data to land, and that defines
    the investments, that defines the ROI, that defines the sustainability.
  topic: business/strategy
- impact_reason: Provides a powerful, memorable analogy for integrating carbon awareness
    into financial planning, treating energy/sustainability as a quantifiable cost.
  relevance_score: 10
  source: llm_enhanced
  text: I still believe somebody used to, when we really started talking about the
    carbon aware, people now call it "use carbon like cash."
  topic: safety/strategy
- impact_reason: A sharp rephrasing of the classic data quality adage, specifically
    highlighting the financial penalty (expensive) associated with poor data quality
    in modern, resource-intensive AI systems.
  relevance_score: 10
  source: llm_enhanced
  text: garbage in doesn't just mean garbage out; it means expensive garbage out.
  topic: business/strategy
- impact_reason: 'This clearly outlines the primary data challenges for AI: volume,
    quality, accessibility, and distribution (silo busting).'
  relevance_score: 9
  source: llm_enhanced
  text: I think we should start with data itself as one of the key challenges because
    in the context of AI, there's a requirement for massive data inflow that's clean,
    accessible, and also it actually is scattered across, not just siloed in one location.
  topic: technical/strategy
- impact_reason: Highlights the common misconception that compute alone solves AI
    scaling issues, pointing directly to hardware constraints (GPUs/accelerators)
    as a bottleneck.
  relevance_score: 9
  source: llm_enhanced
  text: The next big ticket item is compute. I think we have a real misconception
    about thinking that adding more data centers, adding more compute is going to
    simplify stuff. But we haven't really thought through around the shortages of
    the GPUs, accelerators.
  topic: technical/strategy
- impact_reason: Details the inadequacy of legacy storage models for AI workloads
    and calls for a unified approach considering diverse access patterns (file, block,
    object) and lifecycle management.
  relevance_score: 9
  source: llm_enhanced
  text: 'We have really not thought through around the storage infrastructure, especially
    because it''s legacy, it''s actually really focused on a specific model. Are you
    talking about file, block, object? We kind of really looked into that kind of
    framework. We need to really look at how do you mix all of these in the context
    of AI, read/write patterns, scaling linearly, tiering, removing unused data blocks,
    and basically the other important piece: how do you unify all of these things
    together?'
  topic: technical
- impact_reason: 'Provides a concrete strategic solution: unified, AI-ready platforms
    integrating elastic resources and MLOps.'
  relevance_score: 9
  source: llm_enhanced
  text: The last one is you have to have leaders solve this challenge by moving into
    a unified framework where you need AI-ready platforms with elastic compute, storage
    that could auto-tier with an integrated MLOps framework to basically address all
    of these gaps that are kind of influenced at the start.
  topic: strategy
- impact_reason: Reinforces the financial risk of bad data, moving beyond simple conceptual
    understanding to quantifiable business impact.
  relevance_score: 9
  source: llm_enhanced
  text: Garbage out is expensive. Extremely expensive at scale.
  topic: business
- impact_reason: Offers technical dimensions for data quality beyond simple cleaning,
    focusing on diversity, noise reduction, and improving OOD performance.
  relevance_score: 9
  source: llm_enhanced
  text: 'How do you look at the gradient variance for these data sets? You need to
    have a model that basically creates a framework around the quality of data itself,
    which is a specific generalization, not really memorizing it, but calling it out:
    clean, diverse, deduplicated, reducing the noise frameworks, and improve the out-of-the-distribution
    performance requirements for the data quality.'
  topic: technical
- impact_reason: 'A strong strategic metaphor: focus on data quality and efficiency
    (''better needles'') rather than just raw infrastructure scale (''bigger haystacks'').'
  relevance_score: 9
  source: llm_enhanced
  text: We should look at not building bigger haystacks, but looking at how do you
    have better needles in the system? That's when you will improve the infrastructure
    degradation aspect of the data flows.
  topic: strategy
- impact_reason: 'Provides actionable infrastructure requirements for data pipelines:
    freshness, schema checks, anomaly detection, and PII governance integrated early.'
  relevance_score: 9
  source: llm_enhanced
  text: I think it's important to look at the data freshness and the quality gates.
    If you have to take an example, let's take streaming ETL, for example. You need
    schema checks, anomaly detection, and for example, PII, so that we know what kind
    of information that is being used.
  topic: technical/strategy
- impact_reason: 'Provides a clear definition of SLOs in the AI context: contractual
    commitments regarding workflow performance, moving beyond simple latency metrics.'
  relevance_score: 9
  source: llm_enhanced
  text: SLO is nothing but a service level objective, which is what is signed up when
    you basically build a framework or a model for the customer to look at what commitments
    can you provide in terms of each of these workflows that we've been talking about.
  topic: technical
- impact_reason: Connects SLOs directly to measurable outcomes, specifically highlighting
    the critical need to manage offline/online parity in training and serving.
  relevance_score: 9
  source: llm_enhanced
  text: 'You need to see, especially when you define a service level objective, you
    need to look at how to improve outcomes for the infrastructure. Simple example
    again: offline/online parities.'
  topic: technical/business
- impact_reason: Highlights a critical shift in MLOps/SRE focus from simple latency
    metrics to broader Service Level Objectives (SLOs) that encompass degradation,
    moving beyond a single performance indicator.
  relevance_score: 9
  source: llm_enhanced
  text: SLOs. It's not about just setting the SLO, but looking at a breach. We just
    can't have the concept of latency being the only concept.
  topic: technical/strategy
- impact_reason: Establishes SLOs/SLCs as fundamental, non-negotiable requirements
    for deploying and managing AI systems, similar to basic infrastructure guarantees.
  relevance_score: 9
  source: llm_enhanced
  text: service level objectives and service level commitments have become the underlying
    table stakes for everything we do in the AI world.
  topic: business/strategy
- impact_reason: Identifies offline/online parity as a key challenge that SLOs and
    infrastructure design must address for reliable model deployment.
  relevance_score: 9
  source: llm_enhanced
  text: 'Simple example again: offline/online parities. How do you look at transformations
    in the training and serving workflows that could come up with a seamless path
    towards execution...'
  topic: technical
- impact_reason: Directly mentions RAG pipelines and vector stores, indicating these
    complex components must be integrated into the SLO/KPI framework.
  relevance_score: 9
  source: llm_enhanced
  text: '...and then batch and search controls and retrieve worlds and all the RAG
    pipelines with vector stores?'
  topic: technical
- impact_reason: Suggests the need for automated policy engines to govern data placement
    based on desired outcomes (cost, carbon, performance).
  relevance_score: 9
  source: llm_enhanced
  text: where you need to have a policy engine which basically looks at the defined
    outcomes for the data to be resident in its location.
  topic: technical/strategy
- impact_reason: Describes a practical, tunable mechanism for optimizing infrastructure
    placement using weighted priorities (cost, carbon, latency) based on business
    needs.
  relevance_score: 9
  source: llm_enhanced
  text: You could put weights around the cost of the infrastructure. You could have
    carbon savings added to it. You could look at performance latency, and you could
    tune in by like percentage of weights and provide a business priority around it.
  topic: technical/strategy
- impact_reason: 'Defines the modern requirement for AI infrastructure: integration
    and unification across disparate environments, moving beyond simple hardware provisioning.'
  relevance_score: 9
  source: llm_enhanced
  text: infrastructure is no longer just hardware. Enterprises need unified frameworks
    that bring together data, compute, storage, and orchestration across hybrid and
    multi-cloud environments.
  topic: business/technical
- impact_reason: 'Summarizes the core strategic thesis: efficiency (sustainability)
    is not a trade-off but a driver of financial return (ROI) in AI deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: sustainability and ROI go hand in hand. Leaders must map workloads to the
    right execution venue and design with efficiency in mind, from power and cooling
    to carbon-aware storage.
  topic: business/strategy
- impact_reason: Specifies the critical networking requirements unique to large-scale
    AI/ML training (low latency, high bandwidth), essential for distributed systems.
  relevance_score: 8
  source: llm_enhanced
  text: AI clusters demand ultra-low latency, high-bandwidth links with different
    streams around the distributed training frameworks that AI brings up.
  topic: technical
- impact_reason: Connects the evolution of orchestration directly to MLOps and the
    necessity of managing hybrid/multi-cloud environments to optimize pipelines.
  relevance_score: 8
  source: llm_enhanced
  text: With the hybrid environment, the definition of orchestration changed, and
    now with MLOps and workloads specific to the MLOps, you need to really look at
    how do you remove pipeline inefficiencies by bringing together a hybrid multi-cloud
    environment into the orchestration infrastructure.
  topic: technical/strategy
- impact_reason: Pushes monitoring and alerting beyond static thresholds toward self-learning,
    continuous models for data quality management.
  relevance_score: 8
  source: llm_enhanced
  text: This has to become like a self-learning model that could keep on a continuous
    motion, keep its path at every stage of it.
  topic: technical
- impact_reason: Emphasizes proactive monitoring (pass/fail rates) as the mechanism
    to identify the root cause and implement controls within the AI lifecycle degradation
    framework.
  relevance_score: 8
  source: llm_enhanced
  text: You could look at pass rate versus the failure. A lot of things that will
    help us understand within the lifecycle where does this entire degradation framework
    begin so that we could put the control, appropriate controls back into the system.
  topic: strategy/technical
- impact_reason: Details the cascading effect of data location decisions across all
    major business and technical pillars (performance, cost, compliance).
  relevance_score: 8
  source: llm_enhanced
  text: Because it then translates into the requirements for performance, compliance,
    cost models, ROI, all of these great elements.
  topic: strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Data Volume, Quality, and Model Degradation
  for AI at Scale - with Sunitha Rao of Hitachi Vantara


  This episode focuses on the critical, often overlooked, infrastructure and data
  management challenges enterprises face when scaling AI deployments, moving beyond
  the initial hype cycles to achieve measurable ROI and sustainability.


  ---


  **1. Focus Area:**

  The discussion centers on **AI Infrastructure Modernization**, specifically addressing
  the ripple effects of explosive data growth on AI model performance, data quality
  governance, compute resource management (especially GPUs), and integrating sustainability
  (ESG) into IT investment decisions. Key themes include data pipeline quality control,
  mitigating model degradation, and defining infrastructure in the context of hybrid/multi-cloud
  MLOps.


  **2. Key Technical Insights:**

  *   **Expensive Garbage Out:** Poor data quality (noise, duplication, bias) leads
  to significantly expensive failures at scale, necessitating proactive quality gates
  (schema checks, anomaly detection) early in the data pipeline, not just during training.

  *   **Infrastructure Redefined:** Modern AI infrastructure is fluid, requiring unified
  frameworks that seamlessly integrate data, elastic compute, tiered storage, and
  MLOps orchestration across hybrid environments.

  *   **SLOs for Degradation Control:** Service Level Objectives (SLOs) must move
  beyond simple latency metrics to encompass data freshness, training-to-serving skew
  percentages, and overall outcome KPIs to prevent preventable model degradation.


  **3. Business/Investment Angle:**

  *   **Workload Mapping is Key to ROI:** The primary driver for successful investment,
  ROI, and sustainability is accurately mapping specific workloads to the correct
  execution venue (on-prem, cloud, edge).

  *   **Sustainability as Cost Control:** Aligning IT investments with ESG goals is
  crucial; leaders should "use carbon like cash" by implementing policy engines that
  weigh cost, carbon savings, and performance when placing data residency.

  *   **Focus on Better Needles:** Investment should shift from simply building bigger
  compute "haystacks" to deploying "better needles"—smarter, more efficient data management
  and quality tools within the existing system.


  **4. Notable Companies/People:**

  *   **Sunitha Rao (Guest):** Special Vice President and General Manager for Hybrid
  Cloud Business at **Hitachi Vantara**. She provides the expert perspective on data
  infrastructure foundations, hybrid cloud management, and aligning AI scaling with
  sustainability.

  *   **Hitachi Vantara:** Mentioned as a company focused on providing energy-efficient
  storage frameworks and unified platforms to address these infrastructure gaps.


  **5. Future Implications:**

  The industry is moving toward a highly integrated, policy-driven infrastructure
  where data placement, resource utilization, and sustainability metrics are intrinsically
  linked to model performance. The definition of infrastructure will become increasingly
  abstract and fluid, driven by workload requirements rather than just physical hardware.
  Continuous, self-learning monitoring systems will replace static threshold alerts
  to manage complex data flows.


  **6. Target Audience:**

  **AI/ML Leaders, Enterprise IT Executives (CIOs, CTOs), Data Strategy Officers,
  and Infrastructure Architects** who are responsible for scaling AI deployments,
  managing hybrid cloud environments, and justifying AI investment ROI while meeting
  corporate sustainability mandates.


  ---


  ### Comprehensive Summary


  The podcast episode with Sunitha Rao of Hitachi Vantara provides a pragmatic deep
  dive into the infrastructure realities underpinning enterprise AI success, arguing
  that the focus must shift from sheer data volume and compute capacity to data quality,
  infrastructure unification, and sustainable placement strategies.


  **Narrative Arc and Key Discussion Points:**

  The conversation begins by acknowledging the widespread challenge: enterprises are
  investing heavily in AI, yet ROI is often elusive, partly due to the explosive growth
  of data. Rao systematically breaks down the core infrastructure challenges: **data
  silos** requiring "silo busters," **compute bottlenecks** (GPU shortages), **networking
  demands** (ultra-low latency), inadequate **storage frameworks** (legacy systems
  not suited for AI read/write patterns), and the complexity of **orchestration**
  in hybrid/multi-cloud MLOps environments. She concludes this section by emphasizing
  that **cost and sustainability (ESG)** must be primary considerations, not afterthoughts.


  The discussion pivots to the crucial distinction between **data volume and data
  quality**. Rao stresses that "garbage in is an expensive garbage out," highlighting
  that poor quality data introduces security risks (leakage, bias amplification) and
  significantly inflates operational costs. She advocates for establishing rigorous
  quality frameworks—focusing on clean, diverse, and deduplicated data—before training
  begins.


  Rao then redefines **modern infrastructure** as less about "hammers and nails" and
  more about deploying "better needles." To combat model degradation, she outlines
  three critical control frameworks:

  1.  **Data Freshness and Quality Gates:** Implementing checks like schema validation
  and PII detection in streaming ETL pipelines (e.g., Hitachi Vantara’s PII data service).

  2.  **Early Alerting and Detection:** Moving toward self-learning models for continuous
  monitoring and automated playbook responses.

  3.  **Reproducibility and Versioning:** Tightly linking data sets, features, and
  code bases.


  The concept of **Service Level Objectives (SLOs)** is introduced as the mechanism
  to enforce these controls, moving beyond simple latency to track KPIs like training-to-serving
  skew and pass rates.


  Finally, the conversation addresses **cost, scale, and sustainability**. Rao’s core
  recommendation is **mapping workloads to the right execution venue**. This decision
  dictates performance, compliance, cost models, and sustainability outcomes. She
  advocates for a "carbon-aware" approach, using policy engines to weigh infrastructure
  cost against carbon savings when determining where data should reside, aligning
  infrastructure investment directly with measurable ROI and ESG goals.'
tags:
- artificial-intelligence
- ai-infrastructure
- investment
title: Data Volume, Quality, and Model Degradation for AI at Scale - with Sunitha
  Rao of Hitachi Vantara
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 65
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 5
  prominence: 0.5
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 5
  prominence: 0.5
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-09 08:37:13 UTC -->
