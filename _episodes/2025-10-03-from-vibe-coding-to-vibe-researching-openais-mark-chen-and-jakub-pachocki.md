---
companies:
- category: tech
  confidence: high
  context: earcher and can AI discover new ideas on its own? OpenAI's chief scientist,
    Ilya Sutskever, and chief rese
  name: Openai
  position: 503
- category: unknown
  confidence: medium
  context: r new ideas on its own? OpenAI's chief scientist, Ilya Sutskever, and chief
    research officer Mark Chen joined A16Z
  name: Ilya Sutskever
  position: 529
- category: unknown
  confidence: medium
  context: ntist, Ilya Sutskever, and chief research officer Mark Chen joined A16Z
    General Partners, Audine Mita, and Sa
  name: Mark Chen
  position: 572
- category: unknown
  confidence: medium
  context: and chief research officer Mark Chen joined A16Z General Partners, Audine
    Mita, and Sarah Wang to unpack GPT-5's re
  name: General Partners
  position: 594
- category: unknown
  confidence: medium
  context: h officer Mark Chen joined A16Z General Partners, Audine Mita, and Sarah
    Wang to unpack GPT-5's reasoning quest
  name: Audine Mita
  position: 612
- category: unknown
  confidence: medium
  context: en joined A16Z General Partners, Audine Mita, and Sarah Wang to unpack
    GPT-5's reasoning question, why evals m
  name: Sarah Wang
  position: 629
- category: unknown
  confidence: medium
  context: in a little bit of deficit of great evaluations. And I think the big things
    that we look at are actual m
  name: And I
  position: 3773
- category: unknown
  confidence: medium
  context: performance in math and programming competitions. Although I think like
    they are also becoming saturated in a
  name: Although I
  position: 4029
- category: unknown
  confidence: medium
  context: his huge move toward agency in model development. But I think at least
    the state that it's in currently u
  name: But I
  position: 7669
- category: unknown
  confidence: medium
  context: lot of it's being consistent over long horizons. So I think there are very
    related problems. And in fac
  name: So I
  position: 8506
- category: tech
  confidence: high
  context: ot better presets. Coders, they have some kind of notion of this is how
    long I'm willing to wait for a par
  name: Notion
  position: 15150
- category: unknown
  confidence: medium
  context: or a decade now, but I was struck by the story of Lee Sedol, the Go player
    who kind of famously quit Go after
  name: Lee Sedol
  position: 15990
- category: unknown
  confidence: medium
  context: searcher? Persistence is a very key trait, right? Like I think what is
    different about research when you'r
  name: Like I
  position: 19864
- category: unknown
  confidence: medium
  context: e distill their experience into your own process. When I was in grad school,
    there's a big part. I was a f
  name: When I
  position: 21552
- category: unknown
  confidence: medium
  context: the business of discovering new things about the Deep Learning stack and
    I think we're kind of building somethin
  name: Deep Learning
  position: 25495
- category: tech
  confidence: high
  context: Mentioned throughout as the developer of GPT-5, Codex, and ChatGPT, and
    the focus of the discussion regarding research culture, evals, and the automated
    researcher.
  name: OpenAI
  source: llm_enhanced
- category: finance
  confidence: high
  context: Mentioned as the venture capital firm whose General Partners (Audine Mita
    and Sarah Wang) joined the discussion.
  name: A16Z
  source: llm_enhanced
- category: tech
  confidence: high
  context: A specific model developed by OpenAI, discussed in detail regarding its
    reasoning capabilities and launch.
  name: GPT-5
  source: llm_enhanced
- category: tech
  confidence: high
  context: A specific model/series from OpenAI, mentioned alongside GPT-5 and ChatGPT,
    and later discussed specifically regarding its focus on real-world coding.
  name: Codex
  source: llm_enhanced
- category: tech
  confidence: high
  context: Mentioned as one of the successful products developed by OpenAI alongside
    GPT-5 and Codex.
  name: ChatGPT
  source: llm_enhanced
- category: tech
  confidence: high
  context: Mentioned extensively as the company where the speakers work, developing
    models like Codex and GPT-5, and focusing on fundamental research.
  name: OpenAI
  source: llm_enhanced
- category: tech
  confidence: high
  context: Mentioned as a formative milestone in AI development, referencing its famous
    match against Lee Sedol.
  name: AlphaGo
  source: llm_enhanced
- category: tech
  confidence: high
  context: Mentioned as the latest coding tool that is accelerating coding tasks significantly.
  name: GPT-5
  source: llm_enhanced
date: 2025-10-03 20:23:27 +0000
duration: 52
has_transcript: false
insights:
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
layout: episode
llm_enhanced: true
original_url: https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/3f86df7b-51c6-4101-88a2-550dba782de8/episodes/8eb7117c-1a78-4b60-ab9e-b048e53946c9/audio/128/default.mp3?aid=rss_feed&awCollectionId=3f86df7b-51c6-4101-88a2-550dba782de8&awEpisodeId=8eb7117c-1a78-4b60-ab9e-b048e53946c9&feed=JGE3yC0V
processing_date: 2025-10-03 20:23:27 +0000
quotes:
- length: 178
  relevance_score: 4
  text: The biggest things that OpenAI has going for it in terms of keeping the best
    people motivated and excited, as I said, is that we are in the business of doing
    fundamental research
  topics: []
- length: 110
  relevance_score: 3
  text: And so for a lot of them, inching from like 96 to 98% is not necessarily the
    most important thing in the world
  topics: []
- length: 115
  relevance_score: 3
  text: Are you trying to approach, it doesn't work, and you have to think about what's
    the next approach I'm going to take
  topics: []
- length: 105
  relevance_score: 3
  text: So I think maybe the most important part of the mindset is to not assume that
    what is now will be forever
  topics: []
- length: 144
  relevance_score: 3
  text: And you want to persist that, but you have to be honest with yourself about
    when it's working and when it's not so that you can learn and adjust
  topics: []
- length: 184
  relevance_score: 3
  text: A lot of our most successful researchers have started their journey with deep
    learning at OpenAI and have worked in other fields like physics or computer science
    or finance in the past
  topics: []
- length: 95
  relevance_score: 3
  text: I think actually the most important thing is just to make sure you protect
    fundamental research
  topics: []
- impact_reason: 'This clearly states the ultimate research goal: automating the discovery
    process, which is a massive leap beyond current AI capabilities.'
  relevance_score: 10
  source: llm_enhanced
  text: The big thing that we are targeting is producing an automated researcher.
    So automating the discovery of new ideas.
  topic: technology
- impact_reason: This signals a critical shift in AI evaluation from academic benchmarks
    to real-world, economically valuable outcomes, a key concern for business leaders.
  relevance_score: 10
  source: llm_enhanced
  text: The next set of evals and milestones that we're looking at will involve actual
    discovery and actual movement on things that are economically relevant.
  topic: business
- impact_reason: A provocative, memorable statement suggesting a future where research
    intuition (vibe) is automated, contrasting with the current 'vibe coding' mentioned.
  relevance_score: 8
  source: llm_enhanced
  text: I think the future hopefully will be vibe researching.
  topic: technology
- impact_reason: 'This encapsulates the core strategic direction of advanced AI development:
    moving from pattern matching to complex, multi-step reasoning embodied in agents.'
  relevance_score: 9
  source: llm_enhanced
  text: We think the future is about reasoning, more and more about reasoning, more
    and more about agents.
  topic: technology
- impact_reason: 'Highlights a major industry challenge: the current evaluation metrics
    are saturated and inadequate for measuring true progress toward AGI or advanced
    discovery.'
  relevance_score: 9
  source: llm_enhanced
  text: I think we are definitely think like we are in a little bit of deficit of
    great evaluations. And I think the big things that we look at are actual marks
    of the model being able to discover new things.
  topic: technology
- impact_reason: Justifies using high-level competitions (like AtCoder/IMO) as valid,
    real-world proxies for future research capability.
  relevance_score: 7
  source: llm_enhanced
  text: I think a lot of the best researchers in the world have gone through these
    competitions and have gotten very good results. And yeah, I think we are kind
    of preparing for this frontier where we're trying to get our models to discover
    new things.
  topic: startups
- impact_reason: Provides concrete evidence of GPT-5's capability to generate novel,
    albeit small, scientific contributions, signaling a shift in R&D productivity.
  relevance_score: 9
  source: llm_enhanced
  text: We would try the models with some of our friends who are professional physicists
    or professional mathematicians. And you already saw kind of some instances of
    this where you can take a problem and have it discover maybe not very complicated
    new mathematics, but some non-trivial new mathematics.
  topic: technology
- impact_reason: A strong forward-looking statement suggesting that the current perceived
    breakthroughs are just the beginning of an accelerating trend in capability.
  relevance_score: 8
  source: llm_enhanced
  text: I expect that that was quite small compared to what's coming over the next
    year.
  topic: industry trends
- impact_reason: Connects the concepts of 'reasoning' and 'long horizon agency,' defining
    robustness as the ability to maintain depth and consistency over extended operational
    periods.
  relevance_score: 8
  source: llm_enhanced
  text: I think actually like well, the ability to maintain depth is a lot of it's
    being consistent over long horizons. So I think there are very related problems.
    And in fact, I think with the reasoning models we have seen the models greatly
    extend the length of which they are able to reason and work reliably without going
    off track.
  topic: technology
- impact_reason: 'Identifies the primary bottleneck for enterprises adopting advanced
    AI techniques like RL: defining effective reward functions for complex, real-world
    tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: One of the hardest things about RL for folks who are not practitioners of
    RL is the idea of crafting the right reward model.
  topic: business
- impact_reason: This highlights a crucial insight into optimizing AI model performance
    based on problem complexity, suggesting that speed isn't always the priority for
    quality.
  relevance_score: 9
  source: llm_enhanced
  text: for easy problems being a lot lower latency, for harder problems, actually
    the right thing is to be even higher latency, get you the really best solution,
    and just being able to find that preset.
  topic: technology
- impact_reason: This pinpoints a specific inefficiency in early AI model resource
    allocation, valuable for understanding model training and optimization trade-offs.
  relevance_score: 8
  source: llm_enhanced
  text: the previous generation of the Codex models, they were spending too little
    time solving the hardest problems and too much time solving the easy problems.
  topic: technology
- impact_reason: This provides a concrete, personal testimony to the transformative
    speed and quality of modern coding AI tools, signaling a paradigm shift in software
    development.
  relevance_score: 10
  source: llm_enhanced
  text: I've really kind of felt like, okay, this is no longer the way. Like, you
    can do a 35-year-old refactor pretty much perfectly in 15 minutes, you kind of
    have to use it.
  topic: technology
- impact_reason: This defines the current state of advanced AI tools—necessary but
    imperfect—and sets a clear goal for future development (crossing the 'uncanny
    valley').
  relevance_score: 9
  source: llm_enhanced
  text: I think it is a little bit of an uncanny valley still right now where you
    kind of have to use it because it is just like accelerating so many things, but
    it's still like, you know, a little bit like not quite as good as a co-worker.
    So, you know, I think like our priority is getting out of the uncanny valley.
  topic: technology
- impact_reason: This illustrates a generational shift in coding norms, where AI assistance
    is becoming the baseline expectation, not an optional tool.
  relevance_score: 9
  source: llm_enhanced
  text: I do feel like already it's kind of transformed the default for coding. This
    past weekend, I was talking to some high schoolers and they're saying, oh, actually
    the default way to code is vibe coding.
  topic: industry trends
- impact_reason: This offers a fundamental definition of successful research, emphasizing
    the pursuit of the unknown and the necessity of persistence.
  relevance_score: 7
  source: llm_enhanced
  text: What makes a great researcher? Persistence is a very key trait, right? Like
    I think what is different about research when you're actually trying to I think
    the special thing about research is you're trying to create something or learn
    something that is just not known, right?
  topic: business
- impact_reason: 'This provides critical advice on maintaining objectivity in research
    and development: balancing conviction with rigorous, honest self-assessment of
    progress.'
  relevance_score: 8
  source: llm_enhanced
  text: I think a trap many people fall into is going out of the way to prove that
    it works, right? Which is quite different from believing in your idea and thinking
    of it is extremely important, right? And you want to persist that, but you have
    to be honest with yourself about when it's working and when it's not so that you
    can learn and adjust.
  topic: business
- impact_reason: This emphasizes the value of experience in problem selection—finding
    the 'horizon' of difficulty that sustains motivation and leads to meaningful breakthroughs.
  relevance_score: 8
  source: llm_enhanced
  text: I think there are just very few shortcuts for experience. I think through
    experience you kind of learn what's the horizon to be thinking of a problem, but
    you can't pick something that's too hard or it's not satisfying to do something
    that's too easy.
  topic: business
- impact_reason: 'This outlines a key strategy for talent retention in competitive
    tech fields: focusing on frontier innovation and fundamental research rather than
    imitation.'
  relevance_score: 10
  source: llm_enhanced
  text: The biggest things that OpenAI has going for it in terms of keeping the best
    people motivated and excited, as I said, is that we are in the business of doing
    fundamental research. We aren't the type of company that looks around and says,
    oh, what model did company X build, what model did company Y build? We have a
    fairly clear and crisp definition of what it is we're out to build. We like innovating
    at the frontier. We really don't like copying.
  topic: business
- impact_reason: 'This reveals a non-obvious hiring heuristic for top-tier technical
    talent: prioritizing demonstrated ability to solve hard problems over public visibility
    or social media presence.'
  relevance_score: 9
  source: llm_enhanced
  text: We don't purely look for who did the most visible work or is the most visible
    on social media. I think one thing that we look for is having solved hard problems
    in any field.
  topic: startups
source: a16z
summary:
- key_takeaways:
  - The primary research target is building an 'automated researcher' capable of discovering
    new, economically relevant ideas autonomously.
  - GPT-5's main focus was integrating reasoning and more agentic behavior by default,
    moving beyond simple instant responses.
  - Current evaluations (like math/programming competitions) are becoming saturated,
    necessitating a shift to benchmarks that measure genuine discovery and long-horizon
    operation.
  - Reinforcement Learning (RL) remains a highly effective and versatile method, successfully
    combined with large language models to execute complex objectives.
  - The new Codex for real-world coding is designed to handle messy environments and
    optimize latency based on problem difficulty, moving past the 'vibe coding' default.
  - Great research requires persistence, a high tolerance for failure, and the ability
    to be maximally truth-seeking even when deeply convicted in an idea.
  - OpenAI maintains its talent edge by focusing on frontier, fundamental research
    rather than copying competitors, and by valuing deep technical fundamentals over
    social media visibility.
  overview: OpenAI's Mark Chen and Jakub Pachocki discuss the evolution of AI research,
    focusing on the shift from instant-response models to reasoning capabilities exemplified
    by GPT-5, and the ultimate goal of creating an 'automated researcher.' They emphasize
    the necessity of moving evaluation benchmarks beyond saturated metrics toward
    economically relevant discovery and long-horizon agency.
  themes:
  - The Future of AI Research (Automated Researcher)
  - GPT-5 and Reasoning Capabilities
  - Evolution and Deficiencies of AI Evaluations (Evals)
  - The Role and Success of Reinforcement Learning (RL)
  - AI in Coding and Developer Workflow
  - The Nature of Scientific Research and Researcher Culture
  - Talent Acquisition and Organizational Culture at OpenAI
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- startup
- investment
- openai
title: 'From Vibe Coding to Vibe Researching: OpenAI’s Mark Chen and Jakub Pachocki'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 55
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 19
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 4
  prominence: 0.4
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-03 20:23:27 UTC -->
