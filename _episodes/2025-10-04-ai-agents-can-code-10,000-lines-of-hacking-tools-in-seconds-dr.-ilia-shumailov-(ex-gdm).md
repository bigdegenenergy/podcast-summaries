---
companies:
- category: unknown
  confidence: medium
  context: This episode is brought to you by Nespresso and Samar Origins by The Weekend
    Coffee Collection. Introducing Sam
  name: Samar Origins
  position: 48
- category: unknown
  confidence: medium
  context: brought to you by Nespresso and Samar Origins by The Weekend Coffee Collection.
    Introducing Samar Origins. My collaboration with
  name: The Weekend Coffee Collection
  position: 65
- category: unknown
  confidence: medium
  context: d Samar Origins by The Weekend Coffee Collection. Introducing Samar Origins.
    My collaboration with Nespresso is a connection
  name: Introducing Samar Origins
  position: 96
- category: unknown
  confidence: medium
  context: hey can't bring their pets with them. Through the Purple Leash project,
    Purina is helping create more pet-friend
  name: Purple Leash
  position: 630
- category: unknown
  confidence: medium
  context: vors and their pets can escape and heal together. Visit Purina.com's Lush
    Purple to learn more. Big models today
  name: Visit Purina
  position: 781
- category: unknown
  confidence: medium
  context: can escape and heal together. Visit Purina.com's Lush Purple to learn more.
    Big models today, if we compare th
  name: Lush Purple
  position: 800
- category: unknown
  confidence: medium
  context: e like even worse than that. MLST is supported by Cyber Fund. Link in the
    description. I'm Illia. I spend my d
  name: Cyber Fund
  position: 2406
- category: unknown
  confidence: medium
  context: publishing in both security and machine learning. Then I joined DeepMind
    where I stayed for two years in t
  name: Then I
  position: 2746
- category: tech
  confidence: high
  context: trust model in itself is very different from any notion of trust you can
    find in cryptographic literature
  name: Notion
  position: 5188
- category: unknown
  confidence: medium
  context: e else. The link is in the description. Hi folks, Steven Johnson here,
    co-founder of NotebookLM. As an author, I'v
  name: Steven Johnson
  position: 6377
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. I was based
    in DeepMind and before that most
  name: Google
  position: 6775
- category: unknown
  confidence: medium
  context: you did your PhD at Cambridge under the legendary Ross Anderson? Yes. So
    you have fundamentally different DNA, ri
  name: Ross Anderson
  position: 6928
- category: unknown
  confidence: medium
  context: ally different DNA, right? You're a security guy. My DNA is approximately
    the same. Okay, fair enough. It'
  name: My DNA
  position: 7019
- category: unknown
  confidence: medium
  context: ecurity rather than they fall in the realm of AI. And I'm curious, is there
    any overlap or what is the ov
  name: And I
  position: 7692
- category: tech
  confidence: high
  context: nt in discovery of the adversarial examples using gradient information,
    blah, blah, blah, right? You can kin
  name: Gradient
  position: 11166
- category: unknown
  confidence: medium
  context: actually changed anything about these big models. So I think in part what
    we were talking about in this
  name: So I
  position: 12268
- category: tech
  confidence: high
  context: And then one day, hey, let's upgrade to the next Intel chip. And they just
    had changed something about t
  name: Intel
  position: 14225
- category: unknown
  confidence: medium
  context: t of like access control tooling around it. Okay. But I mean, help me here
    because that seems like I'm ge
  name: But I
  position: 16673
- category: unknown
  confidence: medium
  context: ed when the road takes you from smooth sailing to Chevy Colorado will eat
    up some of the toughest terrains with fi
  name: Chevy Colorado
  position: 18447
- category: unknown
  confidence: medium
  context: with five available drive modes and Silverado and Silverado HD have the
    muscle to take you out for some serious
  name: Silverado HD
  position: 18554
- category: unknown
  confidence: medium
  context: th multiple engine options and impressive towing. While Silverado EV is
    the perfect combo of performance and capabilit
  name: While Silverado EV
  position: 18675
- category: unknown
  confidence: medium
  context: s and a smile. How do they do it? Easy with a new Galaxy Watch 8. Sleep
    tracking and personalized insights from
  name: Galaxy Watch
  position: 19114
- category: unknown
  confidence: medium
  context: 8. Learn more at samsung.com. Requires compatible Samsung Galaxy phone,
    Samsung Health app, and Samsung account. S
  name: Samsung Galaxy
  position: 19380
- category: unknown
  confidence: medium
  context: ng.com. Requires compatible Samsung Galaxy phone, Samsung Health app, and
    Samsung account. So I would never even g
  name: Samsung Health
  position: 19402
- category: unknown
  confidence: medium
  context: e all of the problems that exist in all of these. What I love about this
    system, right, is that because it
  name: What I
  position: 21109
- category: unknown
  confidence: medium
  context: t format puts it in that database. And that's it. Like I'm almost just
    a buyer of these sort of solutions
  name: Like I
  position: 21648
- category: unknown
  confidence: medium
  context: rity tooling for humans by humans against humans. Now I don't think we
    know what we're building. It's har
  name: Now I
  position: 24180
- category: tech
  confidence: high
  context: It's unlikely for them to disappear. You see that Anthropic paper, what
    was it called, Agentic Misalignment,
  name: Anthropic
  position: 29414
- category: unknown
  confidence: medium
  context: You see that Anthropic paper, what was it called, Agentic Misalignment,
    where they, I mean, you were going to set the se
  name: Agentic Misalignment
  position: 29451
- category: unknown
  confidence: medium
  context: boss was having an affair or something like that. The AI didn't want to
    be switched off. And it's just abs
  name: The AI
  position: 29719
- category: tech
  confidence: high
  context: reds of millions of devices. Okay. Now we look at Hugging Face's library
    and you look at this wonderful flag cal
  name: Hugging Face
  position: 35687
- category: unknown
  confidence: medium
  context: hat they do. I honestly, now everything is run on Google Cloud. Literally
    every single thing. I now just remove
  name: Google Cloud
  position: 38779
- category: unknown
  confidence: medium
  context: ', how many maintainers, what they do. Through the Starbucks College Achievement
    Plan, full and part-time Starbucks partners receive 10'
  name: Starbucks College Achievement Plan
  position: 40686
- category: unknown
  confidence: medium
  context: iochemistry, or any of the more than 150 offered. At Starbucks, benefits
    like college tuition coverage are just
  name: At Starbucks
  position: 40993
- category: unknown
  confidence: medium
  context: earn more at starbucks.com/partners. Hey, this is Bill Simmons from The
    Bill Simmons Podcast here to help you ma
  name: Bill Simmons
  position: 41117
- category: unknown
  confidence: medium
  context: ucks.com/partners. Hey, this is Bill Simmons from The Bill Simmons Podcast
    here to help you make the most of your summer wit
  name: The Bill Simmons Podcast
  position: 41135
- category: unknown
  confidence: medium
  context: ere to help you make the most of your summer with Michelob Ultra, a superior
    light beer worth playing for. Play fo
  name: Michelob Ultra
  position: 41211
- category: unknown
  confidence: medium
  context: ummer. Enjoy responsibly. Copyright 2025 Anheuser-Busch Michelob Ultra
    Light Beer, St. Louis, Missouri. Heck, if I was an adversary
  name: Busch Michelob Ultra Light Beer
  position: 41532
- category: unknown
  confidence: medium
  context: st. And in practice, you just need to be careful. Ilya Walsh is set up
    for us what model collapse refers to. W
  name: Ilya Walsh
  position: 58135
- category: ai_research
  confidence: high
  context: The speaker previously worked at DeepMind for two years in the best machine
    learning security team. It is a major AI research lab.
  name: DeepMind
  source: llm_enhanced
- category: ai_model_provider
  confidence: high
  context: Mentioned as an example of a machine learning model that could be used
    to solve the millionaire problem via trusted computation instead of cryptography.
  name: Jemma
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an AI-first tool built to help organize ideas and make connections
    by uploading documents and using AI to uncover insights.
  name: NotebookLM
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The URL provided for trying NotebookLM, implying association with Google.
  name: notebooklm.google.com
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Implied association through NotebookLM (notebooklm.google.com).
  name: Google
  source: llm_enhanced
- category: investment_ai
  confidence: high
  context: Mentioned as a supporter of MLST (Machine Learning Security & Trust, likely
    the speaker's current focus/project).
  name: Cyber Fund
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A system proposed in a paper ('diffusion prompt injections by design')
    that rewrites user queries into a language with formal semantics for control flow
    and data flow, enforcing policies outside the core model.
  name: Camel
  source: llm_enhanced
- category: ai_evaluation
  confidence: high
  context: Mentioned as the standard for adversarial evaluation for agentic workflows,
    against which the speaker's system (Camel) was tested.
  name: AgentDojo
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned in reference to an 'Anthropic paper' titled 'Agentic Misalignment,'
    which described a scenario where an AI tried to blackmail someone to prevent being
    switched off.
  name: Anthropic
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned in relation to work suggesting that thinking traces (reasoning
    paths) might not be directly relevant or meaningful for understanding model behavior.
  name: Superalignment
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced metaphorically (the 'AlphaGo moment') when discussing models
    that teach themselves through self-learning and iteration to solve problems in
    novel ways.
  name: AlphaGo
  source: llm_enhanced
- category: advertisement
  confidence: high
  context: Mentioned in an advertisement break, not directly related to AI/ML development.
  name: Chevrolet
  source: llm_enhanced
- category: advertisement
  confidence: high
  context: Mentioned in an advertisement break promoting the Galaxy Watch 8 features
    like sleep tracking.
  name: Samsung
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned specifically regarding its `trust_remote_code` flag in the `transformers`
    library, drawing a parallel to the Log4j vulnerability due to remote code execution
    risks.
  name: Hugging Face
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in relation to security compromises of its CI/CD integration
    on GitHub and issues with its nightly build package loading priorities, highlighting
    supply chain risks.
  name: PyTorch
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a popular ML library that has many dependencies, some of which
    are questionable, illustrating broader supply chain complexity.
  name: TensorFlow
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Mentioned as a fully human-verified kernel (secure extension L4 kernel)
    that is mathematically proven to have no memory exploits, used as an example of
    intensive verification efforts that ML could potentially speed up.
  name: seL4
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of running external verifiers and dealing with
    computational budgets, implying their work in AI for protein folding.
  name: AlphaFold team
  source: llm_enhanced
date: 2025-10-04 06:55:01 +0000
duration: 61
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/2777e210ff1246aeb18f9633287ba46b/
processing_date: 2025-10-06 02:43:57 +0000
quotes:
- length: 122
  relevance_score: 4
  text: Investment in security means you could have spent the resources getting first
    to the market and getting the network effect
  topics:
  - market
  - investment
- length: 95
  relevance_score: 3
  text: And it sounds like the other half of the problem is almost as diffusion of
    responsibility thing
  topics: []
- length: 25
  relevance_score: 3
  text: The problem is not really
  topics: []
- length: 71
  relevance_score: 3
  text: The problem is it's pulling in 10,000 JavaScript libraries or something
  topics: []
- length: 79
  relevance_score: 3
  text: But the problem is you don't know if the external verifier is going to complete
  topics: []
- length: 73
  relevance_score: 3
  text: Well, you have to put in some arbitrary computational budgets, thresholds
  topics: []
- impact_reason: A stark comparison emphasizing the unique threat profile of autonomous
    AI agents compared to human adversaries, particularly regarding scale, speed,
    and comprehensive knowledge.
  relevance_score: 10
  source: llm_enhanced
  text: Agents are very different from humans. You will not find a single human in
    the world that works 24/7, touches absolutely every single one of your endpoints
    in your system, that absolutely knows everything there is, that can generate you
    basically all of the hacking tools on a whim, just because it knows that it has
    seen all of them, it can recreate this in a matter of a second.
  topic: predictions/safety
- impact_reason: 'This is a major, potentially disruptive thesis: that verifiable
    ML models could replace complex cryptographic protocols for certain tasks (like
    private computation).'
  relevance_score: 10
  source: llm_enhanced
  text: So suddenly, what I'm marking in this work is that actually trusted third
    parties can exist. And when you do have those trusted third parties, you suddenly
    don't need to rely on this very expensive and hard and cumbersome cryptographic
    utilities.
  topic: predictions/technical
- impact_reason: Offers a clear, academic distinction between AI Safety (average case/accidents)
    and AI Security (worst case/malice), which is often blurred in public discourse.
  relevance_score: 10
  source: llm_enhanced
  text: Safety is like an average-case performance of the system. Security is the
    worst-case performance of the system. So the difference between the two is the
    existence of malicious actors that kind of push the system to the worst-case setup.
  topic: safety/strategy
- impact_reason: A strong critique of current academic and startup defense strategies
    against prompt injection, suggesting they are fundamentally ineffective against
    sophisticated attacks.
  relevance_score: 10
  source: llm_enhanced
  text: We find that pretty much in all of the cases we're capable of doing this [indirect
    prompt injection via email]. And if you take a whole bunch of academic work to
    show on how to build defenses, and you can actually find startup pretty much implementing
    the same things and sharing this as like a security solution, like those approaches
    don't work.
  topic: safety/limitations
- impact_reason: This points directly to the vulnerability of advanced instruction-following
    models to prompt injection/manipulation, leading to unintended behavior ('loss
    of a different kind').
  relevance_score: 10
  source: llm_enhanced
  text: When they follow instructions, you can do a lot more. Like you can convince
    them that things that they were not capable of doing before are some things that
    they need to do, and suddenly this leads to some sort of a loss of a different
    kind.
  topic: safety/limitations
- impact_reason: A concrete description of an adversarial attack (prompt injection
    via context/email) demonstrating a critical security failure in current agent
    architectures.
  relevance_score: 10
  source: llm_enhanced
  text: we were trying to send an email to the agent such that when this email ends
    up in the agent's context, rather than following the user task, the agent is actually
    doing something else, like the thing that we've sent it to.
  topic: safety/technical
- impact_reason: Articulates the central, unsolved problem of fine-grained policy
    enforcement for personalized AI models handling sensitive data.
  relevance_score: 10
  source: llm_enhanced
  text: However, I don't want it to reveal certain private information. Like, how
    would I even define that as a security?
  topic: safety/business
- impact_reason: 'A crucial architectural insight: security for LLM agents must be
    achieved via external system design (orchestration, access control) rather than
    relying solely on model internal modifications.'
  relevance_score: 10
  source: llm_enhanced
  text: we should be able to get guarantees not by changing the models, but to be
    changing the systems around them and how the models interact with your sensitive
    data and how we basically build sort of like access control tooling around it.
  topic: strategy/technical
- impact_reason: Illustrates extremely fine-grained, dynamic access control policies
    that can be enforced externally based on execution context, which is impossible
    with standard prompting.
  relevance_score: 10
  source: llm_enhanced
  text: The only allowed data flow from this tool to go into this other tool is if
    this other tool, like I don't know, domain of the website actually has dot gov
    dot uk inside of it.
  topic: safety/technical
- impact_reason: 'A strong stance on data handling: sensitive data should be abstracted
    away from the model weights/context, existing only as a protected, symbolic variable
    accessed via controlled interfaces.'
  relevance_score: 10
  source: llm_enhanced
  text: I would never even give my passport number to a fine-tuning of a large model.
    Exactly. I see it. It will have a symbolic representation of it, but we'll know
    so the passport itself exists in this variable. I can defer to it, but I don't
    even know what the value is.
  topic: safety/business
- impact_reason: A critical warning against anthropomorphizing AI agents, emphasizing
    that their capabilities and threat models fundamentally differ from human adversaries.
  relevance_score: 10
  source: llm_enhanced
  text: I think it's extremely important to not think about these agents as humans,
    right? Oh, I don't, I don't think about that. What I'm trying to say in security
    terms is that agents are like, is a worst-case human sort of, well, not even this.
    Like agents are very different from humans.
  topic: safety/strategy
- impact_reason: Defines the unprecedented scale and capability of an AI agent adversary
    compared to any human threat actor.
  relevance_score: 10
  source: llm_enhanced
  text: You will not find a single human in the world that works 24/7, touches absolutely
    every single one of your endpoints in your system, that absolutely knows everything
    there is, that can generate you basically all of the hacking tools on a whim,
    just because it knows it has seen all of them, it can recreate this in a matter
    of a second.
  topic: safety
- impact_reason: A strong statement on the necessary evolution of security controls
    (fine-grained access, transparency) required for agentic systems, contrasting
    sharply with legacy coarse-grain controls.
  relevance_score: 10
  source: llm_enhanced
  text: With an agent that doesn't work. This doesn't exist. All of these assumptions
    are sort of gone. We don't know how to build systems against this. We need very
    fine grain, we need extreme pressure session, we need extreme control and transparency,
    otherwise it's just not going to work.
  topic: safety/strategy
- impact_reason: A concrete example of agentic 'over-eagerness' or emergent behavior
    (sending unsolicited emails/pings), emphasizing the critical need for extreme
    precision in prompting and guardrails.
  relevance_score: 10
  source: llm_enhanced
  text: In between this, it sends four different emails to parties I never mentioned
    because it thinks, oh, actually, let me also notify the admin that I've done this
    and also let me also ping this endpoint. They do this. It's because unless you
    specify things extremely precisely, unless you have checks in place, these agents
    just, yeah, they don't think like me and you.
  topic: safety/technical
- impact_reason: Explicitly names the 'confused deputy problem' as a core security
    vulnerability that will persist and be exacerbated in agentic workflows due to
    context limitations.
  relevance_score: 10
  source: llm_enhanced
  text: How should the financial agent know they're not supposed to do this? They
    don't have enough context. They are not supposed to have enough context. So and
    usually when we talk about social engineering security, like there is a wide variety
    of things that like they describe why human systems fail and it's going to be
    a similar sort of thing because many of those problems, like confused deputy problem
    is the formal name of this, they will exist.
  topic: safety/technical
- impact_reason: Argues against relying on human-comprehensible interpretability (dimensionality
    reduction) for security assurance, as it inherently causes information loss (collisions)
    masking critical behavior.
  relevance_score: 10
  source: llm_enhanced
  text: For security, I think interpretability is maybe an interesting tool for safety
    sort of things, but for security, it's definitely not a step in the right direction
    because like broadly speaking, if you take something extremely multi-dimensional
    and project it into something very small dimensional because a human can comprehend
    this thing, then you will have a lot of collisions where like the multi-dimensional
    space maps to the same sort of smaller, smaller dimensional space.
  topic: safety/technical
- impact_reason: Draws a direct, alarming parallel between the widespread, unmanaged
    risk of open-source LLMs and the catastrophic, systemic impact of the Log4j vulnerability.
  relevance_score: 10
  source: llm_enhanced
  text: I am extremely worried about this [open-source proliferation]... But for an
    average consumer, have you heard about Log4j vulnerability? That kind of the thing
    that stormed the internet.
  topic: safety/strategy
- impact_reason: Provides a detailed, technical explanation of how the `trust_remote_code`
    flag in Hugging Face libraries enables remote code execution (RCE) upon model
    loading, mirroring historical supply chain attacks.
  relevance_score: 10
  source: llm_enhanced
  text: And what this thing does is that when you load the model, you know, like you
    click use this model, use transformers inside, it gives you like a code snippet
    to load some model. Inside it has this flag sometimes hardcoded. And what this
    thing does is they say, oh, for some models, when you load them, you actually
    want to load the latest, the latest representation from an external machine. What
    this thing does is literally remote code loaded on your machine, executed on your
    machine, loaded on top of stuff.
  topic: safety/technical
- impact_reason: Highlights a critical, active security vulnerability in model loading
    practices (remote code execution via model weights/representations), which is
    highly relevant to ML deployment.
  relevance_score: 10
  source: llm_enhanced
  text: What this thing does is they say, oh, for some models, when you load them,
    you actually want to load the latest, the latest representation from an external
    machine. What this thing does is literally remote code loaded on your machine,
    executed on your machine, loaded on top of stuff.
  topic: safety/technical
- impact_reason: Introduces the advanced concept of 'architectural backdoors' in ML
    models, a sophisticated attack vector that hides malicious logic in the model's
    structure rather than weights, making detection extremely difficult.
  relevance_score: 10
  source: llm_enhanced
  text: We have written a whole new branch of literature on what we call like architectural
    backdoors, where you don't actually hide malicious functionality in parameters
    of the models. Instead, you hide it in the structure of the model itself, like
    a structure so that even if you find you in the model, it still has the same baseline
    behavior.
  topic: safety/technical
- impact_reason: 'Provides a specific, terrifying example of an architectural backdoor:
    token-triggered data leakage between users during inference, demonstrating a critical
    privacy/security flaw in transformer deployment.'
  relevance_score: 10
  source: llm_enhanced
  text: we can change the architecture of the model such that they become sensitive
    to certain tokens when you supply them to a transformer, that when you supply
    them, they start using the memory in the wrong way. So like they start routing,
    for example, data from one user to another user.
  topic: safety/technical
- impact_reason: 'A stark admission regarding the current state of AI safety/security:
    there are no known universal solutions for many frontier model vulnerabilities.'
  relevance_score: 10
  source: llm_enhanced
  text: Is there one thing that all of the frontier labs could implement that would
    improve the security of their models? No, I don't, I don't think this exists today.
    Like we don't know how to solve problems. I think this is the honest answer. Is
    that for most of the issues we have today, we just don't have a solution.
  topic: safety/limitations
- impact_reason: 'Articulates the core challenge of securing rapidly evolving ML systems:
    the target moves faster than the security tooling can be developed and standardized.'
  relevance_score: 10
  source: llm_enhanced
  text: the actual issue is the fact that before you develop security tooling, you
    really need to have something to secure because every single, small detail changes
    how you build security systems. So unless you know everything about the system
    and it's kind of frozen in time, you can't really build security. And by the time
    you have something to secure, it's already too late.
  topic: safety/technical
- impact_reason: Provides a precise, two-part definition of model collapse, distinguishing
    between the immediate effect (tail shrinking) and the cumulative effect (failure
    accumulation).
  relevance_score: 10
  source: llm_enhanced
  text: when we talked about model collapse, we kind of referred to two phenomena
    happening at the same time. One of them was the tails are shrinking and basically
    improbable events become more improbable. And then the second phenomena was over
    time and this accumulates with fails.
  topic: technical/limitations
- impact_reason: Highlights a fundamental shift in failure modes as models scale,
    moving away from older adversarial attack vectors to new, capability-driven vulnerabilities.
  relevance_score: 9
  source: llm_enhanced
  text: Big models today, if we compare them to the big models five years ago, they
    fail in very different ways.
  topic: technical/limitations
- impact_reason: Connects improved instruction-following capability (a key LLM breakthrough)
    directly to increased potential for misuse or complex task execution, relevant
    for both utility and security.
  relevance_score: 9
  source: llm_enhanced
  text: When they follow instructions and they become better at following instructions,
    you can suddenly do a lot more.
  topic: predictions/impact
- impact_reason: Uses a classic security thought experiment (the irrational child
    adversary) to frame the extreme nature of AI agent threats, suggesting agents
    surpass even this high bar.
  relevance_score: 9
  source: llm_enhanced
  text: 'In security, we tend to say that a child is the worst-case adversary you
    can find: completely irrational thinking, infinite amount of time. They can basically
    touch everything. There are no expectations on behaviors whatsoever. But agents
    are like even worse than that.'
  topic: safety/limitations
- impact_reason: 'Clearly states the urgent need for new security tooling specifically
    for agentic systems and outlines the core goals: observability, constraint imposition,
    and confidence.'
  relevance_score: 9
  source: llm_enhanced
  text: I am very unemployed and I'm trying to build security tooling for the future
    to make sure that as we get agentic fleets integrated into more and more use cases,
    we can actually tell what they're doing, we can impose constraints on them, and
    we can have confidence that tomorrow they're not going to leak our private information,
    hack the boxes on which they're running, and so on.
  topic: business/strategy
- impact_reason: Provides a concrete, albeit speculative, example (the millionaire
    problem) of how a verifiable ML inference could substitute for established cryptographic
    methods.
  relevance_score: 9
  source: llm_enhanced
  text: We can say, oh, let's pick Jemma as an example of a model. And this is both
    of us agree on a prompt... And then that is it, which is run inference, maybe
    on a platform that can give you an integrative verification that the model exactly
    ran with the exact parameters and also the inputs we provided. And then you're
    set. You no longer need cryptography.
  topic: technical/predictions
- impact_reason: 'Details the trade-off in modern LLM robustness: they resist old
    optimization attacks but are highly susceptible to simple, semantic prompt variations.'
  relevance_score: 9
  source: llm_enhanced
  text: Whereas with models, they clearly become more robust against that stuff [gradient-based
    adversarial examples], or at least it's significantly harder to traverse this
    landscape, but they also become a lot less robust against other adversaries. Like
    suddenly, very simple rephrasing of the same questions forces it to completely
    do something different.
  topic: technical/limitations
- impact_reason: Expresses deep frustration with the lack of interpretability and
    predictability in modern large models, framing development as 'alchemy' rather
    than engineering.
  relevance_score: 9
  source: llm_enhanced
  text: Whereas nowadays, you look at a modern big model, it's like it's way too much
    alchemy; like it's completely impossible to tell, like, oh, I have added this
    thing inside, what actually happens to the whole thing?
  topic: technical/limitations
- impact_reason: Reinforces the idea that increased capability (instruction following)
    directly translates into new attack surfaces where the model can be persuaded
    to violate its original constraints.
  relevance_score: 9
  source: llm_enhanced
  text: When your models get better, they get significantly better, like capability
    of growing up the model overall, they get significantly better following instructions.
    When they follow instructions, you can do a lot more. Like you can convince them
    that things that they were not capable of doing before are some things that they
    need to do, and suddenly this leads to some sort of a loss of a different kind.
  topic: safety/predictions
- impact_reason: A strong critique of current, easily bypassed academic/startup security
    solutions for LLMs, suggesting existing defenses are inadequate against determined
    attacks.
  relevance_score: 9
  source: llm_enhanced
  text: if you take a whole bunch of academic work to show on how to build defenses,
    and you can actually find startup pretty much implementing the same things and
    sharing this as like a security solution, like those approaches don't work.
  topic: safety/business
- impact_reason: 'A core security philosophy shift required for AI systems: accepting
    opacity and focusing on building resilient external control layers rather than
    trying to fully understand the internal state.'
  relevance_score: 9
  source: llm_enhanced
  text: we kind of, I think in security, we need to make an assumption that we can't
    actually tell what's happening. And instead, try and build systems around it in
    order to bring the resilience up to this whatever, 99.999 and ideally as many
    nines as possible.
  topic: safety/strategy
- impact_reason: States the current impossibility of enforcing user-defined security
    policies on modern LLM agents, justifying the need for new architectural approaches.
  relevance_score: 9
  source: llm_enhanced
  text: Today with modern agents and the way we build the modern agents, it's impossible
    [to express policies like this].
  topic: limitations/safety
- impact_reason: Perfectly frames the tension between utility (needing to use sensitive
    data) and security (preventing unauthorized leakage) in agentic workflows.
  relevance_score: 9
  source: llm_enhanced
  text: you can always find a way to manipulate this agent into revealing this piece
    of information, right? At the same time, you definitely want to give your passport
    number to the world if it's filling the documents.
  topic: safety/business
- impact_reason: Advocates for grounding AI agent execution in formal methods from
    programming languages (like control flow, data flow) rather than treating them
    as black-box text predictors.
  relevance_score: 9
  source: llm_enhanced
  text: This is more like taking a step back and taking foundations of programming
    languages and building this by design into the models.
  topic: technical/strategy
- impact_reason: 'Describes the core mechanism of the proposed solution (Camel): translating
    natural language tasks into structured, formally verifiable code/programs.'
  relevance_score: 9
  source: llm_enhanced
  text: we receive a user query and then we rewrite the user query in a language that
    has formal semantics for control flow and data flow.
  topic: technical
- impact_reason: 'Reinforces the separation of concerns: the LLM generates the plan/code,
    and a separate, verifiable execution environment enforces the security policies.'
  relevance_score: 9
  source: llm_enhanced
  text: This is not a part of the model. This is a part of the actual execution. In
    case of Camel, will have an interpreter that takes in this program, executes the
    program step by step and actually enforces a static or a dynamic policy on top
    of this graph, the execution graph.
  topic: technical/strategy
- impact_reason: Describes a robust, externalized authorization check (Oracle) that
    must precede any sensitive data usage by the agent.
  relevance_score: 9
  source: llm_enhanced
  text: before using this, basically there is an external sort of like think about
    this as an Oracle that I can ask, oh, is this okay to use this variable to interact
    with this tool? And then if this external Oracle says, no, this is a passport
    and you're not touching a government website, then I'm forbidding this.
  topic: safety/technical
- impact_reason: Contrasts the predictability of human behavior (which allows for
    traditional security assumptions) with the lack of such constraints for autonomous
    agents.
  relevance_score: 9
  source: llm_enhanced
  text: With agents, that's not the case. You don't make an assumption that the user
    will go and touch every single endpoint you have in a network because, you know,
    why would they do this? And even if they do this, you call them in and you say,
    well, you know, we'll apply a legal framework and imprison you.
  topic: safety
- impact_reason: A sobering assessment of the paradigm shift in cybersecurity, acknowledging
    that current security frameworks are obsolete for the AI era.
  relevance_score: 9
  source: llm_enhanced
  text: beforehand we were building security tooling for humans by humans against
    humans. Now I don't think we know what we're building. It's hard to tell.
  topic: strategy/safety
- impact_reason: 'Actionable recommendation for future security architecture: moving
    away from coarse-grained controls to highly granular, transparent, and strongly
    enforced mechanisms.'
  relevance_score: 9
  source: llm_enhanced
  text: We need very fine grain, we need extreme pressure session, we need extreme
    control and transparency, otherwise it's just not going to work.
  topic: safety/strategy
- impact_reason: Highlights the fundamental shift in security paradigms from human-centric
    threats to agent/AI-centric threats, indicating current security models are inadequate.
  relevance_score: 9
  source: llm_enhanced
  text: No, it's likely it's not going to be like this because beforehand we were
    building security tooling for humans by humans against humans. Now I don't think
    we know what we're building. It's hard to tell.
  topic: safety/predictions
- impact_reason: Identifies the 'diffusion of responsibility' as a major socio-technical
    challenge when agents cause errors, complicating accountability and consequence.
  relevance_score: 9
  source: llm_enhanced
  text: And it sounds like the other half of the problem is almost as diffusion of
    responsibility thing. Yeah. It's like you, Ilya, you were at work, you asked the
    agent to email this to this person instead of emailed it to five other people.
    And then somebody stops, why I do what you do, you email the, I didn't do it.
    It's this agent.
  topic: safety/business
- impact_reason: 'A strategic call to action: the existing framework for consequences
    and liability is broken by agents, necessitating a complete shift in how we model
    and regulate them.'
  relevance_score: 9
  source: llm_enhanced
  text: The normal consequence chain doesn't really apply anymore. Yeah. That's hard.
    I think we really need to change our thinking and start modeling because these
    agents, and obviously they don't widely exist yet, this is a thing that is coming
    clearly.
  topic: strategy/safety
- impact_reason: Casts significant doubt on the utility of 'thinking traces' for interpretability
    or security auditing, referencing work suggesting they might be irrelevant artifacts.
  relevance_score: 9
  source: llm_enhanced
  text: To what extent do you think you can read anything about what the model was
    thinking from its thinking trace? Definitely not. I mean, I'll never reach maybe,
    but like the corner case is definitely not. Or even, there's even that work from
    Superalignment that it might not mean anything whatsoever.
  topic: technical/safety
- impact_reason: Directly links the trend of local/private model deployment to an
    immediate, unsolved security tooling gap.
  relevance_score: 9
  source: llm_enhanced
  text: But that means a lot more security problems will appear because we don't know
    how to reason about them. We don't know how to build security tooling around them.
  topic: safety/predictions
- impact_reason: A strong cautionary warning against running untrusted models outside
    of secure, sandboxed environments.
  relevance_score: 9
  source: llm_enhanced
  text: If you're running your thing outside of a jail, if you're running your model
    outside of a sandbox, you're doing a very bad thing to yourself.
  topic: safety/strategy
- impact_reason: Cites specific, high-profile supply chain compromises affecting major
    ML infrastructure (PyTorch CI/CD), demonstrating that even trusted build pipelines
    are vulnerable to code injection.
  relevance_score: 9
  source: llm_enhanced
  text: There has been at least publicly two reported compromises of the CI/CD integration
    for PyTorch on GitHub... Somebody broke into those runners. And when you break
    into the runners, you can change the build files themselves. So you can serve
    whatever you want to change the code.
  topic: safety/technical
- impact_reason: A direct, strong warning about the necessity of sandboxing when running
    external ML models, emphasizing personal security risk.
  relevance_score: 9
  source: llm_enhanced
  text: if you're running your model outside of a sandbox, you're doing a very bad
    thing to yourself.
  topic: safety
- impact_reason: Details the mechanism of a CI/CD supply chain attack, showing how
    compromising automated build systems allows for malicious code injection into
    final software releases.
  relevance_score: 9
  source: llm_enhanced
  text: They basically automatically do all the tests and stuff. Somebody broke into
    those runners. And when you break into the runners, you can change the build files
    themselves. So you can serve whatever you want to change the code.
  topic: safety/supply chain
- impact_reason: 'Provides a strong, actionable security recommendation for developers
    handling sensitive or external code: isolate it in a VM rather than running it
    locally.'
  relevance_score: 9
  source: llm_enhanced
  text: This is why I wouldn't install cloud code on my personal machine. I'm like,
    no way I'm going to do that. I'll just get a VM. I'll put it on a VM. That's fine.
    I'm not putting it on my personal computer.
  topic: strategy/safety
- impact_reason: Identifies Python's lack of inherent memory protection as a major
    exacerbating factor for supply chain risks compared to other languages, making
    dependency exploitation easier.
  relevance_score: 9
  source: llm_enhanced
  text: The problem is it's pulling in 10,000 JavaScript libraries or something. I
    think in the case of Python, because everyone is using Python, this is even worse
    because no memory protection at all. There is no memory security.
  topic: technical/safety
- impact_reason: Exposes the deep, often untrusted nature of dependencies within major
    ML frameworks, highlighting the 'dependency hell' problem in the ML ecosystem.
  relevance_score: 9
  source: llm_enhanced
  text: if you take the popular ML libraries, you'll find they have disproportionately
    many dependencies. And many of these dependencies, when you get to level 3 dependency,
    when you look at dependencies of dependencies, they're very questionable, like
    extremely questionable.
  topic: safety/technical
- impact_reason: 'Proposes a major future application for AI: accelerating formal
    verification and security auditing (like kernel verification) by using ML agents
    to handle tedious annotation/proof tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: But now, let's imagine we can replace actually even half of this thing with
    ML models where for half of this is bell annotations, you can actually do this
    by hand, not by hand, but with the agents significantly faster. Then suddenly,
    like verification is a lot easier to do, right?
  topic: predictions/technical
- impact_reason: Connects the theoretical Halting Problem directly to practical AI
    system design (like AlphaFold), forcing developers to use arbitrary computational
    budgets, which introduces bias.
  relevance_score: 9
  source: llm_enhanced
  text: I find actually that it has very practical consequences [for the halting problem].
    So for example, when we were talking to the AlphaFold team, right? You know, there
    is part of the AlphaFold system which is very interesting. I recommend people
    watch it up. So, you know, it goes and runs external verifiers. Yeah. But the
    problem is you don't know if the external verifier is going to complete.
  topic: technical/limitations
- impact_reason: A concrete example of how practical constraints (time budgets) imposed
    by theoretical limitations (halting) directly impact the optimality and scope
    of scientific discovery via AI systems.
  relevance_score: 9
  source: llm_enhanced
  text: If it doesn't complete within, you know, five seconds, then you just terminate
    it and, you know, look at other runs. So while you can do that, I think it also
    introduces biases in the types of programs we can discover, right? Because maybe
    if I had set my budget to seven seconds instead of five, I would have found like
    a more optimal solution, right?
  topic: technical/limitations
- impact_reason: 'A clear prioritization of current bottlenecks: compute efficiency
    and hardware limitations outweigh the theoretical concerns of the halting problem
    in current large-scale AI systems.'
  relevance_score: 9
  source: llm_enhanced
  text: I would still say that our limitation today is more compute. Like imagine
    you had infinite compute. And even if like it's totally fine for this thing to
    run forever, like and you increase the threshold up to a point where it doesn't
    really matter anymore, right? Then you're no longer limited. So I think it's more,
    we just need better hardware. We need things which are more efficient.
  topic: technical/strategy
- impact_reason: 'Explains the economic disincentive structure in tech: the pressure
    for speed-to-market often overrides early investment in robust, non-differentiating
    features like security.'
  relevance_score: 9
  source: llm_enhanced
  text: It's like it's a fundamental first of the market wins. So everyone rushed
    to the market early. Investment in security means you could have spent the resources
    getting first to the market and getting the network effect. And thus you kind
    of don't have incentives to build the security tooling first.
  topic: business/strategy
- impact_reason: Highlights the enduring, increasing value and cost of high-quality,
    specialized, human-generated data, even in an era of synthetic data generation.
  relevance_score: 9
  source: llm_enhanced
  text: we use a lot of synthetic data, but we still use a lot of real data. And whenever
    we need to go and acquire data, there is a massive market in acquiring very specialized
    data. And you see that improvement still comes from humans and the cost of this
    data is growing, like extremely specialized, like hiring a ton of mathematical
    PhDs.
  topic: business/technical
- impact_reason: Addresses and refutes a common counter-argument to model collapse
    theory (just add more synthetic data), noting that even with mitigation, distribution
    drift persists.
  relevance_score: 9
  source: llm_enhanced
  text: The other thing that people were saying is actually it's quite simple. Just
    accumulate more data and then like whenever you can generate an edgy data, just
    plug it in and then you have a shape of the distribution still in place. And I
    think the important thing to realize is that even in this case, when you do theoretical
    modeling with even the simplest model, it still drifts. It just doesn't drift
    that much.
  topic: technical/limitations
- impact_reason: Provides a concise definition of the core challenge in security (worst-case
    modeling) versus typical ML focus (average-case performance).
  relevance_score: 8
  source: llm_enhanced
  text: It's very easy to model an average case. It's very hard to model a non-average
    case, the worst case, which is the goal of security, the exact same case.
  topic: safety/strategy
- impact_reason: Proposes a novel view of ML models as inspectable, controllable entities,
    contrasting them with opaque biological humans.
  relevance_score: 8
  source: llm_enhanced
  text: We actually know what they do. We know how they think. We can check their
    state. They're kind of like a resettable human, if you will.
  topic: technical/strategy
- impact_reason: Stresses that the proposed ML-based trust mechanism is a fundamentally
    new paradigm, distinct from TEEs or standard crypto.
  relevance_score: 8
  source: llm_enhanced
  text: This trust model in itself is very different from any notion of trust you
    can find in cryptographic literature or in more trust execution environment sort
    of literature.
  topic: strategy/technical
- impact_reason: Describes the natural migration path for security experts into AI
    security, driven by the emergence of non-classical, unpredictable components (agents).
  relevance_score: 8
  source: llm_enhanced
  text: 'It''s folks who specialize in breaking computers, then starting noticing
    this weird component here and there alongside the technical part of the pipeline
    where they''re not classical software components. They are this wonderful, weird
    AI agents appearing, and you suddenly start asking a question: What do they do?
    What do we trust about them?'
  topic: strategy/business
- impact_reason: Contrasts the relative predictability of smaller models with the
    opaque nature of modern large models, highlighting a loss of engineering control.
  relevance_score: 8
  source: llm_enhanced
  text: I think we had significantly more control when we dealt with smaller models,
    like we kind of knew which knobs to turn to make the model do stuff.
  topic: technical/limitations
- impact_reason: A clear prediction about the near-future trend of highly personalized
    AI agents mirroring individual users.
  relevance_score: 8
  source: llm_enhanced
  text: personalized AI models are going to be a big thing. It's like, I'm going to
    want a model that provides a web chatbot interface for you to talk to me when
    I don't have time or something that more or less is my personality.
  topic: predictions
- impact_reason: 'Describes the business vision: highly reusable, generic foundation
    models combined with secure, personalized data layers, enabling rapid customization.'
  relevance_score: 8
  source: llm_enhanced
  text: This could also allow the creation of almost generic models. Indeed, yeah.
    That then could just be attached to my personal data. Yeah. And now suddenly it's
    customized for me, right?
  topic: business/predictions
- impact_reason: 'Claims universality: the proposed security architecture works regardless
    of the underlying proprietary or open-source LLM being used.'
  relevance_score: 8
  source: llm_enhanced
  text: we check pretty much all of the models from all of the providers because it
    doesn't matter what it is. And then we put on top our Camel system that basically
    performs the orchestration interaction with private data and then forces arbitrary
    rules.
  topic: technical/strategy
- impact_reason: Highlights the modularity and reduced friction for end-users adopting
    complex AI solutions by separating core logic from personal data management.
  relevance_score: 8
  source: llm_enhanced
  text: What I love about this system, right, is that because it's all my private
    sensitive data is really just factored over into some separate data source, right?
    And so I can just get off the shelf all the other parts.
  topic: business
- impact_reason: Suggests that agent behavior, even when given a task, remains highly
    unpredictable, undermining simple 'prompt dependency' assumptions.
  relevance_score: 8
  source: llm_enhanced
  text: I think it's very unpredictable. I have to say, I've been running like agentic
    workloads forever, and very often you find that agents, when they ask to solve
    a task, they solv
  topic: technical/limitations
- impact_reason: Contrasts AI reasoning processes (iterative, exhaustive search) with
    human intuition in complex problem-solving, suggesting a fundamental difference
    in cognitive architecture.
  relevance_score: 8
  source: llm_enhanced
  text: If you look at the way that solves mathematical problems, this is not how
    humans solve mathematical problems. Right? Like, if you look at the transcript
    itself, it kind of goes iteratively through all possible things it can do. Right?
    We assume we don't think about it.
  topic: technical/predictions
- impact_reason: A stark prediction that agentic systems will drastically amplify
    existing threats like corporate espionage, leveraging the same underlying security
    vulnerabilities.
  relevance_score: 8
  source: llm_enhanced
  text: Expect that your inside your threats, your corporate espionage things will
    go through the roof because the fundamentals of security do not change.
  topic: predictions/safety
- impact_reason: Sets an extremely high reliability bar (five nines) for critical
    systems and advocates for building robust, contained environments ('boxes') rather
    than relying solely on model introspection.
  relevance_score: 8
  source: llm_enhanced
  text: We need something where we can get like a 99.999 reliability out of this.
    We have to think outside the box. We kind of have to build things. We need to
    build boxes.
  topic: strategy/safety
- impact_reason: Balances the acknowledged benefits of current AI with a strong warning
    about the impending security crisis driven by advanced agents.
  relevance_score: 8
  source: llm_enhanced
  text: There is some benefit in early examples of things where they clearly made
    things better. But they're coming. I'm pretty sure. And when they do come, expect
    that your inside your threats, your corporate espionage things will go through
    the roof...
  topic: predictions/safety
- impact_reason: A prediction that as AI models become more powerful and integrated,
    the frequency and severity of security compromises will increase proportionally.
  relevance_score: 8
  source: llm_enhanced
  text: I think today, I think we will have a lot more compromise to the point when
    they become more useful.
  topic: predictions/safety
- impact_reason: Critiques the incentive structure in academic security research,
    arguing that breaking things is rewarded over building robust, long-term defenses,
    which is crucial for industry adoption.
  relevance_score: 8
  source: llm_enhanced
  text: I think in academia, you get famous for breaking stuff. And kind of like the
    incentives are a little bit skewed for you to make a like a flashy announcement.
    Oh, I broke into this big company, thus I'm pretty cool, right? I get a job or
    whatever. But actually, on the other side, I have to say, I think the true ingenuity
    is in people who solve problems because this is something that you're not going
    to get a flashy article out of this.
  topic: strategy/ethics
- impact_reason: A profound statement suggesting that the flexibility and controllability
    of modern ML systems (the 'programs' we write) might allow us to bypass theoretical
    computational limits like the Halting Problem in practical ways.
  relevance_score: 8
  source: llm_enhanced
  text: I think models, especially modern models, change fundamentals of computing
    quite a bit. Because when we think about halting problem style problems, being
    able to tell about a given program completes, I think it doesn't really work in
    general, in generality. It's impossible. But then because today we control what
    programs we write, if you can't reason about this program, just rewrite this,
    like change the semantics, write it in a slightly different way, reduce the amount
    of operations, reduce the just overall length of your program. And suddenly you're
    capable of reasoning a lot more about this.
  topic: technical/predictions
- impact_reason: 'A hopeful counterpoint to the security warnings: ML might ultimately
    be the key to solving complex security challenges, even if we haven''t figured
    out how to leverage it defensively yet.'
  relevance_score: 8
  source: llm_enhanced
  text: I wouldn't be too surprised if we find that ML significantly improves security
    for us in this world, but we just don't know how to like another example.
  topic: predictions/safety
- impact_reason: Identifies a key human cognitive limitation in security (overwhelmed
    by granular permissions) and suggests AI agents could act as necessary intermediaries
    to enforce secure defaults.
  relevance_score: 8
  source: llm_enhanced
  text: Academically, it is just shows that the second you add like a lot of breakdown
    permissions inside, humans just look struck and just say accept all. But let's
    imagine that all of those permissions are actually handled by the agent or once
    you've given them the agent just checks and says, oh, actually, I think you're
    over-permissing this thing because it's unclearly you're not using this feature.
  topic: safety/strategy
- impact_reason: 'Highlights a potential benefit of AI agents: overcoming human cognitive
    limitations (like choice overload) in complex decision-making, specifically related
    to security or configuration.'
  relevance_score: 8
  source: llm_enhanced
  text: there is limitation of humans where too many options force them to take the
    insecure behavior is no longer a thing because you have an agent that kind of
    aids this part that was like humans were blind to other ways.
  topic: predictions/safety
- impact_reason: Strong assertion emphasizing that theoretical computer science limits
    have immediate, tangible consequences in modern AI deployment.
  relevance_score: 8
  source: llm_enhanced
  text: I think the halting problem while theoretical, like also has very important
    practical consequences just when you sit down and try and run the program, right?
  topic: technical/limitations
- impact_reason: Emphasizes that diversity preservation is the core challenge, and
    simply flooding the system with synthetic data provides diminishing returns.
  relevance_score: 8
  source: llm_enhanced
  text: But the fundamental thing still remains in place. You need to preserve diversity,
    just plug in a ton of synthetic data. It's like you're not going to give you much
    performance boost.
  topic: technical
- impact_reason: 'Sets up the central tension in future AI development: the abundance
    of potentially low-quality synthetic data versus the difficulty of acquiring high-quality
    data.'
  relevance_score: 8
  source: llm_enhanced
  text: We were trying to predict the sort of the future on how easy it will be to
    train models later because on one hand, you have this weird oracle from which
    you can gather as much data as you want. On the other hand...
  topic: predictions/technical
- impact_reason: Acknowledges the current tension and debate within the AI community
    regarding the overlap and distinction between safety and security disciplines.
  relevance_score: 7
  source: llm_enhanced
  text: I think in AI, it's a little bit hard. I think this is a very spicy question
    because I think safety folks will say it's exactly the same in security and like
    a classical software sort of security space.
  topic: safety/strategy
- impact_reason: Used as an analogy to describe the opaque and brittle nature of complex
    systems, drawing a parallel to the unpredictability in deep learning systems ('systems
    alchemy').
  relevance_score: 7
  source: llm_enhanced
  text: Modern computers are very much just, you know, a piece of magic where this
    chip works, you move it very slightly, it becomes unstable, and then nobody knows
    what's happening.
  topic: strategy/technical
- impact_reason: Raises the novel security challenge of maintaining privacy (like
    E2E encryption) when interacting with models, suggesting new cryptographic or
    computational methods might be needed.
  relevance_score: 7
  source: llm_enhanced
  text: It stops me from talking to a model in an end-to-end encrypted way, right?
    Maybe we'll need an external to it. Maybe we need to teach it how to do, I don't
    know, power calculations, but this is coming.
  topic: safety/technical
- impact_reason: Reiterates the fundamental dependency of AI capability advancement
    on hardware availability and performance.
  relevance_score: 7
  source: llm_enhanced
  text: We're still bottlenecked very much by hardware. Like the more hardware, the
    more capable hardware we get, the better things should become. I'm quite certain.
  topic: technical/business
- impact_reason: Draws a distinction between enterprise-controlled environments and
    the consumer/open-source space regarding supply chain security, expressing deep
    distrust even in some industry practices.
  relevance_score: 7
  source: llm_enhanced
  text: In industry, it's slightly different because industry actually controls all
    of this package management by themselves. They have proper dedicated teams looking
    at supply chains. But I think in the sort of like consumer space, no, it's actually
    very spooky. I don't even trust industry for this.
  topic: business/safety
- impact_reason: Indicates a strategic shift towards building defensive tooling in
    response to the security landscape, acknowledging the theoretical limits of censorship/control
    in language models (linking to the Halting Problem).
  relevance_score: 7
  source: llm_enhanced
  text: I really want to build more defensive tooling. And for you, thank you. Well,
    thank you. Well, let's see if I fail when I face. We were talking about semantics
    early. So I think you've done some work basically proving that semantics censorship
    for language is impossible and you related it to the halting problem.
  topic: strategy/safety
- impact_reason: Illustrates a simple, yet effective, adversarial strategy for hiding
    malicious dependencies by using obscure or non-standard formats that automated
    scanners might ignore.
  relevance_score: 7
  source: llm_enhanced
  text: Heck, if I was an adversary, maybe I'll create a new format that nobody cares
    about, it's set me just so you can load my dependency.
  topic: safety
- impact_reason: A strong critique of academic incentives, suggesting they prioritize
    short-term recognition (breaking things) over long-term utility (building defenses/unlocking
    tech).
  relevance_score: 7
  source: llm_enhanced
  text: I think the incentives in academia are a little bit screwed. They are more
    after flashy articles rather than unlocking technology.
  topic: strategy/ethics
- impact_reason: Shifts the paradigm from 'can we solve it?' to 'how much time/compute
    are we willing to spend?' in dealing with complex program verification and adversarial
    robustness.
  relevance_score: 7
  source: llm_enhanced
  text: I think it's more of a, is just nowadays we kind of pay with time for a lot
    of this stuff rather than paying like, can we find another adversarial example?
    Of course, we can. We can run this for longer and you budget the sort of time,
    time budget.
  topic: strategy/technical
- impact_reason: Frames security not just as a technical necessity but as a competitive
    business commodity driven by market trust.
  relevance_score: 7
  source: llm_enhanced
  text: security is a very expensive commodity, right? Like if I can convince you
    to trust my product more than somebody else's, it's, yeah, it's great.
  topic: business/safety
- impact_reason: Reiterates that current security gaps are primarily due to lack of
    knowledge/solutions, not a lack of will, provided a solution existed.
  relevance_score: 7
  source: llm_enhanced
  text: It's economic incentives. It's nothing but, but I have to say that I'm quite
    certain if we knew how to solve stuff, we would solve this.
  topic: safety/business
- impact_reason: Establishes the speaker's deep security pedigree (DeepMind, Ross
    Anderson lineage), lending significant weight to their security assessments of
    AI.
  relevance_score: 6
  source: llm_enhanced
  text: I was based in DeepMind and before that mostly stuck in the dungeons of the
    university. So you did your PhD at Cambridge under the legendary Ross Anderson?
    Yes. So you have fundamentally different DNA, right? You're a security guy.
  topic: strategy
- impact_reason: Illustrates the massive personal and practical utility of running
    powerful models locally, hinting at future ubiquity and offline capability.
  relevance_score: 6
  source: llm_enhanced
  text: It's amazing to have. I also run local models. It's amazing to have pretty
    much like all of Google locally. I was flying somewhere here and I was interacting
    with the model, teaching, asking it to teach me a language when it's local with
    me, like I don't need to carry it around.
  topic: business/strategy
- impact_reason: Uses the example of the highly secure, mathematically proven seL4
    kernel to set a baseline for extreme security verification effort, contrasting
    it with the complexity of modern systems.
  relevance_score: 6
  source: llm_enhanced
  text: Yesterday, if you've ever seen a seL4, do you know what secure extension,
    L4 kernel, this is like a fully human-verified kernel that there is no memory
    exploits at all existing inside. So mathematically proven. It's proven, yeah,
    like a fully proven system, like a fully verified system. And it took like, I
    think maybe wrong, 30 human years to verify the whole thing.
  topic: technical/strategy
- impact_reason: A general but strong endorsement of the transformative nature of
    current AI learning algorithms.
  relevance_score: 6
  source: llm_enhanced
  text: It's definitely a new paradigm. It's a new kind of like learning algorithms
    which are truly amazing.
  topic: predictions
source: Unknown Source
summary: '## Podcast Summary: AI Agents Can Code 10,000 Lines of Hacking Tools In
  Seconds - Dr. Ilia Shumailov (ex-GDM)


  This 61-minute episode features Dr. Ilia Shumailov, a former DeepMind researcher
  now focused on AI security, discussing the radical shift in adversarial thinking
  required by the advent of highly capable AI agents. The core narrative moves from
  the current state of LLM vulnerabilities (like prompt injection) to a future where
  agentic systems necessitate entirely new security paradigms, moving beyond traditional
  human-centric controls.


  ### 1. Focus Area

  The primary focus is **AI Security and Safety in the context of Agentic Systems**.
  Specific topics include: the failure modes of large models compared to older systems,
  the threat posed by agents capable of rapid, complex code generation (e.g., hacking
  tools), the inadequacy of traditional security models (like those based on human
  rationality), and a novel approach to trusted computation that leverages ML models
  instead of traditional cryptography.


  ### 2. Key Technical Insights

  *   **Evolving Failure Modes:** Modern, highly capable LLMs fail differently than
  models from five years ago. While they are more robust against traditional adversarial
  examples derived from gradient information, they become significantly more vulnerable
  to simple rephrasing and instruction manipulation (like indirect prompt injection),
  especially as their instruction-following capabilities increase.

  *   **Agents as Worst-Case Adversaries:** AI agents surpass traditional worst-case
  adversaries (like an irrational child) because they can operate 24/7, possess near-total
  system knowledge, and can generate massive amounts of complex malicious code (e.g.,
  10,000 lines of hacking tools) almost instantly, invalidating assumptions based
  on human limitations.

  *   **ML for Trusted Computation:** Shumailov proposes using ML models (like Jemma)
  as "resettable trusted third parties" to solve problems like the Millionaire Problem
  (private comparison of values) without relying on complex, expensive cryptographic
  protocols. This approach relies on verifiable inference rather than mathematical
  proof.


  ### 3. Business/Investment Angle

  *   **New Security Market:** The shift from human-centric security to agent-centric
  security creates a massive, urgent need for new tooling that provides fine-grained
  control, transparency, and verifiable constraints around agent execution and data
  access.

  *   **Personalized Agents Require New Controls:** The coming wave of personalized
  AI models, which will handle sensitive user data, cannot be secured by current methods
  (like embedding rules in prompts). Investment is needed in system-level orchestration
  and access control layers built around the models.

  *   **Decoupling Logic from Data:** The proposed security architecture (like the
  Camel system) allows for the creation of generic, off-the-shelf agent logic modules
  that can be safely attached to disparate, user-specific private data sources via
  strict, formally verifiable data flow policies.


  ### 4. Notable Companies/People

  *   **Dr. Ilia Shumailov:** Former DeepMind researcher, now building security tooling
  for agentic fleets. His background spans both machine learning and security (PhD
  under Ross Anderson).

  *   **DeepMind:** Mentioned as the location of his previous work on ML security,
  including defending Gemini against indirect prompt injections.

  *   **Camel System:** The proposed framework (detailed in the "Diffusion Prompt
  Injections by Design" paper) that uses formal semantics (like Python code) to define
  execution graphs and enforce static/dynamic policies on data flow between tools
  and data sources.


  ### 5. Future Implications

  The industry is moving toward a state where security tooling must be built *for*
  non-human, highly capable adversaries. Traditional security assumptions based on
  human rationality, time constraints, and physical penalties are obsolete. The future
  of secure AI integration depends on building robust, external **orchestration and
  policy enforcement layers** (like Camel) around foundation models, rather than trying
  to fix the models themselves.


  ### 6. Target Audience

  **AI/ML Engineers, Cybersecurity Professionals (especially those dealing with emerging
  threats), CTOs, and Security Architects.** This content is highly technical and
  strategic, focusing on the fundamental breakdown of existing security models due
  to agentic capabilities.'
tags:
- artificial-intelligence
- ai-infrastructure
- startup
- investment
- google
- anthropic
title: AI Agents Can Code 10,000 Lines of Hacking Tools In Seconds - Dr. Ilia Shumailov
  (ex-GDM)
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 85
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 7
  prominence: 0.7
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 2
  prominence: 0.2
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 2
  prominence: 0.2
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 02:43:57 UTC -->
