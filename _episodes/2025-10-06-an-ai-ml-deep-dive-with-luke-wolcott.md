---
companies:
- category: unknown
  confidence: medium
  context: Hey everyone, welcome back to the 443 Security Simplified. I'm your host
    Mark Belliberty, and joining me to
  name: Security Simplified
  position: 38
- category: unknown
  confidence: medium
  context: ack to the 443 Security Simplified. I'm your host Mark Belliberty, and
    joining me today is Corey Chat JCN Knockrine
  name: Mark Belliberty
  position: 73
- category: unknown
  confidence: medium
  context: our host Mark Belliberty, and joining me today is Corey Chat JCN Knockriner.
    I'm a deep alert about AI. You don't know my ini
  name: Corey Chat JCN Knockriner
  position: 114
- category: unknown
  confidence: medium
  context: just listening, you're joined by a guest as well. Luke Walcott is the Head
    of MDR Data Science at WatchGuard, an
  name: Luke Walcott
  position: 354
- category: unknown
  confidence: medium
  context: d by a guest as well. Luke Walcott is the Head of MDR Data Science at WatchGuard,
    and as Corey just hinted at it, we
  name: MDR Data Science
  position: 382
- category: unknown
  confidence: medium
  context: ng and some of its applications in cybersecurity. So Luke, thank you for
    hopping on. Alright, thanks for ha
  name: So Luke
  position: 554
- category: unknown
  confidence: medium
  context: answer would be as a kid, we watched *Sneakers*. RIP Robert Redford, loved
    the movie, and just got so excited. I thou
  name: RIP Robert Redford
  position: 997
- category: unknown
  confidence: medium
  context: ly into drumming, realized that I was really into North Indian classical
    music, kind of developed this pattern o
  name: North Indian
  position: 1420
- category: unknown
  confidence: medium
  context: ege, and then I went back to India for two years. Then I got into traveling
    around the world. At this poin
  name: Then I
  position: 1767
- category: unknown
  confidence: medium
  context: omplicated things—the most complicated things—the United States healthcare
    system is on that list. At least for a
  name: United States
  position: 2742
- category: tech
  confidence: high
  context: showed a failed Newton in the movie. Remember the Apple Newton, the first
    PDA before even Windows CE came
  name: Apple
  position: 3677
- category: unknown
  confidence: medium
  context: showed a failed Newton in the movie. Remember the Apple Newton, the first
    PDA before even Windows CE came out? O
  name: Apple Newton
  position: 3677
- category: unknown
  confidence: medium
  context: ember the Apple Newton, the first PDA before even Windows CE came out?
    One of the *Sneakers* guys was on a tra
  name: Windows CE
  position: 3717
- category: unknown
  confidence: medium
  context: cided North Indian tabla was the coolest. I think South Indian rhythm is
    officially the most complicated rhythm
  name: South Indian
  position: 4461
- category: unknown
  confidence: medium
  context: ut it for second, so... Wow, man, that's awesome. Here I thought we were
    going to talk about AI, but I'm a
  name: Here I
  position: 4619
- category: unknown
  confidence: medium
  context: tead. I mean, AI can improve all of these things. When I left college,
    I was like, "Well, I'm going to do
  name: When I
  position: 4777
- category: unknown
  confidence: medium
  context: journey to get you to where you're at right now. But I guess, real quick
    before we dive in, you want to
  name: But I
  position: 5077
- category: unknown
  confidence: medium
  context: spend a lot of time working on this model called Sixth Sense, which we'll
    probably talk about more, which is l
  name: Sixth Sense
  position: 5861
- category: unknown
  confidence: medium
  context: ed. Inside of security, we have a bunch of those. Like I said, they're
    not the best because they're kind o
  name: Like I
  position: 14749
- category: unknown
  confidence: medium
  context: 5 different parameters. The algorithm is called a Random Forest. We trained
    it to basically go off whenever there
  name: Random Forest
  position: 15214
- category: unknown
  confidence: medium
  context: ls. So, that's an example of a purely traditional ML Random Forest model
    that we have. Cool. So, you work every day
  name: ML Random Forest
  position: 15927
- category: unknown
  confidence: medium
  context: that is actually managing the data to train them. And Corey's mentioned
    it a couple times; you've mentioned i
  name: And Corey
  position: 16135
- category: unknown
  confidence: medium
  context: that's a way of creating a synthesized situation. If I didn't say, supervised
    is where you have labels t
  name: If I
  position: 18075
- category: unknown
  confidence: medium
  context: a higher weight for a cat if it were triangular. And I'm curious, in a
    supervised model, I assume the SO
  name: And I
  position: 18972
- category: tech
  confidence: high
  context: a dog. That has happened. That's an example of a Google research paper.
    And my understanding is that adve
  name: Google
  position: 22712
- category: tech
  confidence: high
  context: —the Sixth Sense model can serve almost as threat intel where we pick up
    on, "Wait, attackers are using t
  name: Intel
  position: 26337
- category: tech
  confidence: high
  context: re's so much freaking money being thrown into AI. OpenAI estimated it cost
    them between one and two billio
  name: Openai
  position: 27307
- category: unknown
  confidence: medium
  context: people, that's not cheap, but it's not horrible. The Sixth Sense model
    that I train, a lot of the computation is i
  name: The Sixth Sense
  position: 28250
- category: unknown
  confidence: medium
  context: someone doing that at home. So, what the heck is Dragon NaturallySpeaking's
    problem then? Why can't they actually make func
  name: Dragon NaturallySpeaking
  position: 31174
- category: unknown
  confidence: medium
  context: on bell. It's right over there. It's like I got a Raspberry Pi, and—oh,
    fine, most of it is just a timer. Like,
  name: Raspberry Pi
  position: 31816
- category: unknown
  confidence: medium
  context: regards to false positives versus true positives? Because I'm sure there
    is some fine line that you have to s
  name: Because I
  position: 33673
- category: unknown
  confidence: medium
  context: models that I know—the endpoint team, we have the WatchGuard EDR product—has
    a ton of machine learning on device a
  name: WatchGuard EDR
  position: 34267
- category: tech
  confidence: high
  context: rk. I'm going to remove Tuesday and Wednesday and Monday for that whole
    customer from my training data. I'
  name: Monday
  position: 39355
- category: unknown
  confidence: medium
  context: creative stuff there. And this Japanese company, Sakana AI, is like, "What
    if instead we made these neurons?
  name: Sakana AI
  position: 41417
- category: ai_application
  confidence: high
  context: Company where the guest (Luke Walcott) is the Head of MDR Data Science,
    focusing on using AI/ML for managed detection and response (MDR) services, including
    building AI agents and working on the 'Sixth Sense' foundation model.
  name: WatchGuard
  source: llm_enhanced
- category: technology_historical
  confidence: medium
  context: Mentioned as the first PDA used in the movie *Sneakers* for a hack, showing
    early mobile computing/interface concepts.
  name: Apple Newton
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the product that launched the modern AI era in 2022, based
    on pre-trained transformers combined with supervised fine-tuning and reinforcement
    learning.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the company that trained the latest large model (GPT-5), highlighting
    the massive training costs ($1-2 billion).
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the latest large model released by OpenAI, used as an example
    of massive, expensive LLM training.
  name: GPT-5
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Referenced in the context of an adversarial machine learning research paper
    involving image classification (cat/dog).
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of dictation software that has been stuck at around
    80% accuracy for decades, illustrating the difficulty of achieving the final few
    percent of performance.
  name: Dragon NaturallySpeaking
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as historical dictation software that achieved about 80% accuracy
    for decades, illustrating the difficulty of achieving high accuracy in specific
    applications.
  name: Dragon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a voice assistant that the speaker finds less
    functional than using the OpenAI API for home automation.
  name: Alexa
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a voice assistant that the speaker finds less
    functional than using the OpenAI API for home automation.
  name: Siri
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The product line mentioned that utilizes a ton of machine learning, both
    on-device and in the cloud, including traditional ML and deep learning models
    for security.
  name: WatchGuard EDR product
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: A Japanese tech company mentioned as an example of innovation outside of
    the standard transformer architecture, focusing on biologically inspired neural
    network wiring.
  name: Sakana AI
  source: llm_enhanced
date: 2025-10-06 16:03:53 +0000
duration: 53
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: do that
  text: We should do that.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: curated data
  text: the problem with curated data is to use the stuff you really want, data at
    scale.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/701cabdee386456485214e261da97b20/
processing_date: 2025-10-06 19:22:15 +0000
quotes:
- length: 265
  relevance_score: 8
  text: 'So, really quick question: Throughout this, you kind of talked about the
    evolution of how AI has gone from basic ML use in algorithm growth like nearest
    neighbors to Random Forest, we''ve talked about neural networks, you talked about
    how LLMs are using transformers'
  topics:
  - growth
- length: 114
  relevance_score: 5
  text: You have to learn what's normal, so you take some training data and you say,
    "Okay, let's use that as our training
  topics: []
- length: 201
  relevance_score: 4
  text: And then I spend a lot of time working on this model called Sixth Sense, which
    we'll probably talk about more, which is like, I would say, an interpretable multimodal
    foundation model for cybersecurity
  topics: []
- length: 149
  relevance_score: 4
  text: That was all like what data science meant until the 2000s when we added to
    that algorithms related or broadly called neural networks or deep learning
  topics: []
- length: 198
  relevance_score: 4
  text: So, there was sort of a deep learning era, and then in 2017, a very specific
    type of neural network architecture was discovered called attention, and it was
    built into the thing called a transformer
  topics: []
- length: 162
  relevance_score: 4
  text: And the difference there was that in addition to training the transformer
    on all of the internet's data, they also discovered some really powerful post-processing
  topics: []
- length: 239
  relevance_score: 4
  text: Yes, but especially that we have the outputs of, like, you have enough training
    results, you know, that we're trying to get the SOC to create data, so this is
    the type of thing you would escalate, and this is the type of thing you wouldn't
  topics: []
- length: 105
  relevance_score: 4
  text: And we hear just how expensive it is to train these LLMs and data centers
    full of GPUs and just the power
  topics: []
- length: 208
  relevance_score: 3
  text: I just love how, I guess, when I think of what does it take to be a hacker,
    you have to be kind of obsessed about really esoteric, detailed minutiae, and
    that sort of gives you magic secret power once you are
  topics: []
- length: 234
  relevance_score: 3
  text: '" All you have to do is switch one of those failed logins to a successful
    login, kind of randomly sample which one you''re going to do, and then you can
    basically convert your useless unlabeled data by synthesizing these positive cases'
  topics: []
- length: 146
  relevance_score: 3
  text: Because I'm sure there is some fine line that you have to stay in between
    effective or no false positives but will literally never detect anything
  topics: []
- length: 98
  relevance_score: 3
  text: I'm going to remove Tuesday and Wednesday and Monday for that whole customer
    from my training data
  topics: []
- impact_reason: Introduces a specific, named, proprietary model ('Sixth Sense') focused
    on interpretability and multimodality within the security domain, which is a key
    technical focus area.
  relevance_score: 10
  source: llm_enhanced
  text: And then I spend a lot of time working on this model called Sixth Sense, which
    we'll probably talk about more, which is like, I would say, an interpretable multimodal
    foundation model for cybersecurity.
  topic: Technical insights/model architectures
- impact_reason: A crucial strategic insight arguing that traditional/specialized
    ML models (non-LLM) remain superior or more useful in specific, high-stakes industries
    like cybersecurity.
  relevance_score: 10
  source: llm_enhanced
  text: I think someday people are going to forget that there are other types of AI
    that are not like the chatbots and stuff, but in industries like cybersecurity,
    I think those non-LLM-based machine learning and AI are actually the most powerful,
    most useful.
  topic: AI technology trends/limitations
- impact_reason: Pinpoints the exact architectural breakthrough (Attention/Transformer
    in 2017) that catalyzed the modern AI era.
  relevance_score: 10
  source: llm_enhanced
  text: And so, there was sort of a deep learning era, and then in 2017, a very specific
    type of neural network architecture was discovered called attention, and it was
    built into the thing called a transformer. And this just again was a complete
    revolution.
  topic: AI technology trends/breakthroughs
- impact_reason: Provides the three-stage recipe for modern LLMs (Pre-training + SFT
    + RLHF), marking the shift to generative AI dominance.
  relevance_score: 10
  source: llm_enhanced
  text: And then that kind of went from 2017 up until, I think it was 2022 when ChatGPT
    came out. And the difference there was that in addition to training the transformer
    on all of the internet's data, they also discovered some really powerful post-processing.
    So, you initially pre-train it on the internet, and then there's another phase
    where you do the supervised fine-tuning on question-answer, and then there's another
    phase where you do this reinforcement learning.
  topic: Technical insights/model architectures
- impact_reason: Provides a concrete comparison of parameter count (2 vs. billions/trillions)
    to illustrate the massive increase in model capacity for knowledge storage.
  relevance_score: 10
  source: llm_enhanced
  text: I think for that, there are two parameters. And so, you're trying to find
    this line, and there are two parameters. Well, modern-day models have not two
    parameters, but billions of parameters, sometimes trillions of parameters. And
    that's just a capacity for storing information that's like, 'Hey, I know the internet.
    Give me more. Give me another. Keep coming.'
  topic: Technical insights/Model capacity
- impact_reason: 'Provides insight into the training mechanism of LLMs: initial memorization
    followed by the necessity of learning underlying structure (language rules) due
    to capacity limits.'
  relevance_score: 10
  source: llm_enhanced
  text: the proxy task is to predict the next word in the sentence, as you probably
    heard with these LLMs. And that's hard to do unless you memorize all your content,
    which they do first. And then once there's too much content, then they have to
    actually learn how does English work, or how does the language work?
  topic: technical/AI trends
- impact_reason: 'Excellent, clear explanation of an autoencoder used for anomaly
    detection: training on ''normal'' data via compression/reconstruction loss.'
  relevance_score: 10
  source: llm_enhanced
  text: what an autoencoder is. And what it does is I give it whatever data—just normal
    data from everywhere. It's trained on five months of all of our customers' data...
    Your job, your proxy task, is to take the input data, compress it down into a
    smaller space... and then you have to decode it, push it back out in such a way
    that you compress and then decompress, and I'm going to score you on how well
    you reconstructed the original.
  topic: technical/model architecture
- impact_reason: Directly links feature interpretability (human-engineered features)
    in simpler models to vulnerability against adversarial evasion.
  relevance_score: 10
  source: llm_enhanced
  text: The Random Forest botnet attack, you could definitely evade by doing this
    sort of poking at it and figuring out, because like I said, there are 25 features
    that were created, and I see them, and they're kind of the obvious ones, like,
    "Is it a normal time to log in? Is it a normal location to log in from?" And so,
    an attacker could figure out what we're looking for and fly under the radar, definitely,
    because, like you said, there were humans making these features and basically
    putting together the story of what a botnet attack looks like.
  topic: safety/adversarial ML
- impact_reason: Emphasizes the power of unsupervised learning in security, where
    the model learns 'normal' behavior without explicit labels, allowing it to detect
    novel attacks based on deviations in sequence and context, which is a major trend
    in advanced anomaly detection.
  relevance_score: 10
  source: llm_enhanced
  text: So, how different is good versus bad, right? And then the sequence of when
    you run the different living-off-the-land binaries is something you can manipulate.
    And so, we have constructed this anomaly detector in a way that's like stupid—speaking
    of AI and learning gone wrong—so things that Sixth Sense has going for it is that
    it's unsupervised.
  topic: technical
- impact_reason: Quantifies the scale difference between general-purpose LLMs (trillions
    of parameters) and specialized, high-value models (hundreds of millions), demonstrating
    that efficiency is achievable in specific domains.
  relevance_score: 10
  source: llm_enhanced
  text: So, whereas your ChatGPT or GPT-5 might be one trillion parameters, the Sixth
    Sense model—it's a completely different paradigm. It's not doing text; it's about—it's
    like around a hundred million parameters. So, a hundred million is a lot less
    than a trillion.
  topic: technical
- impact_reason: 'Directly addresses the critical operational challenge in applied
    ML: balancing sensitivity (True Positives) against the cost of noise (False Positives).'
  relevance_score: 10
  source: llm_enhanced
  text: When it comes to the actual capabilities of these models, what considerations
    do you give to signal versus noise? How do you decide when it is a good model
    with regards to false positives versus true positives?
  topic: business
- impact_reason: 'Establishes a non-negotiable business rule for deploying security
    ML: low false positive rate is a prerequisite for service viability, overriding
    raw detection capability due to alert fatigue.'
  relevance_score: 10
  source: llm_enhanced
  text: 'Every detection we put into service has to have a low false positive rate,
    or else we just don''t put it into service. So, in some way, our decision is:
    Can we make it so it has a low false positive rate?'
  topic: business
- impact_reason: Provides a highly specific, quantifiable Service Level Objective
    (SLO) for acceptable model noise in a B2B security context, which is invaluable
    for operationalizing ML metrics.
  relevance_score: 10
  source: llm_enhanced
  text: Acceptable false positives is three false positives per month per customer—it's
    a larger customer—per 500 endpoints, because we kind of keep track per endpoint.
    So, it's very explicit, and I calculate this every month for every detection...
  topic: business
- impact_reason: 'This is a crucial insight on the practical failure mode of ML systems
    in high-volume environments: noise effectively equates to zero detection capability
    due to human behavior (alert fatigue).'
  relevance_score: 10
  source: llm_enhanced
  text: Now, the reason we care about that is because if something is going off too
    much, it's not really a detection anymore because of alert fatigue; they will
    just ignore it. Everyone ignores it. So, it's not like, 'Oh, we have a noisy detection.'
    It's like, 'You don't even have a detection; no one's looking at it.'
  topic: business
- impact_reason: A powerful articulation of the 'alert fatigue' problem, emphasizing
    that high noise effectively equals zero signal in operational environments. Crucial
    for anyone building monitoring or alerting systems.
  relevance_score: 10
  source: llm_enhanced
  text: if something is going off too much, it's not really a detection anymore because
    of alert fatigue; they will just ignore it. Everyone ignores it. So, it's not
    like, 'Oh, we have a noisy detection.' It's like, 'You don't even have a detection;
    no one's looking at it.'
  topic: business/strategy
- impact_reason: 'Directly addresses the fundamental challenge of unsupervised learning
    in security: the ''ground truth'' of ''normal'' might be corrupted by prior, undetected
    compromises.'
  relevance_score: 10
  source: llm_enhanced
  text: 'How important is curated data? We''ve talked about a lot of things, like
    the cool thing about unsupervised is you can start to look at all kinds of stuff,
    non-tagged data, and get stuff from it. But then you have the question: Was the
    environment in the normal state when it was trained, or was there an unbeknownst
    breach behind the scenes?'
  topic: technical/safety
- impact_reason: 'Provides a concrete, actionable example of data curation for anomaly
    detection: using existing detection signals (even noisy ones) as negative examples
    to clean the ''normal'' training set.'
  relevance_score: 10
  source: llm_enhanced
  text: I only want it to learn on normal data. And so, we have a few heuristics where
    we basically remove out. Suppose there's a customer, and on Tuesday they had a
    detection... I'm going to remove Tuesday and Wednesday and Monday for that whole
    customer from my training data. I'm going to just treat that whole customer as
    compromised for three days and just remove them from my training data.
  topic: technical
- impact_reason: A critical perspective on the current dominance of the Transformer
    architecture, suggesting that future breakthroughs may lie outside this paradigm.
  relevance_score: 10
  source: llm_enhanced
  text: The other one is just this whole obsession we have right now with transformers.
    I think it's nearsighted, and there's other stuff out there, too.
  topic: technical
- impact_reason: Highlights cutting-edge, biologically inspired research (Sakana AI)
    focusing on network topology and synchronous activation patterns, suggesting a
    fundamental shift away from standard dense connectivity.
  relevance_score: 10
  source: llm_enhanced
  text: Sakana AI, is like, 'What if instead we made these neurons?' So, the capacity
    for the model is captured in the patterns of synchronous activations, not individual
    activations within the patterns of the activations. And they just rewire a network
    and train, and like, 'Oh yeah, we can do that too.' Now it also learns the internet,
    and it's just that much more like our brains.
  topic: technical
- impact_reason: Directly addresses the application of generative AI/agents for automating
    high-value cybersecurity tasks (SOC workflows).
  relevance_score: 9
  source: llm_enhanced
  text: One of our mandates—we have two mandates. One is to just do whatever cool
    AI stuff we want to and can think of. And so, for over a year now, we've been
    making these AI agents to help automate SOC workflows...
  topic: AI technology trends/predictions
- impact_reason: 'Highlights the primary use case for their specialized foundation
    model: anomaly detection, emphasizing the importance of integrating diverse data
    sources.'
  relevance_score: 9
  source: llm_enhanced
  text: Basically, we use it as an anomaly detector, and so that's sort of—I love
    just connecting new data sources to that, working on that.
  topic: Technical insights/model architectures
- impact_reason: 'Explains *why* the Transformer architecture was revolutionary: its
    scalability, data appetite, and parallelizability suited modern hardware.'
  relevance_score: 9
  source: llm_enhanced
  text: It's just super great because you can scale it out. You can just pump more
    and more data into it. You can kind of grow the size of this, and the model just
    eats up the data you give it, and it's very parallelizable and great for the types
    of chips we use to train models.
  topic: Technical insights/model architectures
- impact_reason: 'Reinforces the earlier point: the vast landscape of useful, non-generative
    ML models often overlooked due to LLM hype.'
  relevance_score: 9
  source: llm_enhanced
  text: I would like to just remind everyone there's a whole tranche of powerful models
    that are neural networks that have nothing to do with chat GPT or generating images
    or video, or generating images or video, all the different, yeah. And then there's
    also all these other algorithms that we could broadly call everything traditional
    ML at this point because it's everything else.
  topic: AI technology trends/limitations
- impact_reason: Confirms that both scale *and* architecture (specifically the attention
    mechanism) are key to the magic.
  relevance_score: 9
  source: llm_enhanced
  text: Yeah, no, I think that's it. And the way that the neural networks are linked
    together is kind of also part of the magic. The fact that there's a sort of attention
    layer that does a certain effect where the input data—it kind of learns what's
    important, what does it need to attend to in the input data—and then followed
    by a certain block of a different type of structure...
  topic: Technical insights
- impact_reason: Highlights the sheer scale and information capacity of modern large
    models (LLMs/Transformers) compared to simple linear models, emphasizing the data
    absorption capability.
  relevance_score: 9
  source: llm_enhanced
  text: modern-day models have not two parameters, but billions of parameters, sometimes
    trillions of parameters. And that's just a capacity for storing information that's
    like, "Hey, I know the internet. Give me more. Give me another. Keep coming."
  topic: technical/AI trends
- impact_reason: 'Crucially explains the fundamental challenge of supervised learning
    in security: extreme class imbalance (lack of labeled attack data).'
  relevance_score: 9
  source: llm_enhanced
  text: In cybersecurity, the type of labels you would want, you don't always have,
    or they're very skewed, because an attack is a true positive, and then everything
    else is just normal, not attacks.
  topic: technical/safety & ethics
- impact_reason: 'Offers a clever, actionable strategy for overcoming label scarcity
    in security: data synthesis/augmentation by flipping outcomes of known events.'
  relevance_score: 9
  source: llm_enhanced
  text: you can synthesize true examples, positive cases, by just saying, "Suppose
    this failed botnet attack was actually successful." All you have to do is switch
    one of those failed logins to a successful login... and then you can basically
    convert your useless unlabeled data by synthesizing these positive cases.
  topic: business/strategy/technical
- impact_reason: A classic, accessible description of adversarial examples, confirming
    their reality and effectiveness in confusing image classification models.
  relevance_score: 9
  source: llm_enhanced
  text: if there are patterns that have been discovered by non-humans—let's say I
    take a picture of a cat, and I know it's a cat, but I could put a little sticker
    in the picture with the cat that looks like it just has random noise, and suddenly
    the AI thinks it's a dog. That has happened.
  topic: safety/adversarial ML
- impact_reason: 'Explains the mechanism of adaptive adversarial attacks: using feedback
    (input/output pairs) to reverse-engineer the model''s decision boundaries.'
  relevance_score: 9
  source: llm_enhanced
  text: adversarial machine learning, by being able to hook to a system that could
    both see the inputs of the questions being asked to the model and the outputs
    of the result, could over a long period of time, having seen that over and over,
    start to figure out some of the underlying features that the ML model was looking
    for to make a decision of yes or no.
  topic: safety/adversarial ML
- impact_reason: Contrasts the vulnerable, human-selected features of the Random Forest
    with the high-dimensional, comprehensive feature set of the autoencoder, suggesting
    robustness through complexity.
  relevance_score: 9
  source: llm_enhanced
  text: 'With the Sixth Sense model, well, first of all, there are a lot more features
    that it collects: weak signals, very weak signals, medium-quality signals, high-quality
    signals. We just dump everything in there. There are thousands of features.'
  topic: technical/strategy
- impact_reason: Identifies 'Living Off The Land' (LOTL) techniques as a key area
    where advanced ML is needed because they blend malicious activity with legitimate
    administrative tools.
  relevance_score: 9
  source: llm_enhanced
  text: They're focused on this living-off-the-land activity that is super hard for
    more naive detections to detect. You know, it's the kind of thing that a security
    attacker uses because they know they can get away with it. PowerShell is something
    admins use.
  topic: predictions/practical application
- impact_reason: This speaks directly to the trade-off between interpretability and
    power in advanced AI. The 'black box' nature of unsupervised learning is framed
    as a strength in security because it detects unknown unknowns.
  relevance_score: 9
  source: llm_enhanced
  text: I couldn't explain to you why it thinks that such and such is anomalous, and
    that's very powerful. All it knows is I gave it a bunch of normal data, and it
    knows what normally happens in a small or medium business customer network, and
    when it sees something that deviates from that...
  topic: safety/technical
- impact_reason: 'Illustrates the core challenge in adversarial ML defense: attackers
    must mimic legitimate behavior closely, but advanced anomaly detection can catch
    subtle deviations in *how* legitimate tools are used (sequence/context).'
  relevance_score: 9
  source: llm_enhanced
  text: So, yes, you're using PowerShell, but you're using it in a way that's not
    the way that PowerShell is used now. That's really hard to evade, right? Because
    if you want to be an attacker, you kind of do have to use PowerShell exactly.
  topic: technical
- impact_reason: A sharp critique of the current AI landscape, suggesting that the
    focus on massive foundation models is overshadowing the value of smaller, domain-specific,
    and potentially more efficient ML applications.
  relevance_score: 9
  source: llm_enhanced
  text: This is the phenomenon of the LLMs just sucking all the oxygen out of the
    air, and people are forgetting that there are these very useful applications.
  topic: strategy
- impact_reason: Provides a concrete, contrasting cost metric for specialized model
    maintenance versus the multi-billion dollar cost of training foundation models,
    offering actionable insight for businesses building niche AI.
  relevance_score: 9
  source: llm_enhanced
  text: So, the models that we have, sort of retraining themselves on some frequency,
    like once a week, once every two weeks—those retraining jobs cost a few thousand
    dollars.
  topic: business
- impact_reason: 'Articulates the ''last mile problem'' in AI adoption: the gap between
    ''good enough'' (80%) and truly usable/reliable performance (99.99%) often requires
    disproportionate effort, especially in nuanced tasks like human language.'
  relevance_score: 9
  source: llm_enhanced
  text: I'm wondering if that last 5%—because if you think of it as dictation software,
    and I've been using Dragon in the past, it's been 80% there for multiple decades.
    But anyone that tries to use it every day knows that 80% is nowhere near good
    enough for grammar. And the last 20% is just tiny little minute changes that get
    it there. And by the way, I still don't think it's usable...
  topic: strategy
- impact_reason: 'Describes a sophisticated, layered ML strategy: using high-precision,
    low-volume traditional models for known threats, and reserving the powerful, potentially
    noisier unsupervised model (Sixth Sense) as a generalized, high-recall backstop.'
  relevance_score: 9
  source: llm_enhanced
  text: I think of it as this sparse mesh of detections that cover as many possible
    things as we can in the MITRE framework or whatever matrix. And then our Sixth
    Sense model is this backstop where if there's something that's too noisy, we just
    add it to there, so it sort of becomes this catch-all backstop that allows us
    to take this approach.
  topic: strategy
- impact_reason: 'Highlights the critical business and operational constraint in security
    monitoring: false positives lead to alert fatigue, rendering detections useless.
    This is a core challenge in real-world ML deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: So, this is, I would say, our decision is first of all, is this detection
    acceptable in terms of its low false positive rate?
  topic: business/strategy
- impact_reason: 'Describes a practical architectural pattern: using a specialized,
    potentially more sensitive model (Sixth Sense) to handle signals that are too
    noisy for the primary, low-false-positive detection layer.'
  relevance_score: 9
  source: llm_enhanced
  text: And then our Sixth Sense model is this backstop where if there's something
    that's too noisy, we just add it to there, so it sort of becomes this catch-all
    backstop that allows us to take this approach.
  topic: technical
- impact_reason: Elevates data curation from a simple cleaning step to a core, scientific
    differentiator in building effective ML systems.
  relevance_score: 9
  source: llm_enhanced
  text: And that's a way so that when I do feed this, 'Okay, learn what's normal,'
    it is actually super normal stuff. So, that's data curation, and there are a lot
    of choices you can make in filtering out. That's probably where the science comes
    in, the data science.
  topic: technical/strategy
- impact_reason: A strong counter-argument to the plateau theory, attributing continued
    rapid advancement to the synergy between open and closed AI communities.
  relevance_score: 9
  source: llm_enhanced
  text: I think there's no plateau inside. The way that the closed-source and open-source
    communities are bouncing ideas off each other, new algorithms come up for a faster
    way to implement attention or a slightly optimized thing, and then everyone starts
    using it like a few days later.
  topic: predictions
- impact_reason: Points to promising, non-deep-learning mathematical techniques (Topological
    Data Analysis) that are currently underutilized in industry but hold significant
    potential.
  relevance_score: 9
  source: llm_enhanced
  text: And then there's also just more traditional ML that has nothing to do with
    neural networks. There's this area called topological data analysis that I used
    to be in, where there are algorithms that have not been put into as far as I know
    much use in industry, but they are cutting-edge mathematical algorithms for analyzing
    data and extracting insights and stuff.
  topic: technical
- impact_reason: 'Quantifies the scale of detection management (236 detections) and
    sets the critical quality metric for security AI: high signal, low noise.'
  relevance_score: 8
  source: llm_enhanced
  text: The second one is to kind of oversee the health of all of our detections.
    So, the MDR service, you know, we have detections for endpoint and cloud and identity
    and mobile network. I think 236 detections in service last time I checked. All
    of those have to be high signal, low noise.
  topic: Business/Strategy/Practical lessons
- impact_reason: 'Clearly demarcates the transition point in data science history:
    traditional statistical methods (like decision trees) gave way to deep learning
    post-2000s.'
  relevance_score: 8
  source: llm_enhanced
  text: And there are a lot of different variations on that, like a golden decision
    tree that captures the logic of this data set, you know, put the data points into
    a new space and then figure out what kind of slice you can put—draw through that—so
    they're like all the 'yeses' are on this side, all the 'noes' are on that side.
    That was all like what data science meant until the 2000s when we added to that
    algorithms related or broadly called neural networks or deep learning.
  topic: Technical explanations
- impact_reason: Explains the computational efficiency underpinning deep learning's
    success—reliance on fast matrix operations.
  relevance_score: 8
  source: llm_enhanced
  text: And that's like a particular type of connecting nodes with activations, and
    it's very, very, very powerful for computers to train these things because it's
    basically just addition and matrix multiplication, things we can do fast.
  topic: Technical insights
- impact_reason: Succinctly summarizes the functional outcome of the modern LLM paradigm.
  relevance_score: 8
  source: llm_enhanced
  text: Basically, the result is you have a chatbot who's good at question-answering,
    who knows the whole internet, and who talks like a human. And that's the three
    pieces that came together to make ChatGPT...
  topic: AI technology trends
- impact_reason: A commentary on the current market hype cycle, suggesting a misallocation
    of focus/capital away from other valuable AI applications.
  relevance_score: 8
  source: llm_enhanced
  text: Everyone else is just distracted talking about how amazing these LLMs are
    because they're amazing. It's absurd how much money is getting spent on these
    things. It's absolutely absurd.
  topic: Business/Strategy/Predictions
- impact_reason: 'Poses the core philosophical question about the source of modern
    AI''s power: is it just scale (compute/data) or architecture?'
  relevance_score: 8
  source: llm_enhanced
  text: The fact that with compute, we can give a model so much data that it can do
    right things more often? Like humans always have to make yes or no, right or wrong
    decisions... But what's different here is big data. Is that true or false?
  topic: Technical/Philosophical
- impact_reason: Illustrates the power of combining multiple features (contextual
    data) in traditional ML (Random Forest) to achieve high-fidelity security detection.
  relevance_score: 8
  source: llm_enhanced
  text: If you take a few dozen different parameters like that—what countries does
    this person normally log in from, and what time of day is it, and stuff like that—you
    can use those traditional ML models. I'm thinking of a model we have that's for
    detecting a botnet attack or a brute force attack. It has maybe 25 different parameters.
    The algorithm is called a Random Forest.
  topic: technical/practical application
- impact_reason: Demonstrates the high level of trust and automation achievable with
    well-tuned traditional ML models in critical security operations (high fidelity
    leading to automated response).
  relevance_score: 8
  source: llm_enhanced
  text: It evaluates every single login. I would say 65 logins, and there are quite
    a few of those people are logging in a lot, and it is so high fidelity that when
    it does go off, we send it directly to the customer. The customer can configure
    it so that to automatically disable the password, kick the user out, and reset
    their password.
  topic: business/practical application
- impact_reason: 'A fundamental limitation of security monitoring: the impossibility
    of knowing the true set of unseen threats (false negatives), which heavily influences
    unsupervised model design.'
  relevance_score: 8
  source: llm_enhanced
  text: We never know the true negatives we miss—missed attacks. We don't know the
    things you're missing all the time.
  topic: safety/limitations
- impact_reason: 'Highlights the primary business advantage of unsupervised models:
    leveraging massive amounts of readily available, unlabeled data without manual
    labeling bottlenecks.'
  relevance_score: 8
  source: llm_enhanced
  text: The power of it is that now I can train on, as I said, all the data we get.
    I can train on it. I don't need someone to say, "This was a good idea. Here's
    this type of data."
  topic: business/strategy
- impact_reason: Frames adversarial attack development as an iterative, adaptive process
    mirroring offensive security tactics.
  relevance_score: 8
  source: llm_enhanced
  text: I think that attacks can be as convoluted as necessary to evade. I mean, that's
    what you're doing when you're attacking; you're figuring out—you try something
    until you're blocked, and then you pivot and you try something else.
  topic: safety/strategy
- impact_reason: 'Highlights a dual utility of advanced anomaly models: not just detection,
    but generating novel threat intelligence by surfacing previously unseen attack
    patterns.'
  relevance_score: 8
  source: llm_enhanced
  text: The power there is like, and we can even—the Sixth Sense model can serve almost
    as threat intel where we pick up on, 'Wait, attackers are using things in new
    ways,' because we're seeing the whole signal. And like I said, it's interpretable.
    When the anomaly score is high, it tells you which of the features it hit flagged
    as being surprised by these features, this collection of sequence of events.
  topic: strategy
- impact_reason: Captures the prevailing public concern regarding the massive capital
    and environmental cost associated with state-of-the-art LLMs, setting up the contrast
    for smaller, specialized models.
  relevance_score: 8
  source: llm_enhanced
  text: OpenAI estimated it cost them between one and two billion just to train that
    latest model they released, was it in three weeks or a month, GPT-5? And we hear
    just how expensive it is to train these LLMs and data centers full of GPUs and
    just the power. So, it makes you think, 'Oh gosh, it costs way too much, and we
    have to kill the earth just to use LLMs.'
  topic: business/predictions
- impact_reason: Uses robotics as a parallel example to reinforce the point that complex
    physical tasks can be solved with relatively small, highly optimized models, challenging
    the 'bigger is always better' narrative in AI.
  relevance_score: 8
  source: llm_enhanced
  text: Another example of small models not being bad is robots. You know, when you're
    watching the videos of the robot folding clothes and stuff, they have a model
    that's a vision computer vision, convert image into numbers, and that is tens
    or hundreds of millions of parameters. And then the actual model that they train
    to tell the robot how to move has only a few thousand or millions of parameters—very
    small, very tiny—and training that is free.
  topic: technical
- impact_reason: Outlines a continuous improvement loop based on post-incident analysis
    (RCA), where failure is systematically converted back into new model features
    or detections, ensuring model evolution.
  relevance_score: 8
  source: llm_enhanced
  text: We do debriefs and RCAs on all of our incidents, all of our major incidents,
    and almost always it results in making more detections. So, the result is that
    we don't—it seems to be working.
  topic: strategy
- impact_reason: A strong, slightly cynical, but realistic assessment of the data
    quality problem, especially relevant when training models on vast, unfiltered
    datasets like the internet.
  relevance_score: 8
  source: llm_enhanced
  text: I mean, cybersecurity is kind of a finite universe. It's protected from—I
    absolutely agree the internet is mostly false.
  topic: safety/strategy
- impact_reason: 'Poses the central question regarding the current state of AI progress:
    are we in an innovation plateau or is rapid advancement continuing?'
  relevance_score: 8
  source: llm_enhanced
  text: Is there still new advancement yet to be discovered as fast, or are we kind
    of at a plateau where companies are talking about it, but they haven't really
    pushed the back-end advancement much lately?
  topic: predictions
- impact_reason: Summarizes the optimistic view that despite decades of research,
    the core technology of AI/ML is still in its early stages of practical application
    and discovery.
  relevance_score: 8
  source: llm_enhanced
  text: So, you're basically saying it's probably in its infancy in technology, even
    though it started in the '60s, '70s, and there's a lot to come?
  topic: predictions
- impact_reason: Provides context on the operational environment (MDR/SOC) where AI/ML
    is being applied, highlighting the target market (SMBs).
  relevance_score: 7
  source: llm_enhanced
  text: We have a SOC, and we sort of this is like a wraparound cybersecurity service
    for all the different platforms, all the different everything's for small and
    medium businesses.
  topic: business/strategy
- impact_reason: Provides a clear, foundational definition of traditional ML rooted
    in statistics, setting the baseline before deep learning.
  relevance_score: 7
  source: llm_enhanced
  text: The start of any kind of machine learning would be, you know, train a function
    using data to learn patterns. And the data, like you've got an XY plot, and there
    are some points on there, and you say, 'What's the line that best captures that
    data, like $y = mx + b$?' And there's an algorithm that sort of helps you... That's
    statistics; it's been around for a long time.
  topic: Technical explanations
- impact_reason: A strong analogy connecting the mindset required for deep expertise
    in esoteric fields (like the speaker's math background) to the mindset of a successful
    hacker/security professional.
  relevance_score: 7
  source: llm_enhanced
  text: I just love how, I guess, when I think of what does it take to be a hacker,
    you have to be kind of obsessed about really esoteric, detailed minutiae, and
    that sort of gives you magic secret power once you are.
  topic: Strategy/General insight
- impact_reason: Provides a concrete, simple example of basic ML (anomaly detection
    based on a single threshold/parameter) used in a real-world security context.
  relevance_score: 7
  source: llm_enhanced
  text: a good example of a very basic, simple thing that I would call ML is, in security,
    you know, if you're interested in a particular parameter and you want to alert
    when that parameter is abnormally large, whatever that needs to be—so, the amount
    of traffic leaving a certain IP address going wherever, day-to-day, there's a
    certain amount of data going. Let me know when there's a spike, right?
  topic: technical/practical application
- impact_reason: Clear, concise definition of supervised learning, useful for grounding
    the subsequent discussion on trade-offs.
  relevance_score: 7
  source: llm_enhanced
  text: So, a supervised model is where the training objective, the task you're giving
    the model to learn on, is explicitly these labels of, "Is this a cat or a dog?
    Is this an attack or not an attack?"
  topic: technical
- impact_reason: 'Defines unsupervised learning by its core mechanism: using a proxy
    task when explicit labels are unavailable.'
  relevance_score: 7
  source: llm_enhanced
  text: unsupervised, they don't have those labels, and you're just saying, "Here's
    a bunch of data, try and learn what you can learn." And so, what you usually end
    up doing is creating like a proxy task.
  topic: technical
- impact_reason: Contrasts the difficulty of achieving human-level nuance (like dictation)
    with tasks that have clear, finite objectives (like games), where AI has already
    surpassed human capability with relatively bounded complexity.
  relevance_score: 7
  source: llm_enhanced
  text: I don't think so, thoroughly. I mean, I think there are definitely scenarios
    where with a finite task, like playing chess and checkers and Go, we didn't have
    to hit an additional wall there; we just cloud-passed the humans. And now the
    reinforcement model beat Mario with the highest score way faster than any human.
  topic: predictions
source: Unknown Source
summary: '## Podcast Episode Summary: An AI/ML Deep Dive with Luke Wolcott


  This 53-minute episode of "443 Security Simplified" features Luke Wolcott, Head
  of MDR Data Science at WatchGuard, focusing on the application of Artificial Intelligence
  and Machine Learning (AI/ML) in cybersecurity, contrasting traditional ML models
  with modern Large Language Models (LLMs).


  ---


  ### 1. Focus Area

  The discussion centers on the **practical application of various Machine Learning
  techniques within Managed Detection and Response (MDR) cybersecurity services**.
  Key topics included the historical evolution of ML, the distinction between supervised
  and unsupervised learning, and the threat of adversarial machine learning.


  ### 2. Key Technical Insights

  *   **Evolution of AI Architectures:** The shift from traditional ML (like decision
  trees) to Deep Learning (neural networks) was catalyzed by the 2017 introduction
  of the **Transformer architecture** (based on the "attention" mechanism), which
  allows for massive scaling with data, leading to modern LLMs.

  *   **Unsupervised Anomaly Detection via Autoencoders:** The "Sixth Sense" model
  at WatchGuard uses an **autoencoder** (an unsupervised technique) as a proxy task:
  compressing input data (thousands of features) into a smaller space and then reconstructing
  it. Poor reconstruction accuracy signals an anomaly, bypassing the need for extensive
  labeled attack data.

  *   **Feature Engineering vs. Scale:** While LLMs rely on massive scale and complex
  wiring (like attention layers) to learn implicitly, traditional, high-fidelity security
  models (like the Random Forest example) often rely on **human-engineered, interpretable
  features** (e.g., login time, location) derived from domain expertise.


  ### 3. Business/Investment Angle

  *   **MDR Data Science Mandate:** WatchGuard''s data science team has a dual mandate:
  developing cutting-edge AI agents to automate SOC workflows and maintaining the
  high fidelity (high signal, low noise) of hundreds of existing security detections.

  *   **Value of Traditional ML in Security:** Despite the hype around LLMs, powerful,
  traditional ML models (like the Random Forest example used for botnet detection)
  remain highly valuable, achieving such high fidelity that they can trigger automated
  customer responses (e.g., password resets).

  *   **Data Labeling Challenge:** In cybersecurity, obtaining sufficient labeled
  attack data for supervised learning is difficult because true attacks are rare (skewed
  data). This necessitates creative solutions like data synthesis or relying on unsupervised
  methods.


  ### 4. Notable Companies/People

  *   **Luke Wolcott (WatchGuard):** Head of MDR Data Science, responsible for developing
  AI agents and the "Sixth Sense" multimodal foundation model for anomaly detection.

  *   **WatchGuard:** The company employing these advanced ML techniques in their
  MDR service for SMBs.

  *   **Corey Chat JCN Knockriner & Mark Belliberty (Hosts):** Facilitated the deep
  dive, providing context on cybersecurity pop culture (*Sneakers*, *WarGames*).


  ### 5. Future Implications

  The conversation suggests a bifurcation in AI development: while the public focuses
  intensely on generative LLMs (built on the Transformer architecture), specialized
  industries like cybersecurity will continue to rely heavily on **highly tuned, often
  simpler, interpretable ML models** for core detection tasks, especially those leveraging
  unsupervised learning to find novel threats. Adversarial ML remains a significant,
  evolving threat that forces continuous adaptation in feature selection and model
  robustness.


  ### 6. Target Audience

  This episode is highly valuable for **Cybersecurity Professionals, Data Scientists
  working in security, Security Engineers, and Technology Leaders** interested in
  the practical, non-hype applications of machine learning for detection, response,
  and SOC automation.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- apple
- google
- openai
title: An AI/ML Deep Dive with Luke Wolcott
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 136
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 19
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 9
  prominence: 0.9
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 19:22:15 UTC -->
