---
companies:
- category: unknown
  confidence: medium
  context: Welcome to Pondering AI. I'm your host, Kimberly Neville. And despite my
  name: Pondering AI
  position: 11
- category: unknown
  confidence: medium
  context: Welcome to Pondering AI. I'm your host, Kimberly Neville. And despite my
    somewhat gravely voice, I am beyo
  name: Kimberly Neville
  position: 40
- category: unknown
  confidence: medium
  context: ravely voice, I am beyond excited to be joined by Karen Kautz. Karen is
    the senior group manager of threat dete
  name: Karen Kautz
  position: 133
- category: unknown
  confidence: medium
  context: And is also a contributor to OWASP, which is the Open Web Application Security
    Project. Did I get that right, Karen? Yeah, it's OWASP. T
  name: Open Web Application Security Project
  position: 289
- category: unknown
  confidence: medium
  context: ich is the Open Web Application Security Project. Did I get that right,
    Karen? Yeah, it's OWASP. That's t
  name: Did I
  position: 328
- category: unknown
  confidence: medium
  context: your interest in working in what is sometimes the Wild West of cybersecurity
    in particular. It is the Wild We
  name: Wild West
  position: 875
- category: unknown
  confidence: medium
  context: startup when it was the days when it only began. So I had to explain to
    ventures what it means that the
  name: So I
  position: 1162
- category: unknown
  confidence: medium
  context: that from the very beginning. And then I went to Special Operations. And
    in there, I exhibited really state actors th
  name: Special Operations
  position: 1368
- category: unknown
  confidence: medium
  context: ther and the stars really aligned, I joined Apex. And Apex was founded
    two years ago. And it was founded by
  name: And Apex
  position: 1738
- category: unknown
  confidence: medium
  context: s ago. And it was founded by Bakes by Sequoia and Sam Almanzar. It all
    sounds very promising. And the team was a
  name: Sam Almanzar
  position: 1817
- category: unknown
  confidence: medium
  context: m was amazing. So I saw the market, I learned it. And I truly recognized
    that there is a difference. Ther
  name: And I
  position: 1922
- category: unknown
  confidence: medium
  context: e seeing here in AI security, and specifically in Generative AI, is two
    different concepts that really make it di
  name: Generative AI
  position: 3489
- category: tech
  confidence: high
  context: this is that when you have access to, let's say, Microsoft Copilot, Google
    Gemini, applications that are the
  name: Microsoft
  position: 3707
- category: unknown
  confidence: medium
  context: this is that when you have access to, let's say, Microsoft Copilot, Google
    Gemini, applications that are the bits an
  name: Microsoft Copilot
  position: 3707
- category: tech
  confidence: high
  context: you have access to, let's say, Microsoft Copilot, Google Gemini, applications
    that are the bits and bytes
  name: Google
  position: 3726
- category: unknown
  confidence: medium
  context: you have access to, let's say, Microsoft Copilot, Google Gemini, applications
    that are the bits and bytes of nowa
  name: Google Gemini
  position: 3726
- category: unknown
  confidence: medium
  context: d dig through my entire, let's say, SharePoint or Google Drive. Now I just
    need to ask Copilot, for example, wha
  name: Google Drive
  position: 4363
- category: unknown
  confidence: medium
  context: my entire, let's say, SharePoint or Google Drive. Now I just need to ask
    Copilot, for example, what is ou
  name: Now I
  position: 4377
- category: unknown
  confidence: medium
  context: like, no, no, no, it's too fast. It won't happen. But I see it on a daily
    basis. I actually see Fortune 5
  name: But I
  position: 4650
- category: unknown
  confidence: medium
  context: en or something like this. The other response was Britney Spears. So it's
    not consistent, right? I've asked these
  name: Britney Spears
  position: 5727
- category: unknown
  confidence: medium
  context: omers asking questions really innocuously, right? Do I get a refund for
    something? And for instance, a c
  name: Do I
  position: 12212
- category: unknown
  confidence: medium
  context: pecifically, right? I had just recently put out a Gen AI security report
    on the state, specifically of Gen
  name: Gen AI
  position: 14217
- category: unknown
  confidence: medium
  context: l that are trying to do hiring decisions with AI. Should I hire this candidate?
    And we see people trying to
  name: Should I
  position: 20102
- category: unknown
  confidence: medium
  context: of had to be an early adopter. But first of all, Gen Z became way more
    early adopters in their nature. A
  name: Gen Z
  position: 26130
- category: unknown
  confidence: medium
  context: to which incidents might occur, and is that true? Because I foresee organizations
    potentially thinking, yeah,
  name: Because I
  position: 36340
- category: ai_application
  confidence: high
  context: Karen Kautz is the senior group manager of threat detection, product management,
    and AI at this company.
  name: Tenable
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Open Web Application Security Project; Karen Kautz is a contributor and
    they recently put out a Gen AI security report.
  name: OWASP
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: A company founded two years ago by Bakes by Sequoia and Sam Almanzar, where
    Karen Kautz joined to focus on AI-first security technology.
  name: Apex
  source: llm_enhanced
- category: financial_entity
  confidence: high
  context: Mentioned as one of the founders/backers of Apex (likely Sequoia Capital,
    a VC firm heavily invested in AI).
  name: Sequoia
  source: llm_enhanced
- category: ai_leader
  confidence: medium
  context: Mentioned as a founder of Apex (likely a misspelling/mispronunciation of
    Sam Altman).
  name: Sam Almanzar
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as an accessible LLM application used by employees, posing an
    inside threat risk.
  name: Microsoft Copilot
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as an accessible LLM application and a system susceptible to
    context injection attacks.
  name: Google Gemini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used as an example of a non-deterministic LLM when asked a subjective question.
  name: ChatGPT
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in relation to addressing context injection threats applicable
    to Google Gemini in June.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A company where Karen Kautz previously led the MXCR product, seeing security
    in AI.
  name: Signea
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a potentially 'weak engine' that an agent might be based on,
    implying it is a foundational model or engine used in AI agents.
  name: DeepSeek engine
  source: llm_enhanced
date: 2025-10-15 09:00:00 +0000
duration: 49
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: question it
  text: we should question it.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: at least know these users and educate them that they should think twice
    before just going for the AI recommendations
  text: we should at least know these users and educate them that they should think
    twice before just going for the AI recommendations.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: involve it, and we should have AI in our main processes
  text: we should involve it, and we should have AI in our main processes.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: have specific guidelines about it
  text: we should have specific guidelines about it.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: be doing instead? So, you know, it's super interesting because this domain
    evolves so fast that my mindset shifts as well, because I was really into real-time
    monitoring, and I'm still thinking it's a very, very important block for recognizing
    and detecting real-time attacks
  text: we should be doing instead? So, you know, it's super interesting because this
    domain evolves so fast that my mindset shifts as well, because I was really into
    real-time monitoring, and I'm still thinking it's a very, very important block
    for recognizing and detecting real-time attacks.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/10dedd8351984c9aa0340ffe18773516/
processing_date: 2025-10-16 04:30:33 +0000
quotes:
- length: 207
  relevance_score: 4
  text: And what I mean by this is that when you have access to, let's say, Microsoft
    Copilot, Google Gemini, applications that are the bits and bytes of nowadays,
    for hydrogen companies, everything is so accessible
  topics: []
- length: 157
  relevance_score: 3
  text: And the problem is that since monitoring and detection are still young in
    this field, we do not know which of the files in our environment are attached
    by AI
  topics: []
- length: 136
  relevance_score: 3
  text: So it means that if you send an email to a user that is using Google Gemini,
    this email can be stored as context in the RAG system of it
  topics: []
- length: 63
  relevance_score: 3
  text: I was looking the first time I was looking at Microsoft Copilot
  topics: []
- length: 64
  relevance_score: 3
  text: There are hundreds of tools you can connect Microsoft Copilot to
  topics: []
- impact_reason: This is a core thesis statement redefining AI security risk away
    from traditional breach metrics to focus on unintended but successful execution
    by agents.
  relevance_score: 10
  source: llm_enhanced
  text: AI agents, as we think about them today, redefine risk, not as breaches, but
    as success in the wrong context.
  topic: strategy
- impact_reason: 'Identifies the dual threat: normalization of risky behavior coupled
    with immature monitoring capabilities in the AI security space.'
  relevance_score: 10
  source: llm_enhanced
  text: The big problem about it is that once it's a normal behavior and combining
    it with the fact that the AI security landscape is not yet to be monitored the
    exact way we would want it to be. So that combination is very, very risky and
    powerful.
  topic: safety/monitoring
- impact_reason: 'A critical warning for organizations deploying AI automation: the
    output is inherently unpredictable, creating potential for silent errors or policy
    violations.'
  relevance_score: 10
  source: llm_enhanced
  text: when we define a generative AI and we automate a specific use case in our
    company, we cannot know what the result of this automation will be.
  topic: safety/governance
- impact_reason: A stark prediction framing how GenAI lowers the barrier to entry
    for sophisticated data exfiltration, potentially bypassing complex traditional
    exploitation chains.
  relevance_score: 10
  source: llm_enhanced
  text: I call that the 2025 ransomware because if back in the days, you had to really
    exploit the servers and do like a privilege escalation and get into like compromise
    users to get into the really core data, now it's so easy. I will just ask Copilot.
  topic: predictions/ransomware evolution
- impact_reason: Provides a concrete, high-profile example of successful AI manipulation
    (bypassing guardrails) and emphasizes that sophisticated attacks are not always
    necessary.
  relevance_score: 10
  source: llm_enhanced
  text: So it's already happening. I think you surely heard about the user who bought
    a luxury car in one go, right? So it's already happening, and you shouldn't be
    that sophisticated to bypass the guardrails. Like from a few questions, a smart
    person will understand how to do that.
  topic: safety
- impact_reason: Identifies context injection as a critical, current threat specifically
    relevant to Retrieval-Augmented Generation (RAG) systems, a core architectural
    pattern in modern enterprise AI.
  relevance_score: 10
  source: llm_enhanced
  text: Context injection is a huge attack vector that was just being addressed by
    Google in June to actually told that they see this threat as applicable to Google
    Gemini. And of course, it's relevant to any RAG-based system.
  topic: technical
- impact_reason: Explains the insidious nature of context injection in RAG systems,
    where malicious context can persist undetected for long periods before being triggered.
  relevance_score: 10
  source: llm_enhanced
  text: So if I put prompt injection in this email, and I'm external to your organization,
    I can send it to any organization now. You will not know that. And once it will
    get into the AI engine when it's being collected for context, at the same time
    that you're asking the questions, it could be months later, the AI will do what
    I want it to do.
  topic: safety
- impact_reason: Identifies 'lack of visibility' into deployed AI assets as the primary
    governance hurdle for CISOs, setting the stage for necessary security controls.
  relevance_score: 10
  source: llm_enhanced
  text: The first foundation will be visibility. I'm talking to, I'd say tens and
    hundreds of CISOs, and what I hear from CISOs and executives is, I don't know
    what AI is running in my organization. I don't know.
  topic: strategy
- impact_reason: 'Provides clear, actionable governance advice: restrict AI from directly
    modifying critical systems; instead, use it for recommendations requiring human
    validation (Human-in-the-Loop).'
  relevance_score: 10
  source: llm_enhanced
  text: So, for example, we shouldn't let the AI just update, in my opinion, our most
    critical database, or we shouldn't let it automate our most critical workflow.
    We can, however, have use cases in which the AI recommends, and then we have human
    in the loop.
  topic: strategy
- impact_reason: Identifies the core problem of 'Shadow AI' proliferation driven by
    accessible tools like Copilot, where IT/Security lacks visibility.
  relevance_score: 10
  source: llm_enhanced
  text: we really don't know in a lot of cases where AI is being used. Everyone is
    a developer today. So maybe you have Copilot or you have some other systems that
    are available and people just have hands on.
  topic: safety
- impact_reason: 'A significant shift in security philosophy: real-time monitoring
    of agentic workflows is becoming infeasible, necessitating a pre-emptive/proactive
    strategy.'
  relevance_score: 10
  source: llm_enhanced
  text: I feel like in real-time it will be very, very hard now to manage it. Ideally,
    I wanted to be monitoring real-time, but I feel like you need to have a more pre-emptive
    approach into it because in real-time it will be super hard to manage it all.
  topic: technical
- impact_reason: 'Defines a practical, proactive risk mapping strategy for GenAI governance:
    focus on users, interactions, and critical workflows.'
  relevance_score: 10
  source: llm_enhanced
  text: I will be putting lots of attention into mapping risky interactions, mapping
    risky users, and mapping risky applications or critical workflows that are using
    Generative AI, and then addressing them up front instead of being, I'd say, proactive
    instead of just being reactive to it.
  topic: strategy
- impact_reason: 'A key conceptual leap: the evolution of UEBA (User Entity Behavior
    Analytics) to AEBA (Agent Entity Behavior Analytics) due to the scale of agent
    activity.'
  relevance_score: 10
  source: llm_enhanced
  text: the new user behavior analysis will be now the agent behavior analysis because
    we were talking about the vastness and volume of activities that happen.
  topic: technical
- impact_reason: 'A stark warning: major firms lack established incident response
    plans specifically for AI breaches (like jailbreaks), contrasting with mature
    plans in traditional security domains.'
  relevance_score: 10
  source: llm_enhanced
  text: companies have haven't figured out yet what they do in case of an AI breach.
    It's very interesting because there is a use case, it's so funny to me, but one
    of my clients, they just had a jailbreak attempt in their environment, like a
    real jailbreak, and I helped them looking at it and helped them sorting into it,
    and they asked me, okay, that's bad, what do I do now? So they haven't sorted
    a plan yet...
  topic: safety
- impact_reason: Provides a detailed, step-by-step forensic/response checklist for
    a specific AI incident (jailbreak), covering data lineage, exfiltration, and downstream
    actions.
  relevance_score: 10
  source: llm_enhanced
  text: if that agent got jailbroken by the user, which data it's connected to, I
    want to look if this data was exfiltrated. I want to look also in the network
    to see if this data got out of my organization. I want to know the ransomware
    is behind my door, right? And then think about which actions does this agent can
    do.
  topic: safety
- impact_reason: 'Offers a pragmatic solution to the speed of change: build flexible,
    template-based incident response plans that cover known attack vectors, allowing
    for context-specific adjustments later.'
  relevance_score: 10
  source: llm_enhanced
  text: You can't create, and you should create a template of, okay, so I know data
    exfiltration can happen. I know that output can be manipulated. What should I
    be doing in each of these cases? And then of course, there needs to be adjustments
    depending on the use case...
  topic: strategy
- impact_reason: 'Crucial insight into risk stratification: the severity of an AI
    incident is highly dependent on the context (use case, data sensitivity, action
    taken), requiring dynamic response scaling.'
  relevance_score: 10
  source: llm_enhanced
  text: And then of course, there needs to be adjustments depending on the use case
    because it's an entirely different use case if the user successfully exfiltrated
    the data to the network after they got it from the AI, or if the data that they
    got is super precious, or if they actually triggered a specific, you know, the
    most risky action ever in the organization, or just an internal email to something
    very not risky.
  topic: safety/strategy
- impact_reason: 'Highlights a primary vector of risk: legitimate users leveraging
    powerful GenAI tools to inadvertently expose sensitive data.'
  relevance_score: 9
  source: llm_enhanced
  text: The first one is the inside threat that's been amplified by Generative AI.
  topic: safety/insider threat
- impact_reason: A concrete, high-stakes example illustrating how easily sensitive,
    time-bound corporate data can be exfiltrated via simple queries to integrated
    AI tools.
  relevance_score: 9
  source: llm_enhanced
  text: Now I just need to ask Copilot, for example, what is our upcoming earnings
    call? What is our forecasted earnings? I'll get a response.
  topic: insider threat/business impact
- impact_reason: Points to a fundamental challenge in AI systems (non-determinism/stochastic
    nature) that complicates traditional security and quality assurance.
  relevance_score: 9
  source: llm_enhanced
  text: The other concept that I see that is different is the fact that the AI is
    not deterministic.
  topic: technical/limitations
- impact_reason: Highlights the current technical gap in tracking AI-mediated data
    changes, making auditing and forensics extremely difficult.
  relevance_score: 9
  source: llm_enhanced
  text: And the problem is that since monitoring and detection are still young in
    this field, we do not know which of the files in our environment are attached
    by AI.
  topic: monitoring/technical gap
- impact_reason: Illustrates a catastrophic scenario where AI-induced data corruption
    leads to flawed decision-making by legitimate users.
  relevance_score: 9
  source: llm_enhanced
  text: Imagine the accountant working on the new report of the company, and he's
    asking about financial data of the company, and the financial data being updated
    by the AI for the wrong values.
  topic: business impact/governance
- impact_reason: Posits that AI adoption acts as an immediate, high-stress test for
    existing security posture.
  relevance_score: 9
  source: llm_enhanced
  text: Is it true then that implementation of AI agents or generative AI, pick your
    term, might be the fastest way for organizations to find out just how good or
    bad their current security protocols are or are not working?
  topic: strategy/security testing
- impact_reason: Emphasizes that accidental or playful interaction, not just malicious
    intent, is a significant source of data leakage in the AI era.
  relevance_score: 9
  source: llm_enhanced
  text: a user can just get this data if it's just been playful with the AI. And that's
    what is carried amassed as I see it.
  topic: insider threat/behavior
- impact_reason: Suggests that AI tools, when used by an attacker or compromised insider,
    can effectively 'flatten' the privilege landscape by accessing data across roles
    easily.
  relevance_score: 9
  source: llm_enhanced
  text: I wouldn't need like the most privileged user in the network. I could just
    gain access to the lower user in the network and get some precious data.
  topic: security/privilege escalation bypass
- impact_reason: Highlights the surprising ease of exploiting security gaps in current
    AI systems, even for non-malicious or simple interactions.
  relevance_score: 9
  source: llm_enhanced
  text: You've said it is horrifyingly simple at times to get into places that you
    really, really shouldn't be able to.
  topic: safety
- impact_reason: Confirms prompt injection and jailbreaking as primary attack vectors
    and stresses that bypassing guardrails is often trivially easy, contrary to public
    perception.
  relevance_score: 9
  source: llm_enhanced
  text: So sure, it's an entirely new vector of how you manipulate the guardrails
    of the AI, and we got prompt injection and jailbreak and the, you know, this already
    the most common attacks that people are aware of, but it shouldn't be that dramatic
    because most of the cases, it's not that complicated to bypass the guardrails.
  topic: safety
- impact_reason: Connects successful data extraction from client-facing AIs to future
    large-scale security incidents (like ransomware), emphasizing low barrier to entry
    for severe damage.
  relevance_score: 9
  source: llm_enhanced
  text: Imagine if I'm successful at linking old information and extracting old sensitive
    data from it, ransomware of 2025, right? I don't need to be that smart. Sorry
    for saying the truth. I don't need to be that smart now.
  topic: safety
- impact_reason: Frames agentic protocols (like MCP) not as fundamentally new security
    paradigms, but as new I/O streams to which existing AI threats apply, simplifying
    the threat model.
  relevance_score: 9
  source: llm_enhanced
  text: MCP is just one type of agentic protocol that we can execute tools and call
    to another AI or to another service from our main agentic applications. So we
    talked a lot about it. And what we want to say about that is that it's just another
    input and output stream. So any AI threat is applicable to it.
  topic: technical
- impact_reason: Presents a stark statistical gap between perceived trust and actual
    implementation of safeguards, while linking trust/safeguards positively to ROI.
  relevance_score: 9
  source: llm_enhanced
  text: Did you know that while 78% of organizations say they fully trust AI, only
    40% actually support it with responsible safeguards? Or that leaders in trust
    for AI are 1.6 times more likely to report high ROI?
  topic: business
- impact_reason: Highlights the democratization of AI deployment via no-code tools
    and agentic frameworks, meaning security oversight cannot rely solely on traditional
    developer pipelines.
  relevance_score: 9
  source: llm_enhanced
  text: Any developer, any user now can just connect to an MCP server, work, and create
    an agentic workflow and connect it to any data of the company. And don't think
    about developers, any people from marketing, it shouldn't be a developer, can
    use the no-code platforms and just connect the organization's most sensitive data
    operations to AI now.
  topic: strategy
- impact_reason: 'Provides the core actionable advice for governance: map unique,
    sensitive operations to define acceptable AI use cases.'
  relevance_score: 9
  source: llm_enhanced
  text: My main recommendation will be to think of what are the use cases that are
    unique to your company. What are the most sensitive operations for your company
    that you want or don't want people to do?
  topic: strategy
- impact_reason: Highlights the critical need for context-specific AI governance and
    use-case identification, moving beyond generic security policies.
  relevance_score: 9
  source: llm_enhanced
  text: it's kind of a custom to each company because in a finance firm, the people
    wouldn't be acting the same way they will be acting in, let's say, a healthcare
    company. And of course, not the same they will do at a restaurant or dining company.
    So there are specific intentions and use cases that are unique to each company.
  topic: strategy
- impact_reason: Praises the integration capability of major tools but immediately
    flags the resulting security complexity and lack of oversight.
  relevance_score: 9
  source: llm_enhanced
  text: I was fascinated. I was looking the first time I was looking at Microsoft
    Copilot. And the team did such a great job because you can connect it to anything.
    There are hundreds of tools you can connect Microsoft Copilot to. But it is like
    a security mess.
  topic: safety
- impact_reason: 'Identifies the primary risks of agents: lack of context, excessive
    agency (permissions), and over-exposure to sensitive data.'
  relevance_score: 9
  source: llm_enhanced
  text: sometimes it wouldn't have the right context. It will have too excessive agency.
    It will have too much data, too much sensitive data that it can approach that
    it can touch.
  topic: safety
- impact_reason: 'Provides a concrete, proactive guardrail: limiting agent permissions
    (e.g., read-only) to mitigate write/modification risks.'
  relevance_score: 9
  source: llm_enhanced
  text: I limit this agent's tools and actions to be only read-only. I don't want
    this agent to update anything or to do any writing. So that will be the proactive
    one.
  topic: technical
- impact_reason: Reiterates the necessity of a business-first policy definition before
    diving into technical monitoring or visibility tools.
  relevance_score: 9
  source: llm_enhanced
  text: before even looking and having the visibility, really thinking from the enterprise
    perspective, what is important to my company? You know, where do I want to involve
    AI and understand or will be risked to apply to it, but it's worth it and I want
    it to be in there...
  topic: strategy
- impact_reason: Emphasizes the need for holistic security correlation—linking AI
    events with network, identity, and workflow data to understand the full impact.
  relevance_score: 9
  source: llm_enhanced
  text: I feel like you need to understand how the AI interactions and activities,
    I'd say, how they come in place into your entire organization workflows, into
    your network activities, into the user identity activities, and how it all comes
    together and what's the effect of it.
  topic: technical
- impact_reason: Advocates for translating generic security threats (like exfiltration)
    into specific, agent-contextualized risks during policy creation.
  relevance_score: 9
  source: llm_enhanced
  text: Understanding the risks first, and that's why I recommend to build your own
    policy, but mapping which are the risks and talking very, very specifically. So
    we know data exfiltration is scary, we know ransomware is scary, but let's talk
    about specific agents we know...
  topic: strategy
- impact_reason: 'Defines a core strategic approach for AI incident response: understanding
    the landscape, identifying incident types, and then mapping potential agents to
    those risks.'
  relevance_score: 9
  source: llm_enhanced
  text: And then there's some combination of having that general plan and understanding
    of what that landscape is and what the different types of incidents that could
    happen are, and then mapping agents to which incidents might occur.
  topic: strategy
- impact_reason: Emphasizes the velocity and dynamism of AI deployment, stressing
    that static security plans will quickly become obsolete.
  relevance_score: 9
  source: llm_enhanced
  text: you can have that plan, and then a day after, like 10 new agents are being
    created.
  topic: strategy/predictions
- impact_reason: Concludes that while the *planning structure* might resemble traditional
    security, the *nature* of AI incidents requires a deep, specific understanding
    of their unique interaction and impact mechanisms.
  relevance_score: 9
  source: llm_enhanced
  text: So it is exactly like in old times in this case, but I feel like it is key
    to understand how the AI incidents really interact or impact
  topic: strategy/safety
- impact_reason: Specific example of non-determinism impacting critical business functions
    (financial/legal data integrity).
  relevance_score: 8
  source: llm_enhanced
  text: if I'm having an automation to update a financial or legal file, I can't know
    what it will be eventually in that file, right?
  topic: business impact/governance
- impact_reason: A common, practical example of AI automation failure leading to direct
    business errors (CRM/sales integrity).
  relevance_score: 8
  source: llm_enhanced
  text: Think about AI that updates sales pipelines and automatically sends email,
    but to the wrong lead.
  topic: business impact/use case failure
- impact_reason: Points out that poor configuration (over-permissive defaults) is
    a major enabler for AI-driven data exposure, even without malicious intent.
  relevance_score: 8
  source: llm_enhanced
  text: most of the time what we see in reality is the unfortunate use case that people
    do not deal with it. They just create a presentation. They do not define and configure
    the exact people who can reach into it.
  topic: strategy/configuration failure
- impact_reason: Reinterprets the security concept of 'dwell time' for the AI context,
    focusing on the speed of information retrieval by curious users.
  relevance_score: 8
  source: llm_enhanced
  text: dwell time for me in the AI landscape will be the time when a user is curious
    and wants to get an answer to get that answer.
  topic: strategy/risk definition
- impact_reason: Urges security leaders to shift focus from catastrophic, external
    threats to the pervasive, everyday risks introduced by internal AI usage.
  relevance_score: 8
  source: llm_enhanced
  text: CISOs are trained to think about attackers, about ransomware, the big stuff.
    But in fact, it's like everyday security.
  topic: strategy/mindset shift
- impact_reason: Acknowledges the inherent user motivation (even if not strictly malicious)
    to 'game' the system, leading to prompt injection or policy abuse.
  relevance_score: 8
  source: llm_enhanced
  text: '...but there are plenty of, I wouldn''t say malicious, but users who want
    to take advantage of the fact that they''re talking to AI. We all want to get
    things free, we all want to leverage ourselves.'
  topic: safety/prompt injection motivation
- impact_reason: References a known real-world example of customer exploitation (likely
    prompt injection leading to unauthorized transactions), proving the threat is
    current.
  relevance_score: 8
  source: llm_enhanced
  text: I think you surely heard about the user who bought a luxury car in one go,
    right? So it's already happening, and you shouldn't be that sophisticated to byp
  topic: business impact/real-world exploit
- impact_reason: 'Clearly delineates two major threat categories for enterprise AI:
    insider threats and adversarial attacks like context injection.'
  relevance_score: 8
  source: llm_enhanced
  text: We talk about enterprise AI, which has a lot to do with the inside threat
    we just talked about. Of course, it has a lot to do with also adversarial, such
    as context injection.
  topic: safety
- impact_reason: Draws an analogy between traditional network protocol attacks (like
    spoofing) and agentic attacks, warning about corrupted or malicious tool responses.
  relevance_score: 8
  source: llm_enhanced
  text: Imagine there's the symbol attack of HTTP. So imagine if someone is saying
    it is the tool of, let's say, Salesforce, but they're not. And then I'm transferring
    and sharing data with this Salesforce tool. What if it's not Salesforce? What
    if I'm getting back data that is corrupted, it's manipulated, or containing just
    jailbreak?
  topic: safety
- impact_reason: Stresses the critical need for user education regarding over-reliance
    on AI, especially in high-stakes areas like hiring or investment decisions.
  relevance_score: 8
  source: llm_enhanced
  text: So we should at least know these users and educate them that they should think
    twice before just going for the AI recommendations.
  topic: safety
- impact_reason: Emphasizes that AI governance and intention detection must be highly
    contextual and customized to the specific industry and operational norms of the
    organization.
  relevance_score: 8
  source: llm_enhanced
  text: Also, what we understood is that it's kind of a custom to each company because
    in a finance firm, the people wouldn't be acting the same way they will be acting
    in, let's say, a healthcare company.
  topic: strategy
- impact_reason: Points out the 'shadow AI' problem where sensitive internal projects
    are discussed with public models, necessitating advanced tooling to surface these
    unknown use cases.
  relevance_score: 8
  source: llm_enhanced
  text: What people are asking about it and getting responses. So how can you know
    about that? So we really worked hard to build models that surface such use cases
    that people are talking about. But it's really hard to keep track unless you've
    got such visibility.
  topic: safety
- impact_reason: Points to the difficulty of tracking emergent, unmanaged AI usage
    and suggests using models for discovery, hinting at the need for AI governance
    tooling.
  relevance_score: 8
  source: llm_enhanced
  text: we really worked hard to build models that surface such use cases that people
    are talking about. But it's really hard to keep track unless you've got such visibility.
  topic: technical
- impact_reason: Directly advises security teams to prioritize monitoring unmanaged,
    user-adopted chat AI applications.
  relevance_score: 8
  source: llm_enhanced
  text: I'm talking about chat AI, so these are the applications that you usually
    wouldn't know that exist and live in your environment. So go monitoring those...
  topic: safety
- impact_reason: Highlights that security risks from AI agents can manifest even without
    explicit identification or tracking of individual agents, suggesting a need for
    broader, systemic security measures.
  relevance_score: 8
  source: llm_enhanced
  text: it can actually happen even outside of knowing explicitly this is agent one,
    two, and three.
  topic: safety/strategy
- impact_reason: 'Identifies a major organizational hurdle in AI security planning:
    the perceived dependency between knowing the agent identity and planning for the
    resulting incidents.'
  relevance_score: 8
  source: llm_enhanced
  text: I foresee organizations potentially thinking, yeah, but if I don't, I don't
    know what agents are, so I don't know what incidents to plan for.
  topic: business/strategy
- impact_reason: Reinforces the necessity of context-aware implementation of security
    protocols, regardless of whether the threat is traditional or AI-driven.
  relevance_score: 8
  source: llm_enhanced
  text: But then in the specific use case, you implement it depending on the way that
    the specific use case happens.
  topic: strategy
- impact_reason: A strong call for cross-functional ownership (developers, product
    managers, security teams) in securing AI systems.
  relevance_score: 7
  source: llm_enhanced
  text: I don't feel like it's only the responsibility of the security people, but
    it's our shared responsibility, both builders and protectors.
  topic: strategy/responsibility
- impact_reason: Shifts the focus to external threats leveraging AI capabilities,
    specifically concerning customer-facing chatbots.
  relevance_score: 7
  source: llm_enhanced
  text: does this also apply to viewing customers as a potential threat vector as
    well?
  topic: external threat/customer interaction
- impact_reason: Describes the standard defense mechanism (system prompts) for customer-facing
    LLMs and notes their effectiveness against accidental misuse.
  relevance_score: 7
  source: llm_enhanced
  text: most of the cases when you do have a client-facing AI, you will usually protect
    it with a system prompt and with specific instructions that the AI follows. So
    most of the time those are good for the non-intentional users...
  topic: technical/defense mechanisms
- impact_reason: Indicates that intention detection is a relatively new, emergent
    security requirement driven by real-world client observations, suggesting current
    tooling may lag.
  relevance_score: 7
  source: llm_enhanced
  text: I feel like it's been a year and a half since we started to talk about [intention
    detection] because that's what we've been seeing in client environments. And there
    wasn't any solution to it back in the day.
  topic: technical
- impact_reason: Explains the rapid, widespread adoption of AI driven by user demand
    and the competitive necessity to adopt.
  relevance_score: 7
  source: llm_enhanced
  text: Gen Z became way more early adopters in their nature. And I feel like it's
    not only Gen Z, but AI is here, and if you wouldn't be adopting it, you will stay
    behind. And people understand it.
  topic: business
- impact_reason: Shows that governance extends to the underlying model choice (engine
    reputation/selection) as a factor in risk assessment.
  relevance_score: 7
  source: llm_enhanced
  text: I don't want my agent personally to be based on DeepSeek. Okay, so it's like
    a combination of lots of events and lots of properties that we want to correlate
    into which agents are there. Can we turn off?
  topic: technical
- impact_reason: Draws a parallel between traditional security planning (based on
    fear of known threats) and AI security planning, suggesting the underlying strategic
    mindset is similar.
  relevance_score: 7
  source: llm_enhanced
  text: So of course, it is fear, but I feel like usual security and traditional security
    is fear as well because you've got this plan for what happens if someone does
    this and that.
  topic: strategy
- impact_reason: Provides a necessary definition of RBAC, setting the stage for explaining
    how AI bypasses or ignores traditional access controls.
  relevance_score: 6
  source: llm_enhanced
  text: RBAC is like role-based access. So it means usually when you define access
    to a presentation or to a file, usually the people or folks who were able to get
    access to it.
  topic: technical/security concept
source: Unknown Source
summary: '## Podcast Summary: Agentic Insecurities with Keren Katz


  This 49-minute episode of Pondering AI, featuring Keren Katz (Senior Group Manager
  of Threat Detection, Product Management, and AI at Tenable and OWASP contributor),
  dives deep into the evolving threat landscape introduced by modern AI agents and
  Large Language Models (LLMs), emphasizing how these technologies fundamentally redefine
  organizational risk.


  ---


  ### 1. Focus Area

  The primary focus is the **security and governance challenges posed by Generative
  AI and AI Agents** within enterprise environments. Key themes include:

  *   The shift from traditional breach-focused risk to risk defined as **"success
  in the wrong context."**

  *   The dual threat of **amplified insider risk** and **non-deterministic AI outputs**.

  *   Specific attack vectors like **context injection** and the risks associated
  with **multi-agent systems (MAS)** and protocols like **MCP (Multi-Agent Communication
  Protocol)**.

  *   The necessity for new governance models to manage the rapid, decentralized adoption
  of AI tools.


  ### 2. Key Technical Insights

  *   **Non-Deterministic Outputs:** LLMs are inherently inconsistent (non-deterministic),
  meaning automated workflows relying on them (e.g., updating financial or legal files)
  can produce unpredictable and potentially erroneous results, even without malicious
  intent.

  *   **Context Injection in RAG Systems:** A major enterprise threat where external,
  malicious data (like a prompt injection hidden in an email) can be stored as context
  in a Retrieval-Augmented Generation (RAG) system and later used by the AI to execute
  unintended actions.

  *   **Agentic Protocol Vulnerabilities:** Protocols like MCP, which enable agents
  to call other services or AIs, introduce new attack surfaces analogous to traditional
  network protocol attacks, where agents might be tricked into interacting with malicious
  or corrupted tools.


  ### 3. Business/Investment Angle

  *   **Amplified Insider Threat:** Accessible tools like Copilot significantly shorten
  the "dwell time" for legitimate users to access and expose sensitive data simply
  by asking questions, bypassing traditional access controls if monitoring is inadequate.

  *   **Trust vs. Safeguards Gap:** A significant disconnect exists where 78% of organizations
  trust AI, but only 40% support it with responsible safeguards, indicating a major
  governance deficit impacting ROI potential.

  *   **The "2025 Ransomware":** Attackers no longer need complex privilege escalation;
  they can leverage low-privilege user access combined with AI tools to extract core
  data, making data exfiltration easier than traditional ransomware entry.


  ### 4. Notable Companies/People

  *   **Keren Katz:** Expert perspective from 12 years at the intersection of AI and
  security, including experience at a founder level and in Special Operations.

  *   **Tenable:** Current employer, highlighting product management in threat detection.

  *   **OWASP:** Contributor, emphasizing the collaborative effort to define AI security
  standards (referenced the recent Gen AI Security Report).

  *   **Apex:** Company founded by Sam Altman and Sequoia, where Katz recognized the
  urgent security gap.

  *   **Google Gemini:** Mentioned specifically regarding the validation of context
  injection as a significant threat vector.


  ### 5. Future Implications

  The industry is moving toward a state where AI is deeply embedded in core operations,
  making robust governance non-negotiable. The future requires a shift from reactive
  security (waiting for breaches) to **proactive governance** focused on **intention
  detection** and **human-in-the-loop** oversight for the most critical workflows.
  Organizations that fail to implement these safeguards risk severe operational failures
  and data exposure, even from non-malicious user curiosity.


  ### 6. Target Audience

  This episode is highly valuable for **Cybersecurity Professionals (CISOs, Threat
  Hunters, Security Architects)**, **AI/ML Product Managers**, and **Technology Executives**
  responsible for risk management and digital transformation strategy. It provides
  a necessary framework for understanding and mitigating emerging AI-native risks.


  ---


  ### Comprehensive Narrative Summary


  Keren Katz frames the current AI security challenge as a paradigm shift: risk is
  no longer just about breaches, but about **unintended functionality**—AI succeeding
  in the wrong context. She highlights two core differences from traditional security:
  the **amplification of insider threats** and the **non-deterministic nature of LLMs**.


  Regarding insider threats, Katz notes that tools like Copilot make it trivially
  easy for employees to query vast amounts of sensitive internal data (like forecasted
  earnings) without triggering traditional alerts, as the activity appears "normal."
  This exposes data that RBAC (Role-Based Access Control) might not adequately cover
  due to poor configuration or the sheer ease of access.


  The non-determinism issue means that even when developers intend for an AI agent
  to perform a specific task (like updating a file), the output is variable and potentially
  incorrect, leading to operational errors (e.g., wrong financial data in reports,
  emails sent to the wrong leads).


  Katz details findings from the recent OWASP Gen AI Security Report, emphasizing
  threats across different use cases:

  1.  **Enterprise AI:** Dominated by insider risk and **context injection** attacks
  targeting RAG systems.

  2.  **Client-Facing Interfaces:** Vulnerable to prompt injection and jailbreaking,
  potentially leading to data extraction or manipulation of customer-facing policies
  (e.g., unauthorized refunds).

  3.  **Multi-Agent Systems (MAS):** Introduce new risks via agentic protocols like
  MCP, where trust in external tools called by the agent can be exploited.


  The solution, according to Katz, begins with **Visibility**: organizations must
  first inventory *what* AI is running and *what data* it touches. Following visibility,
  governance must focus on **intention detection**—understanding the specific, company'
tags:
- artificial-intelligence
- generative-ai
- startup
- investment
- microsoft
- google
title: Agentic Insecurities with Keren Katz
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 131
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 11
  prominence: 1.0
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 3
  prominence: 0.3
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 2
  prominence: 0.2
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-16 04:30:33 UTC -->
