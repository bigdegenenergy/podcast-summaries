---
companies:
- category: unknown
  confidence: medium
  context: pisode number 913. My guest in today's episode is Julian Lone. He is unbelievably
    knowledgeable about training
  name: Julian Lone
  position: 62
- category: unknown
  confidence: medium
  context: tting-edge AI models are made and how his startup Adaptive ML allows enterprises
    to have fine-tuned models for
  name: Adaptive ML
  position: 314
- category: unknown
  confidence: medium
  context: uch more easily than ever before. This episode of Super Data Science is
    made possible by Dell, Nvidia, and AWS. Julian
  name: Super Data Science
  position: 462
- category: tech
  confidence: high
  context: e of Super Data Science is made possible by Dell, Nvidia, and AWS. Julian,
    welcome to the Super Data Scien
  name: Nvidia
  position: 507
- category: unknown
  confidence: medium
  context: by Dell, Nvidia, and AWS. Julian, welcome to the Super Data Science Podcast.
    Thank you very much. I'll be here today. Yeah, i
  name: Super Data Science Podcast
  position: 547
- category: unknown
  confidence: medium
  context: today. Yeah, it's great to have you in person in New York. Totally. Yeah.
    And so it doesn't sound like you
  name: New York
  position: 658
- category: unknown
  confidence: medium
  context: t's a new generation, but it's making a comeback. And I used to write for
    a website called Minecraft.fr,
  name: And I
  position: 2217
- category: unknown
  confidence: medium
  context: aptive ML, who are makers of something called the Adaptive Engine, a flywheel
    for enterprise AI, which continuously
  name: Adaptive Engine
  position: 4121
- category: tech
  confidence: high
  context: ou to talk about based on your rich experience at Hugging Face, also with
    a company called Light On that we'll t
  name: Hugging Face
  position: 4440
- category: unknown
  confidence: medium
  context: ience at Hugging Face, also with a company called Light On that we'll talk
    a lot about more later in the epi
  name: Light On
  position: 4481
- category: unknown
  confidence: medium
  context: eal life and at the biggest scale that LLMs come. So I'd love for you to
    start off by providing us with
  name: So I
  position: 4686
- category: unknown
  confidence: medium
  context: into its larger focus on reinforcement learning. But I would say, you know,
    obviously, props to the Grok
  name: But I
  position: 10212
- category: unknown
  confidence: medium
  context: lems, you know, that the model is going to solve. When I was mentioning
    the feedback before, you know, the
  name: When I
  position: 10946
- category: tech
  confidence: high
  context: le, you know, recently semi-acquired, I guess, by Meta, company like Search
    as well, which has been the
  name: Meta
  position: 12723
- category: unknown
  confidence: medium
  context: de of Super Data Science is brought to you by the Dell AI Factory with
    Nvidia, two trusted technology leaders unite
  name: Dell AI Factory
  position: 19508
- category: unknown
  confidence: medium
  context: I workloads from desktop to data center to cloud. The Dell AI Factory with
    Nvidia paves the way for AI to work seamless
  name: The Dell AI Factory
  position: 19773
- category: unknown
  confidence: medium
  context: paves the way for AI to work seamlessly for you. Integrated Dell and Nvidia
    capabilities accelerate your AI-powere
  name: Integrated Dell
  position: 19854
- category: unknown
  confidence: medium
  context: tari top score that we're trying to reach for, or RL AIF, which we talked
    about most recently where you're
  name: RL AIF
  position: 20783
- category: unknown
  confidence: medium
  context: el to do this task, which is an interesting view. In GRPO, it's a bit different.
    You essentially do a new V
  name: In GRPO
  position: 31163
- category: unknown
  confidence: medium
  context: hink, you know, if you think about it from like a Pareto Frontier point
    of view, reinforcement learning will always
  name: Pareto Frontier
  position: 35861
- category: unknown
  confidence: medium
  context: mentally what we do. Hey, hey, this is your host, John Crone. I'm excited
    to announce that I've launched my ow
  name: John Crone
  position: 37544
- category: unknown
  confidence: medium
  context: '''ve launched my own AI consultancy, a firm called Y Carrot. Yes, the
    letter Y and the deliciously crunchy ve'
  name: Y Carrot
  position: 37636
- category: unknown
  confidence: medium
  context: ise in all the cutting-edge approaches, including Gen AI, multi-agent systems,
    and RL from problem scoping
  name: Gen AI
  position: 37878
- category: unknown
  confidence: medium
  context: to tell us exactly how we can help. Again, that's Y Carrot Y C A double
    R O T dot com. So I guess the target, corr
  name: Y Carrot Y C A
  position: 38149
- category: unknown
  confidence: medium
  context: we can help. Again, that's Y Carrot Y C A double R O T dot com. So I guess
    the target, correct me if I'm
  name: R O T
  position: 38171
- category: unknown
  confidence: medium
  context: l like their work is being, it's being televised. Like I think they don't,
    you know, they don't like it at
  name: Like I
  position: 45607
- category: tech
  confidence: high
  context: e being created. Regular listeners know Claude by Anthropic has been my
    go-to AI for years. Claude is the AI
  name: Anthropic
  position: 54612
- category: unknown
  confidence: medium
  context: roblems? Sign up for Claude today and get 50% off Claude Pro when you use
    my link. Claude.ai/superdata. That's
  name: Claude Pro
  position: 55594
- category: unknown
  confidence: medium
  context: with another model. That's a reward that's good. Then I move to the next
    step and kind of building the th
  name: Then I
  position: 57930
- category: unknown
  confidence: medium
  context: And so different people have different opinions. Ilya Sutskever said that
    if Gen AI's fossil fuel is human data o
  name: Ilya Sutskever
  position: 59569
- category: unknown
  confidence: medium
  context: exhausted our supply. However, other people like Sam Altman, Dario Amodei,
    from OpenAI, Anthropic, and Micros
  name: Sam Altman
  position: 59709
- category: unknown
  confidence: medium
  context: ur supply. However, other people like Sam Altman, Dario Amodei, from OpenAI,
    Anthropic, and Microsoft respective
  name: Dario Amodei
  position: 59721
- category: tech
  confidence: high
  context: other people like Sam Altman, Dario Amodei, from OpenAI, Anthropic, and
    Microsoft respectively, they don'
  name: Openai
  position: 59740
- category: tech
  confidence: high
  context: Altman, Dario Amodei, from OpenAI, Anthropic, and Microsoft respectively,
    they don't seem to think it's a pro
  name: Microsoft
  position: 59763
- category: unknown
  confidence: medium
  context: ng enables model and I'm going to use the word of Richard Sutton because
    he put it in a very elegant way, it enabl
  name: Richard Sutton
  position: 61367
- category: unknown
  confidence: medium
  context: data centers are getting more and more ambitious. So Mark Zuckerberg recently
    announced several multi-gigawatt cluster
  name: So Mark Zuckerberg
  position: 63311
- category: unknown
  confidence: medium
  context: nerally intelligent or super intelligent systems? Because I think general
    intelligence system probably alread
  name: Because I
  position: 68409
- category: unknown
  confidence: medium
  context: where you have, yeah, on relatively small scales. An AI system that can
    control a well. Exactly. It'
  name: An AI
  position: 70556
- category: ai_application
  confidence: high
  context: Julian Lone's startup, which creates the Adaptive Engine, a flywheel for
    enterprise AI that continuously evaluates, tunes, and serves fine-tuned LLMs for
    specific business use cases.
  name: Adaptive ML
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a sponsor of the Super Data Science Podcast.
  name: Dell
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a sponsor of the Super Data Science Podcast. A key player
    in AI infrastructure (GPUs).
  name: Nvidia
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a sponsor of the Super Data Science Podcast. Amazon Web Services,
    a major cloud provider for AI/ML.
  name: AWS
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Julian Lone has rich experience working there, a major hub for open-source
    AI models and tools.
  name: Hugging Face
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A company Julian Lone worked at, relevant to his experience in creating
    large-scale LLMs.
  name: Light On
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned in relation to its R1 model and the trend of using verifiable
    rewards (RLEF) for training.
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company (recently semi-acquired by Meta) that provides human
    annotators for RLHF.
  name: Scale
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned in relation to the semi-acquisition of Scale, indicating their
    involvement in data annotation/RLHF efforts.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned alongside Scale as a company providing annotators for RLHF.
  name: Search
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced historically as an example of reinforcement learning using execution
    feedback (RLEF).
  name: AlphaGo
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a model that spent significantly on post-training, particularly
    RL, achieving high scores on exams.
  name: Grok
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as an example of a recent paper/model showing massive scaling
    up of post-training spend relative to pre-training.
  name: Kimi
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a model used internally for experiments to replace human experts
    in reviewing specialized tasks.
  name: GPT-4
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The company the speaker works for, which focuses on the 'Adaptive Engine'
    for continuously evaluating, tuning, and serving LLMs for enterprises.
  name: Adaptive
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned in the context of complex RL pipelines, likely referring to Hugging
    Face, a major platform for ML models and tools.
  name: HF
  source: llm_enhanced
- category: ai_consultancy
  confidence: high
  context: The host's new AI consultancy firm specializing in machine learning, software
    development, Gen AI, multi-agent systems, and RL.
  name: Y Carrot
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The speaker worked there during their PhD. The company developed a chip
    that used photons (light) instead of electrons for computation, offering advantages
    in power consumption and parallelism. They now mostly focus on Gen AI.
  name: LightOn
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as part of the complex technology chain (alongside ASML and others)
    required to build the latest generation of chips like those from Nvidia.
  name: TSMC
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as part of the complex technology chain required to build the
    latest generation of chips, specifically referencing extreme UV lithography.
  name: ASML
  source: llm_enhanced
date: 2025-08-12 11:00:00 +0000
duration: 75
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD2214012390.mp3?updated=1754677568
processing_date: 2025-10-04 16:20:11 +0000
quotes:
- length: 139
  relevance_score: 5
  text: Through that experience, you have tons of experience in creating LLMs that
    are useful for real life and at the biggest scale that LLMs come
  topics: []
- length: 101
  relevance_score: 4
  text: He is unbelievably knowledgeable about training LLMs to the pre-training part,
    the post-training part
  topics: []
- length: 222
  relevance_score: 4
  text: You are co-founder now and CEO of a firm called Adaptive ML, who are makers
    of something called the Adaptive Engine, a flywheel for enterprise AI, which continuously
    evaluates, tunes, and serves large language models, LLMs
  topics: []
- length: 152
  relevance_score: 4
  text: So I'd love for you to start off by providing us with an overview of the steps
    involved in creating an LLM, like pre-training and reinforcement learning
  topics: []
- length: 210
  relevance_score: 4
  text: But very broadly speaking, the way that historically large language models
    have been kind of approached, first is through a pre-training phase, which is
    the bulk historically of where the compute has been spent
  topics: []
- length: 214
  relevance_score: 4
  text: So essentially enabling models to learn not from an explicit demonstration
    of what they should be doing, which is what supervised fine-tuning and pre-training
    are, but instead from feedback about how are they doing
  topics: []
- length: 149
  relevance_score: 4
  text: Yeah, from a very, very high-level point of view, this is an example of the
    sort of data you could be leveraging to power this phase of post-training
  topics: []
- length: 268
  relevance_score: 4
  text: And so right now, this sort of AI feedback, which some people also bundle
    up into the idea of synthetic data, you know, of training based on data that is
    produced by other models, also is seeing a very big, very big growth and very
    big success because it works so well
  topics:
  - growth
- length: 203
  relevance_score: 4
  text: This is essentially what pre-training or supervised fine-tuning do, where
    you are presenting to the model, maybe once, maybe twice, maybe thrice, the same
    samples, and the model eventually learns from it
  topics: []
- length: 96
  relevance_score: 4
  text: Literally, you are training a large language model to do this task, which
    is an interesting view
  topics: []
- length: 78
  relevance_score: 4
  text: Because when you, firstly, you are going to be blending inference and training
  topics: []
- length: 233
  relevance_score: 4
  text: That was actually for the purpose of what you just said, which was fine-tuning
    an eight billion parameter model, one that can fit on a single relatively inexpensive
    GPU, and get frontier model performance on just a small set of tasks
  topics: []
- length: 148
  relevance_score: 4
  text: You know, they're already extensively specialized to machine learning and
    even people, some people say, it's going to be specialized to transformers
  topics: []
- length: 119
  relevance_score: 3
  text: You do this while you have to run the queries on the SQL database to get the
    answer to be able to do execution feedback
  topics: []
- length: 133
  relevance_score: 3
  text: Like the reality is that is an exceptionally powerful method that should be
    in the end, you know, of every data scientist of everyone
  topics: []
- length: 177
  relevance_score: 3
  text: However, other people like Sam Altman, Dario Amodei, from OpenAI, Anthropic,
    and Microsoft respectively, they don't seem to think it's a problem that scaling
    has no end in sight
  topics: []
- length: 198
  relevance_score: 3
  text: Definitely, I think this is very obvious, given the money that people are
    spending, you know, towards trying to get more, given the insane valuation of
    Nvidia, which is going to continue to increase
  topics:
  - valuation
- length: 122
  relevance_score: 3
  text: For a competitor to come, for, you know, an alternative mean of competing
    to come, well, you have to reproduce all of that
  topics: []
- length: 64
  relevance_score: 3
  text: Like I think the reality is that this is really hard to get that
  topics: []
- impact_reason: 'Crucial insight into the limitation of raw pre-trained models: they
    lack alignment for interactive use cases like chat.'
  relevance_score: 10
  source: llm_enhanced
  text: immediately after pre-training, models are actually a bit unwieldy. If you
    take really pure, pure pre-training and you try a model immediately after, it's
    not going to be very interactive with you. It's not going to be chatty, it's not
    going to answer your questions necessarily in the way that you expect.
  topic: technical/limitations
- impact_reason: Defines the purpose of post-training as the alignment phase necessary
    to make models useful for specific applications.
  relevance_score: 10
  source: llm_enhanced
  text: This led to the development of a second phase in model training, which is
    called post-training. And the idea of post-training is to kind of hone in and
    sharpen the model to really fit how it's going to be used, which typically means
    making it a good chat assistant or something like that.
  topic: technical
- impact_reason: Clearly contrasts RL (learning from feedback) with supervised learning
    (learning from explicit demonstration) in the context of post-training.
  relevance_score: 10
  source: llm_enhanced
  text: the big success of post-training has been the use of reinforcement learning.
    So essentially enabling models to learn not from an explicit demonstration of
    what they should be doing, which is what supervised fine-tuning and pre-training
    are, but instead from feedback about how are they doing?
  topic: technical
- impact_reason: 'A major insight into current state-of-the-art model development:
    the computational spend on post-training is rivaling pre-training, signaling a
    paradigm shift.'
  relevance_score: 10
  source: llm_enhanced
  text: But now if you look at recent papers, you know, like Kimi or even Grok, not
    really a paper, but more something that they mentioned, which is that they spent
    nearly as much on post-training as pre-training, so massive scaling up of this
    post-training phase.
  topic: technical/trends
- impact_reason: A strong prediction about the future direction of LLM development,
    emphasizing alignment and fine-tuning over raw foundational training.
  relevance_score: 10
  source: llm_enhanced
  text: I think most of the modern models are going to be going through much more
    extensive post-training than pre-training.
  topic: predictions
- impact_reason: Introduces and explains the concept of Verifiable Rewards (RLEF),
    a scalable alternative to human feedback for objective tasks like math and code.
  relevance_score: 10
  source: llm_enhanced
  text: A big trend that people are probably aware of, you know, with DeepSeek, with
    DeepSeek R1, and was verifiable rewards, you know, where essentially the model,
    you know, solves the mathematics problem or submits an answer to a mathematics
    problem, and then that answer gets evaluated. And if it's right, that's a positive
    signal. If it's wrong, that's a negative signal.
  topic: technical/trends
- impact_reason: Identifies the limitation of RLEF (non-verifiable tasks like report
    writing) and pivots to RLIF (Reinforcement Learning from AI Feedback) as the scalable
    solution for complex NLP tasks.
  relevance_score: 10
  source: llm_enhanced
  text: But there are many tasks, you know, that are not like this. Maybe like it
    could be writing a report or it could be like pretty much any natural, a lot of
    natural language tasks. And this is where I think in terms of scalability and
    access that has been really, really successful as well as RLIF, where you use
    AI feedback. So feedback from another model.
  topic: technical/predictions
- impact_reason: Links RLIF directly to the growing trend of synthetic data generation
    and notes its current high success rate in improving models.
  relevance_score: 10
  source: llm_enhanced
  text: And so right now, this sort of AI feedback, which some people also bundle
    up into the idea of synthetic data, you know, of training based on data that is
    produced by other models, also is seeing a very big, very big growth and very
    big success because it works so well.
  topic: AI technology trends
- impact_reason: Provides a concrete, high-stakes anecdote demonstrating that a powerful
    LLM (GPT-4) can match human expert review quality on specialized tasks, validating
    RLIF/synthetic data.
  relevance_score: 10
  source: llm_enhanced
  text: And so we thought, well, what if we could use at the time GPT-4 instead of
    the humans? And so we needed the humans to do enough that we had a kind of a sample
    that we could compare and GPT-4, you know, they were comparable. It was the same
    quality of results, indistinguishable.
  topic: practical lessons
- impact_reason: 'Offers a fundamental theoretical underpinning for why AI feedback
    works well: evaluation/verification is an easier task for an AI than novel generation.'
  relevance_score: 10
  source: llm_enhanced
  text: There is also a lack, there is another side to this kind which is that verification
    is much easier than generation. So it's much easier, you know, for a model apostatory
    to come and to check a result than it is to produce it.
  topic: technical
- impact_reason: Introduces the critical technical distinction between Online RL (learning
    immediately from the freshest sample) and Offline RL (learning from a fixed, accumulated
    batch of samples).
  relevance_score: 10
  source: llm_enhanced
  text: I think a big first thing is that reinforcement learning typically, so, will
    be online. There is a difference in literature between online or offline RL...
  topic: technical
- impact_reason: Highlights the crucial concept of 'onlineness' in RLâ€”learning from
    self-generated samples and gradually shifting the data distribution, which is
    key to its stability and adaptability.
  relevance_score: 10
  source: llm_enhanced
  text: in reinforcement learning, the samples come from the model itself, so it's
    always in distribution for the model, come within what it's capable to do. And
    then slowly, you are shaping that distribution away towards what you want it to
    be.
  topic: technical
- impact_reason: 'Pinpoints the core technical difference between modern RL algorithms:
    the method of credit assignment (attributing sparse rewards back to specific actions/tokens).'
  relevance_score: 10
  source: llm_enhanced
  text: What, what they do differently is more in the question of how then do you
    attribute, you know, you have a reward. So I tell you, you know, I tell you, you
    pass the exercise or you fail the exercise. And there is a question of how do
    you attribute these to individual steps in the exercise or to like individual
    parts of the messages.
  topic: technical
- impact_reason: 'A very strong business and strategic claim: RL is positioned as
    the definitive method for optimizing the cost/performance trade-off for specialized
    enterprise AI tasks.'
  relevance_score: 10
  source: llm_enhanced
  text: reinforcement learning is the way to get the best performance out of a given
    model for a specific task. We think, you know, if you think about it from like
    a Pareto Frontier point of view, reinforcement learning will always get you the
    best cost to performance compromise, always. It's like a new Pareto Frontier.
  topic: business/strategy
- impact_reason: Contrasts the algorithmic simplicity and scalability of pre-training
    (pure prediction) with the interaction complexity of RL, explaining why RL deployment
    is harder.
  relevance_score: 10
  source: llm_enhanced
  text: One of the reasons pre-training scales so fast, because pre-training is very
    simple. It's very straightforward. You have this like huge batch of tokens, you
    just predict, you know, just the logits... Whereas with reinforcement learning,
    now you have all of these environments, all of these other models that you interact
    with.
  topic: strategy/technical
- impact_reason: Strong endorsement of synthetic data's power, suggesting it is a
    primary driver for accelerating AI development beyond initial expectations.
  relevance_score: 10
  source: llm_enhanced
  text: But what has really positively surprised us is how well it works, like how
    much you can, how much leverage of synthetic data gives you to, you know, go from
    almost nothing to to a lot.
  topic: technical
- impact_reason: 'A major claim: small, accessible models (8B parameters) can achieve
    frontier performance using synthetic data, democratizing high-end capability.'
  relevance_score: 10
  source: llm_enhanced
  text: you can get started, you can get bootstrapped from much less and you can take,
    you know, very small model, eight billion parameter model to be to the frontier
    performance on the task, mostly entirely with synthetic data, which is quite incredible
    and very easy to do.
  topic: predictions
- impact_reason: Quantifies the massive leverage factor (100 human samples -> 10 million
    data points) achievable through synthetic data generation and RL exploration,
    demonstrating extreme data efficiency.
  relevance_score: 10
  source: llm_enhanced
  text: And from this 70, we are able to generate something like 80,000 synthetic
    conversations through self-playing, about 80,000 synthetic conversations. It's
    of this conversation is made of about 10 turns. So you are looking at, you know,
    nearly a million messages. And during the reinforcement learning process itself,
    we explore, you know, multiple possibilities. So for each of these messages, you
    know, we might explore five, ten, you know, possible answers, which gets rewarded
    by judges. So at the end, you are looking from, you know, less than 100 human
    samples, it's something like nearly 10 million data points from which you can
    learn.
  topic: technical
- impact_reason: 'Crucial insight into the paradigm shift required when moving from
    supervised fine-tuning to RL: the focus shifts from demonstration data to defining
    and measuring success (the reward function).'
  relevance_score: 10
  source: llm_enhanced
  text: having reinforcement learning ask you to think differently. You know, you
    don't think so much about the data that's going to be the explicit demonstration,
    but you think more about measurement of success. So you think more about what
    defines success. And how do I measure it?
  topic: technical/strategy
- impact_reason: Identifies the reward function definition as the single hardest,
    most critical step in deploying RL systems, linking it directly to production
    success.
  relevance_score: 10
  source: llm_enhanced
  text: You're getting that definition of the reward function. That sounds like it's
    one of the hardest parts. You're getting people's mindsets on the reinforcement
    learning cycle. If you don't define that reward function right, your model isn't
    going to end up doing in production what you hoped it would.
  topic: safety/strategy
- impact_reason: A powerful, actionable maxim for the ML community, emphasizing the
    direct link between observability/measurement and optimization capability in RL.
  relevance_score: 10
  source: llm_enhanced
  text: There's this quote that I really like for reinforcement to describe reinforcement
    learning, which is that if you can measure it, you can optimize it. This is literally
    true actually.
  topic: technical
- impact_reason: Shifts the focus from data scarcity in pre-training to the power
    of post-training (RL/fine-tuning) as the next frontier for capability gains, emphasizing
    learning from interaction/experience.
  relevance_score: 10
  source: llm_enhanced
  text: But I think I can answer the question of what's next and what's already actually
    the case, which is we go back to post-training where what's very interesting about
    post-training is that post-training enables model to learn from experience. So
    the model actually does something.
  topic: technical/predictions
- impact_reason: Highlights the critical shift from pre-training on static data to
    post-training (RL/experience-based learning) as the next major frontier for model
    improvement.
  relevance_score: 10
  source: llm_enhanced
  text: we go back to post-training where what's very interesting about post-training
    is that post-training enables model... it enables model to learn from experience.
  topic: technical/trends
- impact_reason: 'Defines the ultimate scalability advantage of reinforcement learning:
    generating unlimited, real-world experience data, solving the data scarcity issue.'
  relevance_score: 10
  source: llm_enhanced
  text: this is infinitely scalable because this is essentially the experience of
    the model in the real world.
  topic: technical/trends
- impact_reason: A strong prediction that RL/post-training will consume the majority
    of future compute/research resources, shifting focus away from just scaling pre-training
    data.
  relevance_score: 10
  source: llm_enhanced
  text: the bulk of the resources are going to be spent in the future on post-training,
    on reinforcement learning because the models are going to learn from trying again
    and again across, you know, billions of virtual environments and eventually, although
    in the real world, against trying, you know, their experiments, their own ideas.
  topic: predictions/trends
- impact_reason: 'Clearly identifies the current major limitation: the inability of
    AI models to autonomously and scalably conduct physical, real-world experiments
    (e.g., in science or engineering).'
  relevance_score: 10
  source: llm_enhanced
  text: Models currently cannot do this [conduct real-world experiments]. There is
    no way, you know, currently for a model to run biological tool to get in the scalable
    way.
  topic: limitations/technical
- impact_reason: Provides a foundational definition of the LLM creation pipeline,
    emphasizing pre-training as the historically compute-intensive first step.
  relevance_score: 9
  source: llm_enhanced
  text: very broadly speaking, the way that historically large language models have
    been kind of approached, first is through a pre-training phase, which is the bulk
    historically of where the compute has been spent.
  topic: technical
- impact_reason: A concise explanation of the core objective function of the pre-training
    phase (next-token prediction).
  relevance_score: 9
  source: llm_enhanced
  text: essentially the model is trained to very costly predict the next word, predict
    the next token.
  topic: technical
- impact_reason: 'Highlights a key trend: the blurring of lines between pre-training
    and post-training, suggesting dynamic data curation even during pre-training.'
  relevance_score: 9
  source: llm_enhanced
  text: I think what's really interesting is, you know, right now I'm giving a description
    where pre-training and post-training are very separate things. In reality, it's
    much less so these days. First, because now pre-training is very dynamic, while
    you shift the data distribution.
  topic: technical/trends
- impact_reason: 'Explains *why* post-training is scaling: the data (feedback/problems)
    can be generated or synthesized more easily than scraping the entire internet.'
  relevance_score: 9
  source: llm_enhanced
  text: The part of the reason why is also because post-training data is, if you think
    about it, I don't want to say more plentiful because it's a very complicated subject.
    But there is, you can generate new data and new problems, you know, that the model
    is going to solve.
  topic: technical/strategy
- impact_reason: Provides a concrete, actionable example of how verifiable rewards
    can be generated at massive scale using existing public code repositories and
    their associated tests.
  relevance_score: 9
  source: llm_enhanced
  text: It's very easy to imagine, for instance, mining all of the GitHub repositories
    that are available, pulling all of the tests from them, having the model, you
    know, write, you know, code that needs to pass this test and using that as a signal
    at a very large scale.
  topic: technical/strategy
- impact_reason: 'A critical business/technical constraint: human annotation is inherently
    limited and expensive, driving the need for automated feedback loops.'
  relevance_score: 9
  source: llm_enhanced
  text: Obviously, human data is only so much scalable, you know, like at some point
    having armies of people annotating data is not, you know, an infinite source or
    something that that really is desirable on getting the model to be more competent.
  topic: business/limitations
- impact_reason: Introduces the alternative acronym RLEF (Reinforcement Learning from
    Execution Feedback), providing technical nomenclature for the verifiable reward
    concept.
  relevance_score: 9
  source: llm_enhanced
  text: So one of them was what we just mentioned, verifiable reward. So some people
    call this RLHF or you see all the lots in the literature, RLEF, so from execution
    feedback, because you are executing what the model is producing, testing the result
    in an environment, looking at that result and being like
  topic: technical
- impact_reason: Challenges the assumption that human-labeled data is inherently perfect,
    pointing out significant noise and low inter-rater agreement, which makes synthetic
    data a viable competitor.
  relevance_score: 9
  source: llm_enhanced
  text: But in general, it actually works really well. And I think, you know, people
    have this idea of human data as being very perfect, but actually if you look at
    the data that comes out of the typical annotation contract... there is a massive
    amount of noise.
  topic: business/strategy
- impact_reason: Quantifies the success of RLIF by stating that model-generated feedback
    agreement rates often align with human-to-human agreement rates, suggesting parity
    in quality.
  relevance_score: 9
  source: llm_enhanced
  text: And when you measure actually the same sort of agreement rate with models
    or between models and humans, you see actually numbers that line up where essentially
    the quality that comes out of the model is as good as what comes out of the annotators.
  topic: technical
- impact_reason: Provides a clear, step-by-step definition of Online RL in the context
    of LLM training loops.
  relevance_score: 9
  source: llm_enhanced
  text: Well, essentially, online means that you are learning based on the sample
    you just produced. So let's say I have a set of weights of my model, I make, I
    run inference, you know, I get an answer to the question, I evaluate that answer...
    and then I use that in the training process to say, okay, so now I update my weights,
    based, you know, on that's, thumbs up, thumbs down. But I do it with fresh data,
    with data that has just, just come off the press.
  topic: technical
- impact_reason: Provides a clear, accessible analogy comparing supervised learning/SFT
    to rote memorization, contrasting it with the iterative nature of RL.
  relevance_score: 9
  source: llm_enhanced
  text: If we make a parallel with human learning, and let's say I'm teaching you
    a course about general relativity... I could show you the solution of the exercise,
    have you memorize it, you know, just memorize it again and again and again. This
    is essentially what pre-training or supervised fine-tuning do...
  topic: technical
- impact_reason: Clearly defines the core mechanism of Reinforcement Learning (RL)
    as iterative feedback and correction, contrasting it sharply with SFT.
  relevance_score: 9
  source: llm_enhanced
  text: If we're doing reinforcement learning, the way that it will work is that you
    will try to do the exercise, and then as a teacher, I would correct it, and I
    will tell you, okay, this is good, this is not good, you know, kind of that iterative
    process.
  topic: technical
- impact_reason: 'Explains the risk of SFT: introducing a potentially large, abrupt
    shift in the data distribution, leading to potential knowledge gaps if the new
    data isn''t perfectly comprehensive.'
  relevance_score: 9
  source: llm_enhanced
  text: Well, in supervised fine-tuning, you are kind of plopping down the new distribution,
    which might be very out of distribution, and you are hoping that, you know, as
    you show more and more samples that are diverse in us, you are going to, you know,
    widen the distribution and hopefully, you know, connect it back to the original
    knowledge that there is no gap in between.
  topic: technical
- impact_reason: A strong assertion that RL inherently leads to better generalization
    due to its trial-and-error nature, a key strategic advantage over static fine-tuning
    methods.
  relevance_score: 9
  source: llm_enhanced
  text: the fundamental idea of much more generalization from post-from, from, from
    reinforcement learning because of this online as because of this trial and error
    and all of that, I think is, is very fundamental...
  topic: strategy
- impact_reason: 'Offers a unique perspective on PPO: it requires training a secondary
    LLM (the advantage/value model) specifically to solve the credit assignment problem,
    adding complexity.'
  relevance_score: 9
  source: llm_enhanced
  text: So in PPO, you are training a model to do this. Literally, you are training
    a large language model to do this task, which is an interesting view [training
    a value model to calculate advantage].
  topic: technical
- impact_reason: 'Identifies a major engineering shift in RL for LLMs: the blurring
    of the line between serving (inference) and training, demanding integrated infrastructure.'
  relevance_score: 9
  source: llm_enhanced
  text: when you are going to be blending inference and training. Because as we mentioned
    before, we are teaching the model based on something that is just produced. So
    we are going to have some time to do rollout, so to do predictions, and then rate
    this prediction, and then use data training. So there is not as much as before,
    you know, this dichotomy between, oh, I serve my LLM to millions of users, and
    I train my LLM on this cluster. Now there is a bit more combined, you know, of
    the two, which poses engineering challenges, obviously.
  topic: technical/engineering
- impact_reason: Details the complexity of modern RL pipelines, which often involve
    multi-agent systems (AI judges, simulators) interacting during the training loop.
  relevance_score: 9
  source: llm_enhanced
  text: The other aspect of this is that these are complex pipelines. So typically,
    you know, we mentioned HF, IF, VF or EF depending on how you want to call it.
    So this means that during training, the model is going to have to interact, maybe
    not with humans... but it will have to interact with other models, maybe two,
    three, five, ten of them...
  topic: technical/engineering
- impact_reason: Clearly defines the core value proposition of optimization (cost/efficiency)
    or capability enhancement (performance) that drives enterprise adoption of advanced
    techniques like RL.
  relevance_score: 9
  source: llm_enhanced
  text: Pareto Frontier. So obviously, this is very attractive for enterprise adopting
    AI, because either they want a cheaper model, you know, same level of performance,
    but they want something that runs as efficiently as possible, or maybe they want
    something that's not possible now, and so they want more performance, reinforcement
    learning in both cases, easy answer to get there.
  topic: business
- impact_reason: Provides a comprehensive definition of synthetic data in the RL context,
    encompassing generation, self-play simulation, and AI-driven feedback loops (critics/judges).
  relevance_score: 9
  source: llm_enhanced
  text: And when I say synthetic data, by the way, it's a very broad word, which means
    many things and it means to me, it means like first problem generation sometimes,
    it means like creating new samples, creating new scenarios, you know, maybe new
    scenarios of conversation, self-play, you know, so maybe simulating a user, you
    know, like having a model stand kind of standing as the user to kind of drive
    a conversation for self-play where you have like that, that's, you know, first
    category. But it also means all of the AIF components of giving feedback, you
    know, of reviewing some of some of these things.
  topic: technical
- impact_reason: 'Offers a crucial insight into human-in-the-loop (HITL) data annotation:
    qualitative, generative feedback (writing critiques) is superior to quantitative
    feedback (thumbs up/down) for expert engagement.'
  relevance_score: 9
  source: llm_enhanced
  text: I think when you ask someone, especially someone skilled, like a lawyer or
    think of psychologist, you know, someone, when you ask them to give thumbs up,
    thumbs down on like thousands of samples, I think it's very, they don't like it...
    But when you ask them to like, give feedback about something, or write something,
    actually they really enjoy it, it's really funny. Like, it feels more, I think,
    engaged in the process and they feel more in control.
  topic: safety/ethics
- impact_reason: Strong strategic advice against training foundation models from scratch
    for most applications, emphasizing the efficiency of fine-tuning existing models.
  relevance_score: 9
  source: llm_enhanced
  text: The one thing I want to be very clear that we don't do is starting from scratch.
    Like, I think, you know, specialized models in the sense of starting from something
    from scratch. I think, you know, there are, there are use cases where it might
    make sense, but I think for vast majority, it doesn't.
  topic: strategy
- impact_reason: 'Explains the historical difficulty of RL: training from scratch
    leads to initial instability due to random policies, contrasting sharply with
    the stability offered by pre-trained LLMs.'
  relevance_score: 9
  source: llm_enhanced
  text: part of the reason is because this was reinforcement learning from scratch.
    And when you are doing reinforcement learning from scratch, the behavior initially
    is fundamentally unstable because you are asking a random policy, like a random
    model to take decisions. But obviously the decisions are random, which is a disaster.
  topic: technical
- impact_reason: Powerful analogy ('starting on easy mode') explaining why LLMs revolutionize
    RL application by providing a strong initialization point.
  relevance_score: 9
  source: llm_enhanced
  text: Whereas, you know, when you start with a large language model, you are starting
    on easy mode because the model is already incredibly smart.
  topic: technical
- impact_reason: Highlights the critical dependency on high-quality foundation models
    (base models) for successful downstream fine-tuning or adaptation (like RL).
  relevance_score: 9
  source: llm_enhanced
  text: But this is only possible because the base model is already amazing, actually.
    And if the base model is not good, you know, you don't really get anywhere.
  topic: technical/strategy
- impact_reason: A strong prediction/advocacy that RL techniques (beyond just prompting)
    will become a fundamental skill for all data scientists.
  relevance_score: 9
  source: llm_enhanced
  text: Like the reality is that is an exceptionally powerful method that should be
    in the end, you know, of every data scientist of everyone. Most likely, you know,
    prompting, you know, is in the end of everyone.
  topic: predictions/strategy
- impact_reason: 'A concise philosophical definition of the core challenge in RL:
    defining success and translating it into measurable signals.'
  relevance_score: 9
  source: llm_enhanced
  text: In reinforcement learning, fundamentally what you are thinking about is what
    defines success. Like what, how do I define a successful outcome or a bad outcome
    for the model? How do I provide the model signal about this? That's really what
    it becomes all about.
  topic: technical/strategy
- impact_reason: Reinforces that the technical power of RL is unlocked once the definition
    problem (reward engineering) is solved, framing RL as a complex system design
    challenge.
  relevance_score: 9
  source: llm_enhanced
  text: That's uniquely powerful. Then it becomes entirely a game of how do I define
    it. This is a part where reinforcement learning becomes more for like, I like
    to describe it as a pipeline or as like as a system because it's not very often
    success is multifaceted.
  topic: technical/strategy
- impact_reason: Cites a high-profile, cautionary viewpoint regarding the impending
    data scarcity crisis for foundational model training.
  relevance_score: 9
  source: llm_enhanced
  text: Ilya Sutskever said that if Gen AI's fossil fuel is human data on the open
    internet, we've exhausted our supply.
  topic: predictions/safety
- impact_reason: Confirms the speaker's belief that the 'golden age' of easily accessible,
    high-quality internet data for pre-training is ending.
  relevance_score: 9
  source: llm_enhanced
  text: I think there is a bit of truth in every one of these statements where definitely
    in terms of really available data, we are starting to eat a limit where there
    was a golden age where we were just starting to crawl the web and starting to
    improve your color.
  topic: technical/predictions
- impact_reason: Emphasizes the virtually limitless data generation potential once
    models can interact with and learn from the real world via experimentation.
  relevance_score: 9
  source: llm_enhanced
  text: it's practically models can conduct as many trials and many experiments in
    the real world as resources alone.
  topic: predictions/technical
- impact_reason: 'Confirms the current industry consensus: compute availability and
    cost (dominated by Nvidia) is the primary constraint in scaling AI.'
  relevance_score: 9
  source: llm_enhanced
  text: today's bottleneck is compute. Definitely, I think this is very obvious, given
    the money that people are spending, you know, towards trying to get more, given
    the insane valuation of Nvidia, which is going to continue to increase.
  topic: business/strategy
- impact_reason: 'Poses a fundamental strategic question: Are hardware breakthroughs
    (like photonics/quantum) necessary prerequisites for AGI, or will AGI itself guide
    us to those necessary tools?'
  relevance_score: 9
  source: llm_enhanced
  text: I think about it in terms of like, do we need it to get to AGI/ASI? Like,
    is it something we are going to discover by yourself that we need to figure out
    by yourself to get there? Or is it something that later we are going to figure
    out with the support, you know, of generally intelligent or super intelligent
    systems?
  topic: predictions/strategy
- impact_reason: Suggests that current silicon technology, when scaled effectively
    through AI development, is sufficient to bootstrap the creation of future, more
    advanced computational paradigms.
  relevance_score: 9
  source: llm_enhanced
  text: my thinking on on these subjects of like photonic chips or even quantum computing
    is that we, we as humans don't really need to worry about this right now. I think
    that we already have the capability, like, you know, what we have, the technology
    that we have are already in us, to take us to the level where we will build systems
    that will help us build this.
  topic: strategy/predictions
- impact_reason: Provides concrete examples of hardware specialization happening *now*
    (e.g., optimizing instructions for Softmax in attention mechanisms) within existing
    GPU architectures.
  relevance_score: 9
  source: llm_enhanced
  text: So it's our instruction set for for this, like they are thinking about, oh,
    can we increase a bit on the chip? The part that is dedicated to this so that
    we get a bit more, you know, a bit more throughput with this.
  topic: technical
- impact_reason: 'This sets the stage, highlighting the core value proposition of
    the guest''s company: making fine-tuned, use-case specific LLMs accessible to
    enterprises.'
  relevance_score: 8
  source: llm_enhanced
  text: We spend tons of time talking about that so you can get a full understanding
    of how cutting-edge AI models are made and how his startup Adaptive ML allows
    enterprises to have fine-tuned models for their particular use case available
    much more easily than ever before.
  topic: business/strategy
- impact_reason: Illustrates the massive, near-comprehensive scale of data collection
    for modern LLM pre-training, including multimodal data.
  relevance_score: 8
  source: llm_enhanced
  text: During pre-training, we essentially collect data from all over the web, pretty
    much every book, every paper. But, anyway, nearly at the scale of modern pre-training,
    nearly every text in existence, I think it sounds very grandiose, but it's not
    far from being true, and even nowadays images, videos, and all of this.
  topic: technical
- impact_reason: 'Summarizes the current focus in advanced RL for LLMs: building scalable
    ''signal generation environments'' (like automated testing frameworks).'
  relevance_score: 8
  source: llm_enhanced
  text: So there is a plurality of signal that you can use that is massive. And I
    think now people are very focused on scaling these massive environments in which
    to run in which to run the models to get these signals.
  topic: strategy
- impact_reason: Defines RLHF and grounds it in the real-world annotation economy
    (mentioning Scale and Surge AI), highlighting its scalability limits.
  relevance_score: 8
  source: llm_enhanced
  text: historically, you know, the big one, the big first one, that's the big acronym,
    you know, that that got up a lot with RLHF, which is reinforcement learning from
    human feedback, where you are using your typical annotation data, company like
    Scale, you know, recently semi-acquired, I guess, by Meta, company like Search
    as well, which has been the new to lots.
  topic: technical/business
- impact_reason: Identifies RLHF as the historically dominant post-training reinforcement
    learning approach and names key players/data sources (Scale, annotators).
  relevance_score: 8
  source: llm_enhanced
  text: So there is, historically, you know, the big one, the big acronym, you know,
    that that got up a lot with RLHF, which is reinforcement learning from human feedback,
    where you are using your typical annotation data, company like Scale, you know,
    recently semi-acquired, I guess, by Meta, company like Search as well, which has
    been the new to lots. Essentially having annotators give these thumbs up, thumbs
    down, or different forms of feedback.
  topic: technical
- impact_reason: Provides historical context, connecting modern RL techniques back
    to foundational successes like AlphaGo and Atari, where success/failure provided
    clear rewards.
  relevance_score: 8
  source: llm_enhanced
  text: If you think about it, if you go back to the roots of reinforcement learning,
    you know, when people used to do AlphaGo or, you know, or even before the Atari
    games, this is essentially execution feedback, you know, as a model plays a game,
    if it gets, you know, if it succeeds at the game, you know, then it gets, it gets
    a reward.
  topic: technical
- impact_reason: 'Articulates the primary skepticism surrounding synthetic data: the
    risk of model collapse or generating low-quality, degenerate outputs.'
  relevance_score: 8
  source: llm_enhanced
  text: A lot of people have pushed back on synthetic, I will say synthetic data as
    a whole, but on like on data that is model generated because they are like, oh,
    this is going to, you know, this is going to be degenerate data, like this is
    going to fall down, collapse into like something that's bad.
  topic: safety/concerns
- impact_reason: Transitions the discussion from the *source* of the reward signal
    (RLHF, RLEF, RLIF) to the underlying *algorithm* used for optimization (PPO, A2C),
    signaling a deeper technical dive.
  relevance_score: 8
  source: llm_enhanced
  text: Regardless of which kind of those approaches we choose, there's also differences
    in what reinforcement learning algorithm we select, right? So there's things like
    PPO, A2C. Do you want to tell us about the big ones there?
  topic: technical
- impact_reason: Suggests that successful modern RL applications, especially with
    LLMs, often operate in a 'proximity online' mode, balancing immediate feedback
    with batch processing.
  relevance_score: 8
  source: llm_enhanced
  text: And then you veer into offline RL. But one of the big, one of the big successes,
    you know, of reinforcement learning is that it is mostly online, but with mostly,
    like, proximity online, where essentially you have some balls that are relatively
    fresh, you evaluate this sample, and you learn from that.
  topic: technical
- impact_reason: 'Uses a powerful analogy (memorizing textbook solutions) to explain
    the nature of SFT/pre-training: learning from repeated, static examples.'
  relevance_score: 8
  source: llm_enhanced
  text: If you think, we make a parallel with human learning, and let's say I'm teaching
    you a course about general relativity... I could show you the solution of the
    exercise, have you memorize it, you know, just memorize it again and again and
    again. And then, you know, when I present you the exercise, you can run through
    it exactly the same again. This is essentially what pre-training or supervised
    fine-tuning do...
  topic: strategy
- impact_reason: Names the current state-of-the-art RL algorithms being actively used
    in LLM alignment (PPO, GRPO, DPO), signaling current research focus.
  relevance_score: 8
  source: llm_enhanced
  text: when thinking about reinforcement learning research, I think it's one of the,
    one of the big things to think about and to go back to your general question on
    the different algorithms. So we are a lot like, HUC is another one, but we are
    a lot of these days for instance about PPO, GRPO, DPO, all of this sort of stuff.
  topic: technical
- impact_reason: 'Captures the historical barrier to entry for RL: powerful in theory,
    but notoriously hard to implement robustly in practice.'
  relevance_score: 8
  source: llm_enhanced
  text: But when we got started a year and a half, two years ago, I think it was still
    true if you had that experience, you know, of going, you are like, oh, these are
    amazing methods. They can do amazing things and there's currently a lot of potential
    in them. But there is a bit of a problem, which is that typically, these are quite
    difficult to put in place.
  topic: business
- impact_reason: Provides concrete examples (compilers, VMs) illustrating the need
    for complex, external execution environments when training models for tool use
    via RL.
  relevance_score: 8
  source: llm_enhanced
  text: Maybe you teach your model to do Rust. And so you have, you need to have a
    Rust compiler, and maybe you need you are teaching the model to use. And so on,
    so you need access to these tools, or maybe you are teaching the model computers,
    in which case, you need a VM box, you know, you lack of virtual machine...
  topic: technical/engineering
- impact_reason: Introduces the concept of 'RLOps' as a necessary discipline to manage
    the complexity of deploying and iterating on RL-aligned models in an enterprise
    setting.
  relevance_score: 8
  source: llm_enhanced
  text: We provide essentially data science teams with what we call the RL ops tooling,
    you know, kind of for the reinforcement learning ops.
  topic: business/strategy
- impact_reason: Highlights the abstraction layer being built over complex distributed
    computing for RL, making advanced techniques accessible via simple Python recipes
    (a key goal for developer tooling).
  relevance_score: 8
  source: llm_enhanced
  text: And then you don't have to worry about any of the actual, you know, implementation
    or tooling essentially does kind of a lot of say compile because that's exactly
    compilation, but essentially interprets, you know, your instruction, your Python
    recipe, and then runs it on the cluster in a distributed way without you having
    to think about it.
  topic: technical
- impact_reason: Reinforces the focus on lowering the barrier to entry for RL, targeting
    the primary user (data scientists) to accelerate value realization.
  relevance_score: 8
  source: llm_enhanced
  text: our core idea is to build better tooling for reinforcement learning, so that
    it's easier to get to value and obviously is something right for the data scientists.
  topic: strategy
- impact_reason: 'Explains the strategic rationale for targeting large enterprises:
    their scale creates acute needs for cost optimization and model efficiency, which
    RL/fine-tuning can address.'
  relevance_score: 8
  source: llm_enhanced
  text: one of our theses was to be very focused on enterprise, on like larger enterprise,
    because these companies when they put generic into production, they have a very
    unique scale, you know, maybe millions, tens, hundreds of millions of users. And
    that comes, you know, with obviously costs, you know, they're much larger, also
    bigger impetus to potentially optimize cost back into smaller models...
  topic: business
- impact_reason: 'Defines the architectural philosophy: using existing powerful foundation
    models as a base rather than training from scratch.'
  relevance_score: 8
  source: llm_enhanced
  text: You've described previously Adaptive as a layer on top of foundation models
    to tailor them to final use cases.
  topic: strategy
- impact_reason: 'Philosophical justification for the ''layer on top'' approach: foundation
    models contain vast latent knowledge that needs refinement, not replacement.'
  relevance_score: 8
  source: llm_enhanced
  text: Because so reality is that the foundation models, you know, are with amazing
    engines, with treasure trove, you know, of knowledge and of understanding, which
    you can sharpen into exactly, you know, what you want.
  topic: strategy
- impact_reason: Identifies the specific open-source ecosystem (Llama, Qwen, Kimi)
    driving current enterprise fine-tuning efforts and highlights licensing/usage
    constraints as a practical consideration.
  relevance_score: 8
  source: llm_enhanced
  text: We start from open-source models. So this might be Llama, Qwen, you know,
    Kimi, you know, whichever one is your favorite flavor and whichever one you are
    allowed to use at work. We start from them and we tune them to perform better.
  topic: technical
- impact_reason: Provides a clear technical contrast explaining why RL from scratch
    is difficult, setting the stage for why pre-trained models are essential starting
    points.
  relevance_score: 8
  source: llm_enhanced
  text: When you are doing reinforcement learning from scratch, the behavior initially
    is fundamentally unstable because you are asking a random policy, like a random
    model to take decisions. But obviously the decisions are random, which is a disaster.
  topic: technical
- impact_reason: Emphasizes the sheer scale and depth of knowledge embedded during
    the pre-training phase, which is the core asset being leveraged.
  relevance_score: 8
  source: llm_enhanced
  text: They still contain insane knowledge. Like if you think about it, in pre-training
    they have seen nearly everything. Like the knowledge that contains this model
    is insane.
  topic: technical
- impact_reason: Provides a nuanced view on post-training, suggesting it's not just
    alignment but can involve knowledge refinement, focusing, and acquisition.
  relevance_score: 8
  source: llm_enhanced
  text: I think it does as well in certain conditions [post-training adding knowledge].
    But essentially of disentangling, you know, the knowledge that's in the model,
    maybe preening the part that you don't need as much, so facing the part that you
    need the most and using that as careful to learn even more, to acquire even more
    capabilities.
  topic: technical
- impact_reason: Provides a real-world validation point (AT&T) that Gen AI adoption
    is deep and creating tangible value, countering skepticism about enterprise readiness.
  relevance_score: 8
  source: llm_enhanced
  text: I'm always surprised by the penetration of Gen AI inside the organization
    and everywhere, you know, in every aspect of the business. They are pushing, you
    know, models to do really amazing things that really create value for the company.
  topic: business/predictions
- impact_reason: Sets up the central debate regarding the future scaling laws of AI
    models.
  relevance_score: 8
  source: llm_enhanced
  text: There's a big question mark around scale these days where the idea of bigger
    compute, bigger networks, bigger data, driving more model capability.
  topic: technical/predictions
- impact_reason: Identifies synthetic data as the primary proposed solution to overcome
    the exhaustion of human-generated training data.
  relevance_score: 8
  source: llm_enhanced
  text: Synthetic data seems to be part of the solution there.
  topic: technical/predictions
- impact_reason: Provides specific engineering tricks (data massaging, ordering) that
    can extend the utility of existing data, even if quantity is limited.
  relevance_score: 8
  source: llm_enhanced
  text: You can massage this data to improve its quality to get better results out
    of what you get. You can order it differently during pre-training, maybe for the
    lower quality data first, so that you get more impact from the later quality data.
  topic: technical
- impact_reason: Provides a pragmatic, cautionary view on hardware innovation, acknowledging
    the difficulty of displacing the entrenched, highly optimized silicon/GPU ecosystem.
  relevance_score: 8
  source: llm_enhanced
  text: I think it's really difficult to bring a new hardware paradigm to life. The
    current hardware paradigm, as it has issues, it's, it's, it's load of energy,
    blah, blah, blah. It's very rigid. But it also has tremendous advantages in that
    it works really well.
  topic: strategy/technical
- impact_reason: Explains the high barrier to entry for new hardware competitors,
    emphasizing the decade-long supply chain, R&D, and manufacturing ecosystem required
    to match current leaders.
  relevance_score: 8
  source: llm_enhanced
  text: For a competitor to come, for, you know, an alternative mean of competing
    to come, well, you have to reproduce all of that. And I think this is going to
    take a while.
  topic: business/strategy
- impact_reason: Marks the end of the 'easy data' era, suggesting that the low-hanging
    fruit of web-scale data collection is largely exhausted for foundational models.
  relevance_score: 8
  source: llm_enhanced
  text: we are starting to eat a limit where there was a golden age where we were
    just starting to crawl the web and starting to improve your color.
  topic: trends
- impact_reason: Notes that GPUs are already evolving rapidly to become domain-specific
    accelerators, even within the existing silicon framework, moving beyond general
    graphics processing.
  relevance_score: 8
  source: llm_enhanced
  text: The current hardware paradigm... is already extensively specialized to machine
    learning and even people, some people say, it's going to be specialized to transformers.
  topic: technical/trends
- impact_reason: Addresses the fuzzy boundary between Supervised Fine-Tuning (SFT)
    and Reinforcement Learning, acknowledging definitional ambiguity in the field.
  relevance_score: 7
  source: llm_enhanced
  text: The moment at which it exactly becomes reinforcement learning might be SFT
    supervised fine-tuning. Supervised fine-tuning, yes, totally. It's like the moment
    at which, you know, the transition might be a bit, a bit of a question of where
    everyone puts it...
  topic: technical
- impact_reason: Addresses community debates by grouping PPO and GRPO as sharing fundamental
    characteristics related to online learning, suggesting convergence in core RL
    principles for LLMs.
  relevance_score: 7
  source: llm_enhanced
  text: I think one of them, they are actually quite similar. The answer is, especially
    PPO and GRPO, there was big debate in the community, PPO and GRPO in terms of,
    of these components on onlineness and everything, share a similar characteristic.
  topic: technical
- impact_reason: 'Provides a snapshot of current high-demand AI consulting areas:
    Gen AI, Multi-Agent Systems, and RL, emphasizing end-to-end production capability.'
  relevance_score: 7
  source: llm_enhanced
  text: I've launched my own AI consultancy, a firm called Y Carrot. Yes, the letter
    Y and the deliciously crunchy veggie at Y Carrot. We combine decades of experience
    in machine learning and software development with internationally recognized expertise
    in all the cutting-edge approaches, including Gen AI, multi-agent systems, and
    RL from problem scoping and proof of concept through to high-volume production
    deployments.
  topic: business
- impact_reason: 'A subtle but important technical distinction: raw pre-trained models
    lack the conversational alignment (chatty nature) that instruction-tuned models
    possess.'
  relevance_score: 7
  source: llm_enhanced
  text: One thing to note about pre-training is, you know, we said all the pre-training
    model, the pre-trained model is not chatty.
  topic: technical
- impact_reason: Offers a practical suggestion for researchers/developers to understand
    the raw capabilities of base models before instruction tuning, emphasizing the
    distinction between pre-training and post-training.
  relevance_score: 7
  source: llm_enhanced
  text: Like one thing to note about pre-training is, you know, we said all the pre-training
    model, the pre-trained model is not chatty. And by the way, something I would
    invite people to do. It's harder these days because I mentioned the lines between
    pre-training and post-training are blurred, but it's to look at some of this older
    pre-trained only model.
  topic: technical/strategy
- impact_reason: Clear business mission statement focused on democratizing advanced
    AI techniques (RL) through tooling and expertise.
  relevance_score: 7
  source: llm_enhanced
  text: Yeah, I think for us, you know, like a lot of what we do now is bringing,
    you know, this expertise in reinforcement learning to be something that anyone
    can do.
  topic: business/strategy
- impact_reason: Directly addresses the fear/skepticism around AI job displacement,
    arguing that in adopting companies, massive value creation is occurring.
  relevance_score: 7
  source: llm_enhanced
  text: there's always a discussion about, oh, is AI, you're pervert, is blah blah
    blah, you know, compy people. And sometimes, you know, it might be like, is it
    like whatever? And I think it's definitely not like I think you know, yes, some
    businesses are slower in adoption. The real world is always slower in adoption.
    But there is a like in companies that are moving forward, it's insane value being
    created.
  topic: safety/business
- impact_reason: A powerful statement illustrating the immense engineering complexity
    and accumulated human knowledge embedded in modern AI accelerators (like Nvidia
    GPUs).
  relevance_score: 7
  source: llm_enhanced
  text: If you hold one in your hands, even a nature one on red or beat your own red,
    you are probably holding the sum total of all of human achievement, like all of
    human achievement, as picked to this thing, which is like absolutely insane in
    terms of engineering to get there.
  topic: strategy/technical
- impact_reason: Reassures that the path to advanced AI is not blocked; significant
    research and engineering challenges remain, particularly in scaling RL.
  relevance_score: 7
  source: llm_enhanced
  text: there is stuff left to do. You know, this is not a negative point of view.
    I saw there is nothing left to do. No, there is there is stuff left to do. You
    know, I, I think, you know, we're thinking of reinforcement learning just before.
    There's plenty of stuff to do in that direction of how do we scale this?
  topic: strategy
- impact_reason: 'Offers a specific, actionable data strategy: sequencing data quality
    during pre-training to maximize learning efficiency.'
  relevance_score: 7
  source: llm_enhanced
  text: You can order it differently during pre-training, maybe for the lower quality
    data first, so that you get more impact from the later quality data.
  topic: technical/business
source: Unknown Source
summary: '## Podcast Episode Summary: 913: LLM Pre-Training and Post-Training 101,
  with Julien Launay


  This 75-minute episode features Julien Launay, co-founder and CEO of Adaptive ML,
  providing a deep dive into the methodologies behind creating and refining Large
  Language Models (LLMs), specifically contrasting the **pre-training** and **post-training**
  phases. The discussion moves from foundational AI training concepts to the cutting-edge
  role of reinforcement learning (RL) in modern model refinement and the commercial
  implications for enterprise AI.


  ---


  ### 1. Focus Area

  The primary focus is on the **lifecycle of Large Language Model (LLM) creation**,
  detailing the distinct stages of **Pre-training** (foundational knowledge acquisition)
  and **Post-training** (alignment and specialization). Key technical concepts covered
  include various forms of Reinforcement Learning (RLHF, RLEF, RLAIF) used in post-training,
  and the shift in compute allocation between these two phases.


  ### 2. Key Technical Insights

  *   **The Evolving Training Paradigm:** Historically, pre-training (predicting the
  next token on massive web data) consumed the vast majority of compute. However,
  modern models (like Grok) are spending nearly as much on post-training, indicating
  a massive scaling up of alignment and refinement efforts.

  *   **Plurality of Reinforcement Learning Signals:** Post-training is increasingly
  driven by sophisticated feedback mechanisms beyond traditional Human Feedback (RLHF).
  These include **Execution Feedback (RLEF)**, where models are rewarded based on
  verifiable outcomes (e.g., passing code tests, solving math problems), and **AI
  Feedback (RLAIF)**, where a specialized model evaluates outputs, offering scalable
  alternatives to human annotation.

  *   **Verification vs. Generation:** A core principle discussed is that **verification
  is often easier and more scalable than generation**. Models can be highly effective
  at checking the correctness of another model''s output (RLAIF/RLEF) even if the
  initial human data used for comparison contains significant "noise" (low inter-rater
  agreement).


  ### 3. Business/Investment Angle

  *   **Enterprise Customization via Adaptive ML:** Launayâ€™s company, Adaptive ML,
  focuses on making fine-tuned models easily accessible for enterprises using smaller,
  cost-efficient models, suggesting a market trend toward specialized, rather than
  monolithic, foundation models.

  *   **The Value of Post-Training Data:** The increasing investment in post-training
  signals that the competitive edge is shifting from simply having the largest pre-trained
  model to having the most effective, aligned, and specialized model, often achieved
  through proprietary, high-quality feedback loops.

  *   **Scalability of Synthetic Data:** The success of RLAIF and RLEF demonstrates
  a viable path for companies to generate massive amounts of high-quality training
  data internally, reducing reliance on expensive, slow human annotation pipelines
  for alignment.


  ### 4. Notable Companies/People

  *   **Julien Launay:** Guest, Co-founder/CEO of **Adaptive ML**, bringing expertise
  from **Hugging Face** and **Light On**.

  *   **Grok (xAI):** Mentioned as a recent example demonstrating the power of extensive
  post-training and reinforcement learning to achieve high benchmark scores (e.g.,
  on law exams).

  *   **Annotation Companies (Scale, Surge):** Referenced in the context of traditional
  RLHF data sourcing, highlighting the limitations of human scalability.


  ### 5. Future Implications

  The industry is moving toward a future where **post-training is as critical, if
  not more so, than pre-training**. This shift implies that innovation will increasingly
  focus on creating scalable, automated, and verifiable feedback environments (RL
  environments) to continuously tune models for specific tasks, moving beyond the
  initial, broad knowledge acquisition phase.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Data Scientists involved
  in model deployment, AI Product Managers, and Technology Strategists** interested
  in the practical engineering and commercial realities of building and aligning state-of-the-art
  LLMs for real-world applications.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- investment
- nvidia
- meta
- anthropic
title: '913: LLM Pre-Training and Post-Training 101, with Julien Launay'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 189
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 90
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 17
  prominence: 1.0
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 4
  prominence: 0.4
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 3
  prominence: 0.3
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-04 16:20:11 UTC -->
