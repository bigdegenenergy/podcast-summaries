---
companies:
- category: unknown
  confidence: medium
  context: This podcast is brought to you by Elbit Systems, a leading global defense
    technology company deli
  name: Elbit Systems
  position: 34
- category: unknown
  confidence: medium
  context: oss multiple platforms. Elbit Systems' I-Star and EW Division is the EW
    House of Israel with solutions integrat
  name: EW Division
  position: 311
- category: unknown
  confidence: medium
  context: rms. Elbit Systems' I-Star and EW Division is the EW House of Israel with
    solutions integrated on dozens of
  name: EW House
  position: 330
- category: unknown
  confidence: medium
  context: chnology in an effective way. Welcome to From the Crows Nest. I'm your
    host Ken Miller from the Association of
  name: Crows Nest
  position: 811
- category: unknown
  confidence: medium
  context: ay. Welcome to From the Crows Nest. I'm your host Ken Miller from the Association
    of Bull Crows. In today's ep
  name: Ken Miller
  position: 837
- category: unknown
  confidence: medium
  context: I'm your host Ken Miller from the Association of Bull Crows. In today's
    episode, we are going to talk about a
  name: Bull Crows
  position: 872
- category: unknown
  confidence: medium
  context: m pleased to be here with a senior scientist from Charles River Analytics,
    Jeff Drews, who is an expert on human-centered A
  name: Charles River Analytics
  position: 1145
- category: unknown
  confidence: medium
  context: a senior scientist from Charles River Analytics, Jeff Drews, who is an
    expert on human-centered AI. Jeff, tha
  name: Jeff Drews
  position: 1170
- category: unknown
  confidence: medium
  context: n of the capability? As you said, I'm Jeff Drews, Senior Scientist here
    at Charles River Analytics. I've been with t
  name: Senior Scientist
  position: 1714
- category: unknown
  confidence: medium
  context: iversity of Minnesota. Did an internship with the Air Force Research Laboratory,
    the AFRL, in Dayton, Ohio. They got me into mach
  name: Air Force Research Laboratory
  position: 2090
- category: unknown
  confidence: medium
  context: started in applied artificial intelligence, which Charles River has been
    involved in for a long time. I think the
  name: Charles River
  position: 2408
- category: unknown
  confidence: medium
  context: problem out of the gate here. We've talked in the National Security Space
    about AI for really decades, but it's really been
  name: National Security Space
  position: 3032
- category: unknown
  confidence: medium
  context: We still have human in the loop, at least in the United States, at all
    these various places, but you can leverag
  name: United States
  position: 5325
- category: unknown
  confidence: medium
  context: chnology in workspaces and missions and so forth. But I want to talk a
    little bit about this notion of tr
  name: But I
  position: 5905
- category: tech
  confidence: high
  context: forth. But I want to talk a little bit about this notion of trusting the
    speed with which AI works, becaus
  name: Notion
  position: 5948
- category: unknown
  confidence: medium
  context: ance is a fascinating topic that goes back to the Industrial Revolution,
    bringing in trusted automation effectively. Will
  name: Industrial Revolution
  position: 7649
- category: unknown
  confidence: medium
  context: thing do a good job? Do you understand its logic? Can I trust it? So I
    think even though the machines can
  name: Can I
  position: 7776
- category: unknown
  confidence: medium
  context: job? Do you understand its logic? Can I trust it? So I think even though
    the machines can—the machines,
  name: So I
  position: 7792
- category: unknown
  confidence: medium
  context: Now, Charles River Analytics, you're an FFRDC, a Federally Funded Research
    and Development Center. You do federal research f
  name: Federally Funded Research
  position: 8208
- category: unknown
  confidence: medium
  context: you're an FFRDC, a Federally Funded Research and Development Center. You
    do federal research for DOD in different cap
  name: Development Center
  position: 8238
- category: unknown
  confidence: medium
  context: king on is the Merlin stack, which is part of the DARPA Sceptre program.
    And that is something that can generate—
  name: DARPA Sceptre
  position: 8905
- category: unknown
  confidence: medium
  context: me or is that an acronym? Relax is an acronym for Reinforcement Learning
    with Adaptive Explainability. So what exactly the
  name: Reinforcement Learning
  position: 9470
- category: unknown
  confidence: medium
  context: lax is an acronym for Reinforcement Learning with Adaptive Explainability.
    So what exactly then is Relax trying to solve? W
  name: Adaptive Explainability
  position: 9498
- category: unknown
  confidence: medium
  context: oblem is that trying to solve in terms of for the Air Force and DOD operators
    in the field? Reinforcement lea
  name: Air Force
  position: 9627
- category: unknown
  confidence: medium
  context: useful example is something that came about when AlphaGo Zero first came
    into the mix. AlphaGo Zero was an AI i
  name: AlphaGo Zero
  position: 13075
- category: tech
  confidence: high
  context: d in a way to help it learn. And then eventually, Google DeepMind developed
    this—they found that if they j
  name: Google
  position: 13420
- category: unknown
  confidence: medium
  context: d in a way to help it learn. And then eventually, Google DeepMind developed
    this—they found that if they just had t
  name: Google DeepMind
  position: 13420
- category: unknown
  confidence: medium
  context: hey're very vulnerable, just like any other tool. And AI systems are tools;
    they are tools to be leveraged
  name: And AI
  position: 14969
- category: unknown
  confidence: medium
  context: ach to the battle? How is that explained to them? Because I can imagine
    that if I'm sitting in a classroom an
  name: Because I
  position: 15784
- category: unknown
  confidence: medium
  context: eally a bit surprising to me to the degree in the Camel XAI program, which
    was a DARPA program, the explanati
  name: Camel XAI
  position: 16493
- category: unknown
  confidence: medium
  context: d deep reinforcement, and also into user systems. The HMI, our human-machine
    interface, what information th
  name: The HMI
  position: 17465
- category: unknown
  confidence: medium
  context: lainability, and this is back from a line paper, "A Linear Method for Adding
    Explainability," and they found for a
  name: A Linear Method
  position: 23343
- category: unknown
  confidence: medium
  context: s is back from a line paper, "A Linear Method for Adding Explainability,"
    and they found for a dog classifier for Huskies
  name: Adding Explainability
  position: 23363
- category: ai_application
  confidence: medium
  context: Sponsor of the podcast; a defense technology company delivering advanced
    solutions, implying the use or development of advanced technologies like AI in
    their defense systems.
  name: Elbit Systems
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The employer of the guest, Jeff Drews. They are involved in applied artificial
    intelligence, focusing on human-centered AI, explainability, and verification/validation
    of AI-driven systems.
  name: Charles River Analytics
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The organization where the guest interned and was introduced to machine
    learning and AI.
  name: Air Force Research Laboratory (AFRL)
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A program under which Charles River Analytics is working on the Merlin
    stack, leveraging deep reinforcement learning agents.
  name: DARPA Sceptre program
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A DARPA program mentioned in relation to user-dependent explanations for
    AI systems (XAI).
  name: DARPA Camel XAI program
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The developer of AlphaGo Zero, mentioned in the context of developing novel,
    highly effective AI strategies.
  name: Google DeepMind
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI developed by Google DeepMind designed to play Go, famous for developing
    novel, hard-to-understand strategies.
  name: AlphaGo Zero
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The primary AI system/effort discussed, focused on adaptable (A) explainability
    (XAI) for complex agents.
  name: Relax
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A DARPA program that informed the development of Relax, focusing on user-dependent
    explanations.
  name: Camel XAI program
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The organization sponsoring the Camel XAI program.
  name: DARPA
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A preceding project that provided the springboard for the current Relax
    effort.
  name: Homer project
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: A program mentioned where neuro-symbolic agents are being used for complex
    scenarios.
  name: Sceptre program
  source: llm_enhanced
- category: organization
  confidence: medium
  context: Mentioned as a service partner for the work being done on Relax.
  name: Air Force
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Referenced as a benchmark dataset where early models showed high training
    accuracy but poor real-world performance (overfitting example).
  name: ImageNet
  source: llm_enhanced
- category: professional_services_consulting
  confidence: high
  context: The organization where the speaker works or is affiliated, directing listeners
    to their website for more information. While a consulting firm, they likely advise
    on AI implementation.
  name: CRA (Charles River Associates)
  source: llm_enhanced
date: 2025-10-08 17:30:00 +0000
duration: 37
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: say more efficiently
  text: we should say more efficiently.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/02f2a74ca8da450eb028ab12d19caada/
processing_date: 2025-10-08 20:03:04 +0000
quotes:
- length: 80
  relevance_score: 5
  text: They got me into machine learning, artificial intelligence, kind of optimization
  topics: []
- length: 206
  relevance_score: 3
  text: In today's episode, we are going to talk about artificial intelligence and
    how we develop complex systems to adapt to novel situations and explain the reasoning
    behind the artificial intelligence capability
  topics: []
- length: 166
  relevance_score: 3
  text: From there, I went to Charles River Analytics and immediately got started
    in applied artificial intelligence, which Charles River has been involved in for
    a long time
  topics: []
- length: 146
  relevance_score: 3
  text: But as soon as you start bringing in electronic reasoning, things done by
    artificial intelligence, then they can happen at a much more rapid level
  topics: []
- length: 191
  relevance_score: 3
  text: To unpack that a little bit, reinforcement learning is a branch of artificial
    intelligence where you're producing autonomous agents that are used to make decisions
    or designed to take actions
  topics: []
- length: 214
  relevance_score: 3
  text: What we found—we ran through this, we ran through several user groups, giving
    some access to the interface and some not—and we asked a series of questions and
    basically tested this where we knew the correct answers
  topics: []
- length: 277
  relevance_score: 3
  text: 'What we found were statistically significant results: that explanations helped
    the user understand the decision-making of the agent, that they were more effective
    at using the agent in scenarios in which they could do better, and that their
    trust of the agent overall increased'
  topics: []
- impact_reason: This is a critical strategic insight, shifting the bottleneck from
    technological capability to human adoption and readiness. It's highly relevant
    for business strategy and organizational change management in the AI era.
  relevance_score: 10
  source: llm_enhanced
  text: At this point, we're seeing such a rapid explosion in the advancement of many
    of these AI technologies. The slowdown may not be the technology being ready,
    but the humans being ready to use the technology in an effective way.
  topic: strategy
- impact_reason: Highlights the speaker's core expertise and the critical need for
    XAI (Explainable AI) and robust V&V processes, especially in high-stakes domains
    like defense.
  relevance_score: 10
  source: llm_enhanced
  text: my particular interest and passion in it is providing explainability and enhanced
    verification and validation to AI-driven systems.
  topic: safety/technical
- impact_reason: A direct warning against prioritizing speed over rigor (trust, validation,
    safety) when deploying AI systems.
  relevance_score: 10
  source: llm_enhanced
  text: if you're only looking at the speed, you're going to relax a lot of the other
    standards that you need to do.
  topic: safety
- impact_reason: This perfectly encapsulates the core tension between AI speed and
    human trust/comprehension, necessitating XAI for high-stakes decision support.
  relevance_score: 10
  source: llm_enhanced
  text: a machine can generate a complex course of action, a COA, very, very rapidly,
    but you need to slow things down so the human can understand what's going on because
    under the hood, you don't know what the reasoning process was for this course
    of action that was generated by an AI entity.
  topic: safety/technical
- impact_reason: Introduces the 'Relax' effort, explicitly linking DRL (the engine)
    with Explainability (the necessary trust layer).
  relevance_score: 10
  source: llm_enhanced
  text: the second one that I think nicely complements that is the Relax effort, which
    endeavors to add explainability to those same or similar deep reinforcement learning
    agents so that you can trust them, we can understand them.
  topic: technical/safety
- impact_reason: A textbook explanation of the 'black box' problem inherent in deep
    learning models, particularly when dealing with high-dimensional inputs like images.
  relevance_score: 10
  source: llm_enhanced
  text: However, the big problem with neural networks is that they're inherently opaque.
    You can bring in a large data structure like RGB images; there can be thousands,
    millions of computations that map that input to the output, and the user has no
    idea of really what happened under the hood in order to produce that output...
  topic: safety/technical
- impact_reason: Emphasizes that in high-stakes environments (like military operations),
    performance alone is insufficient; confidence derived from understanding is mandatory
    for adoption.
  relevance_score: 10
  source: llm_enhanced
  text: But if you don't know what it's doing, it's kind of hard to trust. This is
    where it could be of critical importance. People's lives are at stake. You need
    to be confident that this decision, this recommendation for an action, is legitimate.
  topic: safety/business
- impact_reason: 'A concise summary of the explainability paradox: high capability
    requires high interpretability for trust.'
  relevance_score: 10
  source: llm_enhanced
  text: You can have these awesome AI-made tactics and strategies, but if you can't
    understand what it's trying to do and why, it's not reasonable to trust it.
  topic: safety
- impact_reason: 'Clearly outlines the three core hypotheses tested in XAI research:
    accuracy of mental model, performance improvement, and trust increase.'
  relevance_score: 10
  source: llm_enhanced
  text: 'The fundamental questions that we wanted to answer were: Do explanations
    help a user''s mental model become more accurate for how an AI really operates?
    Will they perform better with an agent they understand? And will they trust that
    agent more if they understand it better?'
  topic: technical/safety
- impact_reason: Provides empirical validation for the value of XAI, confirming that
    explanations lead to better understanding, better performance, and increased trust.
  relevance_score: 10
  source: llm_enhanced
  text: 'What we found were statistically significant results: that explanations helped
    the user understand the decision-making of the agent, that they were more effective
    at using the agent in scenarios in which they could do better, and that their
    trust of the agent overall increased.'
  topic: business/safety
- impact_reason: A powerful anecdote illustrating dataset bias and spurious correlation
    learning in deep networks (the 'Husky vs. Snow' problem), underscoring the danger
    of unexamined accuracy.
  relevance_score: 10
  source: llm_enhanced
  text: It turned out it just looked at snow. If you gave it any image with snow in
    it, it thought it was a Husky. If you gave it a Husky on the beach, it thought
    it was a different—it thought it was a different dog or something else altogether.
  topic: safety/technical
- impact_reason: Directly addresses the danger of 'cheap shortcuts' or label leakage
    in training data, leading to brittle or nonsensical behavior in deployment, a
    major concern in applied ML.
  relevance_score: 10
  source: llm_enhanced
  text: If you give them bad information or sort of like biased information that contains
    a cheap answer, they can behave strangely in a simulated environment.
  topic: safety/technical
- impact_reason: A powerful anecdote illustrating the massive gap between lab performance
    (validation sets) and real-world performance (deployment), emphasizing the necessity
    of rigorous field testing.
  relevance_score: 10
  source: llm_enhanced
  text: Until you get it out in the field, it's pretty hard to know you got it right.
    There have been a lot of cases where models—people thought they did really, really
    well. The early models in ImageNet, for example, you got them out; they get 90%,
    98% in training on the held-out validations. You start testing it with pictures
    on your phone; you get 10%.
  topic: strategy/technical
- impact_reason: A critical insight shifting the bottleneck from technological capability
    to human adoption, trust, and organizational readiness.
  relevance_score: 10
  source: llm_enhanced
  text: The slowdown may not be the technology being ready, but the humans being ready
    to use the technology in an effective way.
  topic: business/strategy
- impact_reason: 'Sets the core theme of the discussion: AI adaptation to novelty
    and the necessity of explainability, which are central challenges in advanced
    AI deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: we are going to talk about artificial intelligence and how we develop complex
    systems to adapt to novel situations and explain the reasoning behind the artificial
    intelligence capability.
  topic: technical/safety
- impact_reason: Illustrates the massive computational advantage AI simulation provides
    over human-paced strategic planning (combinatorial explosion solved).
  relevance_score: 9
  source: llm_enhanced
  text: if you have a simulated environment where you can do a million operations
    per second, you could manually commute through all the different combinations
    that could occur in order to make more effective decisions from a strategic level.
  topic: technical/strategy
- impact_reason: Provides a concrete, contemporary example (ATR in Ukraine conflict)
    of AI's current impact on perception tasks in warfare.
  relevance_score: 9
  source: llm_enhanced
  text: you could also think of bringing in AI, which has happened a lot, for example,
    the Russian-Ukraine conflict for ATR, for automatic target recognition, where
    you can leverage AI for perception, things like classifiers, full-motion video
    processing, tracking objects with high fidelity and high resolution.
  topic: technical/application
- impact_reason: Clarifies the current operational posture ('human in the loop') while
    emphasizing the speed and contextual breadth AI offers over human capacity.
  relevance_score: 9
  source: llm_enhanced
  text: We still have human in the loop, at least in the United States, at all these
    various places, but you can leverage AI for very fast perception, very fast reasoning
    over a larger, broader context than you could do manually, which is two minutes.
  topic: safety/application
- impact_reason: 'Quantifies the benefit of XAI: it allows humans to safely capture
    the speed advantage of AI planning (weeks vs. instant generation).'
  relevance_score: 9
  source: llm_enhanced
  text: if you can have that slowdown and unpack its reasoning, then you can still
    leverage the vast speed-ups of producing the strategic maneuver because it still
    could take humans weeks of planning, literally weeks of planning, to gather all
    the necessary information and run through the combinations to get this, whereas
    an AI could generate it virtually instantly.
  topic: business/strategy
- impact_reason: 'A clear requirement for deploying trustworthy, human-supervised
    AI systems: the mechanism for unpacking logic must exist.'
  relevance_score: 9
  source: llm_enhanced
  text: if you want it to be trusted, if you want a human in the loop, there needs
    to be that mechanism to unpack it.
  topic: safety
- impact_reason: Identifies a specific, cutting-edge government program (Merlin stack/Sceptre)
    utilizing Deep Reinforcement Learning (DRL) for strategic COA generation.
  relevance_score: 9
  source: llm_enhanced
  text: The major one that I'm currently working on is the Merlin stack, which is
    part of the DARPA Sceptre program. And that is something that can generate—that
    is a program that is looking to leverage deep reinforcement learning-based agents
    to generate a large amount of courses of action in order to make better and faster
    decisions in a complex space.
  topic: technical/application
- impact_reason: Pinpoints the shift to Deep RL (late 2010s) as the source of increased
    modeling capacity and sophistication.
  relevance_score: 9
  source: llm_enhanced
  text: So in the late 2010s, we started—we as in the research community—started using
    neural networks to use as the policy. So you had a much higher modeling capacity
    that can make much, much more sophisticated decisions based on the information
    available to generate those actions, given the observations of the world.
  topic: technical
- impact_reason: Articulates the specific, practical questions users have about opaque
    models (counterfactual reasoning, consideration of alternatives) that XAI must
    address.
  relevance_score: 9
  source: llm_enhanced
  text: What if I changed this input to this different scenario? Why didn't you consider
    this other avenue of ingress? There are just a lot of things that are not transparent
    to a user that wants to leverage these actions.
  topic: safety
- impact_reason: Highlights the critical shift from focusing only on decision speed
    to requiring explainability (understanding the 'why') for comprehensive system
    integration and risk assessment.
  relevance_score: 9
  source: llm_enhanced
  text: So with that, then you're basically addressing kind of a huge gap that existed
    in AI where it wasn't just the speed of the decision, but you need to understand
    why they were making the decision because that allows you to understand what other
    capabilities you might need to bring to the table or how it might affect other
    things in theater that might not be apparent to or clear within that determination
    by the AI system.
  topic: safety/strategy
- impact_reason: A classic example illustrating the potential for AI to generate novel,
    superior strategies that are initially opaque to human experts, setting up the
    core tension between performance and trust.
  relevance_score: 9
  source: llm_enhanced
  text: There was a move called the alien move that this AI took that no one understood,
    none of the Go masters understood, and it being of critical importance to defeat
    the human player.
  topic: technical/predictions
- impact_reason: Warns against blindly accepting complex, novel outputs from black-box
    systems, even if they appear effective initially.
  relevance_score: 9
  source: llm_enhanced
  text: Having a large, complex system that generates very strange, novel strategies,
    even by experts, without evidence and reasoning to back up what they're doing,
    that's not something that can be trusted.
  topic: safety
- impact_reason: 'The core finding of user-centric XAI: effective explanation is context-dependent
    and deeply tied to human acceptance factors.'
  relevance_score: 9
  source: llm_enhanced
  text: 'The shorter answer, like every good question, is the answer is: it depends.
    It depends on the person. What are they trying to understand? And it gets very
    deeply entrenched in user acceptance.'
  topic: business/strategy
- impact_reason: Identifies counterfactual explanations as a particularly effective
    method for building user mental models by showing decision boundaries.
  relevance_score: 9
  source: llm_enhanced
  text: We found that, for example, counterfactual explanations are really helpful
    to gain a deeper understanding. A counterfactual, in essence, is what would need
    to happen in order for something different to happen.
  topic: technical
- impact_reason: Provides a clear, accessible definition of neuro-symbolic AI, bridging
    modern deep learning with traditional, interpretable AI methods.
  relevance_score: 9
  source: llm_enhanced
  text: Neuro-symbolic—I've never heard that term before. Yeah. So basically, it is
    something that uses both a modern like deep architecture, like people call deep
    neural networks—so that's what the neural comes from—and symbolic means like you
    are using variables that are human-defined and well-known.
  topic: technical
- impact_reason: Explains the mechanism by which deep networks exploit unintended
    features in the data to minimize loss, leading to brittle or non-generalizable
    behavior.
  relevance_score: 9
  source: llm_enhanced
  text: A neural network will utilize any information you give it and try to maximize
    what's called its forward function or minimize its loss function if that's what
    it finds, and it will do that in whatever way it can, and they can learn some
    strange, unintuitive ways.
  topic: technical
- impact_reason: Another concrete example of data leakage causing models to learn
    irrelevant artifacts instead of the true underlying features, highlighting data
    curation risks.
  relevance_score: 9
  source: llm_enhanced
  text: We saw some COVID classifiers that were just looking at patient digits at
    the bottom on the outside; everyone with COVID actually had like an extra millimeter
    of gray at the top because it was like a leaked dataset.
  topic: safety/technical
- impact_reason: This is a fundamental, concise explanation of how neural networks
    operate, highlighting their opportunistic nature in finding patterns, which is
    crucial for understanding failure modes like spurious correlations.
  relevance_score: 9
  source: llm_enhanced
  text: Neural networks use whatever information they can to make a decision as accurately
    and easily as effectively as easily as they can.
  topic: technical
- impact_reason: Provides an excellent, relatable analogy (the 'Link' shortcut) for
    poor generalization and overfitting to specific, non-robust features in reinforcement
    learning or complex systems.
  relevance_score: 9
  source: llm_enhanced
  text: If you don't dig into them, you think they're doing great. Imagine a video
    game with Link where you stab in the bottom left corner; it was super hard to
    beat, but it was not really a good generalized policy, a disproportion that works
    in that one case.
  topic: technical/strategy
- impact_reason: 'Offers a nuanced view on bias: bias is only harmful if the test
    distribution differs from the training distribution (i.e., lack of generalization).
    This reframes bias as a generalization problem.'
  relevance_score: 9
  source: llm_enhanced
  text: Unfortunately, I think that's like another 'it depends' answer, because as
    you sort of indicated, bias can be good. If you have a testing scenario in which
    the same bias is there as the training set, then you do a great job.
  topic: safety/technical
- impact_reason: 'Acknowledges the trade-off: XAI might slightly reduce peak performance
    on a controlled metric but drastically increases real-world usability and trust.'
  relevance_score: 9
  source: llm_enhanced
  text: I think bolstering these tools with explainability and peering a little deeper
    into them and their functionality can make them a little more usable in the real
    world, even though they may not do as good of a job sometimes if you consider
    this training, for example, on like a baseline test.
  topic: safety/strategy
- impact_reason: Applies the 'No Free Lunch Theorem' concept to AI models, arguing
    against the idea of a single, universally superior model architecture, especially
    relevant when discussing LLMs vs. specialized models.
  relevance_score: 9
  source: llm_enhanced
  text: It's like from computer science, there's a no free lunch theorem; there's
    no one ring to rule them all for these AI models when they can do a really good
    job.
  topic: strategy
- impact_reason: Advocates for a decentralized, swarm-like approach using multiple
    specialized, simpler agents over relying on one massive, monolithic, 'best' model,
    which is a key strategic consideration for robustness.
  relevance_score: 9
  source: llm_enhanced
  text: I think smaller, simpler models that can go and deal with things like divide
    and conquer that can go gather intelligence in an autonomous way such that you
    don't have to have 50 humans sitting there with goggles on... they can go act
    and behave in a reasonable way even if they're not individually the most efficient
    or the best.
  topic: strategy/technical
- impact_reason: 'A concrete five-year prediction focusing on the ''human-in-the-loop''
    problem: developing the methodologies for effective utilization and trust calibration,
    rather than just building bigger models.'
  relevance_score: 9
  source: llm_enhanced
  text: I think the next five years, personally, is going to be understanding how
    to use AI effectively and what do we need to extract from the AI in order to understand
    and trust them.
  topic: predictions/safety
- impact_reason: A timeless strategic principle applied to AI, emphasizing that trust
    is the prerequisite for adoption, regardless of technical superiority.
  relevance_score: 9
  source: llm_enhanced
  text: People don't use things they don't trust; we've seen that with automation
    throughout history.
  topic: business/strategy
- impact_reason: Uses a simple analogy to stress the importance of matching the right
    AI tool to the specific task (tool selection), which requires user understanding
    and appropriate trust levels.
  relevance_score: 9
  source: llm_enhanced
  text: You want to use a screwdriver when a hammer is appropriate; it's just not
    the right thing. You need to—it's the same thing with AI tools, and how can we
    get them in the hands of people is to make them better understood and more trusted
    and appropriately trusted.
  topic: strategy/safety
- impact_reason: Provides a timeline for the perceived acceleration of AI impact,
    specifically framing it as central to future military advantage.
  relevance_score: 8
  source: llm_enhanced
  text: AI for really decades, but it's really been in the last, I would say, maybe
    five to ten years where it's really felt like it's taken off, where you can start
    to see the progression of AI really becoming a centerpiece of future military
    operations and how we employ that and how we can kind of more efficiently gain
    an advantage in the battlespace using AI.
  topic: predictions/strategy
- impact_reason: A concise statement on how AI fundamentally changes the complexity
    landscape of modern operations.
  relevance_score: 8
  source: llm_enhanced
  text: The theater of modern warfare has really become more complex in the past decade
    with the introduction of AI-driven systems.
  topic: predictions
- impact_reason: 'Defines the scope of AI''s current utility: automating perception
    and initial reasoning, reducing the burden on human analysts.'
  relevance_score: 8
  source: llm_enhanced
  text: All of those things are now entering the arena where you can remove the necessity
    of manual human inspection and manual human reasoning from the equation, at least
    at all aspects.
  topic: predictions/application
- impact_reason: Frames the AI trust problem as a historical challenge of automation
    adoption, focusing on logic comprehension as the key to acceptance.
  relevance_score: 8
  source: llm_enhanced
  text: Human trust is a very interesting thing; user acceptance is a fascinating
    topic that goes back to the Industrial Revolution, bringing in trusted automation
    effectively. Will this thing do a good job? Do you understand its logic? Can I
    trust it?
  topic: safety/strategy
- impact_reason: Offers a clear, tiered definition of RL application, ranging from
    low-level control to high-level strategic command.
  relevance_score: 8
  source: llm_enhanced
  text: Reinforcement learning is a branch of artificial intelligence where you're
    producing autonomous agents that are used to make decisions or designed to take
    actions. That could be something as low-level as motor controls or as high-level
    as moving an entire battalion to a particular location via a strategic path.
  topic: technical
- impact_reason: Contrasts traditional, simpler RL policy representations (tables/linear
    models) with modern Deep RL, explaining the historical progression.
  relevance_score: 8
  source: llm_enhanced
  text: It can be enhanced with a deep neural network to actually make that decision.
    So back when it was originally used, you could have like tables where the policy,
    which is the action generator, was just defined by something simple like a table
    or a linear model...
  topic: technical
- impact_reason: 'Reiterates the core function of Relax: augmenting DRL with XAI to
    make its outputs trustworthy.'
  relevance_score: 8
  source: llm_enhanced
  text: The idea is Relax can augment that system by adding explainability to the
    actions generated by the deep reinforcement learning agent.
  topic: technical
- impact_reason: Frames the utility of explainability in terms of actionable insight
    for human adaptation, rather than just justification.
  relevance_score: 8
  source: llm_enhanced
  text: So if they're making a decision like, here's the path you need to take, it
    might be too vague for us to know exactly what's involved in getting to that solution,
    or is it more of just helping military leaders understand, hey, this is exactly
    why we're making this decision along the way so that we can then adapt more quickly
    from the human side to try to keep pace?
  topic: strategy
- impact_reason: Directly links novel AI-generated strategies to achieving tactical
    superiority against adversaries relying on predictable, 'by the book' methods.
  relevance_score: 8
  source: llm_enhanced
  text: So if you can imagine if we have a confrontation that we can leverage something
    like this model that can bring in a novel strategy to do something that is incredibly
    effective, a tactic that is not known because it's an adversary knows what you're
    going to do if you're going by the books. Well, that's pretty easy to defend.
    But if it's a novel tactic, then it can be used to your advantage to gain superiority.
  topic: strategy
- impact_reason: Provides a fundamental framework for viewing AI systems in operational
    contexts—they must be treated like any other calibrated tool.
  relevance_score: 8
  source: llm_enhanced
  text: AI systems are tools; they are tools to be leveraged. Tools need to be calibrated,
    known, and trusted.
  topic: strategy
- impact_reason: Details a practical, empirical approach to XAI development, confirming
    that explanation format must be tailored to the user.
  relevance_score: 8
  source: llm_enhanced
  text: What we did was a collection of user studies with varying types of explanations;
    some of them are narrative, some of them are graphical, or combinations of them,
    in order to test the level of understanding that they really have of the AI under
    the hood.
  topic: technical/business
- impact_reason: Contrasts the opaque, raw input processing of deep networks with
    the meaningful, human-defined nature of symbolic variables.
  relevance_score: 8
  source: llm_enhanced
  text: Whereas in a deep neural network, it just takes in an RGB image that are zero
    to 255 values over a 512 by 512 image—that doesn't mean really anything of itself,
    just a collection of values of integers.
  topic: technical
- impact_reason: Applies the concept of spurious correlation from image classification
    to Reinforcement Learning agents, showing that high simulation reward doesn't
    guarantee generalized competence.
  relevance_score: 8
  source: llm_enhanced
  text: If you don't have something to unpack it, it will use information in strange,
    unintuitive ways to maximize that reward function, and it may not be doing a good
    job overall.
  topic: technical/safety
- impact_reason: Strong advocacy for Explainable AI (XAI) as a necessary component
    for ensuring system reliability and trustworthiness, especially when models use
    unintuitive methods.
  relevance_score: 8
  source: llm_enhanced
  text: If you go with explainability, then you can do a good job.
  topic: safety/strategy
- impact_reason: A clear, accessible definition of overfitting, linking it directly
    to the failure to generalize, which is the core challenge in deploying ML models.
  relevance_score: 8
  source: llm_enhanced
  text: There is the phenomenon in machine learning called overfitting. If you're
    overfit, you basically are very, very tied to your training set and you don't
    generalize outside of it.
  topic: technical
- impact_reason: Highlights the difficulty of hyperparameter tuning, specifically
    model capacity selection, acknowledging that achieving the 'right' balance is
    often empirical and hard to know beforehand.
  relevance_score: 8
  source: llm_enhanced
  text: You basically just need the correct model capacity given the training set
    and the cardinality or the size of the training dataset for the objective you're
    trying to accomplish. And it's really, really hard to know if you got that right.
  topic: technical
- impact_reason: Expresses healthy skepticism and highlights the uncertainty surrounding
    the capabilities and emergent behaviors of frontier, massive-scale models.
  relevance_score: 8
  source: llm_enhanced
  text: how crazy we'll get with LLMs, how massive they can be, how much they can
    be applied toward reinforcement learning—there's a lot of unknowns with the super
    modern, advanced, massive trillion-plus parameter networks with what they can
    do.
  topic: predictions/technical
- impact_reason: Identifies the next major frontier in AI application—foundational
    models for physical systems (robotics)—and notes that their impact is still pending.
  relevance_score: 8
  source: llm_enhanced
  text: Are foundational models—are they called foundational models for robotics?
    They haven't hit their moment yet like LLMs have, but people are thinking that
    could be the case, and that could change the answer to my question.
  topic: predictions
- impact_reason: Provides a sobering comparison to traditional, slow-moving defense/engineering
    cycles, setting realistic expectations for the pace of AI integration in high-stakes
    domains.
  relevance_score: 8
  source: llm_enhanced
  text: If you look at what the process has been for changing a metal in an aircraft,
    it takes 25 years to do that. Are we going to completely replace strategic decision-making
    with AI? That would be crazy to do that in a short amount of time.
  topic: strategy/predictions
- impact_reason: A cautionary note suggesting that speed/efficiency isn't the only
    metric for successful AI adoption.
  relevance_score: 7
  source: llm_enhanced
  text: But sometimes efficiency is something that's a little bit more nuanced in
    terms of how we use technology in workspaces and missions and so forth.
  topic: strategy
- impact_reason: Provides the official definition for a specific XAI project targeting
    DRL agents.
  relevance_score: 7
  source: llm_enhanced
  text: Relax is an acronym for Reinforcement Learning with Adaptive Explainability.
  topic: technical
- impact_reason: Uses AlphaGo Zero as a historical touchstone to discuss the evolution
    of AI understanding and training methods.
  relevance_score: 7
  source: llm_enhanced
  text: I think a useful example is something that came about when AlphaGo Zero first
    came into the mix. AlphaGo Zero was an AI intended or that was designed to play
    Go.
  topic: technical/history
- impact_reason: 'States the standard paradigm for training deep learning models:
    maximizing input data quantity.'
  relevance_score: 7
  source: llm_enhanced
  text: Typically, you give a neural network all the information you can, and it learns
    how to make the best decision.
  topic: technical
- impact_reason: Confirms the current industry standard (feeding maximum data) while
    implicitly suggesting that explainability is the necessary countermeasure to manage
    the resulting complexity.
  relevance_score: 7
  source: llm_enhanced
  text: I guess to answer your question, you can restrict the information in some
    ways, but the general practice is to give it all the information you possibly
    can and let the system figure out the best way to use it.
  topic: technical/strategy
- impact_reason: Outlines the standard, high-information approach in deep learning,
    contrasting it with the need for explainability to audit the resulting complex
    decisions.
  relevance_score: 7
  source: llm_enhanced
  text: The general practice is to give it all the information you possibly can and
    let the system figure out the best way to use it.
  topic: technical/strategy
- impact_reason: 'Pinpoints the core research challenge remaining in AI: achieving
    true generality across diverse operational environments.'
  relevance_score: 7
  source: llm_enhanced
  text: That is a fundamental research endeavor that is yet to be overcome to make
    any model work in a super broad variety of circumstances.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: How to Trust AI on the Battlefield


  This 36-minute episode of "From the Crows Nest," sponsored by Elbit Systems, features
  an in-depth discussion with **Jeff Drews, Senior Scientist at Charles River Analytics**,
  focusing on the critical challenge of **building and maintaining human trust in
  advanced Artificial Intelligence systems deployed in complex, high-stakes military
  environments.**


  The narrative arc moves from acknowledging the rapid integration of AI in modern
  warfare (speed and perception) to dissecting the fundamental barrier to adoption:
  **explainability and verification.** The core argument is that while AI offers unprecedented
  speed in strategic planning and tactical execution (like Automatic Target Recognition
  or rapid Course of Action generation), this speed is useless if human operators
  cannot understand or trust the underlying reasoning.


  ### 1. Focus Area

  The primary focus is **Human-Centered AI (HCAI)**, specifically applied to national
  security and defense. Key topics include:

  *   The impact of AI on modern warfare complexity and decision-making speed.

  *   The necessity of **Explainable AI (XAI)** to foster user acceptance and trust.

  *   The technical challenges of **Deep Reinforcement Learning (DRL)** opacity.

  *   The development of adaptive explanation methodologies tailored to the end-user.


  ### 2. Key Technical Insights

  *   **Opacity of Deep Reinforcement Learning (DRL):** DRL agents, which use deep
  neural networks as their policy generators, offer vastly superior modeling capacity
  for complex decisions but are inherently opaque, making it impossible for users
  to trace the reasoning behind generated actions.

  *   **The Relax Effort:** Charles River Analytics is developing **Relax (Reinforcement
  Learning with Adaptive Explainability)** to augment DRL agents by adding mechanisms
  to unpack their decision-making processes, ensuring actions are legitimate and understandable.

  *   **Neuro-Symbolic Architectures:** The future involves combining the high-capacity
  pattern recognition of **neural networks** with the human-interpretable logic of
  **symbolic AI** (using well-defined variables) to create more robust and explainable
  systems.


  ### 3. Business/Investment Angle

  *   **The Bottleneck is Human Readiness:** The rapid advancement of AI technology
  is outpacing the human capacity to effectively integrate and trust it, creating
  a significant market need for HCAI solutions.

  *   **XAI as a Competitive Advantage:** In defense, systems that can demonstrate
  verifiable, understandable reasoning (like those developed under the DARPA Sceptre
  program) will gain crucial adoption over opaque "black box" alternatives, even if
  the latter are marginally faster.

  *   **FFRDC Role in Maturation:** Federally Funded Research and Development Centers
  (FFRDCs) like Charles River Analytics are driving foundational research (e.g., Homer,
  Sceptre programs) that directly translates into next-generation defense capabilities.


  ### 4. Notable Companies/People

  *   **Jeff Drews (Charles River Analytics):** Senior Scientist and expert in Human-Centered
  AI, focusing on XAI and V&V for AI-driven systems.

  *   **Charles River Analytics:** Long-standing developer of applied AI, tracing
  roots back to fuzzy logic and now focused on deep learning applications for defense.

  *   **DARPA Sceptre Program:** A key research initiative mentioned, involving the
  **Merlin stack** (for generating COAs via DRL) and the **Relax** effort.

  *   **Elbit Systems:** The episode sponsor, highlighting their role in integrating
  next-generation defense technology.


  ### 5. Future Implications

  The industry is moving toward a paradigm where **speed must be balanced by verifiable
  understanding.** Future AI deployment hinges on the ability to translate novel,
  highly effective strategies (like those seen in AlphaGo Zero''s "alien moves") into
  actionable, trusted recommendations for human commanders. This requires **adaptive
  explanation interfaces (HMI)** that cater to the specific knowledge level of the
  user (e.g., narrative vs. graphical explanations, counterfactual reasoning).


  ### 6. Target Audience

  This episode is highly valuable for **Defense Technology Professionals, AI/ML Researchers
  focusing on Trust and Safety, Military Strategists, and Defense Investors** interested
  in the practical adoption and validation hurdles of autonomous systems in operational
  theaters.'
tags:
- artificial-intelligence
- ai-infrastructure
- google
title: How to Trust AI on the Battlefield
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 114
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 11
  prominence: 1.0
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-08 20:03:04 UTC -->
