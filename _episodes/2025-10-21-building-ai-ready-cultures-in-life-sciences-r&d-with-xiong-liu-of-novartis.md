---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2159
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2194
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2559
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2621
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5302
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7026
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 17992
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24112
- category: ai_media_and_research
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, utilizing generative AI and LLMs in R&D.
  name: Novartis
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    indicating enterprise AI adoption.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, indicating enterprise AI adoption.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool for organizing ideas and making connections, built by
    Steven Johnson.
  name: Notebook LM
  source: llm_enhanced
- category: ai_research_infrastructure
  confidence: high
  context: Mentioned as the originator of the transformer architecture that underpins
    modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The URL provided for the Notebook LM tool, linking it directly to Google's
    ecosystem.
  name: NotebookLM.google.com
  source: llm_enhanced
- category: consulting_and_research
  confidence: high
  context: Mentioned for conducting a recent survey on generative AI investment and
    scaling in the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a benchmark for the explosion of LLMs, specifically in contrast
    to earlier models like BERT.
  name: ChatGPT
  source: llm_enhanced
- category: ai_research_infrastructure
  confidence: high
  context: An earlier language model architecture (pre-ChatGPT) that the speaker's
    team applied to clinical trial documents for information extraction.
  name: BERT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a model leveraging the transformer architecture
    for generating new sequences/data, applicable to areas like molecule design.
  name: GPT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The underlying model architecture being discussed as the basis for generative
    AI models like GPT, applicable across various data types (language, molecules).
  name: Transformer architecture
  source: llm_enhanced
- category: general_stakeholder
  confidence: medium
  context: Mentioned as groups that are quite good at building foundation models and
    publishing results.
  name: Academics
  source: llm_enhanced
- category: general_stakeholder
  confidence: medium
  context: Mentioned as sources of open public architectures that can be leveraged
    for building models.
  name: Tech companies
  source: llm_enhanced
- category: general_stakeholder
  confidence: medium
  context: Entities that may leverage published models or build their own domain-specific
    versions.
  name: Healthcare companies
  source: llm_enhanced
- category: general_stakeholder
  confidence: high
  context: Professionals capable of quickly building pipelines and models.
  name: AI modelers
  source: llm_enhanced
- category: general_stakeholder
  confidence: high
  context: General term for experts involved in the AI adoption and development process.
  name: AI professionals
  source: llm_enhanced
- category: general_stakeholder
  confidence: high
  context: Stakeholders whose alignment with AI modelers is crucial for effective
    model validation and adoption.
  name: Scientists (Basic scientists)
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 20:49:22 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 100
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs, foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 70
  relevance_score: 3
  text: So we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: Crucially contrasts the old, task-specific ML approach (requiring
    labeled proprietary data) with the foundation model approach (leveraging massive,
    domain-wide pre-training).
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs, foundation models. So
    the idea is that yes, you still care about the specific data you are working on,
    but at the same time, we want to collect all the available data, relevant data,
    or irrelevant data, but as long as they fall into the same domain, and fill those
    data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical
- impact_reason: 'Explains the core value proposition of foundation models: knowledge
    transfer and utility even with limited task-specific data (few-shot/zero-shot
    learning potential).'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Offers a precise, accessible definition of fine-tuning in the context
    of foundation models—adjusting weights rather than full retraining—which is crucial
    for domain adaptation.
  relevance_score: 10
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right? So we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems...
  topic: technical
- impact_reason: 'A powerful conceptual framing: treating biological/genetic data
    as a ''language'' allows LLM architectures to be applied directly to molecular
    design and testing, suggesting massive efficiency gains.'
  relevance_score: 10
  source: llm_enhanced
  text: Genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process...
  topic: predictions
- impact_reason: Offers a clear technical distinction between BERT-style models (encoder-focused,
    generating embeddings/features) and their role in ML pipelines.
  relevance_score: 10
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they use the decoder
    part of the transformer. So you can think it's as a very good representation model...
    So the outcome is some representations or embeddings of those text tokens, etc.
    So those embeddings are token representations they have predictive power.
  topic: technical
- impact_reason: Crucially explains the generative leap of GPT-style models stemming
    from leveraging the decoder part of the transformer, enabling novel data creation.
  relevance_score: 10
  source: llm_enhanced
  text: But now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: 'Highlights a major breakthrough: conditional generation in specialized
    domains (like chemistry), allowing models to optimize for multiple, complex constraints
    (e.g., toxicity AND solubility) simultaneously.'
  relevance_score: 10
  source: llm_enhanced
  text: So now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/applications
- impact_reason: Pinpoints hallucination as a critical safety/reliability issue, contrasting
    its immediate detectability in text/code versus the delayed, experiment-dependent
    validation required in science.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: 'Proposes a crucial solution for model selection in specialized fields:
    developing hybrid evaluation metrics that combine domain expertise with AI performance
    measures.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is a key here,
    because there could be so many versions of found data models. Now, if we, you
    know, have those kind of domain plus AI-driven those evaluation metrics, so we
    can score across different models, right?
  topic: safety/strategy
- impact_reason: Highlights the significant gap between investment interest and successful,
    scaled deployment of GenAI in the life sciences sector, pointing to a major industry
    challenge.
  relevance_score: 9
  source: llm_enhanced
  text: Flash forward to 2024, a recent Deloitte survey notes that while 73% of life
    sciences leaders are investing in generative AI initiatives, fewer than 20% have
    successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: Provides a concise historical overview of the evolution of AI/ML,
    clearly marking the shift from traditional methods to the current era dominated
    by foundation models.
  relevance_score: 9
  source: llm_enhanced
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks. And more recently, we have a paradigm shift into those language models,
    foundation models.
  topic: technical
- impact_reason: 'Provides a concrete example of foundation model utility in life
    sciences: leveraging public molecular data (like cell atlases) to gain insights
    into specific indications even with limited proprietary data.'
  relevance_score: 9
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data. So for example, cell atlases, etc.
  topic: technical
- impact_reason: 'Highlights an advanced application: using GenAI to synthesize novel
    biological data (cell types, gene expressions) for in silico testing, accelerating
    early discovery.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: 'Identifies three key practical challenges for AI adoption in R&D:
    benchmarking, cross-functional alignment, and measuring return on investment (ROI).'
  relevance_score: 9
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: Highlights the foundational role of the Transformer architecture
    across diverse domain-specific foundation models, emphasizing architectural unification
    in modern AI.
  relevance_score: 9
  source: llm_enhanced
  text: So all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Describes the strategic shift in specialized fields (like healthcare)
    towards creating domain-specific foundation models by adapting the general transformer
    paradigm to proprietary/domain data.
  relevance_score: 9
  source: llm_enhanced
  text: So people in the healthcare have been thinking similarly, can we put all the
    available, you know, data, chemical data, clinical trial data, etc. into those,
    you know, transformer architecture? So that way we can also build our domain-specific
    foundation models to enable a lot of downstream tasks.
  topic: strategy/technical
- impact_reason: Emphasizes the indispensable role of domain experts in data curation,
    model guidance, and, crucially, in establishing benchmarks for specialized AI
    systems.
  relevance_score: 9
  source: llm_enhanced
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves... But, you know, we still need the domain
    knowledge people, you know, people have those kind of expertise to guide the,
    you know, the selection of the data and also more importantly, then these models
    are, you build, now how do we benchmark with them?
  topic: strategy/safety
- impact_reason: 'Clearly articulates the current limitation: AI potential is outpacing
    the experimental/data generation capacity in cutting-edge biology (e.g., single-cell
    sequencing data availability).'
  relevance_score: 9
  source: llm_enhanced
  text: I mean, that is potentially possible. But unfortunately, to get those, you
    know, single-cell RNA-seq data, all those kind of more, you know, experiment-driven
    data, probably we're not exactly there yet, because, you know, we still rely on
    those traditional biotechnology methods to collect those data through experiments
    on cell lines, on human tissues, etc.
  topic: limitations
- impact_reason: Identifies organizational alignment and communication across diverse
    stakeholders (scientists, modelers, leadership) as a major hurdle in AI execution.
  relevance_score: 9
  source: llm_enhanced
  text: And then it goes to the execution, right? So, there's a lot of cultural organizational
    issue. I see the key that really we should align, you know, between different
    parties, like scientists, basic scientists, or AI modelers, AI professionals,
    and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: 'Offers strategic advice to domain experts: they must understand
    the evolving AI paradigm, recognizing that AI integration is hybrid, not a complete
    replacement of existing methods.'
  relevance_score: 9
  source: llm_enhanced
  text: It behooves them [subject matter experts] to kind of have some awareness of
    the new and old paradigm that we were talking about before, how both are still
    present, but how we thought that AI would impact life sciences organization or
    that they'd just be this one school way of doing things is not the case.
  topic: strategy
- impact_reason: Describes a strategic shift toward centralizing and leveraging massive,
    diverse data lakes to power broad, strategic AI applications, moving beyond simple
    document processing.
  relevance_score: 8
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization...
  topic: strategy
- impact_reason: Attributes the current AI revolution directly to the underlying architectural
    innovation (the Transformer), emphasizing its foundational role across modalities
    (text, image, molecular data).
  relevance_score: 8
  source: llm_enhanced
  text: All those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: 'Explains the core value proposition of large foundation models:
    leveraging massive, generic pre-training data to enable broad applicability across
    many downstream tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: So now the difference is that we see the benefits of putting a lot of the
    generic public data like text, images into those transformers so we can do, you
    know, so many downstream tasks.
  topic: technical
- impact_reason: Provides a historical comparison (BERT vs. GPT-like models) in the
    context of information extraction, noting the transition from representation models
    to generative capabilities.
  relevance_score: 8
  source: llm_enhanced
  text: So we, you know, applied those kind of models into a clinical track documents
    so that we found a lot of efficiency gain, accuracy gain in extracting information.
    Now with those GPT-like models.
  topic: technical
- impact_reason: 'Addresses the critical data strategy for domain-specific models:
    the necessity of integrating proprietary data alongside public datasets.'
  relevance_score: 8
  source: llm_enhanced
  text: Now the next question is about the data, right? What are the data? ... you
    know, your own company owned those kinds of proprietary information, etc. So those
    things could potentially are building into those models.
  topic: business/strategy
- impact_reason: Identifies a structural advantage for language-heavy scientific fields
    (like genetics) due to the early maturity and success of NLP/LLMs.
  relevance_score: 8
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: Reiterates that data quality remains a primary bottleneck, even with
    massive foundation models, especially when moving into specialized domains.
  relevance_score: 8
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: technical/business
- impact_reason: 'A concise summary of the current state: AI theory is ahead of experimental
    data collection capabilities.'
  relevance_score: 8
  source: llm_enhanced
  text: So, there's, you know, where AI, the architecture, the concept there, but
    the, sometimes we are limited by the, you know, experimental side.
  topic: limitations
- impact_reason: Highlights the challenge of translating complex model capabilities
    into actionable scientific insights, requiring domain experts to bridge the gap.
  relevance_score: 8
  source: llm_enhanced
  text: how do we digest the models, the capabilities? And again, we have to plug
    in, you know, the right scientists to help us together validate those.
  topic: business
- impact_reason: Poses a critical business/strategy question regarding the build vs.
    buy decision for foundation models, especially in regulated, domain-specific fields
    like healthcare.
  relevance_score: 7
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
- impact_reason: Sets the agenda to discuss practical applications and development
    processes for GenAI/Foundation Models, signaling a shift from high-level discussion
    to concrete examples.
  relevance_score: 7
  source: llm_enhanced
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right?
  topic: strategy
- impact_reason: 'Summarizes the organizational requirement for successful AI implementation
    in science: mandatory collaboration between AI specialists and domain experts
    (biologists, chemists).'
  relevance_score: 7
  source: llm_enhanced
  text: So, yeah, so it's basically a cross-discipline collaboration, whether in house
    or externally, yeah.
  topic: business/strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Building AI-Ready Cultures in Life Sciences
  R&D - with Xiong Liu of Novartis


  This 30-minute episode features an in-depth discussion with **Xiong Liu, Director
  of Data Science in AI at Novartis**, focusing on the transformative shift generative
  AI (GenAI) and foundational models are bringing to Life Sciences Research & Development
  (R&D) and clinical workflows. The central narrative explores the challenges organizations
  face in scaling AI adoption beyond pilot projects and the cultural shifts required
  to integrate these powerful new technologies effectively.


  ---


  ### 1. Focus Area

  The discussion centers on the **paradigm shift in AI methodology** within Life Sciences
  R&D, moving from traditional, task-specific supervised/unsupervised machine learning
  (ML) to leveraging **Large Language Models (LLMs) and Foundational Models**. Specific
  applications discussed include accelerating **molecule design, digital twin creation,
  protocol optimization, and disease pathway analysis**. A significant portion addresses
  the practical challenges of **workflow integration, data quality, and establishing
  domain-specific evaluation standards** for AI-driven research ROI.


  ### 2. Key Technical Insights

  *   **Shift to Foundational Models:** The core technical evolution involves moving
  from training models solely on proprietary, labeled data to utilizing massive, pre-trained
  foundational models (like LLMs) that capture broad domain knowledge. This allows
  for better performance even with small, specific downstream datasets via **fine-tuning**.

  *   **Transformer Architecture Dominance:** The success of modern AI in life sciences
  (including molecular representation via SMILES strings or graphs) is rooted in the
  **Transformer architecture**, enabling models to generate novel outputs (molecules,
  sequences) based on learned embeddings and user-defined constraints (e.g., toxicity,
  solubility).

  *   **Domain-Specific vs. Generic GenAI:** While generic LLMs offer broad knowledge,
  the real value in R&D comes from developing or fine-tuning **domain-specific foundation
  models** trained on chemical, clinical trial, and biological data (like cell atlases)
  to provide deeper, more relevant insights into specific indications.


  ### 3. Business/Investment Angle

  *   **Scaling Gap:** Despite high investment interest (73% of leaders investing),
  scaling GenAI beyond pilots remains a major hurdle (<20% success), indicating significant
  organizational and cultural bottlenecks rather than purely technological limitations.

  *   **Data Dependency vs. Model Power:** While foundation models reduce the immediate
  need for massive proprietary datasets for every task, the quality and availability
  of high-throughput experimental data (e.g., single-cell RNA-seq) remain a limiting
  factor for training the most advanced domain-specific models.

  *   **ROI Evaluation:** A critical business challenge is establishing **domain-driven
  evaluation standards** and objective metrics to measure the ROI of AI outputs (like
  novel molecule suggestions) that cannot be instantly validated, unlike text or code
  generation.


  ### 4. Notable Companies/People

  *   **Xiong Liu (Novartis):** The featured expert, providing insights from the perspective
  of a major pharmaceutical company actively integrating GenAI into R&D.

  *   **Google (Transformer Architecture):** Mentioned as the originator of the foundational
  Transformer architecture that underpins modern LLMs.

  *   **Deloitte:** Referenced for a recent survey highlighting the gap between GenAI
  investment and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is moving toward a **hybrid development model**, leveraging publicly
  available academic foundation models while simultaneously building proprietary,
  fine-tuned versions tailored to specific company data and research needs. The future
  success of AI adoption hinges less on model architecture breakthroughs and more
  on **cross-disciplinary alignment**—ensuring seamless communication and validation
  loops between AI modelers, basic scientists (biologists, chemists), and leadership.
  The challenge of **hallucination** in biomedical contexts necessitates robust, domain-expert-driven
  evaluation frameworks to ensure scientific rigor.


  ### 6. Target Audience

  This episode is highly valuable for **Life Sciences R&D Executives, Chief Data Officers,
  AI/ML Directors in Pharma/Biotech, and Technology Strategy Leaders** navigating
  the operationalization and cultural integration of advanced AI technologies. It
  is targeted at professionals needing strategic, high-level technical context rather
  than deep engineering specifics.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 120
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 13
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 20:49:22 UTC -->
