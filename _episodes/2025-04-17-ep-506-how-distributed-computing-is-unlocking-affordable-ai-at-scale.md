---
companies:
- category: unknown
  confidence: medium
  context: This is the Everyday AI Show, the everyday podcast where we simplify AI
    and br
  name: Everyday AI Show
  position: 12
- category: unknown
  confidence: medium
  context: o boost your career, business, and everyday life. When ChatGPT first came
    out, no one was talking about compute,
  name: When ChatGPT
  position: 197
- category: unknown
  confidence: medium
  context: e dinner table with a bunch of dorks like myself. But I think even more
    so the last few months, as we've
  name: But I
  position: 526
- category: unknown
  confidence: medium
  context: pe you are too. What's going on, y'all? My name's Jordan Wilson, and this
    is Everyday AI. So this is your daily l
  name: Jordan Wilson
  position: 1407
- category: unknown
  confidence: medium
  context: g on, y'all? My name's Jordan Wilson, and this is Everyday AI. So this
    is your daily live stream podcast and fr
  name: Everyday AI
  position: 1434
- category: unknown
  confidence: medium
  context: e. So please help me welcome to the show. We have Tom Curry, the CEO and
    co-founder of Distribute AI. Tom, th
  name: Tom Curry
  position: 2610
- category: unknown
  confidence: medium
  context: how. We have Tom Curry, the CEO and co-founder of Distribute AI. Tom, thank
    you so much for joining the Everyday
  name: Distribute AI
  position: 2647
- category: tech
  confidence: high
  context: k for compute, right? And the big tech companies, OpenAI, whenever they
    roll out a new feature, a lot of t
  name: Openai
  position: 5934
- category: tech
  confidence: high
  context: s demand? Yeah, I mean, it's a crazy system where Anthropic has the same
    issue, right? We're constrained to t
  name: Anthropic
  position: 6202
- category: tech
  confidence: high
  context: nd of the day, OpenAI, I think they use primarily Nvidia for their data
    centers. But once again, Nvidia ha
  name: Nvidia
  position: 6503
- category: unknown
  confidence: medium
  context: an't allocate all their resources only to OpenAI. So OpenAI has certain
    search thresholds that they've run fr
  name: So OpenAI
  position: 6666
- category: unknown
  confidence: medium
  context: ta with the ability to kind of reason a lot more. So I do think we'll get
    there. I think the progress th
  name: So I
  position: 10627
- category: tech
  confidence: high
  context: of months, but I think you also have to call out Google, right? With their
    Gemma 3 model, which I believe
  name: Google
  position: 10993
- category: unknown
  confidence: medium
  context: e edge in the future, right? Like, I even saw the Nvidia GTX, right, formerly
    called Jetson. I did the math on
  name: Nvidia GTX
  position: 13369
- category: unknown
  confidence: medium
  context: be really interesting. I think you're 100% right. And I think five years
    might be a stretch. I think the
  name: And I
  position: 13811
- category: tech
  confidence: high
  context: now. But between whatever we're going to see from Meta in their next Llama
    model, we've already talked a
  name: Meta
  position: 15070
- category: unknown
  confidence: medium
  context: t. And the reality is, it's like whether you love Sam Altman or hate Sam
    Altman, he's pushed things forward or
  name: Sam Altman
  position: 17169
- category: unknown
  confidence: medium
  context: e're almost there already, to be honest with you. Like I said, I don't
    think we're that far away from the
  name: Like I
  position: 19103
- category: ai_application
  confidence: high
  context: Mentioned as a leader in proprietary models (ChatGPT, GPT-4o, GPT-4o mini)
    and a major consumer of GPUs, sometimes pausing signups due to compute constraints.
    They are also rumored to be releasing an open model.
  name: OpenAI
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The company of the guest, Tom Curry. They focus on creating a distributed
    AI app layer to find fair compute (leveraging idle computers globally) to offer
    more affordable AI services and create an open/accessible AI ecosystem.
  name: Distribute AI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned alongside OpenAI as a major player with proprietary models (Claude
    3) facing similar compute constraints and high operational costs.
  name: Anthropic
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the primary supplier of GPUs (H100, A100, H200) used by major
    players like OpenAI for their data centers. Their chip supply is a major constraint
    on the industry.
  name: Nvidia
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned for their AI models, specifically the Gemma 3 model, which is
    noted for outperforming much larger open-source models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An open-source model provider whose models (like DeepSeek V3) are frequently
    discussed and benchmarked against proprietary models. Their 7B parameter model
    is noted for being runnable on consumer-grade chips.
  name: DeepSeek
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in the context of upcoming open models, specifically their next
    Llama model.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the product that brought large language models into the mainstream,
    driving initial compute awareness.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a newer hybrid model alongside Gemini 2.5 Pro.
  name: Claude 3
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a newer hybrid model alongside Claude 3.
  name: Gemini 2.5 Pro
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Google's model (27B parameters) cited as significantly outperforming the
    much larger DeepSeek V3 model in Elo scores.
  name: Gemma 3
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a monster model reportedly 5-10 times larger than GPT-4.
  name: GPT-4o
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a smaller, more efficient model released by
    OpenAI.
  name: GPT-4o mini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used as a benchmark for comparison when discussing the size of GPT-4o.
  name: GPT-4
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced historically regarding OpenAI's past open model releases.
  name: GPT-2
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned in the context of edge computing hardware, noting its high cost
    five years ago.
  name: Jetson (Nvidia GTX)
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a model alongside DeepSeek and Llama.
  name: Gemma
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned in the context of platform monopolies for AI usage, specifically
    referencing Siri and iPhones.
  name: Apple (implied via Siri/iPhones)
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned alongside iPhone in the context of potential platform monopolies
    for AI usage.
  name: Google (implied via Android)
  source: llm_enhanced
date: 2025-04-17 13:00:00 +0000
duration: 22
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: edge computing, and how does edge computing impact compute need or GPU
    demand? Yeah, well, when we started this business, the reality was that although
    we wanted to convince ourselves that open source models were good, we were based
    on open source models being relatively bad. OpenAI was extremely dominant at that
    time. It was like you couldn't even believe that anything could catch up to OpenAI.
    Nowadays, we're probably running at like a one to two-month lag between parity
    of private source and open source models, which
  text: the future of edge computing, and how does edge computing impact compute need
    or GPU demand? Yeah, well, when we started this business, the reality was that
    although we wanted to convince ourselves that open source models were good, we
    were based on open source models being relatively bad. OpenAI was extremely dominant
    at that time. It was like you couldn't even believe that anything could catch
    up to OpenAI. Nowadays, we're probably running at like a one to two-month lag
    between parity of private source and open source models, which is really interesting.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/www.buzzsprout.com/2175779/episodes/16995436-ep-506-how-distributed-computing-is-unlocking-affordable-ai-at-scale.mp3
processing_date: 2025-10-06 13:43:11 +0000
quotes:
- length: 257
  relevance_score: 6
  text: Over the last few years as generative AI and large language models have become
    more prevalent, the concept of GPUs and compute has become almost a dinnertime
    conversation, at least if you're crowding around the dinner table with a bunch
    of dorks like myself
  topics: []
- length: 84
  relevance_score: 5
  text: So, I mean, five years ago, if you go back, gaming was the biggest use case
    for GPUs
  topics: []
- length: 112
  relevance_score: 4
  text: Are we going to have the average smartphone in five years be able to run a
    state-of-the-art large language model
  topics: []
- length: 216
  relevance_score: 4
  text: But the reality is that's one thing that decentralized type of providers offer
    like OpenAI is that they're able to work with a lot of data that would be very
    sensitive, primarily like health data and things like that
  topics: []
- length: 112
  relevance_score: 3
  text: The reality is, is that silicon as it stands today, one of our team members
    actually works on chips a little bit
  topics: []
- length: 142
  relevance_score: 3
  text: So although the models can get better, bigger, larger, more compute demand,
    the reality is, is that the technology is just not able to keep up
  topics: []
- length: 175
  relevance_score: 3
  text: And the big tech companies, OpenAI, whenever they roll out a new feature,
    a lot of times they're like, hey, our GPUs are melting, we're going to have to
    pause new user signups
  topics: []
- length: 77
  relevance_score: 3
  text: Why isn't that—even the biggest tech companies can't keep up with this demand
  topics: []
- length: 48
  relevance_score: 3
  text: But the reality is, there's just too much demand
  topics: []
- length: 150
  relevance_score: 3
  text: So the reality is, like I said, our compute, our power grid cannot possibly
    keep up the demand, and we don't have the latest gen chips, and not enough
  topics: []
- length: 186
  relevance_score: 3
  text: The reality is, is that a year ago, larger models, we were basically throwing
    a million different data points into models, which made the model much larger,
    and they were relatively good
  topics: []
- length: 119
  relevance_score: 3
  text: But the reality is, is that no one wants to run a 7 billion, you know, a 70
    billion, 700 billion parameter model, right
  topics: []
- length: 214
  relevance_score: 3
  text: But the reality is, is that although we're cutting the model size so that
    you can put it on a smaller chip, the reality is, you're still using a million
    tokens, which doesn't really actually help our compute issues
  topics: []
- length: 173
  relevance_score: 3
  text: And, you know, speaking of DeepSeek, I know it's been all the rage to talk
    about DeepSeek over the last couple of months, but I think you also have to call
    out Google, right
  topics: []
- length: 44
  relevance_score: 3
  text: The reality is, you can do that today, right
  topics: []
- length: 179
  relevance_score: 3
  text: But the reality is that right now we're running models that are very close
    to as good as what they have, and it's like, at some point, does the marginal
    gain isn't worth it, right
  topics: []
- length: 169
  relevance_score: 3
  text: When H100 has become a lot cheaper, we'll be able to run some of the biggest
    models very quickly and easily, and the access will just be so good that it might
    not matter
  topics: []
- length: 61
  relevance_score: 3
  text: The problem is that I personally do believe in private source
  topics: []
- length: 121
  relevance_score: 3
  text: And the reality is, it's like whether you love Sam Altman or hate Sam Altman,
    he's pushed things forward or locked, right
  topics: []
- length: 300
  relevance_score: 3
  text: Because aside from OpenAI's $200 a month pro subscription, which they also
    said they're losing money on, aside from that, how else are these big companies
    that so many people rely on going to continue to exist five, ten years after their
    $40 billion of funding might run out if they're not profitable
  topics:
  - funding
- length: 195
  relevance_score: 3
  text: But as we wrap up today's show, what's the one most important or the best
    piece of advice that you have for business leaders when it comes to making decisions
    about how they are using AI at scale
  topics: []
- impact_reason: Connects the rise of capable open-source models directly to increased
    compute necessity for SMEs, democratizing the need for dedicated infrastructure
    beyond just the tech giants.
  relevance_score: 10
  source: llm_enhanced
  text: as we've seen open source models really close the gap with proprietary and
    closed models, I think this concept of compute is even more important because
    now all of a sudden you have a lot of probably millions of companies throughout
    the world, medium-sized companies that maybe weren't concerned or weren't really
    paying attention to having their own compute maybe two years ago, now all of a
    sudden it might be a big priority...
  topic: Business/Adoption
- impact_reason: 'A critical technical bottleneck warning: current chip technology
    is peaking, and fundamental hardware innovation is still a decade away, exacerbating
    the current compute crunch.'
  relevance_score: 10
  source: llm_enhanced
  text: The reality is, is that silicon as it stands today... We're basically reaching
    the peak capacity of what we can do with chips, right? We're definitely stretching
    the current technology that we have for chips. So although the models can get
    better, bigger, larger, more compute demand, the reality is, is that the technology
    is just not able to keep up. We're about 10 years out from actually having new
    technology for chips.
  topic: Technical/Limitations
- impact_reason: 'A strong critique of CoT''s net impact on compute: while it improves
    output quality and potentially allows for smaller models, the underlying token
    consumption can negate the efficiency gains.'
  relevance_score: 10
  source: llm_enhanced
  text: Chain of thought basically enables you to give a better prompt... And it also
    might run through a bunch of tokens to give you a better output. So chain of thought
    is a really cool way to basically reduce the model size. But the reality is, although
    we're cutting the model size so that you can put it on a smaller chip, the reality
    is, you're still using a million tokens, which doesn't really actually help our
    compute issues. It's kind of bad. It's not worth it.
  topic: Technical/Critique
- impact_reason: A powerful data point demonstrating that parameter count is no longer
    the sole determinant of performance; architectural efficiency and training quality
    (as seen in Gemma 3 vs. DeepSeek V3) are paramount.
  relevance_score: 10
  source: llm_enhanced
  text: Google with their Gemma 3 model, which I believe is a 27 billion parameter,
    greatly outperformed DeepSeek V3, which is, I think, 600-plus billion parameter,
    at least when it comes to Elo scores. And it's not even close.
  topic: Technical/Breakthroughs
- impact_reason: Quantifies the dramatic acceleration of open-source catching up to
    proprietary models, shifting the competitive landscape significantly.
  relevance_score: 10
  source: llm_enhanced
  text: We were based on open source models being relatively bad. OpenAI was extremely
    dominant at that time. It was like you couldn't even believe that anything could
    catch up to OpenAI. Nowadays, we're probably running at like a one to two-month
    lag between parity of private source and open source models...
  topic: Trends/Competition
- impact_reason: Poses a critical, forward-looking question about the commoditization
    of high-end LLMs onto consumer hardware and its disruptive potential for the cloud
    industry.
  relevance_score: 10
  source: llm_enhanced
  text: Are we going to have the average smartphone in five years be able to run a
    state-of-the-art large language model? And if so, how does that change the whole
    cloud computing conversation?
  topic: predictions
- impact_reason: Highlights the inflection point where open source surpasses proprietary
    models, forcing a re-evaluation of compute strategy and business models.
  relevance_score: 10
  source: llm_enhanced
  text: what happens when and if open models are more powerful than closed and proprietary
    models? So number one, what happens from a GPU and compute perspective? But then
    how does that change the business leader's mindset as well?
  topic: strategy
- impact_reason: 'A clear economic prediction: when model capability is equalized
    (commoditized), the competitive advantage shifts entirely to infrastructure cost
    (compute).'
  relevance_score: 10
  source: llm_enhanced
  text: once things become commoditized, right, and the models are essentially all
    on the same level... the reality is that compute becomes the last denominator
    of basically being able to offer those models at the cheapest cost, right? So
    at that point, it basically comes down to a race to the bottom in terms of who
    can get the cheapest compute...
  topic: business
- impact_reason: 'This is the core business question for proprietary AI firms in a
    commoditizing environment: the value shifts entirely to integration, UX, and specialized
    applications.'
  relevance_score: 10
  source: llm_enhanced
  text: if large language models become commoditized because of open source models,
    is it just more of the application layer that becomes the differentiator for these
    companies?
  topic: business
- impact_reason: 'This is the single most important piece of actionable advice for
    business leaders regarding AI adoption: prioritize flexibility and avoid vendor
    lock-in due to the extreme pace of change.'
  relevance_score: 10
  source: llm_enhanced
  text: The last thing you can do is lock yourself into one specific provider or model.
    Don't allocate too many resources and sell the house on one specific setup because
    the next week something comes out and totally breaks everything before it, right?
  topic: strategy
- impact_reason: Highlights the massive shift in industry focus from pure model capability
    (ChatGPT launch) to the underlying infrastructure (compute/GPUs) required to run
    and scale these models.
  relevance_score: 9
  source: llm_enhanced
  text: When ChatGPT first came out, no one was talking about compute, right? Over
    the last few years as generative AI and large language models have become more
    prevalent, the concept of GPUs and compute has become almost a dinnertime conversation...
  topic: Strategy/Trends
- impact_reason: This is the core thesis of the discussion, pointing towards decentralized
    infrastructure as the solution to the compute bottleneck.
  relevance_score: 9
  source: llm_enhanced
  text: how distributed computing is unlocking affordable AI at scale.
  topic: Technical/Strategy
- impact_reason: Broadens the compute constraint beyond just hardware scarcity to
    include real-world physical infrastructure limitations (power grid).
  relevance_score: 9
  source: llm_enhanced
  text: We're running to the point where you're basically stretching the power grid,
    then you're stretching every resource that we have in the world to run these different
    models.
  topic: Safety/Limitations
- impact_reason: 'Explains a key trade-off: model size reduction (efficiency gain)
    is often offset by increased token usage in advanced reasoning techniques like
    Chain-of-Thought (compute cost).'
  relevance_score: 9
  source: llm_enhanced
  text: Chain of thought uses a ton of different tokens. So although the models are
    smaller, it still uses a ton of resources.
  topic: Technical/Efficiency
- impact_reason: Highlights the rapid advancement of small, open-source models (like
    DeepSeek 7B) making high capability accessible on consumer hardware (edge computing
    potential).
  relevance_score: 9
  source: llm_enhanced
  text: Even their 7 billion parameter model, which is relatively small, you could
    run on most consumer-grade chips. It's extremely good.
  topic: Technical/Edge Computing
- impact_reason: 'Provides a clear strategic division of labor for AI deployment:
    edge for daily tasks, cloud for complex/private tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: I think there is a huge argument for basically edge compute taking over a
    lot of the smaller daily tasks, and then reserving some of the more private models
    and things like that and the larger models for things that might be a little bit
    more deeper, like research and things like that.
  topic: Strategy/Deployment
- impact_reason: Identifies User Experience (UX) and education, rather than raw capability,
    as the current primary barrier to widespread local/edge AI adoption.
  relevance_score: 9
  source: llm_enhanced
  text: The reality is, you can do that today, right? We could probably be able to
    let me know in prompts that getting it from teaching people to basically use that
    and set it up, right? Takes time for people to learn how to install your own model
    and start running things. So it's more of like the UX more than anything.
  topic: Business/Adoption
- impact_reason: A vivid description of the future vision for personal devices (laptops,
    iPads) becoming powerful, localized AI hubs, emphasizing user control over model
    selection.
  relevance_score: 9
  source: llm_enhanced
  text: basically turning to this little tiny data center that allows you to run whatever
    model you want to run at that time.
  topic: predictions
- impact_reason: Directly links privacy concerns to the acceleration of edge computing
    adoption, suggesting privacy is the primary driver for decentralized AI.
  relevance_score: 9
  source: llm_enhanced
  text: If people are really worried about their privacy, then I think that people
    will push for edge compute to be running, and you'll be able to run your own model
    that only uses access to your own data on your phone device, whatever it is, right?
  topic: safety/business
- impact_reason: Describes the concept of a personalized, private 'local knowledge
    graph' model, a major goal for privacy-preserving AI applications.
  relevance_score: 9
  source: llm_enhanced
  text: essentially you're going to have your little database of information about
    yourself and your life and your wife and whatever else, and essentially you'll
    be able to run all that stuff without ever touching any centralized model for
    obvious reasons, privacy reasons, things like that.
  topic: safety
- impact_reason: A controversial but important assessment of the sustainability of
    current high-burn, general-purpose foundation model companies if open source commoditizes
    their core offering.
  relevance_score: 9
  source: llm_enhanced
  text: there is probably a world where essentially OpenAI and Anthropic eventually
    burn so much money... that they don't get to the point where they're looking to
    get to, and essentially they have to just either change business models or run
    out of money, right?
  topic: business
- impact_reason: A strong prediction about imminent user churn from centralized services
    as open-source alternatives become sufficiently capable and accessible.
  relevance_score: 9
  source: llm_enhanced
  text: I don't think we're that far away from the point where people are like, why
    do I let me just cancel OpenAI and don't use Llama? Like let me go cancel and
    use Jump, you know, with all these different models that are out there.
  topic: predictions
- impact_reason: 'A concise summary of the strategic imperative in the current AI
    environment: constant preparedness for paradigm shifts.'
  relevance_score: 9
  source: llm_enhanced
  text: be ready for someone to come out and completely break the mold and change
    the direction of everything.
  topic: strategy
- impact_reason: 'Pinpoints the current bottleneck for decentralized AI adoption:
    it''s not capability (the tech exists), but user experience (UX) and ease of setup.'
  relevance_score: 9
  source: llm_enhanced
  text: We're just really far away. The reality is, you can do that today, right?
    We could probably be able to let me know in prompts that getting it from teaching
    people to basically use that and set it up, right? Takes time for people to learn
    how to install your own model and start running things. So it's more of like the
    UX more than anything.
  topic: technical/business
- impact_reason: 'A fundamental economic principle applied to the AI race: when software
    becomes free/cheap, the cost of hardware/infrastructure dictates market pricing.'
  relevance_score: 9
  source: llm_enhanced
  text: Once things become commoditized... compute becomes the last denominator of
    basically being able to offer those models at the cheapest cost.
  topic: business
- impact_reason: 'Defines the core value proposition of distributed computing platforms:
    aggregating underutilized resources for cost-effective AI deployment.'
  relevance_score: 8
  source: llm_enhanced
  text: we're basically going around and can find fair compute. It can be your computer,
    it could be anyone's computer around the world. And we're basically leveraging
    that to create more affordable options for consumers, businesses, things like
    that, mid-level businesses.
  topic: Business/Technical
- impact_reason: Illustrates the dramatic shift in GPU demand drivers, moving from
    entertainment to foundational AI infrastructure.
  relevance_score: 8
  source: llm_enhanced
  text: Gaming was the biggest use case for GPUs. Nowadays, it's all about these models
    getting bigger.
  topic: Trends
- impact_reason: Provides concrete examples of the latency and resource intensity
    of generative tasks, explaining why demand outstrips supply even for top-tier
    hardware.
  relevance_score: 8
  source: llm_enhanced
  text: Image generation, these aren't like one-second returns, right? You're talking
    about 10, 20 seconds return. And video models are even worse. You're talking about
    minutes potentially, even on an H100, A100, or H200.
  topic: Technical/Deployment
- impact_reason: 'Defines a near-term milestone approaching AGI: the combination of
    strong reasoning with real-time external data access (RAG/browsing capabilities).'
  relevance_score: 8
  source: llm_enhanced
  text: What you really can find that with the ability to surf the internet and actually
    get more answers and use more data. That's where I think we'll get to. I wouldn't
    call it AGI, but we're very close to that where basically you're adding in real-time
    data with the ability to kind of reason a lot more.
  topic: Predictions/AGI
- impact_reason: Paints a vivid picture of the future of personal AI infrastructure
    driven by efficient local models.
  relevance_score: 8
  source: llm_enhanced
  text: Your house and your couple computers and things like that, maybe your laptop
    or iPad, basically turning to this little tiny data center that allows you to
    run whatever model you want to run at that time.
  topic: Predictions/Edge Computing
- impact_reason: Offers a balanced perspective on proprietary leaders like OpenAI,
    acknowledging their role in accelerating the entire ecosystem despite competitive
    concerns.
  relevance_score: 8
  source: llm_enhanced
  text: The problem is that I personally do believe in private source. I do believe
    that there are great use cases for it. And the reality is, it's like whether you
    love Sam Altman or hate Sam Altman, he's pushed things forward or locked, right?
    He's been really productive for the entire environment.
  topic: strategy
- impact_reason: 'Identifies the key remaining moat for centralized providers: handling
    highly sensitive, regulated data (like health/government) where trust in a known
    entity outweighs privacy concerns about local deployment.'
  relevance_score: 8
  source: llm_enhanced
  text: decentralized type of providers offer like OpenAI is that they're able to
    work with a lot of data that would be very sensitive, primarily like health data
    and things like that. So I'm sure there's a lot of very good business use cases
    that they can buy to very large enterprise consumers or non-consumer businesses.
  topic: business
- impact_reason: Reiterates the core technological question driving the shift toward
    localized AI processing.
  relevance_score: 8
  source: llm_enhanced
  text: I always think with these models becoming smaller, more capable, is will most
    things be edge in the future, right?
  topic: technical
- impact_reason: Provides a more aggressive timeline prediction than the initial 'five
    years' mentioned earlier, suggesting rapid hardware convergence with LLM needs.
  relevance_score: 8
  source: llm_enhanced
  text: I think you're 100% right. And I think five years might be a stretch [for
    smartphones running SOTA LLMs].
  topic: predictions
- impact_reason: Highlights the unexpected speed and success of the open-source AI
    movement over the last couple of years.
  relevance_score: 8
  source: llm_enhanced
  text: even the staunchest open source believers, would believe that we're at the
    point that we are now [regarding open source progress].
  topic: strategy
- impact_reason: Clarifies the unique value proposition of centralized, trusted providers
    (like OpenAI/Anthropic) in regulated industries, even if their general models
    are open-sourced.
  relevance_score: 8
  source: llm_enhanced
  text: The reality is that's one thing that decentralized type of providers offer
    like OpenAI is that they're able to work with a lot of data that would be very
    sensitive, primarily like health data and things like that.
  topic: business
- impact_reason: Describes the two-sided marketplace model for compute sharing (provider
    and consumer), a key business model in decentralized infrastructure.
  relevance_score: 7
  source: llm_enhanced
  text: You can provide on one side and you can also use them on the other side.
  topic: Business
- impact_reason: A strong statement reflecting growing public fatigue and resistance
    to further data sharing with centralized entities, fueling the demand for privacy-preserving
    AI.
  relevance_score: 7
  source: llm_enhanced
  text: We already give so much data to the big tech, right? I think we're good on
    giving any more and sharing any more into the details about our lives.
  topic: safety
source: Unknown Source
summary: '## Podcast Episode Summary: EP 506: How Distributed Computing is Unlocking
  Affordable AI at Scale


  This episode of the Everyday AI Show, featuring Tom Curry, CEO and co-founder of
  Distribute AI, centers on the rapidly evolving landscape of AI compute, driven by
  the proliferation of powerful open-source models and the critical need for affordable,
  scalable infrastructure. The discussion moves from the initial shock of generative
  AI compute demands to the strategic importance of distributed resources in democratizing
  access to large language models (LLMs).


  **Main Narrative Arc:**

  The conversation begins by establishing that compute—once an obscure topic—is now
  central to the AI discussion, especially as highly capable open-source models challenge
  proprietary leaders. Tom Curry introduces Distribute AI''s mission: creating an
  open and accessible AI ecosystem by leveraging distributed, often idle, computing
  resources globally. The discussion then dives into the technical paradox of model
  scaling (models getting simultaneously smaller/more efficient yet demanding more
  tokens via techniques like Chain-of-Thought) and the physical limitations of current
  silicon technology. Finally, the hosts explore the competitive dynamic between open
  and closed models, concluding with strategic advice for businesses navigating this
  volatile compute market.


  ---


  **1. Focus Area:**

  The primary focus is the intersection of **Distributed Computing** and **Affordable
  AI Infrastructure**. Specific topics covered include: the current global GPU shortage,
  the efficiency paradox in LLM scaling (e.g., Chain-of-Thought prompting), the competitive
  parity between open-source (e.g., DeepSeek, Gemma) and proprietary models, the potential
  rise of **Edge Computing** for privacy-sensitive tasks, and the long-term business
  viability of heavily funded, non-profitable AI giants.


  **2. Key Technical Insights:**

  *   **Compute Bottleneck:** Current silicon technology is stretching capacity, with
  projections suggesting new chip technology is about 10 years away, exacerbating
  the demand crunch caused by LLM inference (especially for long-context tasks like
  video generation).

  *   **Efficiency Paradox:** While models are becoming technically smaller (fewer
  parameters), techniques like Chain-of-Thought prompting increase token usage significantly,
  which does not necessarily alleviate the underlying compute strain.

  *   **Open Source Closing the Gap:** The lag time between proprietary model releases
  and open-source parity has shrunk to 1-2 months, making open models increasingly
  viable for many enterprise tasks.


  **3. Business/Investment Angle:**

  *   **Commoditization of Models:** If open models achieve parity or superiority,
  compute cost becomes the final differentiator, leading to a "race to the bottom"
  in pricing for infrastructure providers.

  *   **Proprietary Model Viability:** Large, cash-burning proprietary labs (like
  OpenAI/Anthropic) may need to pivot their business models away from general models
  toward specialized, high-value enterprise use cases (e.g., health data, government
  contracts) to justify their valuations.

  *   **Distributed Opportunity:** Companies like Distribute AI offer a two-sided
  marketplace: allowing entities with idle compute to monetize it while providing
  businesses with affordable access to necessary AI resources via APIs.


  **4. Notable Companies/People:**

  *   **Tom Curry (CEO, Distribute AI):** Guest expert explaining the distributed
  compute marketplace and the goal of an open AI ecosystem.

  *   **OpenAI (ChatGPT, GPT-4o):** Mentioned as the initial catalyst for compute
  awareness and a benchmark for proprietary performance.

  *   **Anthropic (Claude 3):** Cited as another major player constrained by current
  compute supply.

  *   **Google (Gemma 3):** Highlighted as a surprisingly powerful, smaller open model
  that outperformed much larger competitors (like DeepSeek V3) on human preference
  scores.

  *   **Nvidia:** Referenced as the primary supplier of data center GPUs, whose supply
  chain constraints affect all major AI players.


  **5. Future Implications:**

  The industry is moving toward a hybrid compute future. **Edge Computing** is predicted
  to handle many daily, privacy-sensitive AI tasks directly on user devices (smartphones,
  laptops) within the next five years, reducing reliance on centralized cloud providers
  for routine inference. The ultimate differentiator will shift from model quality
  (as models commoditize) to the efficiency of the underlying compute infrastructure
  and superior User Experience (UX).


  **6. Target Audience:**

  This episode is most valuable for **AI/ML Engineers, CTOs, Infrastructure Planners,
  and Business Leaders** focused on AI strategy, cost optimization, and vendor lock-in
  avoidance.


  ---


  **Comprehensive Summary:**


  The podcast episode tackles the critical issue of **AI compute scarcity and cost**,
  positioning distributed computing as a key solution for achieving affordable AI
  at scale. Host Jordan Wilson notes that compute has moved from a niche concern to
  a major economic and national security topic following the rise of generative AI.


  Tom Curry of Distribute AI explains his company’s model: creating a decentralized
  network by connecting idle computing resources globally. This two-sided platform
  allows users to contribute spare compute capacity and, conversely, access pooled
  resources affordably via APIs.


  A significant portion of the discussion addresses the **technical tension in model
  development**. While smaller, more efficient models exist (like GPT-4o mini), complex
  reasoning techniques (Chain-of-Thought) consume massive amounts of tokens, meaning
  the overall compute demand remains high. Furthermore, the physical limits of current
  silicon mean that demand is currently outstripping supply, leading to bottlenecks
  even for giants like OpenAI.


  The conversation highlights the **rapid maturation of open-source models**. The
  performance gap with proprietary models is closing quickly, exemplified by Google’s
  Gemma 3 significantly outperforming much larger models. This trend suggests a future
  where open models dominate many tasks, forcing proprietary companies to justify
  their high operational costs through specialized enterprise services (e.g., handling
  highly sensitive'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- startup
- investment
- openai
- anthropic
- nvidia
title: 'EP 506:  How Distributed Computing is Unlocking Affordable AI at Scale'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 67
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 12
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 6
  prominence: 0.6
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 13:43:11 UTC -->
