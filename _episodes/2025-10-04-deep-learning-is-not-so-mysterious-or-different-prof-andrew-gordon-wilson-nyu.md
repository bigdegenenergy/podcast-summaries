---
companies:
- category: unknown
  confidence: medium
  context: I'm Scott Hansen, host of NFL Red Zone. Lowes knows Sundays are fo
  name: Scott Hansen
  position: 4
- category: unknown
  confidence: medium
  context: I'm Scott Hansen, host of NFL Red Zone. Lowes knows Sundays are for football.
    That's why
  name: NFL Red Zone
  position: 26
- category: unknown
  confidence: medium
  context: o hire the people your company desperately needs? Use Indeed's sponsored
    jobs to hire top talent fast. And eve
  name: Use Indeed
  position: 629
- category: tech
  confidence: high
  context: ively well understood, both intuitively through a notion of soft inductive
    biases, but also formally in te
  name: Notion
  position: 1153
- category: unknown
  confidence: medium
  context: often not in the ways that people might believe. So I'm very excited to
    be talking to you both, even th
  name: So I
  position: 1995
- category: unknown
  confidence: medium
  context: ', knowing that there are so many people watching. But I think there are
    so many fundamental misconception'
  name: But I
  position: 2125
- category: unknown
  confidence: medium
  context: eks ago. It's been on Patreon for a little while. And I updated so much
    based on that interview. Andrew w
  name: And I
  position: 3384
- category: unknown
  confidence: medium
  context: rd that human data is kind of the dirty secret of Silicon Valley. Human
    data is the reason why these AI models wor
  name: Silicon Valley
  position: 3592
- category: tech
  confidence: high
  context: ason why these AI models work so well because the OpenAI and the Anthropic,
    what they do is they hire huma
  name: Openai
  position: 3678
- category: tech
  confidence: high
  context: AI models work so well because the OpenAI and the Anthropic, what they
    do is they hire humans to do things li
  name: Anthropic
  position: 3693
- category: unknown
  confidence: medium
  context: e description. And we are also sponsored by 2 for AI Labs. They are an
    incredible research lab based in Zur
  name: AI Labs
  position: 4241
- category: unknown
  confidence: medium
  context: els from scratch. They've got an amazing culture. And Benjamin Cruzier,
    the director, is also very interested in AI safe
  name: And Benjamin Cruzier
  position: 4657
- category: unknown
  confidence: medium
  context: eems like a fit for you, please get in touch with Benjamin Cruzier. Go
    to 2forlab.ai or look in the description. And
  name: Benjamin Cruzier
  position: 4849
- category: unknown
  confidence: medium
  context: is sponsored by CyberFund. Enjoy the show folks. What Andrew, much of your
    work challenges conventional wisdom
  name: What Andrew
  position: 4975
- category: unknown
  confidence: medium
  context: on of our beliefs to have those hard constraints. And B, because we see
    in practice that when we do have
  name: And B
  position: 10833
- category: unknown
  confidence: medium
  context: I was going to ask you, why are you a theory guy? Because I know you're
    a theory guy. Are you a theory guy or
  name: Because I
  position: 12207
- category: unknown
  confidence: medium
  context: Can you tell the audience about yourself? So I'm Andrew Wilson. I'm a professor
    at the Courant Institute of Math
  name: Andrew Wilson
  position: 15901
- category: unknown
  confidence: medium
  context: elf? So I'm Andrew Wilson. I'm a professor at the Courant Institute of
    Mathematical Sciences and Center for Data Scie
  name: Courant Institute
  position: 15939
- category: unknown
  confidence: medium
  context: lson. I'm a professor at the Courant Institute of Mathematical Sciences
    and Center for Data Science at New York Universit
  name: Mathematical Sciences
  position: 15960
- category: unknown
  confidence: medium
  context: Institute of Mathematical Sciences and Center for Data Science at New York
    University. My work focuses on having
  name: Data Science
  position: 15997
- category: unknown
  confidence: medium
  context: hematical Sciences and Center for Data Science at New York University.
    My work focuses on having a prescription for how
  name: New York University
  position: 16013
- category: unknown
  confidence: medium
  context: on. And observing that almost makes you paranoid. Now I really need to
    represent uncertainty because if I
  name: Now I
  position: 17439
- category: unknown
  confidence: medium
  context: rning can't be fully engaged with the real world. And Bayesian methods
    I think are a really great way of reasoni
  name: And Bayesian
  position: 17590
- category: unknown
  confidence: medium
  context: channel. But we had the previous channel as well. And Keith came to my
    wedding on Friday. It's good to have y
  name: And Keith
  position: 17935
- category: unknown
  confidence: medium
  context: tric deep learning. I interviewed, you know, like Michael Bronstein, Taco
    Hen, Zuen Brunner, Peta Village, Cavirch. T
  name: Michael Bronstein
  position: 18256
- category: unknown
  confidence: medium
  context: I interviewed, you know, like Michael Bronstein, Taco Hen, Zuen Brunner,
    Peta Village, Cavirch. They had th
  name: Taco Hen
  position: 18275
- category: unknown
  confidence: medium
  context: ewed, you know, like Michael Bronstein, Taco Hen, Zuen Brunner, Peta Village,
    Cavirch. They had this geometric d
  name: Zuen Brunner
  position: 18285
- category: unknown
  confidence: medium
  context: ', like Michael Bronstein, Taco Hen, Zuen Brunner, Peta Village, Cavirch.
    They had this geometric deep learning b'
  name: Peta Village
  position: 18299
- category: unknown
  confidence: medium
  context: oft constraints can be aligned. We move, we move. And Doug, here we have
    the Lemmings in their natural habit
  name: And Doug
  position: 23516
- category: unknown
  confidence: medium
  context: omize their core insurance and save hundreds with Liberty Mutual. Fascinating.
    It's accompanied by his natural all
  name: Liberty Mutual
  position: 23647
- category: unknown
  confidence: medium
  context: y Lemmings and Children's Company and Affiliates. Excludes Massachusetts.
    Calling hard seltzer lovers. Searching for the t
  name: Excludes Massachusetts
  position: 23976
- category: unknown
  confidence: medium
  context: can cause it to like spin out of control, right? Where I actually did need
    it to conserve energy and not h
  name: Where I
  position: 27490
- category: ai_application
  confidence: high
  context: Mentioned as a company that uses human data for data curation and evaluation
    in training AI models.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company that uses human data for data curation and evaluation
    in training AI models.
  name: Anthropic
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Sponsor conducting the first survey on how human data is being used in
    AI.
  name: Prolific
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: An incredible research lab based in Zurich, hiring research engineers,
    working on reasoning and the ARC challenge, with ambitions to build foundation
    models from scratch.
  name: 2 for AI Labs
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Andrew Wilson is a professor at the Courant Institute of Mathematical Sciences
    and Center for Data Science at NYU, focusing on intelligent systems and model
    construction principles.
  name: New York University
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Part of NYU, where Andrew Wilson is a professor focusing on mathematical
    sciences related to AI/ML.
  name: Courant Institute of Mathematical Sciences
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Part of NYU, where Andrew Wilson is a professor focusing on data science
    and model construction.
  name: Center for Data Science at New York University
  source: llm_enhanced
date: 2025-10-04 01:27:55 +0000
duration: 124
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: always be trying to do that because otherwise we're just preaching to
    the choir
  text: we should always be trying to do that because otherwise we're just preaching
    to the choir.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: change our model depending on how many data points we happen to have
    available
  text: we should change our model depending on how many data points we happen to
    have available.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: is because we should always honestly represent our beliefs
  text: we should is because we should always honestly represent our beliefs.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: think about model construction and algorithm design
  text: we should think about model construction and algorithm design.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: be making
  text: we should be making.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: have biases
  text: we should have biases.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: represent equivariant symmetries in scientific domains
  text: we should represent equivariant symmetries in scientific domains.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: encode it in our model
  text: we should encode it in our model.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: approach it, I think is fundamentally misguided
  text: we should approach it, I think is fundamentally misguided.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: the world, and collect even more learning. Absolutely beautiful. Andrew,
    we haven't even introduced you yet. Can you tell the audience about yourself?
    So I'm Andrew Wilson. I'm a professor at the Courant Institute of Mathematical
    Sciences and Center for Data Science at New York University. My work focuses on
    having a prescription for how to build intelligent systems, what are the key principles
    involved in model construction? I think although the field has made an extraordinary
    amount of empirical progress towards building more performant machine learning
    systems, we're still at early stages of understanding what principles should we
    broadly embrace when we're approaching our own problems. And so this involves
    work on understanding inductive biases, so what assumptions we should be making.
    And so this relates to symmetries, like equivariances, maybe we're modeling molecules,
    their rotation invariant, images could be translation invariant. How do we represent
    those invariances? How do we learn them automatically? How do we discover interpretable
    scientific structure in our data that tells us something may be surprising that
    we didn't know before that will go beyond a particular application? How do we
    represent uncertainty towards decision making? Arguably a prediction that's just
    a point estimate without any kind of error bars associated with it isn't really
    actionable in the real world. If you have an autonomous car and it says there's
    a stop sign, five feet ahead, plus or minus 10,000 feet, you can't really do anything
    with that information. But if it's plus or minus one foot, then you can really
    act on that information. And observing that almost makes you paranoid. Now I really
    need to represent uncertainty because if I don't have that uncertainty, then machine
    learning can't be fully engaged with the real world. And Bayesian methods I think
    are a really great way of reasoning about uncertainty. And so that also forms
    a big part of my research program. Well, welcome to MLST. We have Dr. Dugger in
    the house. The first time we've met in person for we've been doing this for eight
    years, five years on this channel. But we had the previous channel as well. And
    Keith came to my wedding on Friday. It's good to have you here, man. Yeah, it's
    pretty crazy. We haven't met until now. It's going to be a blast. Absolutely.
    Yeah, we've got some good stuff lined up. Well, I guess just to kick this off
    Andrew, I was inspired by geometric deep learning. I interviewed, you know, like
    Michael Bronstein, Taco Hen, Zuen Brunner, Peta Village, Cavirch. They had this
    geometric deep learning blueprint. Our video on that did half a million views.
    It was an amazing video. And the basic hypothesis
  text: the future of the world, and collect even more learning. Absolutely beautiful.
    Andrew, we haven't even introduced you yet. Can you tell the audience about yourself?
    So I'm Andrew Wilson. I'm a professor at the Courant Institute of Mathematical
    Sciences and Center for Data Science at New York University. My work focuses on
    having a prescription for how to build intelligent systems, what are the key principles
    involved in model construction? I think although the field has made an extraordinary
    amount of empirical progress towards building more performant machine learning
    systems, we're still at early stages of understanding what principles should we
    broadly embrace when we're approaching our own problems. And so this involves
    work on understanding inductive biases, so what assumptions we should be making.
    And so this relates to symmetries, like equivariances, maybe we're modeling molecules,
    their rotation invariant, images could be translation invariant. How do we represent
    those invariances? How do we learn them automatically? How do we discover interpretable
    scientific structure in our data that tells us something may be surprising that
    we didn't know before that will go beyond a particular application? How do we
    represent uncertainty towards decision making? Arguably a prediction that's just
    a point estimate without any kind of error bars associated with it isn't really
    actionable in the real world. If you have an autonomous car and it says there's
    a stop sign, five feet ahead, plus or minus 10,000 feet, you can't really do anything
    with that information. But if it's plus or minus one foot, then you can really
    act on that information. And observing that almost makes you paranoid. Now I really
    need to represent uncertainty because if I don't have that uncertainty, then machine
    learning can't be fully engaged with the real world. And Bayesian methods I think
    are a really great way of reasoning about uncertainty. And so that also forms
    a big part of my research program. Well, welcome to MLST. We have Dr. Dugger in
    the house. The first time we've met in person for we've been doing this for eight
    years, five years on this channel. But we had the previous channel as well. And
    Keith came to my wedding on Friday. It's good to have you here, man. Yeah, it's
    pretty crazy. We haven't met until now. It's going to be a blast. Absolutely.
    Yeah, we've got some good stuff lined up. Well, I guess just to kick this off
    Andrew, I was inspired by geometric deep learning. I interviewed, you know, like
    Michael Bronstein, Taco Hen, Zuen Brunner, Peta Village, Cavirch. They had this
    geometric deep learning blueprint. Our video on that did half a million views.
    It was an amazing video. And the basic hypothesis is as you have outlined the
    three curses in machine learning.
  type: prediction
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: high
  source: llm_enhanced
  text: ''
  type: general
- actionable: false
  confidence: medium
  source: llm_enhanced
  text: ''
  type: general
layout: episode
llm_enhanced: true
original_url: https://traffic.megaphone.fm/APO3101626167.mp3
processing_date: 2025-10-04 01:27:55 +0000
quotes:
- length: 193
  relevance_score: 6
  text: Human data is the reason why these AI models work so well because the OpenAI
    and the Anthropic, what they do is they hire humans to do things like data curation
    and evaluation and post-training
  topics:
  - valuation
- length: 156
  relevance_score: 3
  text: But I think there are so many fundamental misconceptions in the way that people
    understand generalization and model construction and artificial intelligence
  topics: []
- length: 149
  relevance_score: 3
  text: And I think this is one of the most important questions to understand right
    now is like, why does scale rigorously speaking produce a simplicity bias
  topics: []
- length: 67
  relevance_score: 3
  text: You have to give them some epsilon weight, otherwise they would die
  topics: []
- impact_reason: 'This highlights the key differentiator of deep learning compared
    to previous model classes: its broad applicability, which is a major trend in
    modern AI.'
  relevance_score: 9
  source: llm_enhanced
  text: I think deep learning really is distinguished in its relative universality,
    how broadly applicable it is relative to other model classes.
  topic: technical
- impact_reason: Representation learning is a core strength and defining feature of
    deep learning, making this a crucial insight into its success.
  relevance_score: 8
  source: llm_enhanced
  text: It also does representation learning incredibly effectively.
  topic: technical
- impact_reason: This directly challenges the conventional wisdom that larger models
    inherently lead to overfitting or complexity, suggesting scale can enforce simplicity
    (Occam's Razor).
  relevance_score: 10
  source: llm_enhanced
  text: It's completely fine to build a huge model that will also have a stronger
    bias for simple solutions, have more of an Occam's razor-like behavior than even
    smaller models.
  topic: technical
- impact_reason: This is a highly provocative statement that overturns a foundational
    concept in classical statistics and machine learning theory, suggesting modern
    deep learning circumvents this trade-off.
  relevance_score: 10
  source: llm_enhanced
  text: I think the bias variance trade-off is an incredible misnomer. There doesn't
    actually have to be a trade-off.
  topic: technical
- impact_reason: 'This exposes the ''dirty secret'' of leading AI labs: the critical,
    non-algorithmic role of high-quality human feedback (RLHF/data curation) in achieving
    state-of-the-art results.'
  relevance_score: 9
  source: llm_enhanced
  text: Human data is the reason why these AI models work so well because the OpenAI
    and the Anthropic, what they do is they hire humans to do things like data curation
    and evaluation and post-training. And there is a ridiculous uplift from using
    this human data.
  topic: business
- impact_reason: This offers a strategic insight into research stagnation, emphasizing
    the importance of challenging established dogma to unlock new progress in AI research.
  relevance_score: 7
  source: llm_enhanced
  text: So much progress has been stalled, I think by just getting stuck on misconceptions.
    Like once a certain number of people believe something, it's very, very hard to
    change their minds, no matter what you say.
  topic: strategy
- impact_reason: 'This identifies a key, under-recognized benefit of scale: enabling
    good generalization even with limited data, shifting the focus from mere flexibility
    to simplicity bias derived from scale.'
  relevance_score: 9
  source: llm_enhanced
  text: How we can have really large models that also generalize well, even when there's
    a small number of data points is something that is not very well recognized. And
    in fact, I think is one of the primary drivers of scale being important for achieving
    good generalization.
  topic: technical
- impact_reason: This provides strong philosophical and practical advice on model
    selection, arguing against data-dependent architectural changes in favor of models
    that honestly reflect underlying beliefs about the data generating process.
  relevance_score: 8
  source: llm_enhanced
  text: I think another misconception is this idea that we should change our model
    depending on how many data points we happen to have available. The reason I don't
    think we should is because we should always honestly represent our beliefs.
  topic: strategy
- impact_reason: This is a fundamental theoretical critique of model evaluation, suggesting
    that the functional space a model can explore (its inductive bias) is more important
    than its parameter count.
  relevance_score: 9
  source: llm_enhanced
  text: Parameter counting is a very bad proxy for model complexity. Really what we
    care about is the properties of this sort of induced distribution over functions
    rather than just how many parameters the model happens to have.
  topic: technical
- impact_reason: This articulates a long-term scientific vision for AI research, prioritizing
    fundamental understanding (like model selection principles) over ephemeral engineering
    solutions.
  relevance_score: 7
  source: llm_enhanced
  text: I'm really hoping to do research that will be relevant in hundreds of years
    from now. And so I think these questions around model selection, for example,
    in Occam's Razor, people will never stop asking.
  topic: strategy
- impact_reason: Highlights the gap between empirical success and fundamental theoretical
    understanding in ML, urging a focus on core principles.
  relevance_score: 9
  source: llm_enhanced
  text: I think although the field has made an extraordinary amount of empirical progress
    towards building more performant machine learning systems, we're still at early
    stages of understanding what principles should we broadly embrace when we're approaching
    our own problems.
  topic: strategy
- impact_reason: Directly addresses the importance of inductive biases, specifically
    symmetries/equivariances, in model construction, a key area in modern deep learning
    research.
  relevance_score: 10
  source: llm_enhanced
  text: This involves work on understanding inductive biases, so what assumptions
    we should be making. And so this relates to symmetries, like equivariances, maybe
    we're modeling molecules, their rotation invariant, images could be translation
    invariant.
  topic: technical
- impact_reason: Emphasizes the critical need for uncertainty quantification (error
    bars) for real-world, actionable ML predictions, especially in safety-critical
    domains like autonomous driving.
  relevance_score: 9
  source: llm_enhanced
  text: Arguably a prediction that's just a point estimate without any kind of error
    bars associated with it isn't really actionable in the real world.
  topic: safety
- impact_reason: Strong statement on the necessity of uncertainty representation for
    ML deployment and points towards Bayesian methods as a solution.
  relevance_score: 9
  source: llm_enhanced
  text: If I don't have that uncertainty, then machine learning can't be fully engaged
    with the real world. And Bayesian methods I think are a really great way of reasoning
    about uncertainty.
  topic: technical
- impact_reason: Clearly articulates the three major theoretical challenges (curses)
    in machine learning, providing a framework for understanding model limitations.
  relevance_score: 8
  source: llm_enhanced
  text: The basic hypothesis is as you have outlined the three curses in machine learning.
    Right. So there's the statistical curse, which is that you only have so many data
    points. And the distance to those data points, you know, or the density of those
    data points is kind of cursed by the dimensionality of your data. There's the
    optimization curse, which is that you get stuck in these local minima. And there's
    the approximation curse.
  topic: technical
- impact_reason: 'Sets up the central tension in the discussion: balancing the need
    for strong, known biases (like geometric constraints) with the desire for model
    expressiveness.'
  relevance_score: 7
  source: llm_enhanced
  text: I think your ideas are kind of tangentially related to that. You still think
    we should have biases. But you also think we can have our cake and eat it, so
    to speak.
  topic: strategy
- impact_reason: 'Offers a philosophical grounding for model building: honesty about
    real-world complexity combined with a simplicity bias (Occam''s razor).'
  relevance_score: 8
  source: llm_enhanced
  text: Another way to describe my philosophy for model construction is just honestly
    represent your beliefs. And we believe the real world is a complicated place.
    And if we combine that belief with the idea that simple solutions that are consistent
    with our observations are more likely to be true, then we can often see desirable
    behavior in quite a variety of different settings.
  topic: strategy
- impact_reason: A counter-intuitive and highly impactful insight suggesting that
    increasing model size (expressiveness) can paradoxically reinforce simplicity/generalization,
    linking to the double descent phenomenon.
  relevance_score: 10
  source: llm_enhanced
  text: Quite often, you can increase model expressiveness while simultaneously increasing
    its biases. So larger models are often more inclined towards simple solutions.
  topic: technical
- impact_reason: Directly challenges the foundational bias-variance trade-off concept,
    suggesting modern techniques (ensembles, large nets) allow achieving both low
    bias and low variance.
  relevance_score: 10
  source: llm_enhanced
  text: I think the bias variance trade-off is an incredible misnomer. There doesn't
    actually have to be a trade-off.
  topic: technical
- impact_reason: Reverses conventional wisdom on overfitting, arguing that building
    *bigger* models, when combined with simplicity bias, can alleviate overfitting,
    contrasting with the standard advice to constrain model size.
  relevance_score: 9
  source: llm_enhanced
  text: Whereas instead, I think if we just embrace the honest belief that there are
    many possible solutions, even if they're not probable for any given problem, combined
    with this sort of simplicity bias, we won't tend to overfit. And interestingly,
    the prescription is almost the opposite of what people think it perhaps should
    be in principle. Like build a smaller model is usually the prescription for avoiding
    overfitting.
  topic: predictions
source: Machine Learning Street Talk (MLST)
summary:
- key_takeaways:
  - Deep learning is mysterious, but often not in the ways commonly believed; its
    phenomena can be understood through concepts like soft inductive biases and rigorous
    generalization frameworks.
  - The classical bias-variance trade-off is a misnomer; it is possible to achieve
    both low bias and low variance through methods like ensembling or, surprisingly,
    by building very large neural networks.
  - Scale in deep learning appears to induce a simplicity bias, meaning larger models
    are often more inclined towards simpler, better-generalizing solutions, as evidenced
    by phenomena like double descent.
  - Parameter counting is a poor proxy for model complexity; the properties of the
    induced distribution over functions are more critical.
  - The best approach to model construction is to honestly represent beliefs, embracing
    expressiveness while incorporating simplicity biases (like Occam's Razor) as soft
    preferences rather than hard constraints.
  - Challenging conventional wisdom, such as the idea that model size should adapt
    to data availability, is crucial for scientific progress in AI research.
  - Understanding the 'why' behind model performance through scientific inquiry (theory
    and empiricism) leads to knowledge that outlives specific, rapidly obsolete engineering
    solutions.
  overview: Professor Andrew Gordon Wilson argues that the perceived mystery of deep
    learning often stems from fundamental misconceptions about generalization, model
    construction, and the bias-variance trade-off. He posits that deep learning's
    power lies in its relative universality and effective representation learning,
    often exhibiting a surprising simplicity bias even in highly over-parameterized
    models. This perspective suggests that embracing expressiveness alongside soft
    simplicity biases, rather than hard constraints, is a more principled approach
    to building robust AI systems.
  themes:
  - Demystifying Deep Learning
  - Inductive Biases and Model Construction
  - The Bias-Variance Trade-off Reconsidered
  - The Role of Scale and Over-parameterization (Double Descent)
  - Scientific Approach vs. Engineering Iteration
  - Honest Representation of Beliefs in Modeling
tags:
- artificial-intelligence
- ai-infrastructure
- investment
- openai
- anthropic
title: Deep Learning is Not So Mysterious or Different - Prof. Andrew Gordon Wilson
  (NYU)
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 83
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 4
  prominence: 0.4
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-04 01:27:55 UTC -->
