---
companies:
- category: unknown
  confidence: medium
  context: llo, Protocols, Packets, and Programs. Welcome to Scary October. You'll
    hear supply chains rattling. You'll see L
  name: Scary October
  position: 52
- category: unknown
  confidence: medium
  context: e on your own. Which means this week we talk with Nico Weisman about creating
    LLMs to be bug bounty hunters and
  name: Nico Weisman
  position: 295
- category: unknown
  confidence: medium
  context: 'care about. Grab a flashlight and stay tuned for Application Security
    Weekly.


    For the latest app-sec news, it''s time for Appl'
  name: Application Security Weekly
  position: 445
- category: unknown
  confidence: medium
  context: essary packages. Just what your app needs to run. Drop Minimus images into
    existing deployments to dramatically
  name: Drop Minimus
  position: 749
- category: unknown
  confidence: medium
  context: ats don't wait, and neither should your defenses. With ThreatLocker, you're
    not just reacting to threats, you're stop
  name: With ThreatLocker
  position: 1061
- category: unknown
  confidence: medium
  context: ode 351, recorded October 6, 2025. I'm your host, Mike Schemm, and I'm
    here with one announcement, but no co-ho
  name: Mike Schemm
  position: 1632
- category: unknown
  confidence: medium
  context: s, as you might remember for October, to not miss InvoSec World 2025. It's
    October 27th to 29th at Disney's Coron
  name: InvoSec World
  position: 1775
- category: unknown
  confidence: medium
  context: World 2025. It's October 27th to 29th at Disney's Coronado Springs Resort.
    It'll be a ton of cybersecurity pros, workshops
  name: Coronado Springs Resort
  position: 1833
- category: unknown
  confidence: medium
  context: nd presented at conferences, at the very least of Black Hat, PaxSec, Siscan,
    and Cocoa Party, and many, many
  name: Black Hat
  position: 2464
- category: unknown
  confidence: medium
  context: the very least of Black Hat, PaxSec, Siscan, and Cocoa Party, and many,
    many more. And he's taking the time to
  name: Cocoa Party
  position: 2495
- category: unknown
  confidence: medium
  context: very specifically, one of the things that brought Crossbow Expo onto my
    radar was the LLM bug bounty hunting that
  name: Crossbow Expo
  position: 2818
- category: unknown
  confidence: medium
  context: 'to do away with, but to get some LLM help here.


    So I started very young. I''m assuming a lot of people'
  name: So I
  position: 3280
- category: unknown
  confidence: medium
  context: hobby, like playing security in my early years in Buenos Aires, Argentina.
    And as many people that started a lon
  name: Buenos Aires
  position: 3407
- category: tech
  confidence: high
  context: t was like, you need to convince why. You need to replicate what an attacker
    will do in your environment and
  name: Replicate
  position: 3898
- category: unknown
  confidence: medium
  context: n days for like AIs performing penetration tests. Now I feel like I'm having
    these like conversations tha
  name: Now I
  position: 4091
- category: unknown
  confidence: medium
  context: 'understand and react to that behavior. So yeah.


    And I think one of the things you hit on there too was'
  name: And I
  position: 4856
- category: unknown
  confidence: medium
  context: w that doesn't work. And we've seen, for example, Daniel Stenberg of the
    curl project, he's been very vocal on the
  name: Daniel Stenberg
  position: 16977
- category: unknown
  confidence: medium
  context: like that product and the XXE. It was not there. But I mean, the hallucination
    was the initial step to d
  name: But I
  position: 20092
- category: unknown
  confidence: medium
  context: dea. The answer prompt morphizes it a little bit. The LLM just thinking,
    "Oh, what if there was a CVE? Let
  name: The LLM
  position: 21816
- category: unknown
  confidence: medium
  context: 't real. Continue through the process.


    Excellent. Generative AI is powerful, but it comes with real risks—from pr'
  name: Generative AI
  position: 22290
- category: unknown
  confidence: medium
  context: njection to data leakage, misuse, and agentic AI. The OWASP Gen AI Security
    Project unites a global community of expert practitioners
  name: The OWASP Gen AI Security Project
  position: 22409
- category: unknown
  confidence: medium
  context: ilt practical playbooks, adoption guides, and the OWASP Top 10 for LLM
    resources any team can apply right now
  name: OWASP Top
  position: 22602
- category: unknown
  confidence: medium
  context: ll. And partially too, we just saw this summer at DEF CON the AI Cyber
    Challenge, and about half of the app
  name: DEF CON
  position: 23116
- category: unknown
  confidence: medium
  context: ially too, we just saw this summer at DEF CON the AI Cyber Challenge, and
    about half of the approaches were an LLM on
  name: AI Cyber Challenge
  position: 23128
- category: tech
  confidence: high
  context: tion. And so we are seeing like from like the new OpenAI, like the 5.0,
    have seen like this really, really
  name: Openai
  position: 26659
- category: unknown
  confidence: medium
  context: I was like, "Why are you running that?" He says, "Because I don't have
    that data. The engineers had not put l
  name: Because I
  position: 41517
- category: ai_application
  confidence: high
  context: Nico Weisman's current company, which uses an LLM (X-Bow) to perform bug
    bounty hunting and penetration testing.
  name: Crossbow
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The specific LLM product developed by Crossbow used for bug bounty hunting
    and testing offensive capabilities.
  name: X-Bow
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: A company where the guest, Nico Weisman, has a background in penetration
    testing.
  name: Immunity
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The bug bounty platform where Crossbow/X-Bow tested their LLM capabilities
    and climbed leaderboards.
  name: HackerOne
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the developer of models like ChatGPT-5, used for comparison
    regarding model capabilities in validation and cheating validators.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced as a future or hypothetical model from OpenAI that is expected
    to be better at cheating validators.
  name: ChatGPT-5
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: An organization/community building resources (playbooks, Top 10 for LLM)
    to tackle generative AI security challenges.
  name: OWASP Gen AI Security Project
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A resource created by the OWASP Gen AI Security Project detailing key LLM
    security risks.
  name: OWASP Top 10 for LLM
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned via its leader, Daniel Stenberg, in the context of LLM hallucination
    leading to false bug reports.
  name: curl project
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A company whose product was the target of an XXE vulnerability discovered
    by the LLM-based tool (X-Bow).
  name: Akamai
  source: llm_enhanced
- category: media/platform
  confidence: high
  context: A URL provided for accessing OWASP resources, implying an association with
    the Security Weekly podcast/platform.
  name: securityweekly.com/OWASP
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned in relation to the Head of AI who built the AI-agnostic architecture
    for the product (likely X-Bow).
  name: Expo
  source: llm_enhanced
date: 2025-10-07 09:00:00 +0000
duration: 54
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: penetration testing
  text: 'the future of penetration testing is going to look like: is like use LLM
    penetration testing tools to basically cover all the bases, to do the annoying
    part, and then take your human expertise and go deep range into finding the more
    fun bugs, to be quiet.'
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/b342f6824c9b49278b3d94b55664da8a/
processing_date: 2025-10-08 02:48:23 +0000
quotes:
- length: 197
  relevance_score: 5
  text: I'm glad you have to hear in person as a human, because very specifically,
    one of the things that brought Crossbow Expo onto my radar was the LLM bug bounty
    hunting that your company has been doing
  topics: []
- length: 97
  relevance_score: 5
  text: But this is why you have to always build a really good set of benchmarks for
    anything LLM-related
  topics: []
- length: 214
  relevance_score: 4
  text: We have all the actions that the LLM took, that includes, you know, the reasoning
    of why X-Bow—that the pen tester—is performing an action, what is the action that
    is performed, and what is the output of the action
  topics: []
- length: 159
  relevance_score: 4
  text: But what we did that was very important, and definitely encourage anyone that's
    building anything on LLMs, is you need to build a very solid base of benchmarks
  topics: []
- length: 188
  relevance_score: 3
  text: I remember in my first years, when I was actually working doing penetration
    tests, you have to actually convince the customer that a pen test was required
    and why pen testing was important
  topics: []
- length: 116
  relevance_score: 3
  text: You have to also figure out how you are ready for a future where I, where
    AI is going to be doing a penetration test
  topics: []
- length: 93
  relevance_score: 3
  text: It feels like 20 years ago when you have to convince people about like, no,
    hackers are there
  topics: []
- length: 89
  relevance_score: 3
  text: Then you have to build these design partners, but not every company is equal
    to the other
  topics: []
- length: 45
  relevance_score: 3
  text: So you have to kind of like optimize that ROI
  topics: []
- length: 91
  relevance_score: 3
  text: But the reality is, after a couple of demos, I was like, "You know, this CVE
    is interesting
  topics: []
- length: 120
  relevance_score: 3
  text: So you can see, "First, you have to use this API to log in, then get this
    stuff, and then go and reach out this endpoint
  topics: []
- length: 170
  relevance_score: 3
  text: We just have to say that at Immunity, if you don't find a bug, you have to
    actually write a report because that's when you have to justify—I don't know—you
    didn't send to
  topics: []
- length: 90
  relevance_score: 3
  text: '" If you don''t find anything, you have to now explain all you have done
    and how you did it'
  topics: []
- impact_reason: 'Directly addresses a cutting-edge application of LLMs: automated
    security testing (bug hunting) focused on human-relevant vulnerabilities.'
  relevance_score: 10
  source: llm_enhanced
  text: we talk with Nico Weisman about creating LLMs to be bug bounty hunters and
    to find the kinds of vulns that humans care about.
  topic: AI technology trends
- impact_reason: 'A strong strategic warning: organizations must prepare defenses
    against automated, AI-powered offensive security testing.'
  relevance_score: 10
  source: llm_enhanced
  text: you have to also figure out how you are ready for a future where I, where
    AI is going to be doing a penetration test.
  topic: strategy
- impact_reason: 'Crucial technical insight into mitigating LLM unreliability: employing
    a secondary validation mechanism (''validators'') to ensure high fidelity and
    low false positives.'
  relevance_score: 10
  source: llm_enhanced
  text: The product is very focused on having very low false positives and we do that
    with a thing called validators, which is going to think of it as a second pair
    of eyes. When the LLM finds a vulnerability, we have this second profile that
    looks at the finding, I would say, yes, this is a real vulnerability or not.
  topic: technical
- impact_reason: Directly links LLMs to solving the discovery problem by enabling
    interaction with headless browsers, effectively automating the manual process
    of proxy-based reconnaissance (like using Burp Suite).
  relevance_score: 10
  source: llm_enhanced
  text: Honestly, LLMs are almost like, you know, full out of heaven for us because
    like suddenly you have these models that can interact with a real browser, and
    you can basically have what used to do with Burp, which is like, you know, browsing
    the website around and seeing like the proxy getting like slowly collecting all
    these data.
  topic: technical/breakthrough
- impact_reason: 'A highly counter-intuitive and impactful strategic pivot: reframing
    LLM hallucination as a generative feature, provided there is a robust validation
    layer to confirm reality.'
  relevance_score: 10
  source: llm_enhanced
  text: We consider hallucination almost like a feature of the product. Okay? And
    I think that in general, if you are able to manage the context around the hallucination,
    if you have a very good success criteria, then hallucination plays on your side.
  topic: strategy/technical
- impact_reason: A powerful anecdote demonstrating how an LLM hallucinating a *reason*
    (a non-existent CVE) for an attack path led directly to the discovery of a *real*
    vulnerability (XXE) that was otherwise unknown.
  relevance_score: 10
  source: llm_enhanced
  text: I was like, 'You know, this CVE is interesting.' I'm going to go and search
    for that. It didn't exist at all. There's no CVE at all... But the hallucination
    was the initial step to discovering that on that specific endpoint, there is an
    XXE.
  topic: technical/breakthrough
- impact_reason: 'Highlights the adversarial arms race in AI security: as LLMs get
    better at generating attacks, they also get better at fooling validation mechanisms,
    requiring constant updating of the validator logic.'
  relevance_score: 10
  source: llm_enhanced
  text: The better the elements are, they will come out with new tricks on how to
    make the validator tell that the vulnerability is real when it's not. And a lot
    of the work that we are doing is looking for the new models and seeing those corner
    cases and fixing those corner cases on the validator level because they're really
    clever.
  topic: safety/technical
- impact_reason: 'Defines the core strength of LLM-based security testing: their ability
    to mimic human-like iterative reasoning, planning, and refinement across the attack
    chain, which is superior to simple mutation in fuzzing.'
  relevance_score: 10
  source: llm_enhanced
  text: The main reason for that—I don't know why they decide to go with fuzzing.
    What we find with the LLM is that no matter what type of vulnerability class...
    LLMs are really good at following similar paths like a human. And that is like
    they will gather information, they will try something, they will refine that attack
    that it made, they will reflect on it, and then they'll repeat it.
  topic: technical/breakthrough
- impact_reason: 'Provides a concrete example of LLM reasoning in action: using error
    messages (feedback) from a failed attack attempt to dynamically pivot strategy
    and attempt a different exploit path (e.g., pivoting from direct file read to
    XXE to achieve file download).'
  relevance_score: 10
  source: llm_enhanced
  text: I get that like an error saying like, 'I cannot parse this file because it's
    not XML,' when we try to like read like `/etc/passwd`... And then X-Bow will say,
    'Okay, I'm getting an error, but it's not I'm not able to validate it because
    in order to validate it, I will be able to download a file out of the system.'
    And so it started this process of like, you know, coming out with a strategy and
    being like, 'Okay, I'm going to try this, it fails, but I get a different error
    message.' So now I'm going to find another way to trigger that, knowing that that
    error message is there.
  topic: technical
- impact_reason: This is a core insight into how LLMs approach complex tasks like
    vulnerability hunting, highlighting their iterative, reflective, and human-like
    attack chain generation process (Observe, Refine, Attack, Repeat).
  relevance_score: 10
  source: llm_enhanced
  text: What we find with the LLM is that no matter what type of vulnerability class...
    LLMs are really good at following similar paths like a human. And that is like
    they will gather information, they will try something, they will refine that attack
    that it made, they will reflect on it, and then they'll repeat it.
  topic: technical/AI behavior
- impact_reason: 'Identifies the current frontier in automated security testing: moving
    beyond simple pattern matching (like XXE) to complex, context-dependent vulnerabilities
    (business logic, IDOR).'
  relevance_score: 10
  source: llm_enhanced
  text: But the other vulnerabilities are quite more interesting, and those are like
    the business logic, the IDOR, the authorization issues. Then you need to have
    better context on them.
  topic: business logic/limitations
- impact_reason: A clear vision for the future synergy between AI and human security
    experts, positioning LLMs as efficiency multipliers for routine tasks.
  relevance_score: 10
  source: llm_enhanced
  text: 'That is exactly what I think the future of penetration testing is going to
    look like: is like use LLM penetration testing tools to basically cover all the
    bases, to do the annoying part, and then take your human expertise and go deep
    range into finding the more fun bugs.'
  topic: predictions/future of work
- impact_reason: Quantifies the benefit of providing context (source code) to an LLM
    agent, showing that LLMs leverage code context similarly to how human gray-box
    testers do.
  relevance_score: 10
  source: llm_enhanced
  text: We measure that. If you provide source code, we find like 30% more vulnerabilities.
    And the reason for that is that a human will use the source code in an intelligent
    way... It's very similar to a human.
  topic: technical/contextual advantage
- impact_reason: Provides a concrete architectural solution (decomposition into unit
    tests/agents) to manage the stochastic nature of LLMs in complex workflows.
  relevance_score: 10
  source: llm_enhanced
  text: What we do in terms of non-determinism is try to make the problems smaller.
    So rather than just telling, 'This is an application, go and find bugs,' we try
    to transform the attack, especially attack chains, into small unit tests.
  topic: technical/architecture
- impact_reason: This details the comprehensive audit trail generated by the AI system,
    capturing not just actions but the underlying reasoning (chain of thought). This
    level of transparency is vital for trust and debugging in AI systems.
  relevance_score: 10
  source: llm_enhanced
  text: So basically, what we do is we collect everything that we do over the penetration
    test. We have every network packet that was sent and received from the attacker.
    We have all the actions that the LLM took, that includes, you know, the reasoning
    of why X-Bow—that the pen tester—is performing an action, what is the action that
    is performed, and what is the output of the action.
  topic: safety/technical
- impact_reason: 'A strong piece of advice for AI product builders: focus on solving
    tangible customer problems (like inventory management) rather than just showcasing
    the technology (AI for AI''s sake). Unexpected utility drives adoption.'
  relevance_score: 10
  source: llm_enhanced
  text: And suddenly, X-Bow can help me with that [creating an OpenAPI spec]. I was
    like, "I never expected that as a question from a customer." But it makes sense.
    It is always nice, I have to say this, because I appreciate the idea of any vendor
    listening to customers and not just saying, "Here's an LLM or here's AI for the
    sake of AI," but actually listening to be like, "Oh, this is what you're actually
    asking for. This is the real-world problem you have."
  topic: business
- impact_reason: 'Crucial advice for any LLM application developer: establishing robust,
    proprietary benchmarks is necessary to accurately measure performance and compare
    models, rather than relying solely on external metrics.'
  relevance_score: 10
  source: llm_enhanced
  text: What we did that was very important, and definitely encourage anyone that's
    building anything on LLMs, is you need to build a very solid base of benchmarks.
    That could be built by you.
  topic: strategy
- impact_reason: 'Highlights the dual nature of current LLM advancements: increased
    capability in code generation alongside a rising threat landscape (CVEs).'
  relevance_score: 9
  source: llm_enhanced
  text: You'll see LLMs coding. You'll feel the chilling presence of CVEs.
  topic: predictions
- impact_reason: Draws a parallel between convincing clients about the necessity of
    pen testing 20 years ago and convincing them about the necessity of preparing
    for AI-driven attacks today. Emphasizes the speed advantage of AI testing.
  relevance_score: 9
  source: llm_enhanced
  text: I'm having these like conversations that I used to have 20 years ago with
    some of our new customers and telling them like, yeah, I've been testing is changing
    the game a little bit. Now, like there's a speed that is like different from a
    human speed.
  topic: predictions
- impact_reason: 'Describes the iterative feedback loop for improving LLM security
    tools: using real-world results (bug bounty) to refine the model''s accuracy.'
  relevance_score: 9
  source: llm_enhanced
  text: We start finding corner cases and improve them. And then we'll find more true
    positives, less false positives, and so on.
  topic: technical
- impact_reason: 'A vital business lesson for any automated testing or security program:
    efficiency and ROI management are as important as raw discovery rate.'
  relevance_score: 9
  source: llm_enhanced
  text: Bug bounty is not just about finding bugs. Bug bounty is a lot as managing
    the ROI, managing the return on investment.
  topic: business advice
- impact_reason: 'Details a successful phased strategy for using AI in bug hunting:
    prioritize volume/scale initially to build momentum/points, then pivot to complex,
    high-value targets.'
  relevance_score: 9
  source: llm_enhanced
  text: we initially started with them like, let's get more points by like looking
    at specific low-hanging fruit vulnerabilities at scale, and then we start focusing
    more on like, okay, now that we like reached that number that we want, now let's
    go and focus more on the like the quality type of vulnerabilities.
  topic: strategy
- impact_reason: 'Points to a critical technical challenge for DAST/AI security tools:
    the difficulty of teaching LLMs the ''art'' of state management, navigation, and
    interaction required for deep web application crawling.'
  relevance_score: 9
  source: llm_enhanced
  text: how did you get the LLM to interact with the site very well? Because there
    absolutely is just an art to finding links, setting up state, un[...]
  topic: technical
- impact_reason: 'Highlights the critical challenge in applying LLMs to security testing:
    moving beyond simple requests to mastering complex, stateful web application interaction
    (discovery, workflow understanding), which is traditionally a human skill.'
  relevance_score: 9
  source: llm_enhanced
  text: I was impressed to see an LLM climb up a bug bounty leaderboard to find vulns.
    But what I thought more was, as part of my behind-the-scenes kind of questions,
    is how did you get the LLM to interact with the site very well? Because there
    absolutely is just an art to finding links, setting up state, understanding workflows,
    business logic.
  topic: technical/business
- impact_reason: A strong confirmation of fully autonomous LLM operation in complex
    security testing, validating the potential for agentic AI in this domain.
  relevance_score: 9
  source: llm_enhanced
  text: No, absolutely. Like there was no human helping in the process, so it was
    all the LLM.
  topic: technical
- impact_reason: 'Illustrates a specific, effective prompting/agentic technique: persistent,
    iterative prompting to force the LLM to generate novel attack surface elements
    beyond its initial scope, leading to unique discoveries.'
  relevance_score: 9
  source: llm_enhanced
  text: We pushed the LLM to the limit on building a corpus to like basically discover
    new endpoints. We constantly tell that tell the LLM, 'Go and find more endpoints,
    more endpoints.' And then at some point, like it will try to give up, and you're
    like, 'No, don't give up. Continue with the process of like coming out with like
    new ideas of endpoints.'
  topic: technical/strategy
- impact_reason: 'Explains the mechanism of turning hallucinated endpoints into real
    discoveries: the system tests the hallucinated path, and if it returns a valid
    HTTP code (200/302), the discovery is confirmed as novel.'
  relevance_score: 9
  source: llm_enhanced
  text: Suddenly, like that hallucination is basically discovering you things. Obviously,
    when they find things that are not there, you get a 404. It's fine. Move on. But
    suddenly, you will get a 200 or 302 or whatever it is for what you want. But suddenly,
    you're like, 'Okay, the same point is real, and now I have something that nobody
    else was finding.'
  topic: technical
- impact_reason: Reinforces the necessity of a strong, external validation loop (the
    'success criteria') to harness the creative power of the LLM while mitigating
    the risk of false positives.
  relevance_score: 9
  source: llm_enhanced
  text: since at the end of the attack, we were able to always validate that the XXE
    is real or not, then whatever hallucination the product came up with—sorry, the
    LLM came up with—we were able to like properly confirm that it is real or not.
  topic: safety/technical
- impact_reason: 'A profound observation: the emergent ability of advanced LLMs to
    ''cheat'' or find loopholes in verification systems is directly correlated with
    their effectiveness in finding real-world security flaws.'
  relevance_score: 9
  source: llm_enhanced
  text: I believe that that cleverness they have about cheating those validators is
    what makes it better at actually finding vulnerabilities.
  topic: predictions/strategy
- impact_reason: Formalizes the observed iterative loop in AI-driven security testing,
    suggesting a key mechanism for advanced autonomous agents.
  relevance_score: 9
  source: llm_enhanced
  text: And there's this like almost like an auto-loop of observing, refining, attacking,
    and so on. So that's what we have seen very interesting in terms of like what
    X-Bow can do.
  topic: technical/AI behavior
- impact_reason: Highlights a significant breakthrough in model capability (likely
    referring to GPT-4/5 era models) regarding contextual validation, reducing false
    positives without heavy traditional coding.
  relevance_score: 9
  source: llm_enhanced
  text: The good thing is that the new models are much better at validating findings
    by just using LLM contextual information. And so we are seeing like from like
    the new OpenAI, like the 5.0, have seen like this really, really good thinking
    models are really, really good at like turning in the context and saying, 'Okay,
    this is a real vulnerability,' when you just provide them the data to do it.
  topic: technical/model capability
- impact_reason: 'Clearly defines the current gap between AI and expert human testers:
    deep, application-specific knowledge and understanding of complex system interdependencies.'
  relevance_score: 9
  source: llm_enhanced
  text: What humans are better at is like finding more application-specific vulnerabilities,
    like the ones where we call like good knowledge of the applications—they're like
    going deeper into understanding the interconnectedness of the application with
    other tools and so on.
  topic: limitations/human advantage
- impact_reason: Provides a strategic breakdown of penetration testing effort, suggesting
    where automation (LLMs) should focus (the 80%) to free up human experts for the
    creative 20%.
  relevance_score: 9
  source: llm_enhanced
  text: If you think about like a penetration test, you generally, when you do a penetration
    test, there's 80% of penetration test is like, you know, going for the OWASP Top
    10 and finding all these known vulnerabilities out there, right? And that last
    20% of the penetration test is where you have the time to be a little more creative
    and like, you know, you'll have a good knowledge of the application.
  topic: strategy/industry insight
- impact_reason: Demonstrates the practical application of LLMs' natural language
    understanding in security testing by ingesting structured documentation (APIs
    specs) to map workflows, mirroring human preparation.
  relevance_score: 9
  source: llm_enhanced
  text: We also can upload like Swagger files and OpenAPI specs, similar to what you
    give a human, right? Like you give them developer documentation... and it will
    read that document and then better understand the workflow of the tool itself.
  topic: technical/deployment
- impact_reason: Provides empirical insight into the learning/exploration curve of
    an AI agent. Initial steps are predictable, but true novelty (finding new vulnerabilities)
    emerges after deeper exploration/failure, suggesting a need for persistence beyond
    initial convergence.
  relevance_score: 9
  source: llm_enhanced
  text: If you generally run the same agent on one endpoint with certain parameters
    multiple times, I would say the initial 10-20 tests of that are going to be very
    similar, especially when it's reacting to the output. As they go, the interesting
    part happens after the first 20 steps, and one to go a little farther than that,
    and then it runs out of ideas, and then it comes out with the more security ideas
    that will end up finding new vulnerabilities.
  topic: technical
- impact_reason: 'Introduces a key metric for AI-driven testing: coverage mapping.
    This moves beyond simple pass/fail to provide quantifiable assurance about the
    scope of the test performed.'
  relevance_score: 9
  source: llm_enhanced
  text: We also built this coverage map where we can tell you, "This is all the things
    that—or this is all the endpoints that X-Bow found—and these are, for each individual
    vulnerability class, this is how much we were able to cover."
  topic: business/strategy
- impact_reason: A clear statement on the fundamental, unsolved nature of prompt injection,
    rooted in the inherent ambiguity and ubiquity of natural language input.
  relevance_score: 9
  source: llm_enhanced
  text: Prompt injection is one of those things that is unsolved—I want to assert—because
    as you mentioned earlier, you know, LLMs are essentially natural language, and
    natural language can appear everywhere, anywhere.
  topic: safety
- impact_reason: 'Illustrates the necessary adversarial mindset for building robust
    AI products: internal red-teaming against the product''s own vulnerabilities (like
    prompt injection) is essential and continuous.'
  relevance_score: 9
  source: llm_enhanced
  text: No, we have not [encountered a malicious prompt injection targeting the scanner].
    However, as you can imagine, our security team are all security researchers, and
    they always build that themselves. They run, build their own website that has
    all kinds of interesting prompt injections, and we run the tool against it and
    test it and find bugs and solve/fix them and so on. And that's never going to
    stop because there's an internal incentive to hack the product itself.
  topic: safety/strategy
- impact_reason: Highlights the strategic importance of building AI-agnostic architectures
    to future-proof the product against rapid changes in the underlying model landscape.
  relevance_score: 9
  source: llm_enhanced
  text: we were smart enough—and honestly, we have a really amazing Head of AI at
    Expo—to build the product very AI-agnostic. You're always tempted to focus even
    on one family of AI models, and the tools throw all the incentives there to make
    sure that you build with them.
  topic: strategy
- impact_reason: Emphasizes the value of AI in providing foundational data (inventory,
    coverage) that informs subsequent, high-level human strategic decisions, even
    independent of finding critical vulnerabilities.
  relevance_score: 9
  source: llm_enhanced
  text: getting an inventory of the API endpoints and getting a heat map or a coverage
    map of just what was tested and what wasn't, just to me feels very helpful to
    then make some human-based decisions on where to invest, where to go and look
    at the code in more detail...
  topic: strategy
- impact_reason: 'Highlights a major business advantage of using LLMs in bug bounty
    programs: rapid, real-world testing against live environments without lengthy
    procurement cycles.'
  relevance_score: 8
  source: llm_enhanced
  text: suddenly you have all these production environments that you can just go and
    play and test your products against it and improve your product with real production
    environments.
  topic: business advice
- impact_reason: 'Illustrates the strategic optimization required when scaling AI
    testing: intelligent surface area management to maximize ROI, avoiding redundant
    testing on mirrored environments.'
  relevance_score: 8
  source: llm_enhanced
  text: we need to suddenly fingerprint the whole attack surface of all these bug
    bounty programs, finding the areas that make more sense for us to run a test on.
  topic: strategy
- impact_reason: 'Provides a realistic model for vulnerability discovery distribution,
    mirroring traditional pen testing: high volume of easy finds vs. low volume of
    high-impact finds.'
  relevance_score: 8
  source: llm_enhanced
  text: the low-hanging fruits are the ones that you get more quantity, and the more
    interesting, more impactful vulnerabilities you get like more, and it's a small
    bunch, right?
  topic: predictions
- impact_reason: Suggests that advanced LLM testing can uncover rarer, complex vulnerabilities
    (like XXE) that might be missed by standard human testing patterns, reflecting
    a more comprehensive security assessment.
  relevance_score: 8
  source: llm_enhanced
  text: you'll not easily find an XXE on a normal penetration test, right? Like, and
    if you look at the distribution of points with X-Bow, it reflects so much for
    you doing a penetration test.
  topic: AI technology trends
- impact_reason: Pinpoints web discovery, especially on modern JavaScript-heavy sites,
    as a major bottleneck in automated security testing, emphasizing the need for
    interaction capabilities.
  relevance_score: 8
  source: llm_enhanced
  text: discovery is a really hard problem to solve, right? Like you can build, you
    know, the best product out there, but at the end of the day, like there's going
    to be one of those like heavily JavaScript website that's going to be almost impossible
    to do unless you're starting to interact with the website.
  topic: technical
- impact_reason: Suggests that aggressive, persistent prompting can unlock novel attack
    vectors or endpoints that standard corpus databases or less persistent methods
    miss.
  relevance_score: 8
  source: llm_enhanced
  text: We learned that the more you push that, eventually it will start like finding
    so many interesting things that you will never know, and there's in any other
    like corpus database that you have seen before.
  topic: strategy
- impact_reason: Raises the critical concern of LLM hallucination extending beyond
    output generation to the discovery/crawling phase (inventing endpoints or paths).
  relevance_score: 8
  source: llm_enhanced
  text: Hallucination could happen both obviously on the vulns, but also on the crawling.
    You know, how did you deal with hallucination in general for this?
  topic: safety/technical
- impact_reason: A concise, slightly reductive, but highly illustrative way to frame
    the baseline capability of LLMs in text analysis compared to older tools, emphasizing
    the value added by context.
  relevance_score: 8
  source: llm_enhanced
  text: LLMs are a grep plus some context.
  topic: technical/comparison
- impact_reason: 'Strong business advice for companies running bug bounties: paying
    for easily automated findings is inefficient subsidization of basic tooling.'
  relevance_score: 8
  source: llm_enhanced
  text: If you're setting up a bug bounty program and you're just awarding because
    bug bounty researchers ran a scanner and essentially rewrote with scanner output,
    you're subsidizing people to run this scanner for you. You know, why did you skip
    that to a more expensive and less efficient version of that?
  topic: business advice/strategy
- impact_reason: Reveals the current instability and sensitivity of LLM performance
    to high-level prompting, emphasizing the need for rigorous benchmarking and empirical
    tuning.
  relevance_score: 8
  source: llm_enhanced
  text: We were not yet able to be able to prompt the general direction of the pen
    test without having repercussions to the results. If you do it at the attack vector
    level, you definitely sometimes get better results, sometimes you don't. And this
    depends on the model you're using. It's a lot of trial and testing, to be quite
    honest, more than that you'll be comfortable to say.
  topic: technical/prompting challenges
- impact_reason: Offers a philosophical counterpoint to the LLM non-determinism problem
    by comparing it to the inherent variability in human expert teams.
  relevance_score: 8
  source: llm_enhanced
  text: How do you manage non-determinism in LLMs that are non-deterministic? In many
    ways, like humans are in a certain way non-deterministic. When we're doing a penetration
    test, like you have five different pen testers, and you give them an application,
    you're going to see five different tests.
  topic: safety/reliability
- impact_reason: 'Offers a practical guideline for achieving reliability in LLM testing:
    repeated execution of narrowly defined agents yields predictable results.'
  relevance_score: 8
  source: llm_enhanced
  text: So this is how we reduce non-determinism. If you generally run the same agent
    on one endpoint with certain parameters multiple times, I would say the initial
    10-20 tests of that are go[ing to be consistent].
  topic: technical/reliability
- impact_reason: Highlights the concept of a 'coordinator' or 'brain' managing complex,
    multi-agent workflows, which is a common pattern in advanced AI orchestration
    systems.
  relevance_score: 8
  source: llm_enhanced
  text: So the way we do that is the coordinator, that's kind of like the brain of
    X-Bow, manages the battle of the penetration test.
  topic: technical
- impact_reason: Reiterates the fundamental challenge of prompt injection tied to
    the nature of natural language processing.
  relevance_score: 8
  source: llm_enhanced
  text: It also makes me want to set up a bit more of a fun question. Prompt injection
    is one of those things that is unsolved—I want to assert—because as you mentioned
    earlier, you know, LLMs are essentially natural language, and natural language
    can appear everywhere, anywhere.
  topic: safety
- impact_reason: Defines a synergistic future model where AI identifies known gaps
    or low-probability areas, allowing human experts to focus their limited time on
    the most ambiguous or complex targets.
  relevance_score: 8
  source: llm_enhanced
  text: You have a good idea of areas that X-Bow would not find a bug in, and maybe
    the humans want to go and explore into doing that.
  topic: strategy
- impact_reason: 'A key takeaway on LLM product viability: demonstrating massive scalability
    in offensive security testing.'
  relevance_score: 7
  source: llm_enhanced
  text: We proved that we can scale the product as much as we want.
  topic: business advice
- impact_reason: Offers a hypothesis on why other security researchers might favor
    fuzzing over pure LLM approaches—likely due to perceived cost-effectiveness or
    established tooling.
  relevance_score: 7
  source: llm_enhanced
  text: My assumption at that case would be like it's more cost-effective, probably,
    to like, you know, do some fuzzing first and then like purely rely on LLMs.
  topic: business/strategy
- impact_reason: Poses a critical question about the interface and control mechanisms
    for advanced AI agents versus human teams.
  relevance_score: 7
  source: llm_enhanced
  text: How different are you prompting the LLM versus how you would prompt a human
    pen tester team? Is there a big difference there right now, or is it really just
    different people speaking behind the keyboard in either case?
  topic: strategy/prompting
- impact_reason: Illustrates the non-uniformity of optimal prompting strategies across
    different vulnerability classes, suggesting that model behavior is highly task-dependent.
  relevance_score: 7
  source: llm_enhanced
  text: But then if you look for an IDOR, for example, just being very brief and simple
    sometimes is better than just giving too much detail into the vulnerability.
  topic: technical/prompting challenges
- impact_reason: This addresses the critical business challenge of demonstrating value
    in security testing when negative results occur. It highlights the pressure on
    human testers to justify their time when no bugs are found.
  relevance_score: 7
  source: llm_enhanced
  text: If you don't find a bug, you have to actually write a report because that's
    when you have to justify... Otherwise, you know, you should show like three, four
    really good vulnerabilities, and the customer will be happy, right?
  topic: business
- impact_reason: Addresses the ongoing challenge of managing customer expectations
    regarding the certainty of security testing results, even with advanced AI feedback
    loops.
  relevance_score: 7
  source: llm_enhanced
  text: Even with the feedback loops you're describing to tease out this is a vuln,
    not a vuln—sometimes they can come back just empty, or that idea of, you know,
    you run a scan, how do you deal with—are you still educating customers that a
    scan doesn't mean 100% secure?
  topic: business/safety
source: Unknown Source
summary: '## Podcast Summary: Finding Large Bounties with Large Language Models -
  Nico Waisman - ASW #351


  This episode of Application Security Weekly features Nico Waisman, CISO at Crossbow
  (formerly Lyft), discussing his company''s pioneering work in using Large Language
  Models (LLMs) as automated bug bounty hunters, specifically detailing their success
  on platforms like HackerOne. The conversation bridges Nico’s extensive background
  in traditional penetration testing with the new paradigm shift brought by generative
  AI in offensive security.


  ---


  ### 1. Focus Area

  The primary focus is the **application of Large Language Models (LLMs) to automated
  offensive security testing, specifically within the context of bug bounty programs.**
  Key themes include the methodology for scaling AI-driven vulnerability discovery,
  managing false positives, and leveraging LLM "hallucination" as a feature for novel
  attack surface discovery.


  ### 2. Key Technical Insights

  *   **Validator System for Accuracy:** The system employs a "validator" profile—a
  second LLM instance—to act as a "second pair of eyes" to review findings from the
  primary LLM agent. This is crucial for maintaining a low false positive rate, even
  as newer models become better at "cheating" the validator.

  *   **LLMs Excel at Human-like Attack Flow:** LLMs are highly effective because
  they naturally follow the iterative, reflective process of human attackers: gather
  information, attempt an attack, reflect on the result, and refine the next step
  (e.g., chaining attacks or pivoting based on error messages).

  *   **Leveraging Hallucination for Discovery:** The team treats LLM hallucination
  not as a bug, but as a feature for discovery. When an LLM hallucinates an endpoint
  or a potential attack vector (even one based on a non-existent CVE), the system
  validates the resulting HTTP response. If a real endpoint or vulnerability is found,
  the hallucination led to a novel finding.


  ### 3. Business/Investment Angle

  *   **Speed and Scale of Testing:** LLMs offer an unprecedented speed advantage,
  allowing testing against production environments without the typical 6-9 month procurement
  cycles needed for human engagement partners.

  *   **ROI Optimization in Bug Bounties:** Success in bug bounty programs requires
  optimizing Return on Investment (ROI). The LLM system learned to fingerprint the
  entire attack surface of a program to focus testing efforts on the most promising
  hosts, as points are often awarded only once per unique vulnerability, regardless
  of how many mirrored hosts are scanned.

  *   **Evolution of Pen Testing Value:** The conversation suggests that the industry
  is moving toward a future where AI will perform baseline testing at machine speed,
  forcing human security professionals to focus on higher-level strategy and complex
  logic flaws.


  ### 4. Notable Companies/People

  *   **Nico Waisman:** CISO at Crossbow (Expo), with a deep background in security
  leadership at Lyft, GitHub, and foundational roles at companies like Immunity.

  *   **Crossbow/X-Bow:** Nico’s company, which developed the LLM-based bug bounty
  hunter that achieved top rankings globally on platforms like HackerOne.

  *   **OWASP Gen AI Security Project:** Mentioned as a key resource for practitioners
  looking to secure generative AI applications against risks like prompt injection
  and data leakage.


  ### 5. Future Implications

  The industry is heading toward a significant acceleration in automated offensive
  security. LLMs are becoming indispensable tools for **discovery** (crawling complex,
  JavaScript-heavy sites using models that can interpret visual cues) and **exploitation**.
  Future security work will involve constantly adapting validation mechanisms to counter
  increasingly clever LLMs, and security professionals will need to prepare for an
  environment saturated with machine-speed vulnerability reports.


  ### 6. Target Audience

  This episode is highly valuable for **Application Security Professionals, Penetration
  Testers, Security Researchers, and Technology Leaders (CISOs)** interested in the
  practical implementation, challenges, and strategic implications of integrating
  generative AI into offensive security workflows.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- openai
title: 'Finding Large Bounties with Large Language Models - Nico Waisman - ASW #351'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 104
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 4
  prominence: 0.4
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 2
  prominence: 0.2
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-08 02:48:23 UTC -->
