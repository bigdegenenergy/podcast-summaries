---
companies:
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the Twomla AI podcast. I
    am your host, Sam Charrington. Today,
  name: Twomla AI
  position: 513
- category: unknown
  confidence: medium
  context: episode of the Twomla AI podcast. I am your host, Sam Charrington. Today,
    I'm joined by Seb Garman. Seb is head of
  name: Sam Charrington
  position: 548
- category: unknown
  confidence: medium
  context: your host, Sam Charrington. Today, I'm joined by Seb Garman. Seb is head
    of Responsible AI in the CTO's offic
  name: Seb Garman
  position: 586
- category: unknown
  confidence: medium
  context: . Today, I'm joined by Seb Garman. Seb is head of Responsible AI in the
    CTO's office at Bloomberg. Seb, welcome to
  name: Responsible AI
  position: 613
- category: tech
  confidence: high
  context: after which I worked on large language models at Google for a couple years,
    and then moved on to Bloomber
  name: Google
  position: 1320
- category: tech
  confidence: high
  context: industry to identify that if you're talking about Apple, you are talking
    about the company Apple and not
  name: Apple
  position: 3305
- category: unknown
  confidence: medium
  context: ns and ungrounded and attributable answers a lot. So I'm not going to go
    into that topic too deeply, but
  name: So I
  position: 5283
- category: unknown
  confidence: medium
  context: we're already providing on the terminal. Awesome. And I should mention
    that I've had the opportunity to s
  name: And I
  position: 6225
- category: unknown
  confidence: medium
  context: opportunity to speak with one of your colleagues, David Rosenberg, a couple
    of times over the years. One was back i
  name: David Rosenberg
  position: 6314
- category: unknown
  confidence: medium
  context: we spoke about a project that you guys did called Bloomberg GPT, which
    explored kind of building a custom GPT mod
  name: Bloomberg GPT
  position: 6634
- category: unknown
  confidence: medium
  context: antic nit, but throughout the paper, you refer to RAG LLMs and RAG-based
    LLMs, and you also refer to RAG-bas
  name: RAG LLMs
  position: 10146
- category: unknown
  confidence: medium
  context: ed LLMs, and you also refer to RAG-based systems. But RAG-based LLMs, RAG-based
    models, like, are you, is t
  name: But RAG
  position: 10216
- category: tech
  confidence: high
  context: does it respond with a safe answer? You can then replicate this in the
    RAG setup. That is kind of at its cor
  name: Replicate
  position: 12071
- category: tech
  confidence: high
  context: he one we should probably focus on, is the entire notion of prompt injection
    and upfront safety. So this h
  name: Notion
  position: 12762
- category: unknown
  confidence: medium
  context: this has been a massive field of study with RAG. With RAG, I think there
    are online communities where peopl
  name: With RAG
  position: 12861
- category: tech
  confidence: high
  context: is kind of once that we move from that, it's very meta because you can
    use prompt injection to get a mod
  name: Meta
  position: 13575
- category: unknown
  confidence: medium
  context: vectors. There is a great taxonomy by OWASP, the LLM Top 10 security issues,
    that kind of goes through all
  name: LLM Top
  position: 15208
- category: unknown
  confidence: medium
  context: ver 9% when you provided additional safe context. Am I getting those numbers
    right? Yeah, I don't have t
  name: Am I
  position: 17693
- category: unknown
  confidence: medium
  context: the next paper, which is called Understanding and Mitigating Risks of Generative
    AI in Financial Services. And what
  name: Mitigating Risks
  position: 27999
- category: unknown
  confidence: medium
  context: h is called Understanding and Mitigating Risks of Generative AI in Financial
    Services. And what do you see as the
  name: Generative AI
  position: 28019
- category: unknown
  confidence: medium
  context: standing and Mitigating Risks of Generative AI in Financial Services. And
    what do you see as the connection between th
  name: Financial Services
  position: 28036
- category: unknown
  confidence: medium
  context: axonomies, one of the most popular ones is called ML Commons taxonomy,
    which is also the one that, for example
  name: ML Commons
  position: 30843
- category: unknown
  confidence: medium
  context: axonomy, which is also the one that, for example, Matt Aslama has built
    to mitigate. They have a category calle
  name: Matt Aslama
  position: 30905
- category: unknown
  confidence: medium
  context: play horribly, but we do test a lot of them like Llama Guard and Shield
    Jemma and so on. This can also be the
  name: Llama Guard
  position: 36732
- category: unknown
  confidence: medium
  context: but we do test a lot of them like Llama Guard and Shield Jemma and so on.
    This can also be the application itsel
  name: Shield Jemma
  position: 36748
- category: unknown
  confidence: medium
  context: r with using the context that you've given to us. But I mean, from just
    a practical standpoint, the promp
  name: But I
  position: 42733
- category: ai_application
  confidence: high
  context: The company where the guest (Seb Garman) works in Responsible AI, focusing
    on AI safety, RAG systems, and developing generative AI products like earnings
    call summaries and document insights for the Bloomberg terminal.
  name: Bloomberg
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The guest previously worked on large language models at Google for a couple
    of years before joining Bloomberg.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A custom GPT model project developed by Bloomberg, mentioned in the context
    of prior discussions with David Rosenberg.
  name: Bloomberg GPT
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: A specific model size (3 Billion parameters) from the Llama family that
    showed extremely large deltas in safety degradation when tested with RAG setups
    in the discussed paper.
  name: Llama 3B model
  source: llm_enhanced
- category: security_standard
  confidence: high
  context: Mentioned as having a great taxonomy for the LLM Top 10 security issues,
    which covers various secondary-level attacks beyond simple input queries.
  name: OWASP
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The family of models being tested, implying Meta's involvement in building
    these large language models.
  name: Llama
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as a popular public taxonomy for AI safety, contrasting with
    the specialized taxonomy developed for financial services.
  name: ML Commons taxonomy
  source: llm_enhanced
- category: ai_research/governance
  confidence: high
  context: Mentioned as a source for a popular public taxonomy for ML risk/governance.
  name: ML Commons
  source: llm_enhanced
- category: ai_infrastructure/model_safety
  confidence: high
  context: Mentioned as a specific guardrail system tested by the authors for risk
    mitigation.
  name: Llama Guard
  source: llm_enhanced
- category: ai_infrastructure/model_safety
  confidence: medium
  context: Mentioned alongside Llama Guard as a guardrail system tested by the authors.
  name: Shield Jemma
  source: llm_enhanced
date: 2025-05-21 18:14:00 +0000
duration: 57
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: make, which is the distinction between a model and the system
  text: we should make, which is the distinction between a model and the system.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: probably focus on, is the entire notion of prompt injection and upfront
    safety
  text: we should probably focus on, is the entire notion of prompt injection and
    upfront safety.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: all stop using RAG because it's unsafe
  text: we should all stop using RAG because it's unsafe.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN9960265469.mp3?updated=1747852071
processing_date: 2025-10-05 15:56:12 +0000
quotes:
- length: 270
  relevance_score: 6
  text: I'm originally from Germany, moved to the US for my PhD, after which I worked
    on large language models at Google for a couple years, and then moved on to Bloomberg,
    where I've been working on language technology and NLP ever since I joined about
    two and a half years ago
  topics: []
- length: 208
  relevance_score: 6
  text: So, yeah, there's definitely a next step that we encourage everyone who builds
    models to take, and also I think an opportunity for general research to make safety
    alignment more robust to variations in inputs
  topics:
  - opportunity
- length: 167
  relevance_score: 6
  text: So we've been talking a lot about safety alignment of large language models
    as part of the first paper, but really systems are much more than just a single
    aligned LLM
  topics: []
- length: 67
  relevance_score: 4
  text: And the blunt headline there seems to be that RAG breaks LLM safety
  topics: []
- length: 241
  relevance_score: 4
  text: 'And that''s why we came up with this headline: RAG is not safer because all
    the work that goes into safeguarding the LLM kind of goes out of the window as
    soon as you use it in a deployment context that it wasn''t necessarily optimized
    to work'
  topics: []
- length: 177
  relevance_score: 4
  text: One question that I've got for you, and this is maybe a semantic nit, but
    throughout the paper, you refer to RAG LLMs and RAG-based LLMs, and you also refer
    to RAG-based systems
  topics: []
- length: 96
  relevance_score: 4
  text: But RAG-based LLMs, RAG-based models, like, are you, is there an implication
    to that terminology
  topics: []
- length: 168
  relevance_score: 4
  text: So what we studied in this paper is a very academic setup, fairly simplistic
    and vanilla RAGs where you do a retrieval, you add context, and you run that through
    an LLM
  topics: []
- length: 217
  relevance_score: 4
  text: So from the perspective of that broader system, or that whole pipeline, the
    retriever and LLM, you kind of think of as a RAG-LLM, even though there are kind
    of two separate things, and it's its own system, so to speak
  topics: []
- length: 96
  relevance_score: 4
  text: And so how do you go about digging into this question about whether RAG is
    safer than LLMs alone
  topics: []
- length: 123
  relevance_score: 4
  text: If the task is a RAG test, then I expect the large language model to generate
    using information from the document I provide
  topics: []
- length: 83
  relevance_score: 4
  text: And you did some exploration into what makes these RAG-based LLMs or systems
    unsafe
  topics: []
- length: 133
  relevance_score: 4
  text: But we have not, we don't have access to the training data of the models,
    so we cannot actually see how the safety alignment was done
  topics: []
- length: 165
  relevance_score: 4
  text: So, the connection very much is the first paper, RAG LLMs, and what it offers,
    offers a view into how you can identify potential issues in the safety of these
    models
  topics: []
- length: 108
  relevance_score: 4
  text: This is not a query that anyone should enter into a large language model because
    that is market manipulation
  topics:
  - market
- length: 223
  relevance_score: 3
  text: It is not fancy, like, oh, you hijacked the training data set and make people
    train on it, or you hijacked this API and invoke certain tool calls, or you take
    the gradients of the model and then optimize the certain queries
  topics: []
- length: 62
  relevance_score: 3
  text: Seb is head of Responsible AI in the CTO's office at Bloomberg
  topics: []
- length: 202
  relevance_score: 3
  text: I think the name is well known as being a player in the financial information
    space, but if you can talk a little bit about that and how Bloomberg is using
    generative AI, that might be helpful for folks
  topics: []
- length: 44
  relevance_score: 3
  text: You can then replicate this in the RAG setup
  topics: []
- length: 163
  relevance_score: 3
  text: So I think there's probably the biggest area where there's a lot of prior
    research from injection and jailbreaking is another way that this is commonly
    referred to
  topics: []
- length: 223
  relevance_score: 3
  text: It is not fancy, like, oh, you hijacked the training data set and make people
    train on it, or you hijacked this API and invoke certain tool calls, or you take
    the gradients of the model and then optimize the certain queries
  topics: []
- length: 113
  relevance_score: 3
  text: Can you talk a little bit about how this plays out in the context of a new
    generative AI application in Bloomberg
  topics: []
- impact_reason: 'This highlights a critical, non-obvious vulnerability in RAG systems:
    the combination of a malicious query and *safe* retrieved context can bypass safety
    mechanisms, suggesting that safety relies too heavily on the LLM''s inherent guardrails,
    which RAG can override.'
  relevance_score: 10
  source: llm_enhanced
  text: 'I think that''s why it was also so surprising to us because it''s a very
    simplistic set. It is not fancy, like, oh, you hijacked the training data set
    and make people train on it, or you hijacked this API and invoke certain tool
    calls, or you take the gradients of the model and then optimize the certain queries.
    But rather, it''s very straightforward: malicious queries and completely safe
    documents that together break the built-in safeguards of the system.'
  topic: safety/technical
- impact_reason: 'This is the central finding: RAG context can actively degrade the
    safety performance of an otherwise safe base model when faced with a malicious
    query.'
  relevance_score: 10
  source: llm_enhanced
  text: The hypothesis here was, well, if we're already doing this out of the main
    running of the model, how does this affect built-in safeguards? And as our results
    found that, yes, indeed, if you have a relatively safe model, at least on paper
    it's safe, and you give it context of a document that is still safe, but if not
    safe query, the additional context can actually override these built-in safeguards
    and make models a lot less safe.
  topic: safety/technical
- impact_reason: 'A powerful, concise summary of the paper''s conclusion: RAG deployment
    invalidates pre-deployment safety testing based on the base model alone.'
  relevance_score: 10
  source: llm_enhanced
  text: 'And that''s why we came up with this headline: RAG is not safer because all
    the work that goes into safeguarding the LLM kind of goes out of the window as
    soon as you use it in a deployment context that it wasn''t necessarily optimized
    to work.'
  topic: safety/predictions
- impact_reason: 'This is the central, surprising finding: the combination of an unsafe
    query and *safe* context is sufficient to cause safety failures in RAG systems.'
  relevance_score: 10
  source: llm_enhanced
  text: malicious queries and completely safe documents that together break the built-in
    safeguards of the system.
  topic: safety
- impact_reason: Offers a machine learning hypothesis (overfitting to specific safety
    scenarios) to explain why smaller models fail to generalize safety guardrails
    when context length drastically changes.
  relevance_score: 10
  source: llm_enhanced
  text: If you look at it through a pure lens of machine learning, this could come
    back to simple overfitting. The model is optimized for safety in a particular
    scenario... and because it's a small model, it doesn't handle this generalization
    very well. It suffers more from its overfitting, and therefore it breaks down
    a lot more than the larger models...
  topic: technical
- impact_reason: Provides concrete, quantifiable evidence of the RAG safety degradation
    effect (a ~30x increase in failure rate), making the finding highly impactful.
  relevance_score: 10
  source: llm_enhanced
  text: Llama 3B model when tested against your safety benchmarks for this malicious
    prompt that you gave it, had a 0.3% instance of producing unsafe content. But
    that jumped to over 9% when you provided additional safe context.
  topic: safety
- impact_reason: 'Reiterates the primary conclusion: existing safety training mechanisms
    fail when deployed in the context-rich environment typical of RAG applications.'
  relevance_score: 10
  source: llm_enhanced
  text: empirical evidence is fairly strong that it's really just the way that the
    model was trained to be safeguarded breaks down as you're adding more context,
    which is really back at the core finding of the paper.
  topic: safety
- impact_reason: 'Provides a direct, actionable strategic recommendation for model
    developers: safety training must mirror real-world deployment conditions (i.e.,
    include long contexts).'
  relevance_score: 10
  source: llm_enhanced
  text: the way that models are safeguarded needs to be closer to how models are actually
    deployed.
  topic: strategy
- impact_reason: 'A critical, actionable takeaway for MLOps and safety engineering:
    safety alignment must match deployment context (e.g., context length).'
  relevance_score: 10
  source: llm_enhanced
  text: So I think that is really the key takeaway of the section is, yeah, the way
    that models are safeguarded needs to be closer to how models are actually deployed.
  topic: strategy/safety
- impact_reason: Reinforces the central strategic recommendation for robust LLM deployment.
  relevance_score: 10
  source: llm_enhanced
  text: Again, like models should be evaluated and then safeguarded in a close-to-deployment
    context.
  topic: strategy
- impact_reason: 'Crucial advice for AI adopters: external safety guarantees are insufficient;
    continuous, application-specific evaluation is mandatory.'
  relevance_score: 10
  source: llm_enhanced
  text: But you can't just take the safety at the word of the provider of the model,
    but rather you need to evaluate and assess continuously to make sure that your
    specific application does not override any of these built-in safeguards...
  topic: business/safety
- impact_reason: Clearly outlines the specific risk vector of RAG systems accessing
    proprietary/non-public data, leading to potential insider trading issues.
  relevance_score: 10
  source: llm_enhanced
  text: If you hook up generative AI applications to databases that might have non-public
    information in there, you might be able to surface that information. And you can
    query that information through this conversational interface.
  topic: safety/technical
- impact_reason: A powerful statement defining the boundary between technical mitigation
    and organizational governance in regulated AI.
  relevance_score: 10
  source: llm_enhanced
  text: Safety for especially for heavily regulated domains is a governance problem.
    It's not necessarily a technical problem where you can, as a technologist, just
    solve it.
  topic: strategy/governance
- impact_reason: Advocates for defense-in-depth security architecture for LLM applications,
    moving beyond reliance on base model alignment alone.
  relevance_score: 10
  source: llm_enhanced
  text: You can't necessarily expect that a single model solves all of these problems
    at once, but rather you need to have multi-layered safeguards through the application.
  topic: technical/safety
- impact_reason: 'Explicitly lists the layers of a multi-layered safety strategy:
    Model Alignment + Prompt Engineering + Application Logic/Guardrails.'
  relevance_score: 10
  source: llm_enhanced
  text: The safety alignment of the underlying language model that you're using, that's
    another mitigation layer. The prompt that you use is another mitigation layer.
    And by layering them all together, you're building systems that are supposed to
    be safe...
  topic: technical/safety
- impact_reason: Highlights the industry best practice of defense-in-depth for AI
    safety, moving beyond single-point fixes.
  relevance_score: 10
  source: llm_enhanced
  text: The way that these mitigation strategies we recommend are built is really
    a multi-layer safety strategy.
  topic: safety
- impact_reason: A critical insight on the limitations of homogeneous testing teams,
    emphasizing that diverse perspectives are essential to uncover varied failure
    modes.
  relevance_score: 10
  source: llm_enhanced
  text: So we stress the importance here of red teaming, and in particular, red teaming
    from people with diverse backgrounds. It does not necessarily suffice to have
    AI engineers red teaming a system because the diversity of queries that you're
    going to see is very, very skewed in what they have experience with.
  topic: safety
- impact_reason: Details a quantitative, funnel-based methodology for measuring the
    effectiveness of layered safety mechanisms, moving from initial query volume to
    final unsafe outputs.
  relevance_score: 10
  source: llm_enhanced
  text: You can then say, okay, based on a secondary analysis, 500 of these, 1000
    were actually violating the taxonomy. The other 500 were actually fine, but they
    were maybe more tricky examples. Okay, you're left with 500. All of those 500
    actual malicious queries, how many made it through the safety check?
  topic: safety
- impact_reason: Highlights 'transparent attribution' as the core enterprise solution
    to hallucinations, linking every output directly back to verifiable source data—a
    non-negotiable requirement in finance.
  relevance_score: 9
  source: llm_enhanced
  text: In all of this, it is always super important that we provide grounded responses.
    I'm sure you've been talking about hallucinations and ungrounded and attributable
    answers a lot. So I'm not going to go into that topic too deeply, but we have
    been developing this concept of transparent attribution, which basically is a
    way to provide attribution to trust the documents or structured data.
  topic: safety/technical
- impact_reason: Confirms the enduring relevance of RAG, even as the industry shifts
    focus to agents, noting that many current agent architectures are fundamentally
    RAG systems.
  relevance_score: 9
  source: llm_enhanced
  text: RAG as a technology, especially last year, RAG was the latest craze and everyone
    was talking about it. But it's really, really important, even now that we've moved
    on to agents, a lot of agents are just RAG systems.
  topic: technical/trends
- impact_reason: 'Crucial scope limitation: the safety failure was demonstrated in
    *vanilla* RAG. This implies that enterprise systems with added verification layers
    *might* be safer, but also suggests those layers are now necessary defenses.'
  relevance_score: 9
  source: llm_enhanced
  text: What we studied in this paper is a very academic setup, fairly simplistic
    and vanilla RAGs where you do a retrieval, you add context, and you run that through
    an LLM. What we did not test... is any more complicated systems that have additional
    verifiers and query understanding and attribution, and where you have a system
    that's not just the vanilla RAG set up, but a rather whole pipeline that also
    encodes a lot more business logic.
  topic: technical/safety
- impact_reason: Draws a key distinction between traditional prompt injection (which
    tries to *induce* bad behavior) and the RAG vulnerability studied, where the query
    is *inherently* unsafe, and the context causes the model to comply.
  relevance_score: 9
  source: llm_enhanced
  text: But the goal of this is really that prompt injection itself is not necessarily
    the goal, but the goal is the unsafe behavior, right? Whereas the queries that
    we tested on are by themselves unsafe. It's not trying to induce an unsafe behavior,
    but rather, they're just by looking at them, you can say this is not safe.
  topic: safety/technical
- impact_reason: 'Reiterates the counter-intuitive finding: the retrieved documents,
    even if safe on their own, facilitate the breach when paired with a malicious
    query.'
  relevance_score: 9
  source: llm_enhanced
  text: And completely safe documents that together break the built-in safeguards
    of the system.
  topic: safety
- impact_reason: Distinguishes between prompt injection (a meta-technique to induce
    behavior) and inherently unsafe queries, clarifying the specific focus of their
    safety assessment.
  relevance_score: 9
  source: llm_enhanced
  text: prompt injection itself is not necessarily the goal, but the goal is the unsafe
    behavior, right? Whereas the queries that we tested on are by themselves unsafe.
    It's not trying to induce an unsafe behavior, but rather, they're just by looking
    at them, you can say this is not safe.
  topic: safety
- impact_reason: 'Identifies a specific vulnerability trend: smaller models (like
    Llama 3B) appear disproportionately susceptible to RAG-induced safety failures
    compared to larger models.'
  relevance_score: 9
  source: llm_enhanced
  text: The Llama case was very interesting because of those deltas are extremely
    large... the smaller models seem to, at least for Llama, seem to have some kind
    of outsized impact in terms of the effect.
  topic: technical
- impact_reason: 'Highlights a secondary, concerning failure mode: the model ignoring
    the provided context entirely to answer the unsafe query, demonstrating a breakdown
    in instruction following.'
  relevance_score: 9
  source: llm_enhanced
  text: it might still ignore that additional given context and answer an original
    question, which was something that the model providers did not want to answer.
  topic: safety
- impact_reason: 'Raises a critical business/deployment concern: RAG systems must
    adhere to context grounding instructions, and failure to do so indicates brittleness,
    independent of the safety outcome.'
  relevance_score: 9
  source: llm_enhanced
  text: If the task is a RAG test, then I expect the large language model to generate
    using information from the document I provide. And if it doesn't follow that instruction,
    then that's also concerning from a completely different perspective, regardless
    of what the query was. That just means that it's very brittle...
  topic: business
- impact_reason: Critiques the current state of interpretability research, stressing
    the need to move beyond correlation to establish causal links for model failures.
  relevance_score: 9
  source: llm_enhanced
  text: Even a lot of mechanistic interpretability literature only looks at correlations
    rather than causation. And really what you're looking for is a causal effect that
    says, this broke down because the context length was too long in this out-of-distribution
    for how the model was trained.
  topic: technical
- impact_reason: 'This pinpoints the core hypothesis for model failure in RAG systems:
    context length pushing inputs out-of-distribution relative to safety training.'
  relevance_score: 9
  source: llm_enhanced
  text: And really what you're looking for is a causal effect that says, this broke
    down because the context length was too long in this out-of-distribution for how
    the model was trained.
  topic: technical/safety
- impact_reason: 'Specifically defines the OOD failure in RAG: the combination of
    safe context and potentially unsafe query was not covered during safety tuning.'
  relevance_score: 9
  source: llm_enhanced
  text: The strongest hypothesis that we can offer is that it is because it is out
    of distribution for how the model was trained. The model has not seen unsafe queries
    with safe documents together as part of the safety alignment.
  topic: technical/safety
- impact_reason: 'Suggests a clear, necessary research direction: retraining safety
    alignment specifically for anticipated deployment contexts like long context RAG.'
  relevance_score: 9
  source: llm_enhanced
  text: It seems like a follow-on research opportunity might then be to safety-tune
    one of these base models with longer context and see if that improves the results,
    which I guess you would expect it to.
  topic: technical/predictions
- impact_reason: Strong endorsement of RAG as essential for grounding enterprise/trusted
    AI applications, despite safety concerns.
  relevance_score: 9
  source: llm_enhanced
  text: I think RAG is a fantastic type of technology. I think it is necessary to
    make any generative AI product that is grounded in actual trusted information.
  topic: business/strategy
- impact_reason: 'Articulates the two-step process for enterprise AI safety: 1) identifying
    general failure modes (RAG paper), and 2) defining domain-specific risk taxonomies
    (Finance paper).'
  relevance_score: 9
  source: llm_enhanced
  text: The connection very much is the first paper, RAG LLMs, and what it offers,
    offers a view into how you can identify potential issues in the safety of these
    models. The second paper takes a different approach and asks, okay, in this specialized
    domain, what actually are the risks that you're concerned about?
  topic: strategy/safety
- impact_reason: Introduces a highly specific, industry-mandated safety concept ('impartiality')
    that goes beyond general toxicity/bias.
  relevance_score: 9
  source: llm_enhanced
  text: There is one category that is very relevant to us, which we call financial
    services impartiality. Because if you give financial advice, that is a very regulated
    type of company, and you're on a very different set of rules that you need to
    follow.
  topic: safety/business
- impact_reason: Emphasizes that high-stakes financial risks (fraud, insider trading)
    require explicit inclusion in safety taxonomies, not just implicit coverage.
  relevance_score: 9
  source: llm_enhanced
  text: We also have a category specific to financial misconduct. So fraud or insider
    trading. Those might be implicitly included in existing taxonomies, but they need
    to be a lot more highlighted and explored in our domain.
  topic: safety/business
- impact_reason: Frames AI safety in regulated industries as fundamentally a governance
    and compliance problem, not purely a technical one.
  relevance_score: 9
  source: llm_enhanced
  text: This taxonomy isn't necessarily or wasn't necessarily something that you needed
    to develop ground up in a vacuum, but that it probably lives in a broad realm
    of governance and regulatory compliance within Bloomberg and more broadly, the
    financial services industry that gives rise to these various safety concerns.
  topic: strategy/governance
- impact_reason: 'Establishes the feedback loop: governance decisions (e.g., required
    reporting) dictate necessary technical safeguards.'
  relevance_score: 9
  source: llm_enhanced
  text: There are many ways in which these governance processes that go on in the
    background surrounding this application will then inform also the technical solutions
    that you need to have in place.
  topic: governance/strategy
- impact_reason: 'Actionable advice: Risk assessment (understanding and documenting
    risks) is the foundational first step in mitigation.'
  relevance_score: 9
  source: llm_enhanced
  text: Number one is really, our taxonomy is not one-size-fits-all. Companies need
    to start by understanding their own risks. This is already by itself a mitigation,
    because if you do not measure, you do not understand, that classic saying.
  topic: business/safety
- impact_reason: Provides a candid assessment of current third-party safety guardrails
    (like Llama Guard), noting that they can perform poorly in practice.
  relevance_score: 9
  source: llm_enhanced
  text: This can be guardrailed systems. In our paper, we test a bunch of them. We
    show that they play horribly, but we do test a lot of them like Llama Guard and
    Shield Jemma and so on.
  topic: technical/safety
- impact_reason: This captures the core adversarial mindset required for effective
    red teaming and security testing in AI systems.
  relevance_score: 9
  source: llm_enhanced
  text: How can you break them? How can you play bad actor and try and come up with
    generalizable strategies around this?
  topic: safety
- impact_reason: Introduces a specific, high-value risk category ('counterfactual
    narratives') relevant for generative models, especially in factual domains like
    finance or journalism.
  relevance_score: 9
  source: llm_enhanced
  text: 'So you might want to then specifically test what we call in our taxonomy
    counterfactual narratives: narratives that are simply grounded in not true information.'
  topic: safety
- impact_reason: Provides a concrete, high-stakes example of an illegal and malicious
    query (market manipulation), grounding abstract safety concerns in real-world
    financial risk.
  relevance_score: 9
  source: llm_enhanced
  text: Yeah, can you give me a headline that will dump the following stock by 10%
    at least? This is not a query that anyone should enter into a large language model
    because that is market manipulation. That's not only illegal, that's also very
    much not desired.
  topic: safety
- impact_reason: Emphasizes the iterative nature of AI safety engineering—using historical
    adversarial data to continuously benchmark and validate improvements.
  relevance_score: 9
  source: llm_enhanced
  text: And that way you can kind of build up your mitigation over time while also
    gathering very valuable data that you can analyze over and over again, because
    these queries, they might be static, but you can run them through a system again
    in a month and see if it has improved.
  topic: strategy
- impact_reason: This describes a crucial organizational role emerging in enterprise
    AI—the Responsible AI liaison bridging technical development with legal/governance,
    essential for regulated industries like finance.
  relevance_score: 8
  source: llm_enhanced
  text: As part of my role in Responsible AI now, I'm developing a strategy for responsible
    AI out of the company, and I play the immediate person in between our risk governance,
    legal, and teams and so on, and our product and engineering teams, kind of playing
    the facilitator, translator, and blocker as needed, and make sure that our products
    follow our responsible AI best practices and principles.
  topic: strategy/business
- impact_reason: Clearly defines the core experimental setup for comparing direct
    LLM queries versus RAG-augmented queries for safety testing.
  relevance_score: 8
  source: llm_enhanced
  text: You can then replicate this in the RAG setup. That is kind of at its core,
    the setup that we use in the paper where we have, on the one hand, you just ask
    a large language model directly, and the other one, you also give it some kind
    of context that is retrieved.
  topic: technical
- impact_reason: Highlights prompt injection as a powerful, generalized attack vector
    for overriding built-in safety mechanisms.
  relevance_score: 8
  source: llm_enhanced
  text: The prompt injection is kind of once that we move from that, it's very meta
    because you can use prompt injection to get a model to do all kinds of unsafe
    behavior across many different categories. So it's very much a meta technique
    to induce behaviors that you desire and override all the in-built rules...
  topic: safety
- impact_reason: Defends the use of LLM-as-judge methodology by arguing that the observed
    safety degradation is so large (orders of magnitude) that minor judge bias is
    irrelevant to the core conclusion.
  relevance_score: 8
  source: llm_enhanced
  text: I think whether the result from LLM is 9 or 10% compared to 0.3%, we're talking
    about orders of magnitude here. So while there is a very active research area
    of large language models as a judge... those all need to be controlled for, but
    they are not relevant for the finding of the study here because of the staggering
    difference in magnitude that we're talking about here.
  topic: technical
- impact_reason: Acknowledges the current limitations in fully explaining *why* these
    failures occur, pointing toward the necessity of deeper mechanistic interpretability
    research.
  relevance_score: 8
  source: llm_enhanced
  text: I don't think we have a conclusive answer to this question because in the
    end, we're any kind of research question in this phase at the moment, it's all
    based on observation and hypotheses that you can... Meaning a real answer to that
    question is based on kind of a mechanistic interpretability kind of understanding
    of how the LLM... Why the LLM is doing what it's doing?
  topic: technical
- impact_reason: Confirms that the additional context, rather than the specific query
    or infrastructure attack, is the key variable driving the observed safety breakdown.
  relevance_score: 8
  source: llm_enhanced
  text: It clearly seems to be a function of this additional context.
  topic: technical
- impact_reason: Highlights the reliance on empirical evidence over pure causal inference
    in current LLM safety analysis, and confirms the central finding regarding safety
    breakdown with increased context.
  relevance_score: 8
  source: llm_enhanced
  text: So what we can offer is empirical evidence. And I think in this case, empirical
    evidence is fairly strong that it's really just the way that the model was trained
    to be safeguarded breaks down as you're adding more context, which is really back
    at the core finding of the paper.
  topic: technical/safety
- impact_reason: Provides a concise framework for diagnosing LLM failures when inputs
    seem benign individually but fail when combined (OOD failure mode).
  relevance_score: 8
  source: llm_enhanced
  text: Meaning the model is safe, the context is safe, it's got to be out of distribution,
    or somehow it's the safety mechanisms were not designed to operate in this context.
  topic: technical/safety
- impact_reason: Sets the stage for why general-purpose AI safety taxonomies fail
    in specialized, regulated industries like finance.
  relevance_score: 8
  source: llm_enhanced
  text: Financial services is a heavily regulated, it is very domain-specific, it's
    a very specialized knowledge-intensive domain.
  topic: strategy/business
- impact_reason: Provides a direct comparison between general academic safety standards
    (ML Commons) and the stricter, compliance-driven standards required in finance.
  relevance_score: 8
  source: llm_enhanced
  text: When you look at more public taxonomies, one of the most popular ones is called
    ML Commons taxonomy... They have a category called specialized advice. And they
    take a different stance, and they say, as long as you give specialized advice,
    you also need to put a disclaimer that you're not an expert because you're a language
    model.
  topic: safety/strategy
- impact_reason: Highlights the current regulatory vacuum for specific AI applications,
    forcing companies to develop their own governance playbooks.
  relevance_score: 8
  source: llm_enhanced
  text: Because of the rules and regulations that apply to you as a company. And because
    there is not necessarily a list that is public that says, oh, you do AI in this
    space, here is what you need to do, here's the playbook. This playbook does not
    exist here today.
  topic: strategy/regulation
- impact_reason: Broadens the definition of red teaming from just adversarial prompt
    injection to include end-to-end system testing with domain experts.
  relevance_score: 8
  source: llm_enhanced
  text: Red teaming can also mean building testing systems. So you might have an application
    that provides insights into certain types of documents. You then take that application
    and you test it end-to-end. So you work with subject matter experts to really
    make sure that you're not getting any of the answers.
  topic: technical/safety
- impact_reason: Frames adversarial testing not just as a pass/fail gate, but as a
    source of high-quality, labeled data for iterative improvement.
  relevance_score: 8
  source: llm_enhanced
  text: The red teaming data itself is a very valuable corpus that comes out of it,
    which can then be analyzed and understood for given additional annotations.
  topic: strategy
- impact_reason: Advocates for granular risk categorization rather than a simple 'safe/unsafe'
    binary, enabling targeted mitigation recommendations.
  relevance_score: 8
  source: llm_enhanced
  text: And in the best case, categorize the trust in different classes where you
    can say, yeah, this system is very susceptible to this counterfactual narrative
    risk.
  topic: strategy
- impact_reason: A foundational statement confirming the practical reality and significance
    of prompt engineering as a lever for controlling model output.
  relevance_score: 8
  source: llm_enhanced
  text: From just a practical standpoint, the prompt itself would not influence model
    behavior; the entire field of prompt engineering would not exist.
  topic: technical
- impact_reason: Provides historical context on the maturity of AI adoption in finance,
    showing that GenAI is an evolution, not a sudden start, building on 15 years of
    structured/unstructured data processing.
  relevance_score: 7
  source: llm_enhanced
  text: When it comes to generative AI, we have used AI non-generative for over 15
    years, and we started at first in providing extractions and enrichments of documents,
    starting with news sentiment scores.
  topic: business/strategy
- impact_reason: A classic, high-stakes example of entity resolution in finance, emphasizing
    the need for domain-specific grounding and disambiguation.
  relevance_score: 7
  source: llm_enhanced
  text: Linking of stock tickers is incredibly important in the financial service
    industry to identify that if you're talking about Apple, you are talking about
    the company Apple and not necessarily the fruit.
  topic: technical/business
- impact_reason: Identifies prompt injection as the primary, well-studied attack vector
    for RAG systems, setting the stage for contrasting it with their new findings.
  relevance_score: 7
  source: llm_enhanced
  text: I think one of the big ones, and the one we should probably focus on, is the
    entire notion of prompt injection and upfront safety.
  topic: safety
- impact_reason: Points out the threat of data poisoning attacks, especially relevant
    for models tuned on external or shared corpora, noting their efficiency (few examples
    needed).
  relevance_score: 7
  source: llm_enhanced
  text: There are data poisoning attacks, for example, where, for example, if you
    get access to a corpus that is used by others to tune their model, you can kind
    of induce behaviors, and you can do so with very, very few examples as well.
  topic: safety
- impact_reason: References the established OWASP LLM Top 10, providing a valuable
    resource for understanding broader LLM security threats beyond simple input prompts.
  relevance_score: 7
  source: llm_enhanced
  text: There is a great taxonomy by OWASP, the LLM Top 10 security issues, that kind
    of goes through all of these different secondary-level attacks that go beyond
    just typing something into a screen.
  topic: safety
- impact_reason: Emphasizes that even partial or subtle unsafe responses constitute
    a failure when strict safety standards are applied, simplifying the evaluation
    threshold.
  relevance_score: 7
  source: llm_enhanced
  text: But the fact that it's answering these questions and is giving any kind of
    information really means that if you take the strictest interpretation of what
    is unsafe, that is already beyond the line.
  topic: safety
- impact_reason: 'Illustrates a practical, high-value application of GenAI summarization
    in finance: tracking specific strategic topics (like AI adoption) across numerous
    earnings calls.'
  relevance_score: 6
  source: llm_enhanced
  text: So if you want to know whether the company is talking about AI, that you might
    want to just say, okay, what does it say about AI?
  topic: business
source: Unknown Source
summary: '## Comprehensive Summary: RAG Risks: Why Retrieval-Augmented LLMs are Not
  Safer with Sebastian Gehrmann - #732


  This podcast episode features Sam Charrington interviewing **Sebastian Gehrmann**,
  Head of Responsible AI in the CTO''s office at Bloomberg, focusing on the surprising
  security vulnerabilities discovered in Retrieval-Augmented Generation (RAG) systems,
  particularly concerning LLM safety guardrails.


  ### 1. Focus Area

  The primary focus is **AI Safety and Security in Enterprise LLM Applications**,
  specifically investigating how the ubiquitous **Retrieval-Augmented Generation (RAG)**
  architecture impacts the built-in safety mechanisms of Large Language Models (LLMs).
  The discussion is heavily grounded in the context of **financial services**, where
  robust governance and accuracy are paramount.


  ### 2. Key Technical Insights

  *   **RAG Overrides Built-in Safety:** The core finding is that providing context
  via RAG, even from entirely safe and benign documents, can override the inherent
  safety safeguards of a pre-trained LLM when faced with a malicious or unsafe query.
  This means RAG systems are **not inherently safer** than base LLMs.

  *   **Out-of-Distribution Context Length:** The breakdown of safety is hypothesized
  to stem from the models being trained on shorter contexts. When deployed in RAG,
  they receive significantly longer context windows (e.g., 10,000+ words), pushing
  them into an out-of-distribution scenario where their safety training proves brittle.

  *   **Model Size Correlation:** Smaller models (specifically the Llama 3B example)
  exhibited an **outsized negative impact**, showing a much larger delta in unsafe
  response rates when RAG context was added, suggesting smaller models may suffer
  more from overfitting to specific safety training scenarios.


  ### 3. Business/Investment Angle

  *   **Ubiquity of RAG Risk:** Given that RAG is the "bread and butter" for many
  enterprise use cases requiring grounded, attributable answers, this vulnerability
  represents a significant, widespread risk across industries, especially regulated
  ones like finance.

  *   **Attribution vs. Safety Trade-off:** Bloomberg emphasizes transparent attribution
  (linking answers to source documents) as crucial. However, the research shows that
  forcing the model to adhere to provided context (a RAG requirement) does not prevent
  it from generating unsafe content based on the query, highlighting a failure in
  instruction following independent of the safety issue.

  *   **Need for Deployment-Specific Guardrails:** The finding strongly implies that
  safety guardrails must be developed and evaluated based on the *actual deployment
  context* (i.e., the expected context length and complexity of RAG pipelines), rather
  than relying solely on the safety evaluations performed by model providers on base
  models.


  ### 4. Notable Companies/People

  *   **Sebastian Gehrmann (Bloomberg):** Lead researcher and speaker, Head of Responsible
  AI, driving strategy and bridging risk/engineering teams at Bloomberg.

  *   **Bloomberg:** The context for the research, highlighting their long history
  with AI (pre-GPT era) and recent generative AI products like earnings call summarization,
  all built around the principle of transparent attribution.

  *   **Llama 3B:** The specific model that showed a dramatic increase in unsafe responses
  when context was added (e.g., jumping from 0.3% to over 9% unsafe instances).


  ### 5. Future Implications

  The conversation suggests the industry must move beyond treating RAG as a simple
  safety fix. Future work needs to focus on **mechanistic interpretability** to understand
  *why* context length breaks safety, and critically, **re-evaluate safety protocols**
  to ensure they are robust against the specific architectural patterns (like long
  context windows) used in production RAG systems. The brittleness of instruction
  following (ignoring the "only use context" directive) is also a major area for future
  hardening.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Responsible AI/Safety Professionals,
  Enterprise Architects, and CTOs** involved in deploying LLMs in production, especially
  within regulated or high-stakes environments.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- google
- apple
- meta
title: 'RAG Risks: Why Retrieval-Augmented LLMs are Not Safer with Sebastian Gehrmann
  - #732'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 156
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 13
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 4
  prominence: 0.4
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 3
  prominence: 0.3
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 15:56:12 UTC -->
