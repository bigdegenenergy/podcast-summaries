---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: This is episode number 879 with Dr. Greg Michelson, co-founder of ZERV.
    Today's episode is brought t
  name: Greg Michelson
  position: 36
- category: unknown
  confidence: medium
  context: der of ZERV. Today's episode is brought to you by Trainium II, the latest
    AI chip from AWS, and by the Dell AI
  name: Trainium II
  position: 109
- category: unknown
  confidence: medium
  context: inium II, the latest AI chip from AWS, and by the Dell AI Factory with
    Nvidia. Welcome to the Super Data Science po
  name: Dell AI Factory
  position: 162
- category: tech
  confidence: high
  context: AI chip from AWS, and by the Dell AI Factory with Nvidia. Welcome to the
    Super Data Science podcast, the m
  name: Nvidia
  position: 183
- category: unknown
  confidence: medium
  context: y the Dell AI Factory with Nvidia. Welcome to the Super Data Science podcast,
    the most listened-to podcast in the data
  name: Super Data Science
  position: 206
- category: unknown
  confidence: medium
  context: sforming our world for the better. I'm your host, John Kron. Thanks for
    joining me today. And now, let's make
  name: John Kron
  position: 493
- category: tech
  confidence: high
  context: er a year ago. He previously spent seven years as DataRobot's Chief Customer
    Officer and four years as Senior
  name: Datarobot
  position: 939
- category: unknown
  confidence: medium
  context: o. He previously spent seven years as DataRobot's Chief Customer Officer
    and four years as Senior Director of Analytics an
  name: Chief Customer Officer
  position: 951
- category: unknown
  confidence: medium
  context: aRobot's Chief Customer Officer and four years as Senior Director of Analytics
    and Research for Travelers Insurance
  name: Senior Director
  position: 992
- category: unknown
  confidence: medium
  context: as Senior Director of Analytics and Research for Travelers Insurance. He
    was a Baptist pastor while he obtained his Ph
  name: Travelers Insurance
  position: 1038
- category: unknown
  confidence: medium
  context: sts, AI or ML engineers, and software developers. But Greg is such an engaging
    communicator, that Baptist ba
  name: But Greg
  position: 1325
- category: unknown
  confidence: medium
  context: and the innovative ways companies are using RAG (Retrieval Augmented Generation)
    to create more powerful AI applications. All rig
  name: Retrieval Augmented Generation
  position: 2063
- category: unknown
  confidence: medium
  context: Which small town? We're about three hours west of Salt Lake City. Cool.
    So that's the nearest city, like you're no
  name: Salt Lake City
  position: 2433
- category: unknown
  confidence: medium
  context: that's the nearest city, like you're nowhere near Las Vegas, for example.
    Vegas is like eight hours south. Oh
  name: Las Vegas
  position: 2508
- category: unknown
  confidence: medium
  context: experts, but it takes more than one line of code. But ZERV is block-based.
    You've seen it. It's the code is
  name: But ZERV
  position: 4956
- category: tech
  confidence: high
  context: e my first block and it is, say, a SQL query to a Snowflake database, then
    the data frame that gets created w
  name: Snowflake
  position: 7275
- category: tech
  confidence: high
  context: what ZERV reminds me of is the shift from having Microsoft Word documents
    that you emailed around to each ot
  name: Microsoft
  position: 8054
- category: unknown
  confidence: medium
  context: what ZERV reminds me of is the shift from having Microsoft Word documents
    that you emailed around to each other,
  name: Microsoft Word
  position: 8054
- category: tech
  confidence: high
  context: does that same kind of thing in data science that Google Docs did, where
    you could all of a sudden be coll
  name: Google
  position: 8468
- category: unknown
  confidence: medium
  context: does that same kind of thing in data science that Google Docs did, where
    you could all of a sudden be collabora
  name: Google Docs
  position: 8468
- category: unknown
  confidence: medium
  context: science tools. I was there for seven years or so. And I think everybody
    has pretty well realized that low
  name: And I
  position: 10382
- category: tech
  confidence: high
  context: all like DataRobot bought a notebook environment, Databricks introduced
    a notebook environment, like they're a
  name: Databricks
  position: 10735
- category: unknown
  confidence: medium
  context: to get into that. We had an episode recently with Natalie Mumbwe, it's
    episode 873. And in that episode, she talke
  name: Natalie Mumbwe
  position: 11926
- category: unknown
  confidence: medium
  context: It's open source. It's the same code editor that VS Code uses. So each
    node is a little text window. And i
  name: VS Code
  position: 14323
- category: unknown
  confidence: medium
  context: pisode of Super Data Science is brought to you by AWS Trainium 2, the latest
    generation AI chip from AWS. AWS Tr
  name: AWS Trainium
  position: 15651
- category: tech
  confidence: high
  context: y companies across the spectrum, from giants like Anthropic and Databricks
    to cutting-edge startups like Pool
  name: Anthropic
  position: 16068
- category: tech
  confidence: high
  context: y, the most valuable IP, just sending that off to OpenAI in a prompt. Yeah,
    in a prompt. Yeah, exactly. Ye
  name: Openai
  position: 16975
- category: unknown
  confidence: medium
  context: fortable with that. And so we also integrate with AWS Bedrock, which is
    Amazon's hosted large language model se
  name: AWS Bedrock
  position: 18312
- category: tech
  confidence: high
  context: d so we also integrate with AWS Bedrock, which is Amazon's hosted large
    language model service. And they h
  name: Amazon
  position: 18334
- category: tech
  confidence: high
  context: the IP issues there. And then the third thing is Hugging Face. So the open-source
    models that are out there, yo
  name: Hugging Face
  position: 18665
- category: unknown
  confidence: medium
  context: lly make those models yours in a significant way. So ZERV is designed to
    be self-hosted. So all of your dat
  name: So ZERV
  position: 19057
- category: unknown
  confidence: medium
  context: d my prompts? Is it low-risk so I can use OpenAI? Am I comfortable with
    Bedrock? Or do I want to actuall
  name: Am I
  position: 19290
- category: unknown
  confidence: medium
  context: d of just as easy as just as turnkey as using the OpenAI API? Yeah, so
    if you can see it, then you can talk to
  name: OpenAI API
  position: 19646
- category: unknown
  confidence: medium
  context: use it downstream. So those are sort of internal. The AI assistant is kind
    of above that. So you might hav
  name: The AI
  position: 21250
- category: unknown
  confidence: medium
  context: ally an agent and it can do stuff to your canvas. So I might say, hey,
    build me a canvas that does, you
  name: So I
  position: 21473
- category: unknown
  confidence: medium
  context: lking about a completely new era in data science. Like ZERV is at the forefront
    of building this new industri
  name: Like ZERV
  position: 22496
- category: unknown
  confidence: medium
  context: have individuals, you know, working alone in the Jupyter Notebook typing
    out each character of code. Yeah, exactly.
  name: Jupyter Notebook
  position: 22678
- category: unknown
  confidence: medium
  context: ith a bunch of dashboarding things. I just did an AWS QuickSight dashboard
    for a conference that I was speaking at
  name: AWS QuickSight
  position: 23776
- category: unknown
  confidence: medium
  context: tack that includes GPUs and networking as well as Nvidia AI Enterprise
    software, Nvidia inference microservices, models,
  name: Nvidia AI Enterprise
  position: 28586
- category: unknown
  confidence: medium
  context: bula, it was I'd like to hire a data scientist in New York. Then we can
    convert that in real time in millise
  name: New York
  position: 31035
- category: unknown
  confidence: medium
  context: t out and we got, you know, like top podcasts and New York Times reviews,
    and we brought in all sorts of other doc
  name: New York Times
  position: 32979
- category: unknown
  confidence: medium
  context: ody can get on there and get in and give it a go. Damn Bitcoin miners.
    What a world. Everyone. And so yeah, so n
  name: Damn Bitcoin
  position: 41850
- category: unknown
  confidence: medium
  context: to have in the show notes about that. Similarly, Microsoft CEO Satya Nadella
    predicts that business logic will move from SaaS
  name: Microsoft CEO Satya Nadella
  position: 42356
- category: unknown
  confidence: medium
  context: ke you're in that same kind of boat as Klarna and Satya Nadella around
    thinking that with LLMs empowering them, i
  name: Satya Nadella
  position: 42531
- category: unknown
  confidence: medium
  context: be bolting on generative AI onto their products. But I think there's a
    difference between a simple bolt-
  name: But I
  position: 43833
- category: unknown
  confidence: medium
  context: that's kind of interesting. So, a friend of mine, Sean Coshla, a real software
    developer, he has built a really
  name: Sean Coshla
  position: 45418
- category: unknown
  confidence: medium
  context: instead of paying a $20 a month subscription for ChatGPT Plus, he uses
    that interface, simple interface, and he
  name: ChatGPT Plus
  position: 45646
- category: unknown
  confidence: medium
  context: models in the backend. And he's like, my cost of OpenAI APIs went from
    $20 a month to $2 a month. So, that's k
  name: OpenAI APIs
  position: 45771
- category: unknown
  confidence: medium
  context: k I've used something like that, a product called Typing Mind. I think
    that's the name of it. It's doing the sa
  name: Typing Mind
  position: 45983
- category: unknown
  confidence: medium
  context: essional and personal lives through things like a ChatGPT Pro subscription
    and Deep Research. Billions and bill
  name: ChatGPT Pro
  position: 48565
- category: ai_application
  confidence: high
  context: Co-founder Greg Michelson's company; a platform for developing and delivering
    AI products using a collaborative, graph-based (DAG) coding environment.
  name: ZERV
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Sponsor of the podcast, mentioned for their Trainium II AI chip.
  name: AWS
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in connection with the Dell AI Factory, indicating their role
    in AI hardware/infrastructure.
  name: Nvidia
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Greg Michelson's previous employer (Chief Customer Officer); described
    as one of the original low-code, no-code data science tools.
  name: DataRobot
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Greg Michelson's previous employer for analytics and research, implying
    internal use of data science/ML.
  name: Travelers Insurance
  source: llm_enhanced
- category: data_platform
  confidence: medium
  context: Mentioned as a database source that ZERV can query via SQL blocks in its
    DAG.
  name: Snowflake
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as a company choosing Trainium 2 to power their AI workloads.
  name: Anthropic
  source: llm_enhanced
- category: ai_platform
  confidence: high
  context: Mentioned as a company choosing Trainium 2; also noted for introducing
    a notebook environment, shifting towards coding environments.
  name: Databricks
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Mentioned as a cutting-edge startup choosing Trainium 2 to power their
    AI workloads.
  name: Poolside
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned in the context of LLMs and the concern companies have about sending
    proprietary code/IP to their services via prompts.
  name: OpenAI
  source: llm_enhanced
- category: ai_product
  confidence: high
  context: Mentioned as a specific LLM product users are familiar with, often used
    for comparison regarding latency.
  name: ChatGPT
  source: llm_enhanced
- category: ai_tool
  confidence: high
  context: Mentioned alongside ChatGPT as a tool that allows non-developers to generate
    code.
  name: Copilot
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in connection with the Dell AI Factory with Nvidia.
  name: Dell
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Latest generation AI chip from AWS, purpose-built for today's largest AI
    models, offering PetaFLOPS of compute.
  name: AWS Trainium 2
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Amazon's hosted large language model service integrated into ZERV.
  name: AWS Bedrock
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Platform for open-source models, integrated into ZERV allowing users to
    instantiate and potentially host models locally.
  name: Hugging Face
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: A dashboarding tool integrated with ZERV for publishing reports.
  name: AWS QuickSight
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: AWS service mentioned as a deployment option for models created in ZERV.
  name: SageMaker
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A human resources platform co-founded by the speaker, used as a concrete
    example for leveraging RAG over professional profiles.
  name: Nebula
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A third-party product/interface that uses OpenAI keys to provide a consumption-based
    alternative to the ChatGPT Plus subscription.
  name: Typing Mind
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A service or tool used by the speakers that incorporates LLMs and seems
    to offer advanced research capabilities, potentially including fact-checking traces.
  name: Deep Research
  source: llm_enhanced
- category: ai_model_developer
  confidence: high
  context: A specific model name referenced, likely from OpenAI, discussed in the
    context of internet access limitations.
  name: O1
  source: llm_enhanced
- category: ai_model_developer
  confidence: high
  context: A specific model name referenced, likely from OpenAI, noted for having
    internet access.
  name: O3 Mini
  source: llm_enhanced
- category: ai_model_developer
  confidence: high
  context: A specific model name mentioned alongside O1 and O3 Mini, noted for its
    ability to iterate and fact-check responses.
  name: DeepSeek C1
  source: llm_enhanced
- category: ai_model_developer
  confidence: high
  context: A specific, high-capacity model from OpenAI that was noted as lacking internet
    access at the time of the discussion.
  name: O1 Pro
  source: llm_enhanced
- category: ai_model_developer
  confidence: medium
  context: Used generally to refer to OpenAI's models (e.g., GPT-4) when discussing
    bias testing against other LLMs.
  name: GPT
  source: llm_enhanced
- category: ai_model_developer
  confidence: medium
  context: Mentioned as one of the four different large language models used in the
    speaker's bias testing experiment.
  name: Titan
  source: llm_enhanced
date: 2025-04-15 11:00:00 +0000
duration: 67
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: fill them in a bit more
  text: we should fill them in a bit more.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: kind of explain that term a little bit
  text: we should kind of explain that term a little bit.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD7771131066.mp3?updated=1744703767
processing_date: 2025-10-06 14:17:04 +0000
quotes:
- length: 154
  relevance_score: 5
  text: It's a full stack that includes GPUs and networking as well as Nvidia AI Enterprise
    software, Nvidia inference microservices, models, and agent blueprints
  topics: []
- length: 89
  relevance_score: 4
  text: Many companies are struggling with leveraging large language models into their
    businesses
  topics: []
- length: 73
  relevance_score: 4
  text: So the first LLM integration that we built were what we call GenAI blocks
  topics: []
- length: 236
  relevance_score: 4
  text: So you might take and connect a code block to a GenAI block and pass it some
    variables that you want to include in your prompt in order to get that sort of
    query back, and then you can take the output of that query and use it downstream
  topics: []
- length: 181
  relevance_score: 4
  text: So you might have a prompt like a text field, an interaction, a chat space
    with a large language model where the AI assistant is actually an agent and it
    can do stuff to your canvas
  topics: []
- length: 203
  relevance_score: 4
  text: You could have tons of information stored in whatever kind of database and
    a really buzzword technology today, which I would say was even busier in 2024,
    this idea of RAG (Retrieval Augmented Generation)
  topics: []
- length: 87
  relevance_score: 4
  text: So you can take a deep learning model of some kind, often today, a large language
    model
  topics: []
- length: 252
  relevance_score: 4
  text: And so it sounds like you're in that same kind of boat as Klarna and Satya
    Nadella around thinking that with LLMs empowering them, internal processes can
    be simplified, standardized, and lots of different SaaS vendors can be removed,
    cutting down costs
  topics:
  - saas
- length: 133
  relevance_score: 4
  text: So, I think the newer stuff, the more cutting-edge stuff, every SaaS company
    seems to be bolting on generative AI onto their products
  topics:
  - saas
- length: 96
  relevance_score: 3
  text: And so we also integrate with AWS Bedrock, which is Amazon's hosted large
    language model service
  topics: []
- length: 170
  relevance_score: 3
  text: Extend your enterprise with AI and GenAI at scale powered by the broad Dell
    portfolio of AI infrastructure and services with Nvidia industry-leading accelerated
    computing
  topics: []
- length: 115
  relevance_score: 3
  text: Similarly, Microsoft CEO Satya Nadella predicts that business logic will move
    from SaaS applications into AI agents
  topics:
  - saas
- impact_reason: 'Provides a crucial nuance: AI coding assistants lower the barrier
    to entry but do not eliminate the need for foundational knowledge, especially
    for complex tasks. Expertise remains valuable.'
  relevance_score: 10
  source: llm_enhanced
  text: But then she quickly realized, well, you know what, I actually do also need
    to read like an introductory Python book at the same time. Yeah, yeah, there are
    little things. You couldn't sit down with your grandma and just do it. But, you
    know, after some time, grandma could pick it up probably, you know, but there's
    still value in experts, and you still need to know how to read it and stuff like
    that, especially particularly when projects get much more complex.
  topic: limitations
- impact_reason: 'Clearly articulates the primary barrier to enterprise LLM adoption:
    IP security and data leakage concerns when using external APIs like OpenAI.'
  relevance_score: 10
  source: llm_enhanced
  text: Many companies are struggling with leveraging large language models into their
    businesses. And part of why that is a struggle for them... is that they're worried
    about their intellectual property. They're worried about sending off their company
    software, most potentially if they're a software company, the most valuable IP,
    just sending that off to OpenAI in a prompt.
  topic: safety
- impact_reason: 'Outlines a comprehensive, tiered strategy for LLM integration addressing
    security and customization needs: proprietary (OpenAI), secure cloud-hosted (Bedrock),
    and self-hosted open-source (Hugging Face).'
  relevance_score: 10
  source: llm_enhanced
  text: There are really three options at the moment inside of ZERV for interacting
    with large language models. Well, for really. The first is OpenAI... the second
    is AWS Bedrock... And then the third thing is Hugging Face.
  topic: business
- impact_reason: 'Details a sophisticated level of AI interaction: an agent capable
    of planning and executing multi-step modifications (creating/connecting nodes)
    on the visual workflow canvas.'
  relevance_score: 10
  source: llm_enhanced
  text: The AI assistant is actually an agent and it can do stuff to your canvas.
    So I might say, hey, build me a canvas that does, you know, that takes this data
    frame... and write me an analysis that does whatever. And then it would actually
    create a plan around that and then actually it would be able to create the blocks
    and connect them and all that sort of stuff.
  topic: breakthroughs
- impact_reason: 'Defines the paradigm shift: moving from ''artisanal'' (notebook-based,
    individual) data science to ''industrialized'' (platform-based, automated, collaborative)
    data science.'
  relevance_score: 10
  source: llm_enhanced
  text: this now starts to feel like we're talking about a completely new era in data
    science. Like ZERV is at the forefront of building this new industrialized data
    science as opposed to a more artisanal data science where you have individuals,
    you know, working alone in the Jupyter Notebook typing out each character of code.
  topic: strategy
- impact_reason: A strong prediction about the impact of LLMs on the build vs. buy
    dynamic, suggesting a significant shift towards in-house development due to lowered
    barriers to entry.
  relevance_score: 10
  source: llm_enhanced
  text: But the large language model stuff has made it so much easier to build that
    I think the calculation on build versus buy is super different now than it was
    a few years ago. It's just so much, it's going to be so much cheaper to build
    than it is to buy these sort of things.
  topic: business/predictions
- impact_reason: A concrete, relatable example demonstrating the limitation of pure
    generative LLM querying (lack of diversity/hallucination) and the necessity of
    grounding via RAG.
  relevance_score: 10
  source: llm_enhanced
  text: But it turned out that naively querying these large language models would
    often give you results that weren't very good. So, like I recall one example,
    we fed it Dune, you know, the movie Dune, you know, I like Dune, give me more
    movies like that. And it ended up giving the top five responses, four of them
    were other versions of Dune. So I was like, all right, that's not ideal.
  topic: safety/limitations
- impact_reason: 'Clearly defines the classic ''last mile'' problem in MLOps: the
    deployment bottleneck caused by the scarcity of specialized software/MLOps engineers
    relative to data scientists.'
  relevance_score: 10
  source: llm_enhanced
  text: So, on a smaller team or on a team where there's huge demand for software
    engineers, which is often the case, you can end up having more data scientists
    creating models than there are software engineers to deploy them. That creates
    a bottleneck.
  topic: business/limitations
- impact_reason: A highly relatable pain point for the entire data science community,
    illustrating the fragility and non-reproducibility of prototype code (Jupyter
    notebooks) when handed off.
  relevance_score: 10
  source: llm_enhanced
  text: If you've ever been sent or emailed a Jupyter notebook and tried to run it,
    you know what some of them are, like you have the wrong version of this package
    installed. Oh, you got a `pip install` a whole bunch of other stuff to make that
    work.
  topic: practical lessons
- impact_reason: 'This is the core value proposition: abstracting away packaging complexity
    so users can reference trained models directly for deployment.'
  relevance_score: 10
  source: llm_enhanced
  text: Well, then I just say, hey, remember that random forest and I just point at
    it instead of having to figure out how to package that thing up so that it could
    be deployed as an API. So we handle all that stuff.
  topic: business/technical
- impact_reason: 'Introduces a major industry prediction: AI enabling companies to
    internalize functions previously outsourced to specialized SaaS vendors, leading
    to vendor consolidation.'
  relevance_score: 10
  source: llm_enhanced
  text: You mentioned earlier in this episode how AI could kill a lot of SaaS software
    and service businesses. So some companies like Klarna are combining AI standardization
    and simplification to shut down SaaS providers.
  topic: predictions
- impact_reason: Citing a major industry leader (Nadella) to support the thesis that
    core business processes will be embedded within autonomous agents rather than
    residing in packaged SaaS applications.
  relevance_score: 10
  source: llm_enhanced
  text: Microsoft CEO Satya Nadella predicts that business logic will move from SaaS
    applications into AI agents.
  topic: predictions
- impact_reason: Identifies a fundamental shift in software economics driven by API
    consumption models (pay-per-token/use) challenging traditional subscription models.
  relevance_score: 10
  source: llm_enhanced
  text: Instead of wanting to pay a monthly fee that's just kind of this flat fee,
    you instead, we've kind of gotten used to now with calling APIs and paying per
    number of tokens that we send or receive back from that API. We as developers...
    are getting more and more used to this idea of an economic model where it's consumption-based
    on specifically what I need instead of subscription-based.
  topic: business
- impact_reason: 'Describes the dangerous psychological shift: as model accuracy improves,
    users lower their guard and increase trust, potentially leading to catastrophic
    errors when inaccuracies *do* occur.'
  relevance_score: 10
  source: llm_enhanced
  text: I start to notice it's so less frequently that I start to just trust the AI
    systems, which is maybe risky. So, when they were frequently inaccurate or having
    biases, as like, you know, you were constantly on the lookout for those kinds
    of issues. But now that I rarely see those... it just seems like they're basically
    always right.
  topic: safety
- impact_reason: A concrete, high-impact example of hallucination, specifically inventing
    non-existent functionality (an API) due to the model's drive to be helpful, illustrating
    a major current limitation.
  relevance_score: 10
  source: llm_enhanced
  text: I asked ChatGPT how, you know, for some code to do that. And it invented an
    API out of nothing, like that didn't exist. And it was like, oh, here's the code
    to do that. And I have no doubt that when they do introduce that feature that
    the code would run, but no, it was just made up. It just completely hallucinated
    an API because they want to be so helpful.
  topic: limitations/technical
- impact_reason: Emphasizes the necessity of 'trace' or 'chain-of-thought' visibility
    in complex AI systems to detect hidden failures (like assuming data when access
    is denied). This strongly advocates for explainability features.
  relevance_score: 10
  source: llm_enhanced
  text: Luckily, at least I have that trace. Like when you do Deep Research, it's
    kind of an explanation of what it's thinking about. And so, in what it's thinking
    about, it was like, because I don't have internet access, I'm just going to assume
    what kind of thing was there. And I'm like, no, that's like, so at least I can
    see that it had that trace, because otherwise with the output itself, you're kind
    of like, oh, cool, that makes sense. And so, if I hadn't gone and looked at the
    website or looked at the trace, I would have been led completely astray.
  topic: safety/practical lessons
- impact_reason: Introduces a key new technical feature ('fleet') designed to solve
    the latency problem of sequential LLM calls through massive, cost-effective parallelization
    using serverless compute.
  relevance_score: 9
  source: llm_enhanced
  text: We also have added a feature called the fleet, which is coming out this week.
    And that's a way to massively parallelize code execution using serverless technology.
  topic: technical/breakthroughs
- impact_reason: Explains the core technical advantage of the DAG/block architecture
    combined with serverless execution for achieving parallelization without added
    coding complexity or increased compute cost (only time compression).
  relevance_score: 9
  source: llm_enhanced
  text: ZERV is block-based. You've seen it. It's the code is arranged as a DAG. And
    so each block, when you execute it, spins up serverless compute and executes.
    Well, it turns out that it's dead easy to just parallelize by spinning up lots
    of serverless compute. And that side is, you know, you don't have to write any
    code to do it. And it doesn't cost more because it's the same amount of compute.
    It just happens all at the same time.
  topic: technical/model deployment
- impact_reason: 'A significant business prediction: LLMs lower the barrier to entry
    for custom solutions, potentially disrupting existing SaaS models that rely on
    proprietary, hard-to-replicate logic.'
  relevance_score: 9
  source: llm_enhanced
  text: why the rise of LLMs might build trouble for many SaaS businesses as building
    in-house solutions becomes increasingly viable
  topic: business/predictions
- impact_reason: 'Uses a powerful, relatable analogy (Word vs. Google Docs) to explain
    the paradigm shift ZERV brings to data science workflow management: moving from
    sequential file sharing to real-time collaboration.'
  relevance_score: 9
  source: llm_enhanced
  text: what ZERV reminds me of is the shift from having Microsoft Word documents
    that you emailed around to each other... ZERV does that same kind of thing in
    data science that Google Docs did, where you could all of a sudden be collaborating
    all together on the same... document.
  topic: strategy/analogy
- impact_reason: 'Describes the new paradigm of coding driven by LLMs: shifting from
    pure generation to iterative refinement and prompt engineering of AI-generated
    code.'
  relevance_score: 9
  source: llm_enhanced
  text: You're not coding the same way that you were where you'd sit down at a blank
    screen and just type code. Instead, you're sort of describing to the large language
    model what you want to build, and then you're taking that code and maybe you're
    tweaking it, or maybe you're interacting with the large language model to make
    it do what you want it to do.
  topic: AI technology trends/coding experience
- impact_reason: Highlights the immediate democratization effect of generative AI
    tools on coding ability, even for non-traditional developers.
  relevance_score: 9
  source: llm_enhanced
  text: she never learned how to be a developer, but now she can pick up tools and
    use something like Copilot or ChatGPT and be able to generate code herself.
  topic: predictions
- impact_reason: A strong statement countering the hype that AI will completely replace
    skilled practitioners in the near term.
  relevance_score: 9
  source: llm_enhanced
  text: We're not exactly to the place where it's no expertise required. And I don't
    think we will be for a while.
  topic: predictions
- impact_reason: 'Highlights a major productivity gain: integrated, non-intrusive
    data previewing that eliminates the need for constant debugging print statements
    common in notebooks.'
  relevance_score: 9
  source: llm_enhanced
  text: if I run my code, and I say I do something to a data frame, maybe I convert
    a date variable from characters to numeric or something like that, then I can
    click on it on the left side and have a preview, and I can click on it on the
    right side and have a preview, and I can compare what my code actually did without
    having to type, you know, `print df.head()` and all that sort of stuff in order
    to actually see it.
  topic: practical lessons
- impact_reason: 'A key business metric for infrastructure decision-making: quantifiable
    cost/performance advantage over incumbent GPU solutions for AI workloads.'
  relevance_score: 9
  source: llm_enhanced
  text: These instances offer 30 to 40 percent better price performance relative to
    GPU alternatives.
  topic: business
- impact_reason: Emphasizes the platform's self-hosting capability, which is critical
    for enterprises with strict data governance and compliance requirements.
  relevance_score: 9
  source: llm_enhanced
  text: ZERV is designed to be self-hosted. So all of your data and all of your compute
    lives in your environment anyway. So you have some choices around, okay, where
    do I, where can I send my prompts?
  topic: safety
- impact_reason: Paints a picture of future collaborative data science, where human
    experts are augmented by multiple dedicated AI agents working simultaneously on
    the same project.
  relevance_score: 9
  source: llm_enhanced
  text: So you could have three people plus their three assistants all kind of generating
    nodes and figuring out how do these flows work in some complex machine learning
    flow.
  topic: predictions
- impact_reason: Lists the breadth of MLOps capabilities integrated into the platform,
    covering training, API creation, deployment flexibility (in-platform or external),
    and scheduling.
  relevance_score: 9
  source: llm_enhanced
  text: You can train models, you can use GPUs, and then when you come to deploy,
    you can really easily build your own APIs that you could host with ZERV or download
    and take somewhere else. You can deploy using SageMaker, you can schedule jobs.
    Like the whole lifecycle of these data science projects is built into one application.
  topic: practical lessons
- impact_reason: 'Highlights crucial interoperability: supporting polyglot environments
    (SQL, R, Python) within a single workflow, unified by standard source control
    integration.'
  relevance_score: 9
  source: llm_enhanced
  text: And you can do it in any language you want. So you could have your data engineers
    writing SQL, you could have your old-school statistician folks writing R code,
    you could have your machine learning engineers doing Python, and it all syncs
    to GitHub or Bitbucket or whatever source control you use.
  topic: technical
- impact_reason: 'Highlights a major trend in MLOps/Data Science platforms: unifying
    the entire lifecycle (engineering, statistics, ML) and supporting polyglot environments,
    which addresses organizational friction.'
  relevance_score: 9
  source: llm_enhanced
  text: Like the whole lifecycle of these data science projects is built into one
    application. And you can do it in any language you want. So you could have your
    data engineers writing SQL, you could have your old-school statistician folks
    writing R code, you could have your machine learning engineers doing Python, and
    it all syncs to GitHub or Bitbucket or whatever source control you use.
  topic: strategy/technical
- impact_reason: A provocative statement suggesting that foundational LLM capabilities
    will disrupt specialized, high-cost SaaS vendors whose value proposition is now
    easily replicable.
  relevance_score: 9
  source: llm_enhanced
  text: I actually think it's sort of an existential crisis for a lot of these SaaS
    vendors that are building kind of custom build use case type solutions and then
    charging a mint for them.
  topic: business/predictions
- impact_reason: Identifies RAG as a key, powerful technology trend in the current
    AI landscape.
  relevance_score: 9
  source: llm_enhanced
  text: This idea of RAG (Retrieval Augmented Generation). It is a really powerful
    thing.
  topic: technical
- impact_reason: Provides a clear, concise explanation of the vectorization step fundamental
    to RAG systems, linking it directly to LLMs.
  relevance_score: 9
  source: llm_enhanced
  text: So you can take a deep learning model of some kind, often today, a large language
    model. And so you can take the natural language on each of those hundreds of millions
    of documents encoded into a vector, so just a series of numbers.
  topic: technical
- impact_reason: Explains the synergy between RAG (retrieval) and modern LLMs (large
    context windows) as the core mechanism for advanced Q&A systems.
  relevance_score: 9
  source: llm_enhanced
  text: And then you can, with today's big context windows, you could take all of
    those documents potentially, like the top 100 documents that come back, throw
    them all into the huge context window of a generative large language model.
  topic: technical
- impact_reason: Illustrates that the need for parallelization extends beyond simple
    vector indexing to complex, multi-stage tasks like ASR (Audio-to-Text) followed
    by LLM processing, making it critical for enterprise feasibility.
  relevance_score: 9
  source: llm_enhanced
  text: You know, processing all of that stuff, parallelization is key for a lot of
    different use cases. So, we like one use case that we just worked on was trying
    to evaluate the performance of a call center based on audio files, audio recordings
    of MP3 file recordings of conversations with customers. That was at a bank. You
    know, that's a lot of processing, right? Converting all of the recordings into
    text that you could then pass to a large language model. You need to be able to
    do parallelization for stuff like that. Otherwise, it's going to take, you know,
    it's not feasible to do it.
  topic: technical/predictions
- impact_reason: Addresses the skill gap and organizational permission barriers that
    prevent data scientists from owning the full deployment lifecycle, justifying
    the need for automated deployment tools.
  relevance_score: 9
  source: llm_enhanced
  text: But it's not obvious to me that data scientists know how to do that [Dockerizing,
    managing servers]. And it's really not obvious that they have the privileges to
    do those kinds of things in terms of just like the infrastructure.
  topic: business/limitations
- impact_reason: 'Defines the current inefficient workflow: prototyping by data scientists
    followed by a costly, time-consuming rewrite/re-engineering phase by software
    teams.'
  relevance_score: 9
  source: llm_enhanced
  text: At the end of the day, what data scientists spend most of their time doing
    today is building prototypes. And then those prototypes get handed off to another
    team to kind of like recode in another environment with, you know, Dockerized
    and deployed and managing servers and stuff like that.
  topic: practical lessons
- impact_reason: Highlights the friction point between data science prototyping and
    software deployment, specifically noting that developers often lack the necessary
    data science context, creating deployment hurdles.
  relevance_score: 9
  source: llm_enhanced
  text: RV's built-in API builder and GPU manager remove those kinds of barriers?
    Yeah, it's not just a bottleneck. It's also kind of a problematic dependency,
    because at the end of the day, the software developers that are deploying these
    things are probably not data scientists.
  topic: business/strategy
- impact_reason: Presents a core technical solution (containerization per project/canvas)
    to the dependency management problem, making environments reproducible and shareable.
  relevance_score: 9
  source: llm_enhanced
  text: So, ZERV kind of handles all of those problems. So, every canvas inside of
    ZERV has a Docker container that's supporting it. So, anybody that logs into that
    canvas doesn't have to worry about dependencies because it's all saved in that
    project.
  topic: technical
- impact_reason: Describes an automated serialization and artifact management system
    integrated directly into the workflow, simplifying the path from training to API
    endpoint creation.
  relevance_score: 9
  source: llm_enhanced
  text: So every block inside ZERV when you execute it, it creates serialized versions
    of all of the variables that you've worked through. So if I train a random forest
    in a block, then it's there and it's accessible. So I can access it from external
    to ZERV using an API.
  topic: technical
- impact_reason: Addresses organizational friction by providing a smooth handoff mechanism,
    while simultaneously redefining the data scientist's role from 'prototyper' to
    'software builder' within the platform.
  relevance_score: 9
  source: llm_enhanced
  text: And that means that data scientists can start to deploy their own stuff. But
    in some organizations, they may still not be allowed. So then we have like a handoff
    system where it's really easy to take something that a data scientist has done,
    who by the way aren't building prototypes anymore. Now they're building software
    that can actually be deployed in ZERV, and we can hand that off to other teams
    to actually do the deployments.
  topic: strategy/business
- impact_reason: Connects the rise of powerful, accessible AI tools directly to a
    fundamental shift in the build vs. buy calculus for enterprise software.
  relevance_score: 9
  source: llm_enhanced
  text: I think this goes back to the whole build versus buy thing. It's just becoming
    so much easier to build stuff in-house.
  topic: business/strategy
- impact_reason: Differentiates between superficial AI integration (prompt boxes)
    and truly transformative, deeply integrated AI agents that deliver 'magical' user
    experiences.
  relevance_score: 9
  source: llm_enhanced
  text: But I think there's a difference between a simple bolt-on where you add a
    prompt, a place where you can write a prompt or something like that. That's kind
    of a dime a dozen as compared to a SaaS offering that is deeply integrated with
    these kinds of things, actually seeing magical in some sorts of ways when the
    agents begin doing things that are just intuitive and just sort of work.
  topic: strategy
- impact_reason: Provides a concrete, quantitative example showing how decoupling
    the UI/access layer from the underlying LLM provider can drastically reduce operational
    costs for the end-user.
  relevance_score: 9
  source: llm_enhanced
  text: He uses that interface, simple interface, and he calls OpenAI models in the
    backend. And he's like, my cost of OpenAI APIs went from $20 a month to $2 a month.
  topic: business
- impact_reason: 'Highlights a key feature of advanced LLMs: their ability to perform
    metacognition or intelligent clarification, mimicking the critical thinking of
    a highly skilled human analyst.'
  relevance_score: 9
  source: llm_enhanced
  text: The way that it asks me before it goes off and does a long research request
    for clarification on some of the key points, even just those questions that it
    asks, and like those are incisive and exactly spot on the questions you should
    be asking as a well-educated analyst or PhD student or PhD graduate being asked
    to take on this task.
  topic: technical
- impact_reason: 'Provides a crucial operational insight for users: the difficulty
    of steering or correcting a model once it has fundamentally misunderstood a prompt
    or task, suggesting a ''first-shot'' success is often indicative of overall solvability.'
  relevance_score: 9
  source: llm_enhanced
  text: The one thing I found about these models is that if they get it right or at
    least close to right the first time, you're probably okay. But like, you know,
    there are some problems that I've given them, and it's like, all right, it didn't
    get that right at all. And then I'm like, okay, there's no hope. Like you take
    another route or something, because they don't, if they don't get it, then they're
    not, in my experience anyway, easy to kind of correct.
  topic: practical lessons
- impact_reason: 'Reveals a critical, non-obvious architectural/deployment constraint:
    the largest, most capable models (O1 Pro) might lack features like internet access,
    forcing trade-offs between raw capability and real-time data access, which changes
    based on model version/tier.'
  relevance_score: 9
  source: llm_enhanced
  text: I was trying to use the O1 model. And at least at the time that I was doing
    this a week or two ago, it turns out O1 doesn't have internet access yet. Or was
    it O3? Was it O1 Pro? It was O1 Pro specifically that I was trying to use. And
    so, you know, I was like, I want to use, this is a big problem, I want to get
    the chunkiest model that OpenAI can give me running in the backend in my Deep
    Research. And it turns out that they, at least at the time that I ran this, did
    not yet have internet access. So, if I use something like O3 Mini, I could get
    internet access, but not with O1 Pro.
  topic: technical/deployment
- impact_reason: 'Poses the central ethical and governance question for the future
    of AI adoption: how to manage and balance inherent biases across both human and
    automated decision-making layers.'
  relevance_score: 9
  source: llm_enhanced
  text: So, as we increasingly automate judgment and decision-making, how should we
    balance these kinds of biases that happen with both humans and AI evaluation processes
    to create more fair, transparent systems?
  topic: safety/ethics
- impact_reason: 'Clearly articulates a major pain point in current LLM usage: sequential
    latency when batch processing or running many calls, setting the stage for why
    the ''fleet'' feature is necessary.'
  relevance_score: 8
  source: llm_enhanced
  text: if you wanted to make, say, you wanted to make a call to a large language
    model, but you wanted to do it, say, a thousand times. You know, they're slow,
    right? Everybody's used ChatGPT. You know, you can type it, ask it a question.
    It might take, you know, 20, 30, 40 seconds for it to come back. So if you try
    to do that a thousand times, doing it in series is kind of a pain.
  topic: AI technology trends/limitations
- impact_reason: Highlights the integration of AI assistants directly into the development
    workflow (building blocks) and asserts that LLMs are fundamentally revolutionizing
    the coding process itself.
  relevance_score: 8
  source: llm_enhanced
  text: our AI assistant that can help you to write code within the app can build
    its own blocks, can do some of that stuff. It's sort of modern coding, you know,
    because the large language models really revolutionize the way people code.
  topic: AI technology trends/impact
- impact_reason: Highlights the platform's strength in enabling true, heterogeneous,
    real-time collaboration and execution across different languages within the same
    workflow.
  relevance_score: 8
  source: llm_enhanced
  text: The nice thing about the architecture is that you can run as many blocks simultaneously
    as you want. So you can have multiple people in the same canvas, all running code
    at the same time, writing code in Python, in R, in SQL. It's just kind of a wild
    mix-and-match environment where anybody can do anything.
  topic: business/collaboration
- impact_reason: Articulates the critical, widely experienced pain point of version
    control and collaboration within the standard Jupyter notebook ecosystem.
  relevance_score: 8
  source: llm_enhanced
  text: if you've ever worked in a Jupyter notebook, then you know that sharing and
    collaborating on those is a nightmare, right? Like, you know, and commit, and
    like, like, you open it, the metadata changes. So you end up with tons of merge
    conflicts and all that stuff, which is a nightmare.
  topic: practical lessons/pain points
- impact_reason: Provides a quantifiable, high-impact business metric (9x cycle time
    reduction) for model development.
  relevance_score: 8
  source: llm_enhanced
  text: it says that it empowers code-first data teams to cut cycle times by up to
    nine times.
  topic: business/metrics
- impact_reason: A strong critique of the limitations of the low-code/no-code movement
    for complex, real-world data science problems.
  relevance_score: 8
  source: llm_enhanced
  text: I think everybody has pretty well realized that low-code, no-code, you know,
    the emperor has no clothes on that. You know, anytime you run into a complicated
    problem, you're going to exceed the bounds of what you can do in a low-code, no-code
    tool.
  topic: strategy/critique
- impact_reason: Asserts that true value generation in data science remains tied to
    expert coders, even as tools evolve.
  relevance_score: 8
  source: llm_enhanced
  text: they're all kind of shifting to coding environments because they're realizing
    that the only people that actually generate value from data are the experts, the
    ones who were writing code.
  topic: strategy/expert value
- impact_reason: 'Provides a balanced view on AI coding assistance: it lowers the
    barrier but does not eliminate the need for foundational knowledge, especially
    for complex tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: it's not as turnkey as you might like it to be today, where she described
    how she kind of she was like, okay, cool, I can just code. But then she quickly
    realized, well, you know what, I actually do also need to read like an introductory
    Python book at the same time.
  topic: AI limitations/practical lessons
- impact_reason: 'Illustrates a sophisticated, practical use case for the DAG structure:
    managing heterogeneous data streams (weather, stocks, images) requiring distinct
    preprocessing steps in a unified view.'
  relevance_score: 8
  source: llm_enhanced
  text: you can arrange your directed graph so that you're taking in some data inputs
    or maybe multiple data inputs, you're doing data pre-processing on each of those
    streams separately. And because they're different things, like one is weather,
    another one's stock prices, another one's images, you have all these different
    data inflows that need completely different kinds of data pre-processing. And
    so it makes sense to see that laid out in ZERV's DAG.
  topic: technical/architecture application
- impact_reason: Explains the user interface design philosophy for node-based coding,
    emphasizing immediate visibility of data flow (inputs/outputs) alongside the code.
  relevance_score: 8
  source: llm_enhanced
  text: The central area is the actual code, and then on the left side of the screen,
    you have all of the inputs, like what data is being fed in from upstream blocks.
    And then you have all the outputs on the right side of the screen.
  topic: technical
- impact_reason: Provides specific, high-impact performance metrics for new dedicated
    AI hardware (Trainium 2), relevant for large-scale model training.
  relevance_score: 8
  source: llm_enhanced
  text: This episode of Super Data Science is brought to you by AWS Trainium 2, the
    latest generation AI chip from AWS. AWS Trainium 2 instances deliver 20.8 PetaFLOPS
    of compute, while the new Trainium 2 Ultra servers combine 64 chips to achieve
    over 83 PetaFLOPS in a single node, purpose-built for today's largest AI models.
  topic: technical
- impact_reason: Positions the platform as a comprehensive, end-to-end environment
    covering the entire ML lifecycle, excluding only raw infrastructure.
  relevance_score: 8
  source: llm_enhanced
  text: We think about ourselves as kind of a full-stack data science environment.
    We don't do like the hardware and the data warehousing type stuff, but we connect
    to all of those solutions. But everything else we do, we do.
  topic: business
- impact_reason: 'Addresses the common organizational friction point: ensuring platform
    work integrates seamlessly with existing developer tools (like VS Code) via Git.'
  relevance_score: 8
  source: llm_enhanced
  text: So if you know you've got an engineer down the line and they want to stay
    in VS Code, it's fine. You know, it all pulls together on GitHub.
  topic: strategy
- impact_reason: 'Articulates a core architectural principle for modern data platforms:
    data gravity dictates that the processing layer must be flexible enough to meet
    the data where it resides, rather than forcing migration.'
  relevance_score: 8
  source: llm_enhanced
  text: So our approach is just like people's data is going to be where it is, and
    you can access it via code. And so whatever wherever it is, we want you to be
    able to get to it.
  topic: strategy
- impact_reason: Offers practical insight into the hyperparameter tuning for vector
    embeddings (dimensionality), highlighting the trade-off between precision (more
    dimensions) and compute cost.
  relevance_score: 8
  source: llm_enhanced
  text: So you kind of find this sweet spot for your particular application. And yeah,
    so you might have 64 or 128 or 3000 columns depending on your needs.
  topic: technical
- impact_reason: Highlights the complexity of scaling RAG systems, implying that manual
    orchestration of vectorization, retrieval, and LLM querying is difficult.
  relevance_score: 8
  source: llm_enhanced
  text: So, how does ZERV's approach to parallelizing these RAG workflows compare
    to if you try to do that on your own, if you try to figure out all the pieces
    of what I just described on your own?
  topic: technical
- impact_reason: Emphasizes the importance of visualization and workflow orchestration
    (DAGs) for rapid iteration and debugging complex AI pipelines like RAG.
  relevance_score: 8
  source: llm_enhanced
  text: So yeah, the RAG stuff is super, super convenient. The being able to lay it
    out in a canvas style kind of like a DAG makes it really easy to kind of see what's
    going on. And it makes it really fast to sort of experiment with how it works.
  topic: technical/strategy
- impact_reason: 'Highlights the business value of environment standardization: drastically
    reducing onboarding time and eliminating version conflict troubleshooting.'
  relevance_score: 8
  source: llm_enhanced
  text: And so when you have a new data scientist join your team, they don't have
    to spend their first week getting their Python installed and making sure everything,
    oh, we use NumPy 0.19 and you got 0.23 installed, and like none of those conversations
    have to really happen anymore as we manage all of that.
  topic: business
- impact_reason: Explains the complexity of model serialization and deployment beyond
    simple linear models, emphasizing that packaging complex artifacts (like RFs or
    NNs) requires specialized MLOps knowledge.
  relevance_score: 8
  source: llm_enhanced
  text: But if it's a more complicated model like a random forest or an XGBoost or
    a neural network or something like that, it's not as simple as just like here's
    some weights to put into a formula. You know, it's a more complex thing. And so
    then you've got to figure out, okay, I'm going to serialize this model, you know,
    pickle it and then dump all the dependencies out and dockerize it and then hand
    that thing off.
  topic: technical
- impact_reason: Details the serverless deployment strategy (using Lambdas), which
    minimizes operational overhead (DevOps/infrastructure management) for the end-user.
  relevance_score: 8
  source: llm_enhanced
  text: And then when you deploy in ZERV, you also don't have to worry about the infrastructure
    stuff, because all of our APIs utilize Lambdas, like serverless technology again.
    You don't have like long-running services that are out there. It's just there.
  topic: technical
- impact_reason: Presents the counter-argument to the 'AI kills SaaS' thesis, suggesting
    AI will create new specialized market opportunities.
  relevance_score: 8
  source: llm_enhanced
  text: So, for example, the CEO of Zoho argues that in contrast, AI will fuel new
    vertical SaaS companies because you get new problems to tackle.
  topic: predictions
- impact_reason: A direct warning to existing software vendors whose value proposition
    is easily replicated by in-house AI development.
  relevance_score: 8
  source: llm_enhanced
  text: It seems to me like that's going to be a real problem for a lot of software
    vendors.
  topic: predictions
- impact_reason: Justifies the high cost of premium LLM access (like ChatGPT Plus)
    based on the immense value derived from a single, high-quality output (ROI perspective).
  relevance_score: 8
  source: llm_enhanced
  text: I know it's expensive. $200 a month sounds like a lot. But as soon as you
    start using it, there are so many everyday use cases where a single report that
    it creates for me, I'm like, that is worth $200 to me.
  topic: business
- impact_reason: Acknowledges the massive investment in LLMs while immediately pivoting
    to the persistent, critical challenges of bias and inaccuracy.
  relevance_score: 8
  source: llm_enhanced
  text: Billions and billions of dollars have been spent on developing these LLMs
    in aggregate across the planet, LLM-based services and projects. But there are
    still some shortcomings today around bias and inaccuracy.
  topic: safety/technical
- impact_reason: Describes a novel, self-referential testing methodology to probe
    for model ego or self-preference bias, a sophisticated approach to evaluating
    model fairness.
  relevance_score: 8
  source: llm_enhanced
  text: I wanted to just figure out if the models were biased towards their own answers,
    like if they thought that their own answers were better than the answers coming
    out of other large language models.
  topic: technical/safety
- impact_reason: Highlights the instability and high variance in LLM evaluation results,
    even when testing for specific biases. This suggests that evaluation methodologies
    themselves need robustness checks.
  relevance_score: 8
  source: llm_enhanced
  text: It was really interesting and wildly unstable. Like I could rerun that analysis
    and it would dramatically change. But there didn't seem to be a bias there in
    terms of like ChatGPT likes its own answers, that sort of thing.
  topic: technical/limitations
- impact_reason: 'Offers a practical, albeit potentially circular, strategy for quality
    assurance: using one LLM to grade the output of another LLM, suggesting a path
    for scalable, automated quality iteration.'
  relevance_score: 8
  source: llm_enhanced
  text: But in terms of like, I found it real effective to use LLMs to evaluate the
    answers from LLMs. So, that whole iterative thing
  topic: practical lessons/strategy
- impact_reason: Signals RAG as a current, innovative application technique for building
    more powerful, grounded AI systems.
  relevance_score: 7
  source: llm_enhanced
  text: the innovative ways companies are using RAG (Retrieval Augmented Generation)
    to create more powerful AI applications.
  topic: technical/applications
- impact_reason: Provides a clear, concise, and accessible definition of a DAG, which
    is fundamental to understanding ZERV's architecture and modern workflow orchestration.
  relevance_score: 7
  source: llm_enhanced
  text: a directed acyclic graph, it's basically what it says. Everything's in there
    in the term. But to break it down, you have a graph, which means you have nodes
    and edges connecting those nodes. It's directed, which means you have some kind
    of flow. It's not just that the points in the graph are connected... And then
    the final term there, the A, means that nowhere in your directed graph is there
    a loop.
  topic: technical/concepts
- impact_reason: 'Defines the core abstraction layer of the platform: code in nodes,
    data flow via edges, making complex pipelines visually explicit.'
  relevance_score: 7
  source: llm_enhanced
  text: In ZERV's graph, we call it a canvas. The code lives in the nodes in your
    graph, and then the connections indicate data and memory flow.
  topic: technical/architecture
- impact_reason: Clarifies the product positioning against the historical trend of
    low-code/no-code, acknowledging that LLMs are blurring these lines but asserting
    the necessity of code expertise.
  relevance_score: 7
  source: llm_enhanced
  text: Code-first is it's definitely not a no-code, low-code tool. Although large
    language models are making that a little bit fuzzier.
  topic: strategy/positioning
- impact_reason: Emphasizes the unprecedented speed of transformation in the software/data
    science development lifecycle due to recent AI advancements.
  relevance_score: 7
  source: llm_enhanced
  text: it's really wild how coding has changed over the last in such a short time
    in the last two, three years.
  topic: AI technology trends
- impact_reason: A cautionary note against over-optimism regarding full automation;
    expertise remains crucial for the foreseeable future.
  relevance_score: 7
  source: llm_enhanced
  text: you know, we're not exactly to the place where it's no expertise required.
    And I don't think we will be for a while.
  topic: safety/ethics/predictions
- impact_reason: Describes a practical application of Directed Acyclic Graphs (DAGs)
    in a data science platform for managing complex, heterogeneous data pipelines
    (weather, stocks, images).
  relevance_score: 7
  source: llm_enhanced
  text: It's very easy for me to imagine how in a tool like ZERV, you can arrange
    your directed graph so that you're taking in some data inputs or maybe multiple
    data inputs, you're doing data pre-processing on each of those streams separately.
  topic: technical
- impact_reason: Coined the term 'node-code environment,' positioning ZERV as a hybrid
    evolution beyond traditional no-code platforms, emphasizing code integration.
  relevance_score: 7
  source: llm_enhanced
  text: You talk about no-code environments. You've created a node-code environment.
    I appreciate that so much actually. It's a code node.
  topic: strategy
- impact_reason: Confirms the key players in the modern enterprise data stack (Snowflake,
    Databricks), grounding the discussion in current industry infrastructure.
  relevance_score: 7
  source: llm_enhanced
  text: By data stacks, you mean like warehousing type stuff, like Snowflake, Databricks
    type?
  topic: technical
- impact_reason: Reinforces the importance of RAG while acknowledging that the audience
    might be split between experts and newcomers, suggesting broad applicability.
  relevance_score: 7
  source: llm_enhanced
  text: So, usually powerful technology, definitely worth exploring for people. There's
    probably some listeners out there that are like, I know, John, I know what RAG
    is. But for those who don't, it's definitely worth exploring and understanding.
  topic: strategy
- impact_reason: 'Defines a successful infrastructure strategy: focusing on generalized
    compute orchestration rather than being tied to a single AI workload type.'
  relevance_score: 7
  source: llm_enhanced
  text: We're agnostic in terms of what loads are actually getting parallelized. We
    just make it easy to do it.
  topic: strategy
- impact_reason: A memorable, catchy term ('RAG-DAG') that encapsulates the combination
    of RAG architecture and Directed Acyclic Graph workflow management.
  relevance_score: 7
  source: llm_enhanced
  text: So, in addition to your code nodes, did you realize that you have RAG-DAGs?
    Oh, I don't hate that. That is definitely going to be reused.
  topic: strategy
- impact_reason: 'Acknowledges the necessary shift in perspective: valuing the data
    scientist''s domain/model expertise over traditional software engineering deployment
    skills.'
  relevance_score: 7
  source: llm_enhanced
  text: I like how you kind of turn that on its head as well and made me feel good
    about myself, about the skills that I do have that a software engineer might not
    have in terms of understanding the model that I built.
  topic: strategy
- impact_reason: Details the technical implementation of the 'code node' in the ZERV
    environment, leveraging familiar, high-quality tooling (Monaco/VS Code editor).
  relevance_score: 6
  source: llm_enhanced
  text: The node itself is a text editor. It's a code window. We use Monaco. It's
    open source. It's the same code editor that VS Code uses.
  topic: technical
- impact_reason: 'Describes the first level of LLM integration: dedicated blocks for
    prompt engineering, allowing dynamic input variables to shape the output.'
  relevance_score: 6
  source: llm_enhanced
  text: The first LLM integration that we built were what we call GenAI blocks. So
    in those blocks, you're not typing code, you're typing in a prompt.
  topic: technical
- impact_reason: A classic, enduring strategic dilemma in enterprise technology adoption,
    relevant to how companies choose to implement AI solutions.
  relevance_score: 6
  source: llm_enhanced
  text: There's definitely this businesses have always had to sort of juggle between
    build versus buy.
  topic: business
- impact_reason: A humorous but relevant cautionary tale about the resource demands
    and potential abuse vectors (like crypto mining) when offering free compute resources
    in an ML environment.
  relevance_score: 6
  source: llm_enhanced
  text: We originally had a free tier, but we had to shut it off for a little bit
    because of the Bitcoin miners. They went a little bananas.
  topic: business/safety
source: Unknown Source
summary: '## Podcast Episode Summary: 879: Serverless, Parallel, and AI-Assisted:
  The Future of Data Science is Here, with Dr. Greg Michaelson


  This episode of the Super Data Science podcast features Dr. Greg Michaelson, co-founder
  of ZERV, discussing the significant evolution of the ZERV platform over the past
  year, focusing on how it addresses the challenges of modern, collaborative, and
  AI-augmented data science workflows. The conversation charts a course from artisanal
  notebook-based development toward an industrialized, platform-centric approach.


  ### 1. Focus Area

  The primary focus is the **industrialization of data science workflows** through
  platform engineering, specifically highlighting ZERV''s **graph-based coding environment
  (DAGs)**, its new **serverless parallelization capabilities ("Fleet")**, and the
  integration of **AI assistants** to augment coding and project creation. Secondary
  topics include the shift away from low-code/no-code tools, IP concerns regarding
  LLMs, and the flexibility in model hosting (OpenAI, Bedrock, Hugging Face).


  ### 2. Key Technical Insights

  *   **Serverless Parallelization (The Fleet):** ZERV introduced the "Fleet" feature,
  enabling massive, cost-effective parallelization of code execution (e.g., running
  thousands of LLM calls simultaneously) by leveraging serverless compute spun up
  for each node in the Directed Acyclic Graph (DAG) without requiring manual multiprocessing
  management.

  *   **Node-Code Environment:** ZERV operates as a code-first environment where nodes
  in the DAG represent code blocks (written in Python, R, or SQL) using the Monaco
  editor (VS Code''s engine). Data and memory flow are explicitly visualized between
  nodes, allowing for real-time previewing of inputs and outputs, eliminating the
  need for constant `print()` statements common in notebooks.

  *   **AI Agent Integration:** Beyond simple GenAI blocks that execute prompts, ZERV
  features an AI assistant that acts as an agent capable of modifying the canvas itself—creating
  new blocks, connecting flows, and building entire analysis pipelines based on natural
  language instructions.


  ### 3. Business/Investment Angle

  *   **The Failure of Pure Low-Code/No-Code:** The industry consensus, supported
  by Greg’s experience at DataRobot, is that low-code/no-code tools fail for complex
  problems, leading major platforms to pivot back toward supporting expert coders.
  Value generation remains concentrated among those writing code.

  *   **LLMs Threaten SaaS Models:** The increasing viability of building in-house,
  customized solutions using LLMs (especially when self-hosted) poses a significant
  challenge to traditional SaaS businesses that rely on proprietary, black-box functionality.

  *   **Full-Stack Data Science Platform:** ZERV positions itself as a full-stack
  environment covering the entire data science lifecycle—from data connection and
  exploration to model training (with GPU support) and deployment (via custom APIs,
  SageMaker, or job scheduling)—all within a single, collaborative application.


  ### 4. Notable Companies/People

  *   **Dr. Greg Michaelson:** Co-founder of ZERV, bringing experience from DataRobot
  (CCO) and Travelers Insurance, emphasizing the need for industrialized data science.

  *   **ZERV:** The platform central to the discussion, focusing on collaborative,
  DAG-based coding.

  *   **AWS Trainium 2 & Dell AI Factory with Nvidia:** Sponsors mentioned, highlighting
  the importance of specialized hardware for large-scale AI.

  *   **OpenAI, AWS Bedrock, Hugging Face:** Key providers discussed regarding LLM
  integration and data security/hosting choices.


  ### 5. Future Implications

  The future of data science is moving toward **industrialized, collaborative, and
  AI-assisted development**. Platforms like ZERV are enabling teams to manage complex,
  multi-language workflows seamlessly, drastically cutting cycle times (up to 9x mentioned).
  The integration of AI agents will further democratize complex pipeline creation,
  allowing multiple human experts and their AI assistants to work concurrently on
  the same project canvas.


  ### 6. Target Audience

  This episode is highly valuable for **hands-on practitioners**, including **Data
  Scientists, ML Engineers, and Software Developers**, particularly those involved
  in MLOps, platform engineering, or struggling with collaboration and version control
  in traditional notebook environments. Professionals interested in the strategic
  shift toward platform-based data science will also benefit.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- startup
- investment
- nvidia
- microsoft
- google
title: '879: Serverless, Parallel, and AI-Assisted: The Future of Data Science is
  Here, with Zerve’s Dr. Greg Michaelson'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 129
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 23
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 6
  prominence: 0.6
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 3
  prominence: 0.3
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 14:17:04 UTC -->
