---
companies:
- category: unknown
  confidence: medium
  context: This is the Everyday AI Show, the everyday podcast where we simplify AI
    and br
  name: Everyday AI Show
  position: 12
- category: unknown
  confidence: medium
  context: o boost your career, business, and everyday life. Responsible AI used to
    be much more straightforward. I don't thi
  name: Responsible AI
  position: 197
- category: unknown
  confidence: medium
  context: . I hope you are too. What's going on? My name is Jordan Wilson, and welcome
    to Everyday AI. This is your daily l
  name: Jordan Wilson
  position: 1297
- category: unknown
  confidence: medium
  context: oing on? My name is Jordan Wilson, and welcome to Everyday AI. This is
    your daily live stream podcast and free
  name: Everyday AI
  position: 1327
- category: unknown
  confidence: medium
  context: it's something I think about probably every day. And I have so many questions,
    and maybe you do too, and
  name: And I
  position: 2284
- category: unknown
  confidence: medium
  context: eam audience, please help me welcome to the show, Sarah Bird, the Chief
    Product Officer of Responsible AI at M
  name: Sarah Bird
  position: 2459
- category: unknown
  confidence: medium
  context: ease help me welcome to the show, Sarah Bird, the Chief Product Officer
    of Responsible AI at Microsoft. Sarah, thank you
  name: Chief Product Officer
  position: 2475
- category: tech
  confidence: high
  context: d, the Chief Product Officer of Responsible AI at Microsoft. Sarah, thank
    you so much for joining the Everyda
  name: Microsoft
  position: 2518
- category: unknown
  confidence: medium
  context: responsibly, that people can use it responsibly. But I think the big thing
    that's changed is people's aw
  name: But I
  position: 4013
- category: unknown
  confidence: medium
  context: of how important this is and level of engagement. So I feel like before
    generative AI really took off, I
  name: So I
  position: 4126
- category: unknown
  confidence: medium
  context: tion. But what does that mean? So whether it's in Copilot Studio, I think
    this is also maybe in the Azure AI Found
  name: Copilot Studio
  position: 7604
- category: unknown
  confidence: medium
  context: Copilot Studio, I think this is also maybe in the Azure AI Foundry, but
    how does that actually work, multi-agentic o
  name: Azure AI Foundry
  position: 7654
- category: unknown
  confidence: medium
  context: e. One of the areas that we released at Build was Foundry Observability,
    and this is exactly giving you a monitoring syst
  name: Foundry Observability
  position: 9547
- category: unknown
  confidence: medium
  context: y, right, like Christmas. It's like, if you get a Super Nintendo in 1990,
    the last thing you're doing is reading t
  name: Super Nintendo
  position: 13702
- category: tech
  confidence: high
  context: ery podcast. Companies like Adobe, Microsoft, and Nvidia have partnered
    with us because they trust our exp
  name: Nvidia
  position: 17684
- category: unknown
  confidence: medium
  context: ducing harmful content? Can my user jailbreak it? Did I accidentally produce
    copyrighted material? And so
  name: Did I
  position: 19713
- category: unknown
  confidence: medium
  context: system. And people are pretty excited about this. Our Work Trends Index,
    for example, showed that I think 81% of employer
  name: Our Work Trends Index
  position: 20067
- category: unknown
  confidence: medium
  context: of the things we released at Build coming out of Microsoft Research is
    a system that is called the Agentic UI. And it
  name: Microsoft Research
  position: 23311
- category: unknown
  confidence: medium
  context: Microsoft Research is a system that is called the Agentic UI. And it is
    a research system for people to play w
  name: Agentic UI
  position: 23361
- category: unknown
  confidence: medium
  context: experts at coding. And we had, when we inside of Microsoft Teams come for
    and say, "We build this great thing for
  name: Microsoft Teams
  position: 24034
- category: unknown
  confidence: medium
  context: icrosoft, the categorization that came out in the International Safety
    Report that came out of the AI Action Summit in Paris, a
  name: International Safety Report
  position: 25668
- category: unknown
  confidence: medium
  context: International Safety Report that came out of the AI Action Summit in Paris,
    and it has three categories. So, the fi
  name: AI Action Summit
  position: 25717
- category: unknown
  confidence: medium
  context: k at this as we all grapple with what this means. But Sarah, as we wrap
    up the show here, we've covered a lot
  name: But Sarah
  position: 30377
- category: big_tech
  confidence: high
  context: The employer of the guest (Sarah Bird), focusing heavily on building and
    deploying generative AI, agentic AI, and responsible AI tools within their ecosystem
    (Copilot, Azure AI Foundry).
  name: Microsoft
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Microsoft's flagship AI assistant/product suite, mentioned as having been
    out for almost three years and heavily integrating agentic AI features.
  name: Copilot
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: A platform mentioned in relation to building and orchestrating multi-agentic
    systems.
  name: Copilot Studio
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: A Microsoft platform mentioned in connection with multi-agentic orchestration
    capabilities.
  name: Azure AI Foundry
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company that has partnered with the podcast host's organization
    for generative AI education.
  name: Adobe
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a company that has partnered with the podcast host's organization
    for generative AI education.
  name: Nvidia
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The research division within Microsoft that developed the 'Agentic UI'
    research system.
  name: Microsoft Research
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A Microsoft product/service used for user ID and access control, now being
    extended for agent identity and governance.
  name: Entra
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A Microsoft security product monitoring systems for threats, now relevant
    in the context of securing AI agents.
  name: Defender
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: A platform (likely Microsoft-related, given context) where developers build
    agents.
  name: Foundry
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced as a benchmark for training services offered by the podcast
    host's company.
  name: ChatGPT
  source: llm_enhanced
date: 2025-07-03 16:00:00 +0000
duration: 33
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/www.buzzsprout.com/2175779/episodes/17444730-ep-560-inside-multi-agentic-ai-3-critical-risks-and-how-to-navigate-them.mp3
processing_date: 2025-10-05 04:46:32 +0000
quotes:
- length: 223
  relevance_score: 6
  text: You know, I think that when we, when we switched when generative AI started
    and we were in the era of the chatbots, right, I think that a lot of the focus
    in responsible AI was just, is this system producing harmful content
  topics: []
- length: 219
  relevance_score: 5
  text: But maybe specifically when it comes to responsible AI and agentic AI in your
    own internal testing, what has been maybe the biggest surprise or learning that
    you think would be helpful for business leaders to understand
  topics: []
- length: 195
  relevance_score: 5
  text: So whether you're looking for ChatGPT training for thousands or just need
    help building your front-end AI strategy, you can partner with us just like some
    of the biggest companies in the world do
  topics: []
- length: 195
  relevance_score: 5
  text: So whether you're looking for ChatGPT training for thousands or just need
    help building your front-end AI strategy, you can partner with us just like some
    of the biggest companies in the world do
  topics: []
- length: 272
  relevance_score: 4
  text: I don't think it was ever simple necessarily, but with the rate of change
    when it comes to generative AI and everything we've seen from big tech companies
    everywhere with agentic AI, and not even that, multi-agentic AI, I think it changes
    responsible AI drastically, right
  topics: []
- length: 81
  relevance_score: 4
  text: But how has even just the growth of agents changed what responsible AI even
    means
  topics:
  - growth
- length: 134
  relevance_score: 4
  text: Maybe your company has been tinkering with large language models for a year
    or more but can't really get traction to find ROI on GenAI
  topics: []
- length: 157
  relevance_score: 4
  text: Companies like Adobe, Microsoft, and Nvidia have partnered with us because
    they trust our expertise in educating the masses around generative AI to get ahead
  topics: []
- length: 134
  relevance_score: 4
  text: Maybe your company has been tinkering with large language models for a year
    or more but can't really get traction to find ROI on GenAI
  topics: []
- length: 157
  relevance_score: 4
  text: Companies like Adobe, Microsoft, and Nvidia have partnered with us because
    they trust our expertise in educating the masses around generative AI to get ahead
  topics: []
- length: 81
  relevance_score: 3
  text: We're going to be recapping the most important insights from today's conversation
  topics: []
- length: 126
  relevance_score: 3
  text: Live stream audience, please help me welcome to the show, Sarah Bird, the
    Chief Product Officer of Responsible AI at Microsoft
  topics: []
- length: 147
  relevance_score: 3
  text: But before we dive into this topic, could you please just let everyone know
    what you do as the Chief Product Officer of Responsible AI at Microsoft
  topics: []
- length: 88
  relevance_score: 3
  text: It just seems like it's everywhere now within the Microsoft ecosystem and
    within Copilot
  topics: []
- length: 240
  relevance_score: 3
  text: And we've also lost kind of one of our most important mitigations, which is
    having the human just directly in the loop, having oversight, because now you're
    going to have agents working for longer periods of time without a human in the
    loop
  topics: []
- length: 143
  relevance_score: 3
  text: 'Yeah, you know, I think the probably the biggest thing is exactly that: you''re
    going to have a more complex system that you''re trying to govern'
  topics: []
- length: 37
  relevance_score: 3
  text: And so you have to be able to do that
  topics: []
- length: 76
  relevance_score: 3
  text: But that's where I think that's one of the most important things with agents
  topics: []
- impact_reason: Highlights the fundamental shift in the complexity of Responsible
    AI due to the emergence of agentic and multi-agentic systems, setting the central
    theme of the discussion.
  relevance_score: 10
  source: llm_enhanced
  text: Responsible AI used to be much more straightforward. I don't think it was
    ever simple necessarily, but with the rate of change when it comes to generative
    AI and everything we've seen from big tech companies everywhere with agentic AI,
    and not even that, multi-agentic AI, I think it changes responsible AI drastically,
    right?
  topic: safety/predictions
- impact_reason: Clearly links agentic capability (taking action) directly to increased
    risk surface area and higher potential negative implications.
  relevance_score: 10
  source: llm_enhanced
  text: But the challenge is that now if an agent is actually able to go and do tasks
    on your behalf, then there's more that can go wrong because it can actually take
    an action. And that can be a bigger surface area that can be higher implications
    because of the action it's taking.
  topic: safety/technical
- impact_reason: 'Presents the core benefit of the subtask decomposition pattern for
    safety: enabling granular testing and targeted guardrails for each component.'
  relevance_score: 10
  source: llm_enhanced
  text: And what's really great about that is that you can make each individual agent
    really good at the smaller tasks that it is doing. And so you can test it specifically
    to do that task. We can put guardrails around it that ensure that it's only doing
    that task.
  topic: safety/technical
- impact_reason: 'Provides a clear, memorable conceptual shift in human interaction
    with AI: moving from ''inner loop'' (constant intervention) to ''outer loop''
    (monitoring and intervention only when necessary).'
  relevance_score: 10
  source: llm_enhanced
  text: the previous era, we had humans really in the inner loop, and it's like you
    did a small task, and then the human checked, and you did a small task, and the
    human checked. And so what happens now is we're really moving humans more to the
    outer loop.
  topic: strategy/predictions
- impact_reason: Defines key performance indicators (KPIs) for monitoring agentic
    systems, focusing on alignment, intent understanding, and tool selection accuracy.
  relevance_score: 10
  source: llm_enhanced
  text: How well is the agent doing at staying on task? How well is the agent doing
    at understanding the user intent? How accurate is it in picking the right tool
    for the job?
  topic: technical/monitoring
- impact_reason: Lists specific, high-priority risks (prompt injection, copyright
    infringement) that must be included in AI testing paradigms.
  relevance_score: 10
  source: llm_enhanced
  text: is the system vulnerable to new types of prompt injection attacks, or is the
    system producing copyrighted material?
  topic: safety/testing
- impact_reason: Advocates for 'co-developing' testing alongside development (Shift-Left
    testing for AI), integrating quality assurance from the very beginning.
  relevance_score: 10
  source: llm_enhanced
  text: we start with what is the system supposed to do, and we build the testing
    we're going to be doing right alongside with the development of the system, so
    we don't wait till the final last mile and then test and find out we have an issue.
    We're co-developing...
  topic: strategy/testing
- impact_reason: 'Signals a crucial strategic pivot in AI governance: moving from
    focusing solely on novel AI risks (hallucinations) to applying established enterprise
    security and governance frameworks (like identity management) to agents.'
  relevance_score: 10
  source: llm_enhanced
  text: The first question people start asking is, 'Well, how do we secure and govern
    agents in that same way?' And so a lot more focus actually on not just the novel
    risk that we see with AI, but just being able to secure and govern AI like any
    other thing.
  topic: safety/strategy
- impact_reason: 'Defines a critical product strategy for agentic AI: building interfaces
    that translate complex AI actions into understandable and governable steps for
    human oversight.'
  relevance_score: 10
  source: llm_enhanced
  text: Where we can provide tools that bridge the gap between what the AI system
    is doing and what the user or the administrative system is doing, so if the administrator
    wants to specify, then we can help make those interfaces actually feel natural,
    and AI and humans work together.
  topic: technical/strategy
- impact_reason: Provides concrete examples of 'malfunctions' specific to agentic
    AI, including prompt injection, which is a key technical vulnerability.
  relevance_score: 10
  source: llm_enhanced
  text: The first is malfunctions, and that is the AI system doing something that
    it's not supposed to be doing. And that could, for example, be producing harmful
    content, or that could be it getting confused and going off task. It could be
    that it's leaking sensitive data accidentally, right? And those are some of the
    big ones we see with agents. It's vulnerable to prompt injection attacks, right?
    Those are all types of malfunctions.
  topic: safety/technical
- impact_reason: Identifies workforce readiness and skill transformation as a critical,
    often overlooked, 'systemic risk' of widespread agentic AI deployment.
  relevance_score: 10
  source: llm_enhanced
  text: The last risk, and then something is very top of mind for me, is the systemic
    risk with AI. And so, for example, with agents, I mentioned that people are going
    to deploy these alongside their workforce... preparing the workforce to actually
    be ready for this new skill set and collaborate with these tools, that's some
    systemic type of risk that we need to go and address.
  topic: safety/predictions
- impact_reason: Directly poses the critical governance challenge that business leaders
    face with autonomous agents—moving beyond simple chatbot guardrails to managing
    autonomous action.
  relevance_score: 9
  source: llm_enhanced
  text: But what about now when we talk about agentic AI? How does that change the
    ethics, the governance, the responsibility that we as business leaders need to
    have in order to make this thing work the right way?
  topic: safety/governance
- impact_reason: 'Identifies the next level of complexity: inter-agent communication
    and task delegation, which complicates accountability and oversight.'
  relevance_score: 9
  source: llm_enhanced
  text: And then when it comes to multi-agentic AI, when agents are talking among
    themselves, divvying tasks up and executing on our behalf, how does that change
    things?
  topic: safety/predictions
- impact_reason: Illustrates a massive, positive shift in customer maturity and prioritization
    regarding AI ethics and safety, moving it from a late-stage concern to a prerequisite.
  relevance_score: 9
  source: llm_enhanced
  text: I feel like before generative AI really took off, I was working in this space
    and I would meet with Microsoft's customers and share what we were doing and they're
    like, it's so great that Microsoft is doing this, but we're really early in our
    AI journey, so we have to get a lot more sophisticated before we even think about
    responsible AI. And now the first thing that people do is ask about responsible
    AI before they even get started with AI, which is excellent.
  topic: business/strategy
- impact_reason: Pinpoints the loss of direct human oversight as a major challenge
    introduced by autonomous agents operating over extended periods.
  relevance_score: 9
  source: llm_enhanced
  text: And we've also lost kind of one of our most important mitigations, which is
    having the human just directly in the loop, having oversight, because now you're
    going to have agents working for longer periods of time without a human in the
    loop.
  topic: safety
- impact_reason: 'Provides a crucial conceptual framework for classifying agents:
    they are novel entities requiring new governance models beyond traditional user/application
    paradigms.'
  relevance_score: 9
  source: llm_enhanced
  text: Agents are a little bit like an application, a little bit like a user, but
    not exactly like either. And so we need to adapt our systems to manage these new
    entities and secure and govern them in the way that we do users.
  topic: technical/governance
- impact_reason: Offers a pragmatic, current view of multi-agent systems (task decomposition)
    that contrasts with more futuristic, chaotic visions, suggesting a manageable
    path forward for responsible implementation.
  relevance_score: 9
  source: llm_enhanced
  text: I think what we're seeing in practice right now is something much simpler,
    but still extremely powerful, and I think much easier to deal with from a responsible
    AI point of view, which is that people are really using multi-agent systems to
    break their task into a bunch of subtasks.
  topic: technical/strategy
- impact_reason: Emphasizes the necessity of component-level governance and visibility,
    even in multi-agent systems, to avoid opaque complexity.
  relevance_score: 9
  source: llm_enhanced
  text: you're going to have a more complex system that you're trying to govern. And
    so you do need to break it into these components so that you're still governing
    each individual agent and not just the system as a whole because you still need
    visibility into what's happening in there.
  topic: governance/technical
- impact_reason: 'Lists the key performance indicators (KPIs) for agent safety and
    reliability that monitoring systems must track: task adherence, intent comprehension,
    and tool selection accuracy.'
  relevance_score: 9
  source: llm_enhanced
  text: guardrails that look and say, "Okay, how well is the agent doing at staying
    on task? How well is the agent doing at understanding the user intent? How accurate
    is it in picking the right tool for the job?"
  topic: technical/safety
- impact_reason: 'Summarizes the evolution of human oversight: shifting focus from
    real-time interaction to rigorous pre-deployment testing and post-deployment monitoring/administration.'
  relevance_score: 9
  source: llm_enhanced
  text: We still have to build these same human-in-the-loop mechanisms; it's just
    where the human goes in the loop is different now, and it's much more in the pre-development
    setting what you actually care about and appropriate testing, and in the post-deployment
    monitoring and administration of the system.
  topic: strategy/safety
- impact_reason: Highlights the critical need for human oversight and intervention
    mechanisms in autonomous AI systems (agents).
  relevance_score: 9
  source: llm_enhanced
  text: need to be able to monitor it so if it's going off task, then a human can
    come in and intervene.
  topic: safety/strategy
- impact_reason: Emphasizes that testing for AI systems must evolve beyond traditional
    software testing to cover novel AI-specific risks.
  relevance_score: 9
  source: llm_enhanced
  text: what we need to test for are different behaviors and different types of risk.
  topic: safety/testing
- impact_reason: Identifies 'trust' built via rigorous testing as the primary bottleneck
    for AI adoption and deployment readiness.
  relevance_score: 9
  source: llm_enhanced
  text: The thing that is sort of delaying them into getting an introduction is building
    that trust, and some of that comes through testing.
  topic: business/strategy
- impact_reason: Provides a strong, quantifiable prediction about the rapid adoption
    rate of AI agents in the enterprise within the near future.
  relevance_score: 9
  source: llm_enhanced
  text: 81% of employers are looking at deploying agents alongside their workforce
    in the next 18 months, right?
  topic: predictions/business
- impact_reason: Highlights a concrete technical solution (Agent ID) for integrating
    AI agents into existing enterprise security and identity management infrastructure.
  relevance_score: 9
  source: llm_enhanced
  text: a lot of what we released at Build, for example, is a new Entra agent ID so
    that agents can be tracked in your system just like anything else.
  topic: technical/security
- impact_reason: Poses a critical question about the necessary upskilling required
    for human supervisors as AI systems become more complex and autonomous.
  relevance_score: 9
  source: llm_enhanced
  text: does this human-in-the-loop model create a different level of users with higher
    skill levels to understand hallucinations and derailment?
  topic: safety/strategy
- impact_reason: Explains that HITL roles shift from micro-level validation to macro-level
    statistical decision-making based on aggregated performance metrics.
  relevance_score: 9
  source: llm_enhanced
  text: If you're in a different point in the loop, you are doing a different job...
    You're often then looking at aggregates and looking at numbers overall and saying,
    'Okay, if 99.8% of the time it does the right thing, is that good enough for my
    task?'
  topic: strategy/monitoring
- impact_reason: Calls for focused research and development on new Human-AI interface
    designs specifically tailored for monitoring distant, agentic systems.
  relevance_score: 9
  source: llm_enhanced
  text: 'I''d actually love to see a lot more innovation: innovation in this human-AI
    interface and how we design it for the world of agents where humans are farther
    out in the loop.'
  topic: technical/strategy
- impact_reason: Describes a practical AI tool designed to bridge the communication
    gap by monitoring the agent's understanding of user intent, thereby simplifying
    the human monitoring task.
  relevance_score: 9
  source: llm_enhanced
  text: one of the things we've built, for example, is a system that looks at, 'Does
    it understand the user intent?' and then alerts you if it seems to be confused
    about what you want...
  topic: technical/monitoring
- impact_reason: Defines AI agents fundamentally as new, governable entities within
    the enterprise IT landscape, necessitating new security protocols.
  relevance_score: 9
  source: llm_enhanced
  text: I think when it comes to agents, agents are, as I was saying earlier, basically
    a new entity that you're deploying in your system.
  topic: strategy/definition
- impact_reason: Highlights the fundamental usability and debugging challenge when
    empowering non-coders with code-generating AI, emphasizing the need for better
    human-AI interfaces.
  relevance_score: 9
  source: llm_enhanced
  text: We build this great thing for people that can't code, and it's going to code
    for them. But if there's a bug, they're just going to find the bug, and you're
    like, "How are they going to find the bug? The whole thing is that they don't
    code."
  topic: limitations/business
- impact_reason: Introduces a structured, recognized framework (from a major summit)
    for discussing and managing AI risk, which is highly relevant for governance and
    compliance.
  relevance_score: 9
  source: llm_enhanced
  text: The categorization that came out in the International Safety Report that came
    out of the AI Action Summit in Paris, and it has three categories.
  topic: safety/strategy
- impact_reason: Clearly separates accidental misuse (due to lack of understanding)
    from intentional misuse, requiring distinct mitigation strategies (education vs.
    security).
  relevance_score: 9
  source: llm_enhanced
  text: The next category is misuse, and you can see two types of misuse. I might
    misuse an AI system because I don't understand what it does... And then, of course,
    unfortunately, we live in a world where people are also intentionally misusing
    these systems.
  topic: safety
- impact_reason: 'Offers a strategic roadmap: risk mitigation must be tailored—technical
    fixes for malfunctions, policy/education for systemic risks.'
  relevance_score: 9
  source: llm_enhanced
  text: When we think about it, we have to look holistically across all of these,
    but it's pretty different tools that we're using to solve each of these risks,
    with malfunctions going much more technical, and then with systemic risk, we're
    looking much more at policy and education and upskilling programs...
  topic: strategy/safety
- impact_reason: 'Articulates the core challenge for leadership: the definition of
    human ''agency'' and critical thinking is being fundamentally redefined by advanced,
    multi-agentic AI systems.'
  relevance_score: 9
  source: llm_enhanced
  text: How should business leaders be doing that? Because it's one thing I even struggle
    with, because even agency seems like it's changing so much as these models now
    can think and they can plan and they can reason on top of working with each other.
  topic: predictions/strategy
- impact_reason: Emphasizes the necessity of shared, peer-to-peer learning and pattern
    sharing in the early stages of complex AI adoption, acknowledging current tool
    inconsistency.
  relevance_score: 9
  source: llm_enhanced
  text: The tools work well for some things; they don't work well for everything.
    And so, one of the things that we do even within my own team at Microsoft is have
    learning sessions where people share, 'Look, I built an agent that did this, and
    it worked really well,' and 'I've tried this, and I'm struggling to get it to
    work.'
  topic: business/strategy
- impact_reason: 'Defines the core mission of a Chief Product Officer of Responsible
    AI: identifying, mitigating, and democratizing risk management tools.'
  relevance_score: 8
  source: llm_enhanced
  text: We look at kind of risk we see emerging in new AI systems and then figure
    out how do we actually go address those risks? What does it take to test them?
    What does it take to mitigate them? And then how do we make it easy for everyone
    to do that?
  topic: strategy/business
- impact_reason: 'Articulates the core value proposition driving the adoption of agents:
    moving from conversational AI to task automation.'
  relevance_score: 8
  source: llm_enhanced
  text: I don't want a system that I just chat with. I want a system that's going
    to go and complete tasks for me so that I don't have to think about it. And so
    it's not surprising that we're seeing this huge excitement and people really starting
    to get value from these systems.
  topic: predictions/business
- impact_reason: Introduces a specific, actionable tool (Foundry Observability) designed
    to address the monitoring needs of agentic systems, focusing on task adherence
    and tool selection.
  relevance_score: 8
  source: llm_enhanced
  text: One of the areas that we released at Build was Foundry Observability, and
    this is exactly giving you a monitoring system for agents so you can see did the
    agent go off task, is it struggling to find the right tool for the job, and all
    that.
  topic: technical/business
- impact_reason: Highlights that traditional security practices, like least privilege,
    must be rigorously applied and monitored at the individual agent level within
    a multi-agent framework.
  relevance_score: 8
  source: llm_enhanced
  text: It's going to be difficult to ensure that you're doing important security
    practices like least privilege access and everything.
  topic: safety/security
- impact_reason: 'Defines the new role of the human in the outer loop: intervention
    triggered by automated monitoring of task deviation.'
  relevance_score: 8
  source: llm_enhanced
  text: But then you also need to be able to monitor it so if it's going off task,
    then a human can come in and intervene.
  topic: safety
- impact_reason: Acknowledges the significant challenge organizations face in testing
    complex, multi-agent AI systems.
  relevance_score: 8
  source: llm_enhanced
  text: How should organizations be testing multi-agentic systems? It seems like a
    Herculean task.
  topic: strategy/testing
- impact_reason: Addresses the challenge of enabling non-expert users (e.g., non-coders)
    to effectively govern, debug, or intervene in complex AI outputs.
  relevance_score: 8
  source: llm_enhanced
  text: we have to be more thoughtful about how can the humans speak in a way that
    they have the ability to and the governance in the way they have the ability to
    and still play their part.
  topic: safety/usability
- impact_reason: Provides a historical context for Responsible AI evolution, contrasting
    early chatbot risks with the new challenges posed by agents.
  relevance_score: 8
  source: llm_enhanced
  text: when we switched when generative AI started and we were in the era of the
    chatbots, right, I think that a lot of the focus in responsible AI was just, is
    this system producing harmful content? Can my user jailbreak it? Did I accidentally
    produce copyrighted material?
  topic: safety/history
- impact_reason: Reinforces the business reality that technical capability alone is
    insufficient; customer trust, driven by demonstrable quality/testing, is the key
    adoption hurdle.
  relevance_score: 8
  source: llm_enhanced
  text: 'And so, we hear from a lot of customers exactly that: that the thing that
    is sort of delaying them into getting an introduction is building that trust...'
  topic: business/adoption
- impact_reason: Prioritizes the cultural and motivational aspect of AI adoption over
    pure technical rollout, stressing that leadership must drive excitement and incentive.
  relevance_score: 8
  source: llm_enhanced
  text: First, I think it's motivating people to want to do it... you might not even
    know that new tools are available. And so, an important part is the leadership
    and the cultural element of incentivizing people and making people excited to
    try new tools.
  topic: business/strategy
- impact_reason: Provides a strong testament to the transformative power of modern
    generative AI, suggesting it enables entirely new categories of risk management
    tools.
  relevance_score: 8
  source: llm_enhanced
  text: Most of the systems I told you that we were developing to address these different
    risks, those are AI-powered. Those are all things we couldn't have done five years
    ago without generative AI.
  topic: technical/business
- impact_reason: Advocates for a collaborative, community-driven approach to mastering
    new AI capabilities, crucial for scaling internal expertise.
  relevance_score: 8
  source: llm_enhanced
  text: And so, having people learn from each other about the patterns of what's working
    and what's not, and really having this be a shared learning journey and not something
    that just everyone is on their own.
  topic: strategy
- impact_reason: Expresses optimism about the componentized, assembly-line approach
    to multi-agent systems as a viable and safe path for adoption.
  relevance_score: 7
  source: llm_enhanced
  text: And so I think that, actually, I'm really excited about this multi-agent pattern
    as something that we think people can be really successful with.
  topic: strategy
- impact_reason: A humble acknowledgment that best practices for testing and deploying
    advanced AI systems are still nascent and evolving across the industry.
  relevance_score: 7
  source: llm_enhanced
  text: we're all in a learning journey on how to do this well.
  topic: strategy
- impact_reason: In the current phase of AI maturity, experimentation and personalization
    are key drivers for finding value, rather than waiting for standardized best practices.
  relevance_score: 7
  source: llm_enhanced
  text: But in the earlier days right now, it's a lot of also getting people to experiment
    and find the things that are going to work best for them and their job.
  topic: business
source: Unknown Source
summary: '## EP 560: Inside Multi-Agentic AI: 3 Critical Risks and How to Navigate
  Them - Comprehensive Summary


  This episode of the Everyday AI Show, featuring **Sarah Bird, Chief Product Officer
  of Responsible AI at Microsoft**, dives deep into the evolving landscape of Responsible
  AI as it confronts the complexity of **Agentic AI** and, specifically, **Multi-Agentic
  Systems**. The core narrative revolves around how the shift from simple human-chatbot
  interaction to autonomous, task-executing agents drastically expands the surface
  area for risk and necessitates a fundamental rethinking of governance, testing,
  and human oversight.


  ### 1. Focus Area

  The discussion centers on **Responsible AI (RAI) governance and risk mitigation**
  specifically within the context of **Agentic AI** and **Multi-Agentic Orchestration**
  (systems where multiple agents collaborate to break down and execute complex tasks).
  Key technologies discussed include Microsoft''s **Copilot Studio**, **Azure AI Foundry**,
  and new security/governance tools like **Entra Agent ID**.


  ### 2. Key Technical Insights

  *   **Multi-Agent Structure as Componentization:** The most practical current form
  of multi-agent systems involves breaking a large task into smaller, specialized
  subtasks, allowing individual agents to be rigorously tested and governed for specific
  functions (like an assembly line).

  *   **Shift from Inner Loop to Outer Loop Oversight:** As agents work for longer
  durations without direct human intervention, the human role shifts from being "in
  the inner loop" (checking every small step) to being "in the outer loop" (monitoring
  aggregated performance and intervening only when alerts trigger).

  *   **Need for Agent Identity and Traditional Security:** A major realization is
  that agents must be governed like any other entity (users, devices). Microsoft is
  addressing this by creating tools like **Entra Agent ID** to provide agents with
  traceable identities for access control and security monitoring, mirroring existing
  enterprise security paradigms.


  ### 3. Business/Investment Angle

  *   **High Adoption Trajectory:** The market is rapidly moving toward agent deployment;
  Microsoft''s Work Trends Index indicated that **81% of employers** are looking to
  deploy agents alongside their workforce within the next 18 months.

  *   **Testing as a Critical Bottleneck:** Many organizations are delaying deployment
  because they underestimate the required testing rigor for agentic systems, leading
  to trust issues when unexpected behaviors or hallucinations occur late in the development
  cycle.

  *   **ROI Depends on Trust and Governance:** Achieving tangible ROI from GenAI hinges
  on building robust governance and testing frameworks *alongside* development, rather
  than as an afterthought, to ensure systems are fit for purpose, especially in regulated
  sectors like finance or healthcare.


  ### 4. Notable Companies/People

  *   **Sarah Bird (Microsoft):** Chief Product Officer of Responsible AI, driving
  the strategy and tooling for mitigating emerging risks in new AI systems.

  *   **Microsoft:** Mentioned as a key developer of agentic tools (Copilot, Azure
  AI Foundry) and governance solutions (Entra, Foundry Observability).

  *   **Microsoft Research:** Mentioned for developing experimental interfaces like
  the **Agentic UI** to explore better human-agent interaction patterns.


  ### 5. Future Implications

  The industry is moving toward complex, autonomous workflows managed by interconnected
  AI entities. This requires significant innovation in **Human-AI Interface (HAI)
  design** to effectively support humans operating in the "outer loop." Furthermore,
  the focus of RAI is broadening from purely novel AI risks (like content generation)
  to integrating agents into existing, mature **enterprise security and governance
  frameworks**.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Leaders, Product Managers, Chief Risk
  Officers (CROs), and Enterprise Architects** who are responsible for deploying,
  securing, and scaling generative AI applications beyond basic chatbot interfaces.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- microsoft
- nvidia
title: 'EP 560: Inside Multi-Agentic AI: 3 Critical Risks and How to Navigate Them'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 108
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 18
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 2
  prominence: 0.2
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 04:46:32 UTC -->
