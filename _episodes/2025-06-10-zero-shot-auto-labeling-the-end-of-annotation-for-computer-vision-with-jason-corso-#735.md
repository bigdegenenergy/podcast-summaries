---
companies:
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the Twemal AI podcast. I
    am, of course, your host Sam Charringt
  name: Twemal AI
  position: 580
- category: unknown
  confidence: medium
  context: the Twemal AI podcast. I am, of course, your host Sam Charrington. Today
    I'm joined by Jason Corso. Jason is co-fou
  name: Sam Charrington
  position: 626
- category: unknown
  confidence: medium
  context: cast. I am, of course, your host Sam Charrington. Today I'm joined by Jason
    Corso. Jason is co-founder of V
  name: Today I
  position: 643
- category: unknown
  confidence: medium
  context: e, your host Sam Charrington. Today I'm joined by Jason Corso. Jason is
    co-founder of Voxel 51 and a professor
  name: Jason Corso
  position: 663
- category: unknown
  confidence: medium
  context: 1 where we make a software dev tool, kind of like VS Code for computer
    vision or VS Code for visual AI. My
  name: VS Code
  position: 1349
- category: unknown
  confidence: medium
  context: personally fallen prey to that direction as well. When I was a postdoc
    at UCLA, I built my own 3D—I was wo
  name: When I
  position: 5274
- category: tech
  confidence: high
  context: ilt my own one at the time. In fact, I think that notion was so prevalent
    in our strategic thinking early
  name: Notion
  position: 5476
- category: unknown
  confidence: medium
  context: ations with common annotation tools like CVAT and Label Studio, Labelbox,
    other annotation tools, mostly because
  name: Label Studio
  position: 5725
- category: tech
  confidence: high
  context: mmon annotation tools like CVAT and Label Studio, Labelbox, other annotation
    tools, mostly because there wer
  name: Labelbox
  position: 5739
- category: unknown
  confidence: medium
  context: ill integrate with all the other tools out there. And I think it's primarily
    because 51 has become a litt
  name: And I
  position: 7064
- category: unknown
  confidence: medium
  context: primarily because 51 has become a little bit of a Rosetta Stone for visual
    data formats. It's very easy to get yo
  name: Rosetta Stone
  position: 7131
- category: unknown
  confidence: medium
  context: ', is you can add your own panels. So 51 is both a Python SDK and a web-based
    app, right? And so in the app, yo'
  name: Python SDK
  position: 7737
- category: unknown
  confidence: medium
  context: owing the boundary? How do you get there quickly? So I wrote a blog in
    January of 2024 with the title "A
  name: So I
  position: 12148
- category: unknown
  confidence: medium
  context: learning is not new, active learning is not new. But I think what we're
    seeing is that with the creation
  name: But I
  position: 14157
- category: unknown
  confidence: medium
  context: ke—why am I forgetting the name of the model now? Not COCO, but the very
    classic vision like which embedding
  name: Not COCO
  position: 15767
- category: unknown
  confidence: medium
  context: recently released a report on arXiv called "Auto-Labeling Data for Object
    Detection," as well as a blog post, "Z
  name: Labeling Data
  position: 19964
- category: unknown
  confidence: medium
  context: a report on arXiv called "Auto-Labeling Data for Object Detection," as
    well as a blog post, "Zero-Shot Auto-Labelin
  name: Object Detection
  position: 19982
- category: unknown
  confidence: medium
  context: Object Detection," as well as a blog post, "Zero-Shot Auto-Labeling Rivals
    Human Performance," that kind of
  name: Shot Auto
  position: 20031
- category: unknown
  confidence: medium
  context: tection," as well as a blog post, "Zero-Shot Auto-Labeling Rivals Human
    Performance," that kind of captures all of these ideas. Can y
  name: Labeling Rivals Human Performance
  position: 20041
- category: unknown
  confidence: medium
  context: ation models we used are like YOLO-World, YOLO-E, Grounding DINO, concretely.
    So, they're not really VLMs, but I t
  name: Grounding DINO
  position: 21176
- category: tech
  confidence: high
  context: abeling, like GPU rental on AWS, was $1.18 for an Nvidia L40S, $1.18 total
    for the 400-something models. A
  name: Nvidia
  position: 25092
- category: unknown
  confidence: medium
  context: four datasets, and this is based on a paper from Kristen Grauman's group
    in I think ICCV 2017 or 2019 where they w
  name: Kristen Grauman
  position: 26329
- category: unknown
  confidence: medium
  context: challenging, like the old visual verbs work from Ali Farhadi a decade ago
    or something like that, right? Like
  name: Ali Farhadi
  position: 37895
- category: ai_application
  confidence: high
  context: Company co-founded by Jason Corso that makes software (51) for computer
    vision/visual AI development, acting as a dev tool for dataset creation, model
    analysis, and visualization.
  name: Voxel 51
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Where Jason Corso is a professor and where Voxel 51 originated some of
    its work on analysis tooling for computer vision.
  name: University of Michigan
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a common, existing annotation tool that Voxel 51 integrates
    with.
  name: CVAT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a common, existing annotation tool that Voxel 51 integrates
    with.
  name: Label Studio
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a common, existing annotation tool that Voxel 51 integrates
    with.
  name: Labelbox
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Where Jason Corso was a postdoc and built his own 3D medical image annotation
    tool.
  name: UCLA
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an example of a classic perceptual embedding model used in
    their research report.
  name: ResNet-50
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an example of a classic perceptual embedding model used in
    their research report.
  name: ResNet-18
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a contemporary foundation model/embedding model whose embeddings
    are used in their research report for measuring classification difficulty.
  name: CLIP
  source: llm_enhanced
date: 2025-06-10 16:54:00 +0000
duration: 57
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: annotation, if you will, or dataset building, really,
  text: the future of annotation, if you will, or dataset building, really, is models
    will be more like—we will have agents, almost like embedding space agents, whose
    whose—trained whether or not with RL or I don't know—but we'll train to be asking
    the domain experts when they're not sure directly by generalizing the notion of
    uncertainty, as you said, active learning for the situation where you're not necessarily
    training one model downstream; you just kind of enrich the embedding space to
    ensure that decision boundaries are well separated.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN5226479586.mp3?updated=1749575933
processing_date: 2025-10-05 10:54:02 +0000
quotes:
- length: 188
  relevance_score: 5
  text: And what we've learned over the years generally is you don't just get data
    dropped into your lap like a student does in my computer vision class, right,
    and then you have to find the model
  topics: []
- length: 216
  relevance_score: 5
  text: I often hear that one of the biggest lift investments that teams can do is
    to build custom annotation tools because every application or use case is kind
    of a little bit different versus using something off the shelf
  topics:
  - investment
- length: 152
  relevance_score: 5
  text: 'So, basically, the assumptions are: you have a foundation model that is—the
    most important thing is to get it in the simplest setup that you can imagine'
  topics: []
- length: 191
  relevance_score: 5
  text: There's some recent work like there was a paper, Florence 2 Model at CVPR
    24, they have this interesting notion of a data engine where they're weighting
    over multiple foundation model outputs
  topics: []
- length: 172
  relevance_score: 4
  text: We released an open source version both in the computer vision community and
    the machine learning community, and it's become a pretty widely adopted tool for
    everyday usage
  topics: []
- length: 125
  relevance_score: 4
  text: People in the community of machine learning and computer vision have been
    looking at this for at least a decade or two, right
  topics: []
- length: 111
  relevance_score: 4
  text: But there is enough structure in these embedding spaces that we can start
    to even leverage some classical ideas
  topics: []
- length: 157
  relevance_score: 4
  text: So, for all the models, actually, to produce the labels, which is only done
    once—not inference, not the creation of the models, not the training of the model
  topics: []
- length: 173
  relevance_score: 4
  text: So, VOC, COCO, even BDD—most or all the classes in those three datasets are
    at least covered in some way or another in the training data or the foundation
    models that we use
  topics: []
- length: 194
  relevance_score: 4
  text: Like it's actually what they found was it was better to train separate detectors
    for like the compositions because these models were training these features were
    leveraging are not compositional
  topics: []
- length: 196
  relevance_score: 4
  text: I guess first of all, I'm a little worried about using the LLM or LLMs to
    generate synthetic datasets because they assume knowledge of the underlying embedding
    space or the manifolds in that space
  topics: []
- length: 189
  relevance_score: 3
  text: In our auto-labeling scenario, take the same unlabeled data, take a foundation
    model, generate those auto-labels—we call them now—similarly train an apples-to-apples
    sort of object detector
  topics: []
- length: 109
  relevance_score: 3
  text: Instead, what we found is that lowering your confidence threshold to somewhat
    egregiously low numbers, like 0
  topics: []
- impact_reason: This is a strong, provocative statement challenging the status quo
    of traditional supervised learning data preparation, signaling a major shift in
    data strategy.
  relevance_score: 10
  source: llm_enhanced
  text: Wait a second, what we've been doing as annotation, just blindly sending everything
    out for labeling, is not the future.
  topic: strategy
- impact_reason: Directly links the rise of foundation models to the obsolescence
    of routine human labeling tasks, highlighting a key technological driver for change.
  relevance_score: 10
  source: llm_enhanced
  text: We're seeing foundation models come along that could indeed actually replace
    a lot of those typical case human labels.
  topic: AI technology trends
- impact_reason: Provides a clear, conceptual framework (Annotation 1.0 vs 2.0) for
    the evolution of data labeling, shifting the human role from mass labeler to interactive
    validator/question-answerer.
  relevance_score: 10
  source: llm_enhanced
  text: If what was annotation 1.0 was 'blindly send me data and get me labels,' annotation
    2.0 might only be a human answering questions asked by the agent, if you will,
    agent labeling or something like that.
  topic: predictions
- impact_reason: Describes specific, advanced analysis techniques (hardness/mistakenness
    metrics derived from logits) used to surface failure modes for human review.
  relevance_score: 10
  source: llm_enhanced
  text: We have capabilities in the system to, for example, compute what we call hardness
    or mistakenness that take your model logits on the data and manipulate them in
    certain ways to then provide to you clusters of outputs that you can then visually
    analyze because in our view, the human expert... is the one who really can make
    the decision to look at whatever we're training at.
  topic: technical
- impact_reason: Provides a concrete, high-stakes business case (losing $800k on a
    $1M labeling budget) illustrating the inefficiency of current annotation practices.
  relevance_score: 10
  source: llm_enhanced
  text: About two years ago, we began to see a shift in the types of conversations
    we would have with users of 51 where they were going from, 'I'm strictly sending
    all of my data to annotation,'... and spending, say, a round number, spending
    a million dollars on this, and then I'm getting it back, and instead of being
    able to use it all, I'm finding out that actually I can only use 10% of it or
    20% of it, which means they're throwing away a significant amount of money.
  topic: business
- impact_reason: 'Articulates the core challenge of data selection: the need to focus
    on hard, boundary-defining examples rather than abundant, easy examples.'
  relevance_score: 10
  source: llm_enhanced
  text: It wasn't clear that the right subset of the data, right? Data—it's easy to
    find typical plus or minus a couple sigma cases, but what really matters is finding
    data along the decision boundaries.
  topic: technical
- impact_reason: Identifies uncertainty quantification as the critical, long-standing
    hurdle for advanced ML deployment.
  relevance_score: 10
  source: llm_enhanced
  text: What are the linchpins in fully getting there, if you will, is being able
    to better characterize or quantify uncertainty in these models, and that's been
    a challenge in the community for a really long time.
  topic: safety/technical
- impact_reason: 'Defines a highly valuable research goal: predicting model performance/data
    difficulty using only pre-trained embeddings and minimal labels.'
  relevance_score: 10
  source: llm_enhanced
  text: Can we measure the difficulty, the expected classification difficulty in that
    space without training the downstream classifier, assuming we only had labels
    on a small subset of the data, like 100 samples per class, for example?
  topic: technical
- impact_reason: Presents a novel, practical proxy for uncertainty (ratio of autoencoder
    reconstruction errors) that correlates with actual model performance.
  relevance_score: 10
  source: llm_enhanced
  text: If you take ratios of reconstruction errors, which is a measure of uncertainty
    to some degree, you can—there are certain classes of those ratios that correlate
    very strongly with downstream classifier performance.
  topic: technical
- impact_reason: 'Articulates the core business value proposition for auto-labeling:
    eliminating wasted spend on easy, redundant data points.'
  relevance_score: 10
  source: llm_enhanced
  text: '"Hey, we spent a million dollars on labeling, and it wasn''t all that useful."
    It would be useful just to auto-label the stuff that''s the same. You don''t necessarily
    have to auto-label the outliers to create value.'
  topic: business
- impact_reason: 'Identifies the second critical business challenge: the QA bottleneck
    negates the efficiency gains of auto-labeling if not optimized.'
  relevance_score: 10
  source: llm_enhanced
  text: How do I minimize the human work needed to do QA on the output of that? Because
    if I have to have humans go and validate 100% of the auto-labels, then I've not
    saved any time at all, right?
  topic: business
- impact_reason: Provides a staggering, concrete comparison demonstrating the potential
    cost savings of using foundation models for labeling generation.
  relevance_score: 10
  source: llm_enhanced
  text: We estimate that collectively these four datasets would cost about $124,000
    to have humans annotate them... And the comparable cost in auto-labeling, like
    GPU rental on AWS, was $1.18 for an Nvidia L40S, $1.18 total for the 400-something
    models.
  topic: business/predictions
- impact_reason: This is a staggering quantification of the cost efficiency of auto-labeling
    versus human labeling, showing a massive ROI potential.
  relevance_score: 10
  source: llm_enhanced
  text: And the comparable cost in auto-labeling, like GPU rental on AWS, was $1.18
    for an Nvidia L40S, $1.18 total for the 400-something models. A dollar eighteen
    total to produce all the labels.
  topic: business
- impact_reason: A concise summary of the primary finding regarding cost difference,
    which is a major business driver for adopting auto-labeling.
  relevance_score: 10
  source: llm_enhanced
  text: So, that's six orders of magnitude more expensive to have humans label than
    models label.
  topic: business
- impact_reason: 'A counterintuitive technical finding: using lower confidence labels
    (more noise) leads to better final model performance, suggesting noise aids generalization.'
  relevance_score: 10
  source: llm_enhanced
  text: lowering your confidence threshold to somewhat egregiously low numbers, like
    0.1 or 0.2, where you will have no easy outputs in the auto-labels, ultimately
    maps to better downstream performance.
  topic: technical
- impact_reason: A direct critique of the current state of embedding spaces, suggesting
    they lack the necessary compositional structure required for robust generalization
    to novel combinations.
  relevance_score: 10
  source: llm_enhanced
  text: I still don't think the current semantically enriched embedding spaces we
    have today are compositional sufficiently compositional still.
  topic: technical
- impact_reason: 'This is a forward-looking technical vision: using ''agents'' to
    intelligently query experts to refine the embedding space itself, moving beyond
    simple sequential model training.'
  relevance_score: 10
  source: llm_enhanced
  text: we'll have agents, almost like embedding space agents, whose whose—trained
    whether or not with RL or I don't know—but we'll train to be asking the domain
    experts when they're not sure directly by generalizing the notion of uncertainty,
    as you said, active learning for the situation where you're not necessarily training
    one model downstream; you just kind of enrich the embedding space to ensure that
    decision boundaries are well separated.
  topic: predictions
- impact_reason: A significant caution regarding synthetic data generation using LLMs,
    pointing out that the generator model might inherently bias the synthetic data
    towards its own learned manifold, limiting true diversity.
  relevance_score: 10
  source: llm_enhanced
  text: I am a little worried about using the LLM or LLMs to generate synthetic datasets
    because they assume knowledge of the underlying embedding space or the manifolds
    in that space
  topic: safety
- impact_reason: A concise and powerful analogy that immediately frames the product's
    utility and target audience (ML engineers/data scientists working on vision).
  relevance_score: 9
  source: llm_enhanced
  text: Voxel 51 where we make a software dev tool, kind of like VS Code for computer
    vision or VS Code for visual AI.
  topic: business
- impact_reason: Reveals a strategic decision to focus on analysis/workflow integration
    rather than competing directly in the crowded annotation tool space, prioritizing
    interoperability.
  relevance_score: 9
  source: llm_enhanced
  text: In order to annotate in 51, we provided integrations with common annotation
    tools like CVAT and Label Studio, Labelbox, other annotation tools, mostly because
    there were so many of them out there, and we felt it was going to be hard to distinguish
    any new tool like ours from another tool that was already out there.
  topic: strategy
- impact_reason: 'Excellent metaphor describing the value proposition: providing visibility
    and diagnostic capability within the core ML iteration loop.'
  relevance_score: 9
  source: llm_enhanced
  text: We inject 51 into the middle of that [data -> train -> analysis -> repeat]
    as the analysis work, right? Kind of like turning the lights on in a dark hallway.
  topic: strategy
- impact_reason: Defines the core analytical questions that drive model improvement,
    which the software is designed to answer through data clustering and visualization.
  relevance_score: 9
  source: llm_enhanced
  text: What are the corner cases? Where is it performing poorly? Exactly. Are there
    commonalities or is there structure to that lack of performance?
  topic: technical
- impact_reason: Highlights the critical role of embedding visualization (e.g., using
    t-SNE/UMAP) as a debugging tool for identifying label noise and data structure
    issues.
  relevance_score: 9
  source: llm_enhanced
  text: Another analysis we use is visualization of embeddings. So you can compute
    your embeddings... and then you can visually interact in the app to find situations
    for mislabel on your actual dataset.
  topic: technical
- impact_reason: A direct, attention-grabbing declaration that serves as a public
    marker for the speaker's stance on the future of data labeling.
  relevance_score: 9
  source: llm_enhanced
  text: I wrote a blog in January of 2024 with the title 'Annotation is Dead.'
  topic: strategy
- impact_reason: 'Clarifies the nuance behind the ''Annotation is Dead'' claim: it''s
    the *process* of blind labeling that is dead, not the need for high-quality, targeted
    labels.'
  relevance_score: 9
  source: llm_enhanced
  text: It wasn't so much that no one's going to need annotated data. Obviously, it's
    a well-oiled machine. We know how supervised machine learning works... but it
    was really intended to catalyze this notion that wait a second, what we've been
    doing as annotation, just blindly sending everything out for labeling, is not
    the future.
  topic: safety/ethics
- impact_reason: Provides a specific timeline estimate for this paradigm shift, suggesting
    the industry is already deeply engaged in the transition.
  relevance_score: 9
  source: llm_enhanced
  text: Annotation 2.0 might only be a human answering questions asked by the agent,
    if you will, agent labeling or something like that. To me, we're on that timeline
    and we're probably somewhere in the middle of it.
  topic: predictions
- impact_reason: A provocative title intended to spark a necessary conversation about
    the sustainability and future of traditional data labeling practices in ML.
  relevance_score: 9
  source: llm_enhanced
  text: '"Annotation is Dead."'
  topic: strategy
- impact_reason: Connects the rise of foundation models and their embedding spaces
    to a deeper, exploitable structure in data representation.
  relevance_score: 9
  source: llm_enhanced
  text: With the creation of these semantically enriched foundation models and embedding
    spaces, there's more of a structure underlying structure to the way data is represented
    in the world...
  topic: technical
- impact_reason: A candid admission that fundamental progress on classical uncertainty
    metrics remains elusive, despite architectural advances.
  relevance_score: 9
  source: llm_enhanced
  text: If we really want a true measure of uncertainty, like a classical P of X type
    uncertainty, I don't think we have—I still don't think we have made it quite there
    yet. We haven't made much progress, right? We still don't really know how to do
    that.
  topic: technical
- impact_reason: Suggests that foundation model embeddings offer a structural shortcut
    to estimating uncertainty, even without perfect theoretical tools.
  relevance_score: 9
  source: llm_enhanced
  text: This is evidence, if you will, that there seems to be some structure in these
    more rich, semantically enriched embedding spaces that even though we still don't
    have the right machinery to go and fully compute uncertainty, at least we can
    get, say, this is using the term loosely, kind of like marginal uncertainty in
    that space.
  topic: technical
- impact_reason: 'Frames the first key challenge for AI adoption: building a ''model
    of predictability'' to manage expectations for automated labeling quality.'
  relevance_score: 9
  source: llm_enhanced
  text: How much of that auto-labeling can I expect to be useful? And I think that's
    a key challenge.
  topic: business
- impact_reason: Provides a concrete, high-stakes baseline cost for traditional human
    annotation for a specific task/dataset size.
  relevance_score: 9
  source: llm_enhanced
  text: Using off-the-shelf established numbers for how much things cost to annotate—so,
    7 cents per box, for example—we estimate that collectively these four datasets
    would cost about $124,000 to have humans annotate them, just one pass, a few minutes
    at a time, so no QA, nothing like that.
  topic: business
- impact_reason: Quantifies the time savings (6,000 human hours vs. 1 hour GPU time),
    grounding the estimate in academic literature.
  relevance_score: 9
  source: llm_enhanced
  text: We estimate that it would take about 6,000 hours of humans to label these
    four datasets, and this is based on a paper from Kristen Grauman's group in I
    think ICCV 2017 or 2019 where they were kind of measuring the value of a label.
  topic: business
- impact_reason: Highlights severe, systematic errors ('ontological failure') that
    models might avoid, suggesting auto-labeling can sometimes be more logically consistent
    than rushed human labeling.
  relevance_score: 9
  source: llm_enhanced
  text: Similarly, they might group together six donuts as one donut, and actually
    one example we have is the whole entire tray, essentially the whole image, is
    labeled as donuts, which is just like this is a complete ontological failure of
    the domain, right?
  topic: safety/quality
- impact_reason: Provides empirical evidence that models trained on auto-labels achieve
    downstream performance very close (e.g., 80-90% relative) to models trained on
    human labels.
  relevance_score: 9
  source: llm_enhanced
  text: we can get within relative numbers in the 80s and 90s performance of whatever,
    like, YOLOv11's human performance is... In auto-labeled, so auto-label-trained
    YOLOv11s can get like 55% performance or something like that, very close performance.
  topic: technical
- impact_reason: 'Offers a strategic business trade-off: reinvest annotation savings
    into larger, more capable downstream models to achieve superior performance.'
  relevance_score: 9
  source: llm_enhanced
  text: if you're willing to save the money you would have spent on annotation and
    instead train a bigger model for compute and spend that money over time, you can,
    apples-to-apples, you can actually outperform the smaller model, right?
  topic: business/strategy
- impact_reason: Connects the auto-labeling noise finding to established ML principles
    (like dropout), framing it as a form of beneficial regularization.
  relevance_score: 9
  source: llm_enhanced
  text: the general idea is consistent with machine learning just seems to perform
    better when there's the right amount of noise, just like dropout and all these
    other things that we've done to create noise when we're training models.
  topic: technical
- impact_reason: A philosophical summary suggesting quantity and diversity of data
    (even noisy data) might outweigh the pursuit of perfect, scarce labels.
  relevance_score: 9
  source: llm_enhanced
  text: I think it's just another chip or another tile in the mosaic of messaging
    around machine learning needs data, and the data does not have to be perfect,
    but more is better than better in some sense.
  topic: strategy
- impact_reason: 'Clearly defines the primary limitation of zero-shot auto-labeling:
    performance degradation on novel or complex concepts not well-represented in the
    foundation model''s training data.'
  relevance_score: 9
  source: llm_enhanced
  text: how do you handle the decision boundary classes, or how do you handle complex
    classes that are out of domain?
  topic: technical/limitations
- impact_reason: 'Identifies the next necessary evolution in data labeling pipelines:
    intelligent filtering of auto-labels.'
  relevance_score: 9
  source: llm_enhanced
  text: The key nugget here is being able to identify, either automatically or semi-automatically
    with the human in the loop, when these auto-labels should be accepted and when
    they should be rejected.
  topic: strategy
- impact_reason: Introduces a concrete framework ('verified auto-labeling') for integrating
    human oversight efficiently into the auto-labeling workflow.
  relevance_score: 9
  source: llm_enhanced
  text: this notion of what we're calling verified auto-labeling. So, generate your
    auto-labels, and it's still on the human whether or not it's a QA person that
    you think about the company, or a team member, actually trained MLE—someone really
    should be saying yes, no, yes, no, yes, no.
  topic: business/strategy
- impact_reason: Proposes a practical, tiered system (Green/Yellow/Red) for prioritizing
    human review based on model confidence/uncertainty.
  relevance_score: 9
  source: llm_enhanced
  text: Our approach is what we think will work—we're still working on this, admittedly—but
    what we think will work is being able to identify what we're thinking of as a
    green, yellow, and red light, think a bit like a stoplight, right?
  topic: strategy
- impact_reason: Pinpoints the 'Yellow' zone—the ambiguous cases—as the critical area
    where human expertise provides the highest marginal value.
  relevance_score: 9
  source: llm_enhanced
  text: the challenge is the yellow, the cases in the middle, right? Are there cases
    where it's unclear if it is a decision boundary problem... or if it's a mislabel
    from the auto-label or someone? So, let's have humans look at that.
  topic: strategy
- impact_reason: This clearly outlines the three-tier approach to data uncertainty
    (red/clear rejection, yellow/human review, green/auto-accepted) which is fundamental
    to active learning and efficient data labeling pipelines.
  relevance_score: 9
  source: llm_enhanced
  text: the cases are clearly at the decision boundary; we're not confident about
    these; we don't know what they are, so let's just throw away those. And then the
    challenge is the yellow, the cases in the middle, right? Are there cases where
    it's unclear if it is a decision boundary problem, like a child near the stoplight,
    stop sign example you gave earlier, or if it's a mislabel from the auto-label
    or someone?
  topic: technical
- impact_reason: 'Highlights the critical, unsolved challenge in modern ML: generalization
    to truly out-of-distribution (OOD) domains, moving beyond standard benchmarks
    like COCO.'
  relevance_score: 9
  source: llm_enhanced
  text: Do you have any intuition around how this approach will perform for out-of-distribution
    domains, like fault detection in electron microscopy images, or some industrial
    use case that is not represented in COCO and some of these other datasets?
  topic: technical
- impact_reason: 'A key insight into the limitations of current feature learning:
    models struggle with novel compositional concepts (like ''person on horse'') if
    the underlying features aren''t sufficiently compositional.'
  relevance_score: 9
  source: llm_enhanced
  text: it's better—it's if you have person on horse, you have person on bicycle,
    person climbing, or something like that, right? Like it's actually what they found
    was it was better to train separate detectors for like the compositions because
    these models were training these features were leveraging are not compositional.
  topic: technical
- impact_reason: A crucial reality check on the hype surrounding data efficiency methods;
    the speaker notes that massive, publicized reductions in human labeling dependency
    are not yet mainstream reality.
  relevance_score: 9
  source: llm_enhanced
  text: I still don't hear people seeing from the mountaintops that, 'Oh look, I don't
    need humans anymore for labeling this,' and 'My need for human labeling is 10%
    of what it was last year.' So, either they've discovered it and they're not talking
    about it, which is a possibility, or it's still just in this experimental stage.
  topic: business
- impact_reason: 'Identifies a critical area for maturation in ML: adopting rigorous
    uncertainty quantification and performance guarantees from traditional engineering/control
    theory.'
  relevance_score: 9
  source: llm_enhanced
  text: that notion of how you minimize human work but still provide some guarantees
    on it—there's a whole area in controls and other spaces in engineering that have
    mechanisms for measuring those types of uncertainties or performance guarantees
    and so on. We just haven't done that in our space.
  topic: safety
- impact_reason: Emphasizes the iterative, intertwined nature of dataset creation
    and model development, contrasting with idealized academic workflows.
  relevance_score: 8
  source: llm_enhanced
  text: The real world is making the dataset alongside making the model and really
    understanding how these things coincide together.
  topic: strategy
- impact_reason: Identifies a major pain point in the industry—the high cost and effort
    of bespoke tooling—which the speaker's product aims to solve or circumvent.
  relevance_score: 8
  source: llm_enhanced
  text: I often hear that one of the biggest lift investments that teams can do is
    to build custom annotation tools because every application or use case is kind
    of a little bit different versus using something off the shelf.
  topic: business
- impact_reason: Positions the tool as a critical interoperability layer or universal
    standard for handling diverse visual data types.
  relevance_score: 8
  source: llm_enhanced
  text: 51 has become a little bit of a Rosetta Stone for visual data formats.
  topic: technical
- impact_reason: Details the extensibility model (plugins/panels) that allows deep
    customization within the platform, catering to users with specific needs.
  relevance_score: 8
  source: llm_enhanced
  text: You can add your own panels. So 51 is both a Python SDK and a web-based app,
    right? And so in the app, you can extend the capabilities of the app through what
    we call panels, which are essentially plugins that have a visual component to
    them.
  topic: technical
- impact_reason: Sets up the narrative for the shift towards auto-labeling, exploring
    whether it was a proactive strategic pivot or a reactive response to external
    technological shifts.
  relevance_score: 8
  source: llm_enhanced
  text: Talk a little bit about the evolutionary path towards automating labeling
    or auto-labeling. Was this something that you started out intending to do, or
    is it something that arose as a result of powerful foundation models, for example,
    or something else?
  topic: strategy
- impact_reason: Implies that foundation models will excel at common, easy cases,
    forcing human effort to focus exclusively on rare, complex, or boundary cases.
  relevance_score: 8
  source: llm_enhanced
  text: What can we expect for l[abels]... We can expect to get decent labels for
    common classes. What can we expect for l[abels]...
  topic: AI technology trends
- impact_reason: A strong statement asserting that current, resource-intensive annotation
    methods are unsustainable given AI advancements.
  relevance_score: 8
  source: llm_enhanced
  text: The future cannot be that way [blindly sending everything out for labeling].
  topic: strategy
- impact_reason: Highlights the inefficiency of Annotation 1.0, contrasting it with
    the potential of foundation models to reduce this burden.
  relevance_score: 8
  source: llm_enhanced
  text: What we've been doing as annotation, just blindly sending everything out for
    labeling, is not the future.
  topic: business
- impact_reason: 'Identifies a critical limitation: the proposed uncertainty/difficulty
    measurement method struggles with highly skewed datasets dominated by hard outliers.'
  relevance_score: 8
  source: llm_enhanced
  text: If a dataset is dominated by those rare cases that are hard to disentangle
    from the truth, then it would be a kryptonite for this modeling that we did.
  topic: technical/limitations
- impact_reason: 'A fundamental principle for applied ML research and product development:
    establish a simple, reproducible baseline first.'
  relevance_score: 8
  source: llm_enhanced
  text: I think simple works, right? You need to get it in the simplest case before
    we can talk about more complex settings.
  topic: strategy/product
- impact_reason: Highlights that the innovation lies in the *experimental setup and
    methodology* (comparing auto-labels vs. human labels) rather than novel model
    design.
  relevance_score: 8
  source: llm_enhanced
  text: There's no new architecture here; there's no technical contribution that way.
    The only parameter we vary is the threshold on the model confidence per object
    label for object auto-label.
  topic: technical/strategy
- impact_reason: Highlights the massive scale of experimentation required for robust
    research in this area, emphasizing the computational cost of thorough model comparison.
  relevance_score: 8
  source: llm_enhanced
  text: we ultimately had to train on the order of 445 different models.
  topic: technical
- impact_reason: 'Poses the central, critical question following the cost/speed analysis:
    performance validation.'
  relevance_score: 8
  source: llm_enhanced
  text: So, you can do it fast and you can do it cheaply, but does it work? Does it
    matter?
  topic: strategy
- impact_reason: Illustrates the inherent unreliability and inconsistency of human
    labeling on tedious tasks, providing a strong justification for automation.
  relevance_score: 8
  source: llm_enhanced
  text: humans sometimes—this is real data—they will only label three of the donuts,
    a handful of the donuts, or about right. This stuff is tedious.
  topic: safety/quality
- impact_reason: Suggests that foundation models, when tuned correctly via confidence
    thresholding, can achieve near-human performance parity in labeling accuracy.
  relevance_score: 8
  source: llm_enhanced
  text: if you're able to find the right confidence threshold, you can essentially
    get almost perfect F1 score—maybe that's reaching a little—you can get very good,
    let's say, comparable F1 score to make it usable.
  topic: technical
- impact_reason: 'Identifies a key practical hurdle in deploying auto-labeling: the
    need for a small, calibrated human-labeled subset to set effective confidence
    thresholds.'
  relevance_score: 8
  source: llm_enhanced
  text: The challenge though in practice is you don't necessarily have—you can't calibrate
    that well. You'll have to get some data labeled to do your calibration of your
    confidence, and then go and hope that that extra place in the rest of the dataset.
  topic: business/strategy
- impact_reason: 'Defines the ''Red Light'' category: samples where the model is highly
    uncertain, suggesting they should be discarded or sent for immediate expert review.'
  relevance_score: 8
  source: llm_enhanced
  text: the red light—these cases are clearly at the decision boundary; we're not
    confident about these; we don't know what they are, so let's just throw away those.
  topic: safety/quality
- impact_reason: Concludes that human verification remains essential, but the goal
    of new annotation paradigms is to minimize and predictably quantify that necessary
    human effort.
  relevance_score: 8
  source: llm_enhanced
  text: Humans are still—I think humans are needed to verify that. You can minimize
    that work and quantify and predictably quantify how much work there would be.
    I think that's when we get to annotation next version of annotation, whether it's
    1.5 or 2.0 or whatever.
  topic: strategy
- impact_reason: Proposes a concrete, comparative methodology (a 'foundation model
    t-test') for assessing model robustness and generalization across different domain
    models.
  relevance_score: 8
  source: llm_enhanced
  text: I would take a dataset domain like medical imaging for which there are in-domain
    and condition models, and then use out-of-domain foundation models like the ones
    we've used here, as well as the in-domain one. Interesting. And kind of predict
    the performance, compare the performance, see if there is a predictable distribution
    over them, almost like a foundation model t-test, if you will, that might be able
    to do that for truly out-of-domain...
  topic: technical
- impact_reason: Suggests a meta-learning or multi-agent system where the optimization
    objective itself is dynamic and refined by interactions, a sophisticated concept
    in AI architecture.
  relevance_score: 8
  source: llm_enhanced
  text: I do think that the underlying optimization function or cost function that's
    being optimized may need to be evolving based on what the feedback is. And so
    there may actually be this notion of a net—a model, if you will—that's exploring
    the space with other models, and they may need to be refining how the space is
    being characterized.
  topic: technical
- impact_reason: A clear, ambitious goal for product adoption, positioning the tool
    as essential infrastructure rather than an optional add-on.
  relevance_score: 7
  source: llm_enhanced
  text: 51 should be one of those things [the first three things someone does when
    starting a new computer vision project].
  topic: business
- impact_reason: Highlights the extreme heterogeneity and customization required in
    real-world ML workflows, justifying the need for flexible tooling.
  relevance_score: 7
  source: llm_enhanced
  text: We used to say for every 100 machine learning engineers out there, there are
    1,000 ways of working because everyone has their own way of doing things, really.
  topic: strategy
- impact_reason: 'A strategic prediction about large datasets: common cases will become
    overwhelmingly predictable, shifting focus away from rare events.'
  relevance_score: 7
  source: llm_enhanced
  text: My intuition says that the closer we get to large-scale internet-scale datasets,
    the more the mean behavior is going to dominate.
  topic: predictions
- impact_reason: 'Emphasizes the methodological rigor required for valid comparisons:
    isolating variables to prove the impact of auto-labeling itself.'
  relevance_score: 7
  source: llm_enhanced
  text: We want to make it as apples-to-apples as possible; there's nothing special
    about the domain or the use case or what have you, aside from the fact that we're
    only using—only doing object detection.
  topic: technical
- impact_reason: Illustrates the binary nature of success/failure in industrial defect
    detection versus the vast, unpredictable failure modes in general vision tasks.
  relevance_score: 7
  source: llm_enhanced
  text: the way I joke about that problem is, in some sense, it's very—the way things
    can go well is, in terms of performance, it's like building things on the manufacturing
    line; it's correct or it's not correct, right? It's right. Whereas the way things
    can go wrong is this really massive and hard-to-predict space.
  topic: business
- impact_reason: 'Expresses the core motivation in applied AI research: bridging the
    gap between experimental results and tangible, deployed systems.'
  relevance_score: 6
  source: llm_enhanced
  text: I'm really excited about the impact that these types of results or these types
    of systems will have in practice. I think we all want to see better AI models
    or better vision models being trained to be deployed.
  topic: strategy
source: Unknown Source
summary: '## Podcast Summary: Zero-Shot Auto-Labeling: The End of Annotation for Computer
  Vision with Jason Corso - #735


  This episode of the Twemal AI podcast features Sam Charrington in conversation with
  Jason Corso, co-founder of Voxel 51 and a professor at the University of Michigan,
  focusing on the paradigm shift from traditional human-based data annotation to automated,
  zero-shot auto-labeling driven by powerful foundation models in computer vision.


  ---


  ### 1. Focus Area

  The primary focus is the **evolution of data annotation in Computer Vision**, specifically
  exploring the viability and performance of **Zero-Shot Auto-Labeling** using large
  vision-language models (VLMs) and object detection foundation models (like YOLO-World,
  Grounding DINO) to replace or drastically reduce the need for expensive, time-consuming
  human labeling (Annotation 1.0). The discussion centers on the tooling (Voxel 51)
  that supports this iterative development loop.


  ### 2. Key Technical Insights

  *   **Annotation 2.0 Paradigm:** The future of labeling is shifting from "blindly
  sending all data out for labeling" (1.0) to an **"agent labeling"** model where
  humans primarily validate or answer specific questions posed by an AI agent, suggesting
  a middle ground in the transition timeline.

  *   **Leveraging Semantic Embeddings for Uncertainty:** While true classical uncertainty
  quantification remains challenging, the rich structure within contemporary foundation
  model embeddings (e.g., combining perceptual embeddings with CLIP embeddings) allows
  for measuring **"marginal uncertainty"** using simple shallow autoencoders and reconstruction
  error ratios, which correlate with downstream classifier performance.

  *   **Zero-Shot Auto-Labeling Methodology:** The core experiment involved taking
  unlabeled data, generating auto-labels using foundation models (YOLO-World, Grounding
  DINO) based on class prompts, training standard object detectors (YOLOv11, RT-DETR)
  on these auto-labels, and comparing the resulting performance against detectors
  trained on human-annotated ground truth.


  ### 3. Business/Investment Angle

  *   **Annotation Cost Reduction:** Traditional annotation for standard datasets
  (VOC, COCO, BDD, LVIS) can cost significant amounts (estimated at $124,000 for one
  pass without QA), making auto-labeling a massive potential cost-saver by eliminating
  labeling for common, typical cases.

  *   **Focus Shift from Data Acquisition to Analysis:** The value proposition of
  tools like Voxel 51 is shifting from facilitating annotation to injecting **analysis**
  into the model development loop—helping engineers identify corner cases, visualize
  embedding clusters, and determine *where* new labels are truly needed.

  *   **The Value of Typical Data:** A key realization is that teams often over-spend
  on labeling typical data points. Auto-labeling creates immediate value by accurately
  labeling the "mean behavior" data, allowing human effort to focus only on the difficult
  decision boundaries and outliers.


  ### 4. Notable Companies/People

  *   **Jason Corso:** Co-founder of Voxel 51, driving the research and tooling around
  visual AI development and data analysis.

  *   **Sam Charrington:** Host of the Twemal AI podcast.

  *   **Voxel 51:** Creator of the **51** software, positioned as "VS Code for visual
  AI," which acts as a universal format translator ("Rosetta Stone") and analysis
  hub for computer vision workflows.

  *   **Foundation Models Mentioned:** YOLO-World, Grounding DINO, YOLO-E (used for
  auto-labeling); RT-DETR, YOLOv11 (used as downstream detectors).


  ### 5. Future Implications

  The industry is moving toward **fewer, more targeted human interventions** in the
  labeling process. The future involves leveraging foundation models to handle the
  bulk of common labeling tasks, reserving human expertise for complex QA, validating
  model uncertainty, and refining prompts. The success of zero-shot methods suggests
  that the bottleneck is shifting from data acquisition to better understanding and
  quantifying model uncertainty and performance predictability *before* full training.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Computer Vision Researchers,
  Data Curation Managers, and Technology Investors** focused on the infrastructure
  and tooling supporting large-scale visual AI deployment.'
tags:
- artificial-intelligence
- ai-infrastructure
- startup
- investment
- nvidia
title: 'Zero-Shot Auto-Labeling: The End of Annotation for Computer Vision with Jason
  Corso - #735'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 99
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 12
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 2
  prominence: 0.2
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 10:54:02 UTC -->
