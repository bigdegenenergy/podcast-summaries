---
companies:
- category: unknown
  confidence: medium
  context: ick of procurement bottlenecks? Get the cure with Bill Procurement. Bill
    is the all-in-one platform trusted by nearl
  name: Bill Procurement
  position: 51
- category: tech
  confidence: high
  context: powerful, and built for serious productivity with Intel Core Ultra processors,
    blazing speed, and AI-powe
  name: Intel
  position: 755
- category: unknown
  confidence: medium
  context: powerful, and built for serious productivity with Intel Core Ultra processors,
    blazing speed, and AI-powered perform
  name: Intel Core Ultra
  position: 755
- category: unknown
  confidence: medium
  context: e on new tech. Win the tech search at Lenovo.com. Unlock AI experiences
    with the new tech. And now, we're goi
  name: Unlock AI
  position: 978
- category: unknown
  confidence: medium
  context: create, and boost productivity all on one device. Hey Richard. Hey Carl,
    what do you know? Well, I know that ou
  name: Hey Richard
  position: 1234
- category: unknown
  confidence: medium
  context: oost productivity all on one device. Hey Richard. Hey Carl, what do you
    know? Well, I know that our friend,
  name: Hey Carl
  position: 1247
- category: unknown
  confidence: medium
  context: ', what do you know? Well, I know that our friend, Michelle Rubustamonte,
    is with us to tell us about something that''s goi'
  name: Michelle Rubustamonte
  position: 1305
- category: unknown
  confidence: medium
  context: on adjacent to DevIntersection. What is it? It's Cybersecurity Intersection.
    Let's let Michelle tell that story. Hey Michelle
  name: Cybersecurity Intersection
  position: 1428
- category: unknown
  confidence: medium
  context: Intersection. Let's let Michelle tell that story. Hey Michelle. Hey Carl,
    hey Richard. How are you? Tell us abou
  name: Hey Michelle
  position: 1492
- category: unknown
  confidence: medium
  context: ring with the group that does DevIntersection and NextGen AI, and we are
    putting on a new conference dedicated
  name: NextGen AI
  position: 1667
- category: unknown
  confidence: medium
  context: ly, the lineup of speakers is incredible. We have Paul Legend, who's here
    from Poland, and does keynotes all ov
  name: Paul Legend
  position: 1821
- category: unknown
  confidence: medium
  context: rld, and is one of the top-rated RSA speakers and Black Hat speakers. We're
    so lucky to have her, but she's n
  name: Black Hat
  position: 1941
- category: tech
  confidence: high
  context: like that as well. But we also have speakers from Microsoft. We have speakers
    that specialize in secure codin
  name: Microsoft
  position: 2254
- category: unknown
  confidence: medium
  context: azing group of speakers, really excited about it. And I think I can count
    myself among the group of speak
  name: And I
  position: 2584
- category: unknown
  confidence: medium
  context: pplications talk. And also, I think we're doing a Security This Week live
    show there somewhere. That is correct. Yeah,
  name: Security This Week
  position: 2777
- category: unknown
  confidence: medium
  context: see you there. Hey, get down, Rock and Roll. It's Carl Franklin and Richard
    Campbell for .NET Rocks. Hey Richard.
  name: Carl Franklin
  position: 4129
- category: unknown
  confidence: medium
  context: ', get down, Rock and Roll. It''s Carl Franklin and Richard Campbell for
    .NET Rocks. Hey Richard. How you doing, bud?'
  name: Richard Campbell
  position: 4147
- category: unknown
  confidence: medium
  context: oll. It's Carl Franklin and Richard Campbell for .NET Rocks. Hey Richard.
    How you doing, bud? Getting psyched
  name: NET Rocks
  position: 4169
- category: unknown
  confidence: medium
  context: you want to start? Well, the unhappy things, the Kent State shootings.
    Yes, terrifying. On May 4th, National
  name: Kent State
  position: 5070
- category: unknown
  confidence: medium
  context: hings, the Kent State shootings. Yes, terrifying. On May 4th, National
    Guard troops killed four students d
  name: On May
  position: 5109
- category: unknown
  confidence: medium
  context: ent State shootings. Yes, terrifying. On May 4th, National Guard troops
    killed four students during a protest agai
  name: National Guard
  position: 5121
- category: unknown
  confidence: medium
  context: killed four students during a protest against the Vietnam War at Kent State
    University, Ohio, leading to nation
  name: Vietnam War
  position: 5193
- category: unknown
  confidence: medium
  context: dents during a protest against the Vietnam War at Kent State University,
    Ohio, leading to nationwide outrage. And the son
  name: Kent State University
  position: 5208
- category: unknown
  confidence: medium
  context: utrage. And the song, what is it, "Ohio"? Is that Neil Young or Crosby,
    Stills, Nash & Young? I'm not sure. Ni
  name: Neil Young
  position: 5310
- category: unknown
  confidence: medium
  context: ng or Crosby, Stills, Nash & Young? I'm not sure. Nigerian Civil War. The
    conflict ended in January when Biafran force
  name: Nigerian Civil War
  position: 5368
- category: unknown
  confidence: medium
  context: r a 32-month struggle for independence. The first Earth Day was observed
    on April 22nd. The Beatles broke up,
  name: Earth Day
  position: 5504
- category: unknown
  confidence: medium
  context: . The first Earth Day was observed on April 22nd. The Beatles broke up,
    and let it be. McCartney said he was le
  name: The Beatles
  position: 5542
- category: unknown
  confidence: medium
  context: the band on April 10th. That was the end of that. But John Lennon, "Instant
    Karma," he wrote and recorded this hit
  name: But John Lennon
  position: 5659
- category: unknown
  confidence: medium
  context: 10th. That was the end of that. But John Lennon, "Instant Karma," he wrote
    and recorded this hit song in a single
  name: Instant Karma
  position: 5677
- category: unknown
  confidence: medium
  context: a single day, showcasing his prolific creativity. Diana Ross and the Supremes
    gave their final concert in Las
  name: Diana Ross
  position: 5782
- category: unknown
  confidence: medium
  context: Ross and the Supremes gave their final concert in Las Vegas on January
    14th. Back to the bad stuff, the Tangs
  name: Las Vegas
  position: 5838
- category: unknown
  confidence: medium
  context: ngshan earthquake. Devastating earthquakes struck Tangshan County, China,
    on January 5th, resulting in significant
  name: Tangshan County
  position: 5944
- category: unknown
  confidence: medium
  context: eautiful rendering of more or less what happened. The HBO *From the Earth
    to the Moon* series, if you ever
  name: The HBO
  position: 6727
- category: unknown
  confidence: medium
  context: pparently. Yeah. And the computer side of things, Nicholas Wirth releases
    Pascal. Wow. He worked previously on the
  name: Nicholas Wirth
  position: 7912
- category: unknown
  confidence: medium
  context: also the same year that the first version of the IBM System/370 comes out
    with all semiconductor RAM. But tha
  name: IBM System
  position: 8843
- category: unknown
  confidence: medium
  context: All right. Well, I guess we should carry on with Better No Framework. Roll
    the crazy music. All right, man, what do yo
  name: Better No Framework
  position: 9302
- category: unknown
  confidence: medium
  context: ike Claude, Cursor to interact directly with your Unity Editor via a local
    MCP, Model Context Protocol. We've be
  name: Unity Editor
  position: 9873
- category: unknown
  confidence: medium
  context: directly with your Unity Editor via a local MCP, Model Context Protocol.
    We've been talking about those, a local MCP clie
  name: Model Context Protocol
  position: 9903
- category: unknown
  confidence: medium
  context: cally using Playwright to, with the code agent in Visual Studio Code. Nice.
    And we're using Claude Sonnet. And we basi
  name: Visual Studio Code
  position: 10382
- category: unknown
  confidence: medium
  context: gent in Visual Studio Code. Nice. And we're using Claude Sonnet. And we
    basically, one prompt told it to create u
  name: Claude Sonnet
  position: 10424
- category: unknown
  confidence: medium
  context: ne prompt told it to create user documentation of Jeff Fritz's Copilot.dot.com
    website. And it did a pretty go
  name: Jeff Fritz
  position: 10508
- category: unknown
  confidence: medium
  context: didn't show was what's involved in setting up the Playwright MCP so that
    the agent can use it. And it turns out th
  name: Playwright MCP
  position: 10634
- category: unknown
  confidence: medium
  context: 969. Yes, that's last week's show with our friend James Montemagno. When
    we talked a little bit about the AI tooling
  name: James Montemagno
  position: 10994
- category: unknown
  confidence: medium
  context: e of Visual Studio Code and its relationship with Visual Studio and so
    on. And our friend Richard Rukima, also kn
  name: Visual Studio
  position: 11116
- category: unknown
  confidence: medium
  context: 'ship with Visual Studio and so on. And our friend Richard Rukima, also
    known as CodePuter, has this comment: "I th'
  name: Richard Rukima
  position: 11156
- category: unknown
  confidence: medium
  context: a beginner, especially after listening to James. But I felt that vibe of
    joy in getting things done so f
  name: But I
  position: 11384
- category: unknown
  confidence: medium
  context: rd, I'm pretty sure you've got a copy of Music to Code By. But thanks so
    much for your comment. But if you'
  name: Code By
  position: 12560
- category: tech
  confidence: high
  context: mment on the website at dotnetrocks.com or on the Facebook's and publish
    every show there. And if you commen
  name: Facebook
  position: 12712
- category: unknown
  confidence: medium
  context: icToCodeBy.net. Okay, let's bring back our friend Joseph Finney. Joseph
    is a mobile product owner and MVP by day,
  name: Joseph Finney
  position: 13028
- category: unknown
  confidence: medium
  context: hich is pretty basic. It's also the basis for the PowerToys Text Extractor,
    which is basically select a region on your scree
  name: PowerToys Text Extractor
  position: 13587
- category: financial_tech
  confidence: low
  context: Mentioned as a financial operations platform, not directly an AI/ML company,
    but part of the broader tech ecosystem.
  name: Bill Procurement
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned for their ThinkPad X1 Carbon laptops featuring AI-powered performance
    and Intel Core Ultra processors.
  name: Lenovo
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned for their Core Ultra processors which power the AI capabilities
    in the ThinkPad X1 Carbon.
  name: Intel
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Speakers from Microsoft are presenting at the Cybersecurity Intersection
    conference, focusing on Azure security and AI tooling.
  name: Microsoft
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in the context of Microsoft speakers discussing Azure security
    and zero-trust architectures.
  name: Azure
  source: llm_enhanced
- category: ai_conference
  confidence: high
  context: A conference running alongside DevIntersection and Cybersecurity Intersection,
    indicating focus on AI topics.
  name: NextGen AI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Proudly sponsors and maintains MCP for Unity, described as the best AI
    assistant for Unity.
  name: Co-Pilot
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an AI assistant (LLM) that can interact with the Unity Editor
    via MCP.
  name: Claude
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an AI assistant (LLM) that can interact with the Unity Editor
    via MCP.
  name: Cursor
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A platform or project using AI tools (Claude Sonnet and a code agent) to
    generate documentation.
  name: CodeItWithAI.com
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Used in conjunction with a code agent in Visual Studio Code for automation
    tasks.
  name: Playwright
  source: llm_enhanced
- category: software_development
  confidence: high
  context: Mentioned as the environment where AI tooling and code agents are being
    used.
  name: Visual Studio Code
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific LLM model used in a demonstration for generating documentation.
  name: Claude Sonnet
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in relation to taking over the Tesseract OCR project.
  name: Google
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: An open-source OCR project, now widely used and loved, which Google took
    over.
  name: Tesseract
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A middle layer/API released by Microsoft for local Windows app developers
    to integrate OCR, image, and language models.
  name: WinML (Windows Machine Learning)
  source: llm_enhanced
- category: hardware
  confidence: medium
  context: Mentioned regarding the System/370 which used semiconductor RAM shortly
    after Intel's 1103.
  name: IBM
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Older, fast APIs used previously by TextGrab for OCR.
  name: Windows 10 OCR APIs
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Windows Machine Learning, a middle layer API for running ONNX models locally
    on Windows devices (CPU, GPU, NPU).
  name: WinML
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Hardware released with new Windows AI APIs and featuring a local LLM and
    NPU.
  name: Copilot Plus PCs
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific feature on Copilot Plus PCs that takes screenshots, powered
    by local AI.
  name: Recall
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a tool for running models locally on laptops or gaming PCs,
    similar to the local LLM capability in Copilot Plus PCs.
  name: Ollama
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Open Neural Network Exchange format, models of which can be run via WinML.
  name: ONNX
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Described as a huge, open-source repository/platform for downloading various
    types of AI models (OCR, image segmentation, etc.).
  name: Hugging Face
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as backing Hugging Face.
  name: Facebook
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A Microsoft open-source app/playground available in the Microsoft Store
    for trying out models, including those from Hugging Face.
  name: AI Dev Gallery app
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned as being open source like the AI Dev Gallery app, and also hosting
    some models.
  name: GitHub
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as a playground for practicing ML skills, known for running competitions
    (e.g., Titanic survival prediction).
  name: Kaggle
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An LLM model from China noted for using fewer resources and being cheaper
    to run than early ChatGPT models; can be run locally.
  name: DeepSeek
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned in comparison to DeepSeek regarding market impact and resource
    usage.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An LLM developed by Microsoft, optimized for CPU/GPU rather than NPU in
    its current released versions.
  name: Phi-3
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Hosts the AI Toolkit extension for model refinement and fine-tuning.
  name: VS Code (Visual Studio Code)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: An extension that acts as a playground for interacting with and refining
    models, including DeepSeek.
  name: AI Toolkit for Visual Studio Code
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned in an advertisement regarding .NET 8 support.
  name: AWS
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an existing large language model that understands programming
    languages.
  name: ChatGPT
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in the context of local AI processing capabilities (NPU TOPS
    rating). Copilot is a Microsoft product.
  name: Copilot Plus PC
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A version of GPT-3 that can be run locally, implying an open-source or
    locally deployable variant of OpenAI's technology.
  name: GPT-OSS
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the seller of high-end GPUs (like the 5080 and implied 5090)
    used for data center processing and local ML workloads.
  name: NVIDIA
  source: llm_enhanced
date: 2025-10-02 01:22:52 +0000
duration: 55
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: carry on with Better No Framework
  text: we should carry on with Better No Framework.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/18c57d77486348beb5c6631ccab834ab/
processing_date: 2025-10-06 04:18:09 +0000
quotes:
- length: 99
  relevance_score: 5
  text: So here's what it is, proudly sponsored and maintained by Co-Pilot, the best
    AI assistant for Unity
  topics: []
- length: 34
  relevance_score: 5
  text: You have to have a Copilot Plus PC
  topics: []
- length: 212
  relevance_score: 5
  text: I mean, you could, but you're going to have to put a speech recognition model
    in front of the LLM, or an object detection model plus an OCR model plus that,
    you know, you have to maybe chain these models together
  topics: []
- length: 127
  relevance_score: 4
  text: But on the hardware side, for me, the showstopper is my, you know, IBM, it's
    Intel's most important product, the 1103, the DRAM
  topics: []
- length: 94
  relevance_score: 4
  text: So in case anyone hasn't figured out by now, the Copilot Plus PC has a local
    LLM built into it
  topics: []
- length: 182
  relevance_score: 4
  text: And, but you know, I think if I had a great Copilot Plus PC, you know, with
    a lot of RAM and a lot of storage, and I just set it over in a closet somewhere,
    I could probably use that
  topics: []
- length: 148
  relevance_score: 4
  text: I would love to hear an expert who actually knows more about context and how
    that differs from the training data and how it differs from fine-tuning
  topics: []
- length: 123
  relevance_score: 4
  text: And it's been an argument now that you can jack up a PC enough with a couple
    of those big GPUs and run a mid-size LLM on it
  topics: []
- length: 95
  relevance_score: 3
  text: But shortly after that, Intel's RAM just dominates the market and sends Intel
    on its trajectory
  topics:
  - market
- length: 75
  relevance_score: 3
  text: So there's definitely some hoops you have to jump through to get it working
  topics: []
- length: 37
  relevance_score: 3
  text: So you have to download the languages
  topics: []
- length: 80
  relevance_score: 3
  text: But again, you have the complexity where you have to download the models locally
  topics: []
- length: 125
  relevance_score: 3
  text: You left the impolite part, Joe, which is like, so for me, the term artificial
    intelligence means something that doesn't work
  topics: []
- length: 67
  relevance_score: 3
  text: And it's an easy way, but again, there you have to manage the model
  topics: []
- length: 38
  relevance_score: 3
  text: So Microsoft makes an LLM called Phi-3
  topics: []
- length: 30
  relevance_score: 3
  text: You have to be able to test it
  topics: []
- length: 186
  relevance_score: 3
  text: With thousands of the best free-to-play casino-style games, chances to earn
    millions of bonus coins and win real money, Legends is revolutionizing the biggest
    experience wherever you are
  topics: []
- length: 189
  relevance_score: 3
  text: You have an entire language that you have to train these models on, or you
    have an entire data set of images with boxes drawn around the dogs or dog breeds
    or very specific things like that
  topics: []
- length: 179
  relevance_score: 3
  text: So, and when you look at what Nvidia's selling to data centers and things
    is their giant GPUs with huge amounts of memory, this super-fast memory in them
    for a scale of processing
  topics: []
- length: 138
  relevance_score: 3
  text: And then you can get that multi-modal experience where you can drop images,
    you could put PDFs in, but you have to be able to read the PDF
  topics: []
- length: 198
  relevance_score: 3
  text: So, if you were going to build a local LLM, Joe, yourself using some existing
    technology, would you first reach for DeepSeek, or would you go for just the stuff
    that Microsoft is exposing in Windows
  topics: []
- length: 94
  relevance_score: 3
  text: There's an infinite number of decisions that you have to make when you're
    picking all of these
  topics: []
- impact_reason: A specific technical example of bridging large language models (LLMs)
    with a complex development environment (Unity) using a defined protocol (MCP),
    showing integration depth.
  relevance_score: 10
  source: llm_enhanced
  text: MCP for Unity accesses a bridge allowing AI assistants like Claude, Cursor
    to interact directly with your Unity Editor via a local MCP, Model Context Protocol.
  topic: technical
- impact_reason: 'Articulates the core value proposition and psychological shift when
    using AI coding assistants: prioritizing outcome over implementation detail.'
  relevance_score: 10
  source: llm_enhanced
  text: But I felt that vibe of joy in getting things done so fast. So do I like the
    if-then-else, or do I like asking for a feature, reviewing the result?
  topic: AI adoption
- impact_reason: 'This is the key benefit of the new Windows AI APIs: abstraction
    of model management, drastically lowering the barrier to entry for local AI features.'
  relevance_score: 10
  source: llm_enhanced
  text: So they basically in the code when you're building, you just have to check,
    does this device support these APIs? If so, do it. Very simple. And like that's
    it. You don't have to manage models. You don't have to manage memory or downloading.
    And you don't have to worry about shipping a five-gig model with your app.
  topic: technical/deployment
- impact_reason: Strong endorsement of Hugging Face as the central repository for
    non-LLM, task-specific models, reminding the audience of the breadth of available
    ML tooling.
  relevance_score: 10
  source: llm_enhanced
  text: If you go to Hugging Face and you look at all the different categories, I
    mean, OCR, image segmentation, image detection, object detection. Hugging Face.
    Hugging Face. Hugging Face.
  topic: technical/trends
- impact_reason: Provides a clear, concise definition of the NPU and its role in enabling
    efficient local AI inference, linking it to the Copilot Plus PC requirements.
  relevance_score: 10
  source: llm_enhanced
  text: That's the Neural Processing Unit. So you kind of have your CPU, your GPU,
    and your NPU. And this was the core, the chip, the part of the CPU in these ARM
    devices that really made it easy to run these models locally and efficiently.
  topic: technical
- impact_reason: Gives a specific, quantifiable hardware benchmark (40 TOPS NPU) for
    the next generation of local AI-enabled PCs, signaling industry direction.
  relevance_score: 10
  source: llm_enhanced
  text: And part of the requirement for a Copilot Plus PC is that it has an NPU of
    at least what is it? 40 TOPS. Yeah, or trillion operations per second.
  topic: technical
- impact_reason: A fundamental reminder about the probabilistic nature of ML, contrasting
    it with traditional deterministic software, which impacts user expectation management.
  relevance_score: 10
  source: llm_enhanced
  text: But it really is that data. I mean, that being said, this is all sort of non-deterministic
    thing. Like you're never going to get 100% out of a machine learning model. It's
    probabilistic.
  topic: safety
- impact_reason: Emphasizes that data quality, structure, and cleanliness are the
    true bottlenecks and assets in the current AI era, not just model access.
  relevance_score: 10
  source: llm_enhanced
  text: Data is the new oil, right? This is the new black gold of, do you have the
    data? Do you have the databases? Is it structured? Is it consistent? Is it clean?
    Is it real? Is it good?
  topic: strategy
- impact_reason: Identifies inference speed as the primary performance trade-off when
    moving from cloud APIs to local hardware.
  relevance_score: 10
  source: llm_enhanced
  text: I think part of the challenge that you'll see if you start using them is speed.
    So the response speed of a local model is going to be much slower actually than
    the cloud-hosted one because your computer cannot compete with a server with a
    rack of GPUs.
  topic: technical
- impact_reason: Directly links new hardware (Intel Core Ultra) with enabling AI experiences
    locally, signaling a trend toward on-device AI processing for productivity.
  relevance_score: 9
  source: llm_enhanced
  text: Unlock AI experiences with the ThinkPad X1 Carbon, powered by Intel Core Ultra
    processors, so you can work, create, and boost productivity all on one device.
  topic: AI technology trends
- impact_reason: Illustrates the practical, high-level automation capabilities LLMs
    offer in game development workflows, moving beyond simple code generation.
  relevance_score: 9
  source: llm_enhanced
  text: Use your LLM tools to manage assets, control scenes, edit scripts, and automate
    tasks within Unity.
  topic: AI technology trends
- impact_reason: Captures the sentiment of experienced developers embracing AI tools—the
    focus shifts from the mechanics of coding to achieving high-level solutions.
  relevance_score: 9
  source: llm_enhanced
  text: I'm long past the joy of knowing how to write procedural code.
  topic: AI adoption
- impact_reason: Mentions the concept of 'local' AI interaction protocols (MCP), suggesting
    a trend toward decentralized or on-device LLM interaction rather than purely cloud-based.
  relevance_score: 9
  source: llm_enhanced
  text: We've been talking about those, a local MCP client.
  topic: AI technology trends
- impact_reason: Provides a clear framework for developers to understand the spectrum
    of integrating AI/ML capabilities into local Windows applications, from easiest
    to hardest.
  relevance_score: 9
  source: llm_enhanced
  text: I would say there's like three general levels of intensity. If you are a local
    Windows app developer and you want to get OCR, image, language models, like all
    of that stuff, if you want to do that in your app, I would say there's like three
    different tiers of complexity that you can engage in.
  topic: strategy/technical
- impact_reason: Pinpoints the emergence of new, simplified developer APIs tied directly
    to the hardware launch of Copilot Plus PCs, signaling a major shift in platform
    support.
  relevance_score: 9
  source: llm_enhanced
  text: The first one is the new Windows AI APIs. And these were released kind of
    around the time the Copilot Plus PCs were released. Okay. Last June.
  topic: technical/trends
- impact_reason: A common, insightful critique of the term 'AI'—it's an umbrella term
    that loses meaning once specific, working technologies (like OCR or LLM) are named.
  relevance_score: 9
  source: llm_enhanced
  text: I have AI features and, well, AI, I should say. I know this show, Richard
    has talked a lot about how you have these big amorphous buckets of AI, and then
    as soon as you start explaining it and giving a more clear, straightforward name
    to it, it stops really being AI.
  topic: strategy
- impact_reason: Explicitly confirms the presence of a local Large Language Model
    as a core feature of the new hardware generation.
  relevance_score: 9
  source: llm_enhanced
  text: In case anyone hasn't figured out by now, the Copilot Plus PC has a local
    LLM built into it.
  topic: trends/predictions
- impact_reason: A critical observation that the LLM hype cycle has overshadowed significant,
    mature tooling and progress in other areas of applied machine learning.
  relevance_score: 9
  source: llm_enhanced
  text: If you're thinking, yeah, before the insanity of LLMs, we had, we had good
    tooling around just maybe building machine models for object detection and recognizers
    and OCR, all these good things. Like, it's just, there was so much going on before
    ChatGPT showed up and just overwhelmed the messaging.
  topic: strategy/trends
- impact_reason: Provides a specific, actionable, low-friction tool (AI Dev Gallery)
    for developers and enthusiasts to experiment with local models (including Hugging
    Face downloads).
  relevance_score: 9
  source: llm_enhanced
  text: You can download an app from Microsoft called the AI Dev Gallery app. And
    what this is, it's kind of a playground for people who are curious about models
    and different models and how this all works. It is a really low barrier to entry
    if you are interested in trying some of these models out on your own device.
  topic: business/product
- impact_reason: 'Defines the third and most complex tier: custom model training,
    reserved for highly niche or specific application needs.'
  relevance_score: 9
  source: llm_enhanced
  text: I would say that is the like the farthest, the highest tier of integrating
    AI models into your app, your local Windows app, is making your own models, training
    your own models from scratch.
  topic: technical/strategy
- impact_reason: Points to model optimization (size and hardware-specific tuning)
    as a key trend for local deployment.
  relevance_score: 9
  source: llm_enhanced
  text: So one of the nice things about DeepSeek is how small it is, but they also
    have NPU-optimized models, which you can go download.
  topic: technical
- impact_reason: Articulates a common skepticism regarding building proprietary core
    infrastructure (like models or compilers), emphasizing the risk/reward calculation.
  relevance_score: 9
  source: llm_enhanced
  text: Why would you want to own a model? Because it sounds like a lot of work. It's
    like owning a framework. Yeah, it is like, don't trust somebody who says, 'I can
    write their own language and write their own IDE.' You're like, 'Oh, their own
    garbage collector, you know, their own crypto library.' These are all scary things
    to me. So when someone says, 'I'll just make our own model,' I'm like, 'Why do
    we need to do that?'
  topic: strategy
- impact_reason: 'Provides the primary business case for building custom models: proprietary,
    niche, high-volume data sets.'
  relevance_score: 9
  source: llm_enhanced
  text: Well, if you're in the industry, if you have insane amounts of data in a niche,
    in a specific industry, it might be worth it for you to look into doing this.
  topic: business
- impact_reason: 'Poses a key challenge for local LLMs: achieving the deep, domain-specific
    code understanding that cloud models possess, likely requiring significant fine-tuning.'
  relevance_score: 9
  source: llm_enhanced
  text: If I was going to use a local LLM, I would want it to understand C#, JavaScript,
    Blazor, you know, and CSS. And I don't know how realistic that is. Like I know
    that the current models like Claude Sonnet and even ChatGPT understand it, but
    for lack of a better word, sorry Richard, didn't mean to offend you there, they're
    programs, you know, they're trained against it. But what does it take to do that
    locally?
  topic: technical
- impact_reason: Highlights context window size as a critical limiting factor for
    local models compared to their cloud counterparts.
  relevance_score: 9
  source: llm_enhanced
  text: Another challenge is going to be context, which is how big of a context window
    can the model actually hold in the provider?
  topic: technical
- impact_reason: Directly links context window size to performance (speed) and resource
    consumption (RAM), posing a key architectural question for local deployment.
  relevance_score: 9
  source: llm_enhanced
  text: You want it to remember everything we've said, like I want as big a context
    as I can possibly get. So is that just a measure of more RAM, or is it the more
    that context you have, the slower it's going to be to come up with a new answer?
  topic: technical
- impact_reason: Connects the abstract concepts of speed and context directly to a
    practical, high-value application (code generation), emphasizing the need for
    contextual awareness over simple generation.
  relevance_score: 9
  source: llm_enhanced
  text: speed and context, I would say, are going to be your biggest risks where you
    don't necessarily just want it to give you new greenfield CSS. You want it to
    give you new CSS in the right spot for your code, which is that I want a much
    harder question.
  topic: technical
- impact_reason: 'Provides a concrete, hardware-centric insight for practitioners
    running local AI: VRAM capacity is the primary bottleneck for executing complex
    local workloads.'
  relevance_score: 9
  source: llm_enhanced
  text: The thing that's made a difference for me has been the video card and the
    amount of memory in the video card, like playing with FramePack and a couple of
    other models.
  topic: technical
- impact_reason: Identifies data sovereignty and privacy concerns as a major driver
    for high-end local LLM deployment, validating the market for powerful local inference
    rigs.
  relevance_score: 9
  source: llm_enhanced
  text: And it's been an argument now that you can jack up a PC enough with a couple
    of those big GPUs and run a mid-size LLM on it. So, you know, certainly I've had
    conversations with folks where it's like, 'I am not prepared to send any of this
    data to the cloud.'
  topic: safety
- impact_reason: 'Points out a significant current limitation in platform-level local
    AI APIs (specifically Windows): the lack of native multimodal support, requiring
    manual model chaining.'
  relevance_score: 9
  source: llm_enhanced
  text: These models, these local models, at least the Windows AI APIs, are not multi-modal.
    So, you will have to... In other words, you can't talk to them and write to them.
    Exactly.
  topic: technical
- impact_reason: 'Crucial practical insight for enterprise AI adoption: unstructured
    data (like PDFs) requires pre-processing (conversion to text) before LLMs can
    utilize it, presenting a data engineering hurdle.'
  relevance_score: 9
  source: llm_enhanced
  text: These LLMs don't read PDFs by default. Luckily, you do have to get them into
    a text format. So, if you're thinking about how you can apply this into your work,
    and I know a lot of enterprises, a lot of companies, a lot of their data is not
    in raw text format.
  topic: business
- impact_reason: 'Crucial practical insight for enterprise AI adoption: unstructured
    data (like PDFs) requires pre-processing (conversion to text) before LLMs can
    utilize it, presenting a data engineering hurdle.'
  relevance_score: 9
  source: llm_enhanced
  text: So, these LLMs don't read PDFs by default. Luckily, you do have to get them
    into a text format. So, if you're thinking about how you can apply this into your
    work, and I know a lot of enterprises, a lot of companies, a lot of their data
    is not in raw text format.
  topic: business
- impact_reason: 'Provides a strong recommendation for rapid prototyping/development:
    leverage platform-provided models (like Phi-3 via Windows APIs) as a robust starting
    point.'
  relevance_score: 9
  source: llm_enhanced
  text: Yeah, I just reach for this stuff. A lot of Microsoft is exposing in Windows,
    and their Phi-3 model. It's pretty good. It's pretty robust. And I would say it's
    a nice middle ground there for building on top of and fine-tuning.
  topic: business
- impact_reason: Identifies a specific market gap in specialized security training
    for the Microsoft ecosystem, suggesting a strategic opportunity.
  relevance_score: 8
  source: llm_enhanced
  text: The interesting thing here is we don't really have a Microsoft and .NET and
    Azure-focused security conference yet.
  topic: business
- impact_reason: A counter-intuitive observation about AI adoption, suggesting that
    deep expertise allows senior developers to leverage tools for maximum leverage,
    rather than just novelty.
  relevance_score: 8
  source: llm_enhanced
  text: The more experienced folks that are going to embrace these tools faster because
    it's typically the more junior people that tend to jump on the bandwagon, new
    things.
  topic: business
- impact_reason: Raises an important ethical/behavioral point regarding human interaction
    with AI systems, suggesting that prompt quality and tone reflect on the user.
  relevance_score: 8
  source: llm_enhanced
  text: In terms of respectful interaction with AI, I don't prescribe to the harsh
    language as I feel it reveals character.
  topic: safety/ethics
- impact_reason: Highlights a pivotal moment in hardware history (the introduction
    of commercial DRAM) that enabled modern computing density and cost structures.
  relevance_score: 8
  source: llm_enhanced
  text: Intel's most important product, the 1103, the DRAM... they were able to make
    a silicon substrate for an 18-pin DIP can with one K of RAM in it for $60. It
    was one cent per bit.
  topic: technical
- impact_reason: 'Provides a crucial reality check on AI integration: while the high-level
    prompt is easy, the underlying infrastructure setup (dependencies, environment
    configuration) remains complex.'
  relevance_score: 8
  source: llm_enhanced
  text: What we didn't show was what's involved in setting up the Playwright MCP so
    that the agent can use it. And it turns out that's pretty complex. You need Node.js
    and NPM, and all that stuff.
  topic: practical lessons
- impact_reason: 'Illustrates the classic trade-off in software development: older,
    built-in APIs offer speed and simplicity but lack the accuracy of newer, more
    complex models (like Tesseract).'
  relevance_score: 8
  source: llm_enhanced
  text: Originally, TextGrab was built using the Windows 10 OCR APIs, which are definitely
    older, not as good. But they're very fast.
  topic: strategy/technical
- impact_reason: A direct commentary on the default privacy settings of a major new
    AI feature (Recall), noting that it was disabled by default, which is a positive
    sign for user control.
  relevance_score: 8
  source: llm_enhanced
  text: Recall. Recall, that's it. It was turned off by default. So that's good.
  topic: safety/ethics
- impact_reason: Emphasizes the broad compatibility and ease of use of the new Windows
    AI APIs across legacy and modern Windows UI frameworks.
  relevance_score: 8
  source: llm_enhanced
  text: The easiest, simplest, like lowest level, easiest for any developer out there
    to integrate into their Windows app, any Windows app, by the way. So WPF or WinUI
    or WinForms, you can do them all.
  topic: technical/deployment
- impact_reason: 'Defines the second tier of complexity: using WinML to run custom
    or specific ONNX models, requiring developers to manage the model files themselves.'
  relevance_score: 8
  source: llm_enhanced
  text: WinML, and that's, there's a little bit of a middle layer there where you
    can go download your own ONNX models and run those.
  topic: technical
- impact_reason: 'Clarifies a fundamental concept for newcomers: ML models are inert
    data files requiring an inference engine or framework to execute, unlike traditional
    applications.'
  relevance_score: 8
  source: llm_enhanced
  text: You can't just download them and then run them. They are not programs. They're
    models. Right. So then you need to interface with them somehow.
  topic: technical
- impact_reason: Points to the emergence of resource-efficient, cost-effective models
    (like DeepSeek) as a viable alternative to the largest, most expensive proprietary
    models.
  relevance_score: 8
  source: llm_enhanced
  text: This was the model that came out of China that uses a lot less resources and
    is there for cheaper to run than, you know, ChatGPT was,
  topic: trends/business
- impact_reason: 'Provides a strong justification for custom model training: niche
    applications with specific data sets where off-the-shelf models fail.'
  relevance_score: 8
  source: llm_enhanced
  text: So if you have a specific application where you need a model that can do very
    niche things or very specific data sets, it's possible. It's doable. And there's
    ways to do it.
  topic: business
- impact_reason: Discusses the competitive landscape, specifically highlighting resource-efficient,
    cheaper models like DeepSeek, even if they didn't immediately dethrone market
    leaders.
  relevance_score: 8
  source: llm_enhanced
  text: Guys, I know we've talked about DeepSeek a bit on this show, and Joe is nodding
    his head, so he knows about it. And this was the model that came out of China
    that uses a lot less resources and is there for cheaper to run than, you know,
    ChatGPT was, and everybody was like, "Oh my God, OpenAI is going down," and it
    didn't.
  topic: business
- impact_reason: Provides insight into Microsoft's small model strategy (Phi-3) and
    notes that optimization targets (NPU vs. CPU/GPU) vary across different model
    releases.
  relevance_score: 8
  source: llm_enhanced
  text: Microsoft makes an LLM called Phi-3. And this model they have, they've been
    releasing 3, 3.5, they just released 4. It's optimized for the CPU and the GPU
    and not the NPU right now.
  topic: technical
- impact_reason: Highlights a specific, accessible tool (VS Code AI Toolkit) for experimentation,
    including advanced tasks like fine-tuning, lowering the barrier to entry.
  relevance_score: 8
  source: llm_enhanced
  text: And you can also do that in VS Code. There's an extension called AI Toolkit
    for Visual Studio Code. And that's another kind of playground to ask place, but
    you can also do the model refinement and fine-tuning in there.
  topic: technical
- impact_reason: Describes a tiered approach to AI deployment (local vs. cloud) and
    Microsoft's framework for comparing them, essential for enterprise decision-making.
  relevance_score: 8
  source: llm_enhanced
  text: So there's the local foundry, and that's what Microsoft has branded their...
    I called it, I think, the second tier, kind of where you have WinML and you have
    your local models and you're doing that work. So you have your local models and
    you can compare those to cloud-hosted models and test them.
  topic: strategy
- impact_reason: Reiterates that for specific, high-value data processing tasks where
    existing models are inadequate, custom building remains the only viable path.
  relevance_score: 8
  source: llm_enhanced
  text: And if you have a hard time processing large amounts of data to get insights
    and actions out of it... If that's what you need to do is something where it's
    not available or it's not good enough, there's really no other way around it than
    to build your own model today.
  topic: business
- impact_reason: 'Describes the current practical limitation of local LLMs: context
    must often be manually injected via the prompt rather than seamlessly referencing
    large document sets.'
  relevance_score: 8
  source: llm_enhanced
  text: In my experiences with local AI, I have a pretty narrow context window that
    you could basically feed it, 'Hey, here's everything I know,' and you feed it
    with the prompt. Yeah. And you say, 'Okay, now do this and give it back to me.'
    But you're not feeding it documents.
  topic: technical
- impact_reason: Articulates the user desire for perfect long-term memory in AI interactions,
    setting a high bar for future context management solutions.
  relevance_score: 8
  source: llm_enhanced
  text: I want it to remember everything we've said, like I want as big a context
    as I can possibly get.
  topic: technical
- impact_reason: Quantifies the extreme hardware investment required for high-context
    local AI execution, illustrating the current cost barrier for advanced local inference.
  relevance_score: 8
  source: llm_enhanced
  text: It's a $10,000 card, but that seems to be the thing that makes the most difference
    for a lot of these kinds of tools when you want to handle a lot of context.
  topic: business
- impact_reason: Provides a stark comparison between consumer/prosumer hardware TOPS
    (1300) and the scale of data center GPUs, emphasizing the massive compute gap
    for large-scale training/inference.
  relevance_score: 8
  source: llm_enhanced
  text: My 5080 has 1300 TOPS. I see. So, and when you look at what Nvidia's selling
    to data centers and things is their giant GPUs with huge amounts of memory, this
    super-fast memory in them for a scale of processing.
  topic: technical
- impact_reason: 'Explains the strategic role of NPUs: optimizing for low-power, continuous
    background tasks on mobile/laptops, contrasting with the high-power needs of foreground
    LLM tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: The NPU, I think, was more of a play for a continuous operation or in the
    background, and on mobile devices where battery and power consumption is a much
    bigger concern for individuals...
  topic: strategy
- impact_reason: 'Provides a clear architectural roadmap for achieving multimodality
    locally: chaining specialized models (ASR, OCR, Vision) before the core LLM.'
  relevance_score: 8
  source: llm_enhanced
  text: So, you're going to have to build that. I mean, you could, but you're going
    to have to put a speech recognition model in front of the LLM, or an object detection
    model plus an OCR model plus that, you know, you have to maybe chain these models
    together.
  topic: technical
- impact_reason: Highlights the strategic value of vendor consolidation and abstraction
    layers (like Microsoft's ecosystem) in reducing developer cognitive load and decision
    fatigue.
  relevance_score: 8
  source: llm_enhanced
  text: I don't have enough time to be building all these applications and learning
    the APIs and learning the political history of where all these models come from.
    So, it is a benefit of Microsoft as a software provider. It's the one throat to
    choke, right?
  topic: strategy
- impact_reason: 'Offers pragmatic business advice: perfection is the enemy of progress;
    sometimes ''good enough'' integrated tooling is superior to chasing the absolute
    best component in every category.'
  relevance_score: 8
  source: llm_enhanced
  text: Is it the best of any of the worlds? The absolute best? No, but when you're
    doing a lot of different stuff, sometimes you just have to have some heuristics
    here and just make the decision making.
  topic: strategy
- impact_reason: Provides a concrete, high-end hardware specification and associated
    cost estimate for running a significant, legacy-level model (GPT-3 variant) locally,
    setting a benchmark for enthusiast/prosumer local AI.
  relevance_score: 8
  source: llm_enhanced
  text: GPT-OSS is a version of GPT-3 that can be run locally on a machine with 64
    gigs of RAM and a 5090 with 24 gigs of VRAM. So, that's roughly a $6 or $7,000
    PC, somewhere in that neighborhood, depending on how much you pay for the video
    card.
  topic: technical
- impact_reason: Highlights the growing necessity and specialization within cybersecurity,
    moving beyond general development conferences.
  relevance_score: 7
  source: llm_enhanced
  text: We're putting on a new conference dedicated to 100% security-focused topics.
  topic: strategy
- impact_reason: A strong philosophical statement on the self-reflective nature of
    communication, even with non-sentient tools.
  relevance_score: 7
  source: llm_enhanced
  text: Putting those mean words out there has as much impact on you as it does on
    anything else. And the software is not affected. Yeah. Things, you know, to be
    affected is you.
  topic: safety/ethics
- impact_reason: Provides historical context, linking the fundamental driver of computing
    progress (Moore's Law) directly to memory density (RAM).
  relevance_score: 7
  source: llm_enhanced
  text: 'This is what Moore''s Law actually was about: was making RAM, right?'
  topic: technical
- impact_reason: A philosophical stance on interacting with AI, suggesting that ethical
    and respectful communication standards should apply to AI interactions, mirroring
    how we treat other humans or systems.
  relevance_score: 7
  source: llm_enhanced
  text: I don't think AI should be treated any different, not for the benefit of AI,
    or the benefit of myself.
  topic: safety/ethics
- impact_reason: Provides a concrete, practical example of an existing productivity
    tool leveraging local OCR, setting the baseline before discussing newer AI models.
  relevance_score: 7
  source: llm_enhanced
  text: TextGrab, which is pretty basic. It's also the basis for the PowerToys Text
    Extractor, which is basically select a region on your screen of somebody sent
    you text that you can't actually select and put somewhere where you want it. And
    it does some on-device local OCR.
  topic: technical/product
- impact_reason: Reflects the current market saturation and developer fatigue regarding
    LLMs, suggesting a need to focus on other valuable AI domains like vision and
    segmentation.
  relevance_score: 7
  source: llm_enhanced
  text: Hey, I'm just appreciating you're talking about something other than an LLM,
    like because it's just, it's just overwhelming right now.
  topic: trends
- impact_reason: Introduces Kaggle as a key platform for practicing and competing
    in ML, linking theoretical knowledge to practical application.
  relevance_score: 7
  source: llm_enhanced
  text: Did you ever play with Kaggle? Because I, we talked about this on the show
    ages ago. Or just like, there is another playground for practicing your ML skills.
  topic: technical/business
- impact_reason: Highlights the benefit of open-source practices in the current AI
    integration landscape, allowing for learning through inspection of working examples.
  relevance_score: 7
  source: llm_enhanced
  text: One of the nice things about this current age of programming is a lot of these
    big popular apps are open source. So you can just see how it's done.
  topic: strategy
- impact_reason: Confirms the availability of specialized, optimized models for low-power/edge
    devices, encouraging experimentation beyond standard cloud models.
  relevance_score: 7
  source: llm_enhanced
  text: So if you do have a device that is a device or low-power device and you want
    more of an optimized model, you can find them and run them.
  topic: technical
- impact_reason: Practical warning about the physical resource requirements (storage
    size) for running high-quality local models.
  relevance_score: 7
  source: llm_enhanced
  text: That is the one thing that I'll say. I recently upgraded my Surface hard drive
    from a 512 to a two terabyte because these models are big. And if you want accurate
    ones, they're very large.
  topic: business
- impact_reason: Explains why non-LLM AI (like image classification) is less 'exciting'
    to the general public—the output is probabilistic confidence scores, not conversational
    fluency.
  relevance_score: 7
  source: llm_enhanced
  text: And a lot of times, they'll give you back a number, a fraction of confidence.
    Yeah. And I think maybe this is why they don't get as much play as they're not
    as exciting for individuals to use.
  topic: predictions
- impact_reason: Defines TOPS (Trillion Operations Per Second) as the key metric for
    measuring NPU compute power for neural networks, clarifying industry jargon.
  relevance_score: 7
  source: llm_enhanced
  text: No, because they're just... No, you know, they talk about that Copilot Plus
    PC has 40 TOPS. I don't know what that means. Yeah, that's trillion operations
    per second. It's the measure of its compute power for neural nets.
  topic: technical
- impact_reason: Defines the current state of applied AI development as 'assembly
    required'—it's about integrating components (models, APIs) rather than deploying
    a single, monolithic application.
  relevance_score: 7
  source: llm_enhanced
  text: Yeah, but you will have to do the gluing, some assembly required. This is
    the job, right? Yeah, like, you know, this is not just an app you run, but we
    are assembling parts to try and get to a place where a model can be built.
  topic: strategy
- impact_reason: 'Poses a key strategic decision for developers: choosing between
    specialized open-source models (DeepSeek) versus platform-integrated, curated
    solutions (Microsoft/Phi-3).'
  relevance_score: 7
  source: llm_enhanced
  text: If you were going to build a local LLM, Joe, yourself using some existing
    technology, would you first reach for DeepSeek, or would you go for just the stuff
    that Microsoft is exposing in Windows?
  topic: strategy
- impact_reason: 'Actionable advice for getting started with local AI development:
    prioritize platform-native tools before exploring complex, external dependencies.'
  relevance_score: 7
  source: llm_enhanced
  text: But I would say start with the built-in stuff, definitely at first. And if
    they don't work for you, then you can start making other questions and decisions.
  topic: business
- impact_reason: A powerful realization about the compressed timeline of the Apollo
    13 crisis, emphasizing the speed of high-stakes engineering problem-solving.
  relevance_score: 6
  source: llm_enhanced
  text: The explosion in the tank happens on April, on April 13th, the splashdown
    is April 17th. It was four days. Wow. The whole thing's four days.
  topic: strategy
- impact_reason: Notes the historical significance of Pascal's creation as an attempt
    to synthesize different programming paradigms.
  relevance_score: 6
  source: llm_enhanced
  text: Nicholas Wirth releases Pascal. He worked previously on the language ALGOL
    60, and there are some derivations there. He was trying to do a combination of
    sort of procedural and algorithmic programming.
  topic: technical
- impact_reason: Highlights the self-reflective nature of communication; harshness
    impacts the user more than the software itself, a subtle point on digital well-being.
  relevance_score: 6
  source: llm_enhanced
  text: The software is not affected. Yeah. Things, you know, to be affected is you.
  topic: safety/ethics
- impact_reason: Reinforces the idea that practical software development relies on
    specific functional APIs rather than the abstract 'AI' namespace.
  relevance_score: 6
  source: llm_enhanced
  text: So there's a bunch of, hey, there's a bunch of APIs after the namespace that
    actually point to the real APIs, the real functionality of what you're actually
    trying to do.
  topic: strategy
- impact_reason: Illustrates the rapid deflation and scaling of storage technology,
    which directly enables larger local AI deployments.
  relevance_score: 6
  source: llm_enhanced
  text: I just saw Richard probably knows about this, but there are now 22 terabyte
    SSD drives for like around $500. Can you wrap your mind around that? It's a lot
    of storage.
  topic: strategy
- impact_reason: Acknowledges the difficulty of manual model evaluation and points
    to resources (AI Dev Gallery) designed to streamline experimentation.
  relevance_score: 6
  source: llm_enhanced
  text: It can be tedious to test manually. But there are a lot of tools out there
    to experiment, get started. And if anybody's curious, I definitely you should
    check out the AI Dev Gallery for sure.
  topic: business
- impact_reason: Historical context on Tesseract, a foundational open-source OCR technology,
    showing the lineage of current capabilities.
  relevance_score: 5
  source: llm_enhanced
  text: Tesseract was the open-source project that Google took over. I think I think
    I actually HP started it way back there, and then kind of Google took it over.
  topic: technical
source: Unknown Source
summary: '## Podcast Episode Summary: Local AI Models with Joe Finney


  This episode of .NET Rocks, featuring guest Joe Finney, pivots away from the overwhelming
  focus on Large Language Models (LLMs) to explore the practical implementation and
  ecosystem of **local, on-device AI models** for Windows application development,
  particularly focusing on Computer Vision tasks like Optical Character Recognition
  (OCR).


  The discussion centers on the increasing accessibility of local AI capabilities
  for developers, contrasting older methods with new, integrated Windows APIs.


  ---


  ### 1. Focus Area

  The primary focus is on **Local AI Model Integration in Windows Applications**,
  specifically covering:

  *   **On-Device Computer Vision:** OCR, image segmentation, and object detection.

  *   **Microsoft''s Evolving AI Infrastructure:** The new Windows AI APIs, WinML,
  and their relationship to Copilot Plus PCs.

  *   **Model Management:** The complexity trade-offs between using built-in models,
  running standardized ONNX models, or training custom models.

  *   **Community Resources:** Highlighting platforms like Hugging Face and Kaggle
  for accessing and practicing with non-LLM models.


  ### 2. Key Technical Insights

  *   **Three Tiers of Local AI Integration:** Developers face three complexity levels:
  **Tier 1 (Easiest):** Using the new, built-in Windows AI APIs (requiring identity/Copilot+
  hardware compatibility) which abstract away model management. **Tier 2 (Medium):**
  Using WinML to run custom or downloaded ONNX models, requiring developers to manage
  model files. **Tier 3 (Hardest):** Training and integrating proprietary models from
  scratch for highly niche tasks.

  *   **The Role of Copilot Plus PCs:** These devices are central to the easiest integration
  tier, as they ship with necessary local LLMs and hardware (NPUs) capable of running
  these models, allowing developers to "light up" features instantly if the device
  supports the new Windows AI APIs.

  *   **Beyond LLMs:** The conversation strongly advocates for recognizing and utilizing
  the wealth of specialized models available (OCR, segmentation, detection) hosted
  on platforms like Hugging Face, which predated the current LLM hype cycle.


  ### 3. Business/Investment Angle

  *   **Developer Friction Reduction:** Microsoft''s new APIs significantly lower
  the barrier to entry for integrating sophisticated AI features into existing Windows
  apps (WPF, WinUI, WinForms) without requiring developers to manage large model downloads
  (e.g., 5GB files).

  *   **Market Segmentation:** The existence of specialized, focused conferences like
  the new **Cybersecurity Intersection** (running alongside DevIntersection and NextGen
  AI) shows the industry is segmenting to provide deep-dive expertise beyond general
  AI topics.

  *   **Productivity Gains:** The discussion on AI tooling (like using LLMs via Playwright/Code
  Agents) reinforces the theme that experienced developers are embracing these tools
  faster, valuing the speed of "solutioning" over the joy of procedural coding.


  ### 4. Notable Companies/People

  *   **Joe Finney:** Guest, mobile product owner, and MVP known for productivity
  apps like TextGrab, which uses OCR technology.

  *   **Microsoft:** Central player due to the release of new Windows AI APIs and
  the hardware push with Copilot Plus PCs.

  *   **Tesseract:** Mentioned as the established, open-source standard for OCR before
  modern AI integration.

  *   **Hugging Face:** Highlighted as the essential repository for downloading and
  exploring thousands of non-LLM models (OCR, object detection, etc.).

  *   **Kaggle:** Mentioned as a platform for practicing ML skills through competitions.


  ### 5. Future Implications

  The industry is moving toward a bifurcated approach to local AI:

  1.  **Easy Integration:** Leveraging standardized, managed APIs provided by the
  OS vendor (Microsoft) for common tasks on compatible hardware.

  2.  **Advanced Customization:** Utilizing standardized formats like ONNX and repositories
  like Hugging Face for developers needing specific, fine-tuned models that go beyond
  the built-in capabilities. The focus will shift from *if* local AI is possible to
  *how* to best manage and deploy diverse model types efficiently.


  ### 6. Target Audience

  This episode is highly valuable for **Windows Application Developers (.NET/C# developers)**,
  **Product Owners**, and **Technical Architects** who are looking to integrate practical,
  privacy-preserving, on-device machine learning capabilities into their desktop applications
  without relying solely on cloud APIs or wrestling with complex model deployment
  pipelines.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- microsoft
- google
- openai
- nvidia
title: Local AI Models with Joe Finney
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 103
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 11
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 9
  prominence: 0.9
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 04:18:09 UTC -->
