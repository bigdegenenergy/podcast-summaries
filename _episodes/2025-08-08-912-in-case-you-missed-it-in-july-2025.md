---
companies:
- category: unknown
  confidence: medium
  context: ou missed it in July episode. Welcome back to the Super Data Science Podcast.
    I'm your host, John Cron. This is an in case you
  name: Super Data Science Podcast
  position: 91
- category: unknown
  confidence: medium
  context: to the Super Data Science Podcast. I'm your host, John Cron. This is an
    in case you missed it episode that hi
  name: John Cron
  position: 134
- category: unknown
  confidence: medium
  context: onversation I had in episode 901 in which I asked Lillith Batlia why data-centric
    machine learning research, or DM
  name: Lillith Batlia
  position: 348
- category: unknown
  confidence: medium
  context: ata. So that led me to be very interested in what Andrew Ng coined data-centric
    AI. And I ended up getting in
  name: Andrew Ng
  position: 1402
- category: unknown
  confidence: medium
  context: erested in what Andrew Ng coined data-centric AI. And I ended up getting
    involved with a working group at
  name: And I
  position: 1436
- category: unknown
  confidence: medium
  context: ended up getting involved with a working group at ML Commons called Data
    Perf, where we were looking to benchm
  name: ML Commons
  position: 1492
- category: unknown
  confidence: medium
  context: nvolved with a working group at ML Commons called Data Perf, where we were
    looking to benchmark data-centric
  name: Data Perf
  position: 1510
- category: unknown
  confidence: medium
  context: hings going on. We're working in partnership with Common Crawl, the foundation
    that curates the data sets that m
  name: Common Crawl
  position: 2039
- category: unknown
  confidence: medium
  context: figure out which data points are worth labeling. From DMLR, we turn to
    AI benchmarks in episode 903. In it,
  name: From DMLR
  position: 5254
- category: unknown
  confidence: medium
  context: ', we turn to AI benchmarks in episode 903. In it, Simon Osindero and I
    discuss approaches to circumventing the lim'
  name: Simon Osindero
  position: 5313
- category: unknown
  confidence: medium
  context: n interesting idea. So what about a solution like Chatbot Arena where in
    Chatbot Arena, there's no, there's no co
  name: Chatbot Arena
  position: 6554
- category: unknown
  confidence: medium
  context: he LMSys lab, if I remember correctly, I think is Joey Gonzalez's lab.
    And so Joey Gonzalez has actually been on
  name: Joey Gonzalez
  position: 6724
- category: unknown
  confidence: medium
  context: has actually been on this show talking about it. If I can find that episode
    quickly, yes, episode 707,
  name: If I
  position: 6815
- category: unknown
  confidence: medium
  context: of leads to the whole "sycophancy" thing, right? When OpenAI said, well,
    we rely too much on people's thumbs u
  name: When OpenAI
  position: 8274
- category: tech
  confidence: high
  context: eads to the whole "sycophancy" thing, right? When OpenAI said, well, we
    rely too much on people's thumbs u
  name: Openai
  position: 8279
- category: unknown
  confidence: medium
  context: nd thumbs down and that's what got us in trouble. The LLM arena is pretty
    much a thumbs up and a thumbs dow
  name: The LLM
  position: 8388
- category: unknown
  confidence: medium
  context: at question, which is, well, if we all agree that Pablo Picasso painted
    this thing and that's one of the answers
  name: Pablo Picasso
  position: 9781
- category: unknown
  confidence: medium
  context: us over the years. In episode 905, my guest, Dr. Sebastian Gammon and I,
    continued this conversation specifically r
  name: Sebastian Gammon
  position: 10915
- category: unknown
  confidence: medium
  context: mitted to arXiv in April called Understanding and Mitigating Risks of Generative
    AI in Financial Services. So mostly
  name: Mitigating Risks
  position: 11205
- category: unknown
  confidence: medium
  context: pril called Understanding and Mitigating Risks of Generative AI in Financial
    Services. So mostly so far in this e
  name: Generative AI
  position: 11225
- category: unknown
  confidence: medium
  context: standing and Mitigating Risks of Generative AI in Financial Services. So
    mostly so far in this episode, we've been tal
  name: Financial Services
  position: 11242
- category: unknown
  confidence: medium
  context: wledge. So what are the limitations this creates? For LLMs in general,
    but particularly for RAG. And I'm ass
  name: For LLMs
  position: 11578
- category: unknown
  confidence: medium
  context: you use them out of the box and say, look, I use Llama Guard, I use Chill
    Gemma, I use Aegis, I'm safe now, ri
  name: Llama Guard
  position: 12882
- category: unknown
  confidence: medium
  context: f the box and say, look, I use Llama Guard, I use Chill Gemma, I use Aegis,
    I'm safe now, right? You're protect
  name: Chill Gemma
  position: 12901
- category: unknown
  confidence: medium
  context: 'ation, but especially for these kinds of domains. All I can do is spray
    my mantra: evaluate the system in'
  name: All I
  position: 14621
- category: unknown
  confidence: medium
  context: case is this practice that we had started in the Cold War where you have
    users trying to be malicious. So w
  name: Cold War
  position: 15733
- category: unknown
  confidence: medium
  context: for AI practitioners. Are we so predictable? Dr. Zohar Brompton thinks
    we may be and he has the research to back
  name: Zohar Brompton
  position: 17286
- category: unknown
  confidence: medium
  context: from episode 907. Something very interesting that Benjamin Libet and countless
    others have shown is that you can h
  name: Benjamin Libet
  position: 17467
- category: unknown
  confidence: medium
  context: lief in AI's ability to anticipate user behavior. So I think we were talking
    about, you know, once you g
  name: So I
  position: 18245
- category: unknown
  confidence: medium
  context: nd machine learning together in one nice package. Having AI knowing your
    next move might sound far-fetched, b
  name: Having AI
  position: 21981
- category: unknown
  confidence: medium
  context: set to change? My final guest in this episode of In Case You Missed It
    is Microsoft Research's Dr. Robert Ness. In episo
  name: In Case You Missed It
  position: 22388
- category: tech
  confidence: high
  context: guest in this episode of In Case You Missed It is Microsoft Research's
    Dr. Robert Ness. In episode 909, Dr. N
  name: Microsoft
  position: 22413
- category: unknown
  confidence: medium
  context: guest in this episode of In Case You Missed It is Microsoft Research's
    Dr. Robert Ness. In episode 909, Dr. Ness inves
  name: Microsoft Research
  position: 22413
- category: unknown
  confidence: medium
  context: In Case You Missed It is Microsoft Research's Dr. Robert Ness. In episode
    909, Dr. Ness investigates how we can
  name: Robert Ness
  position: 22438
- category: unknown
  confidence: medium
  context: cool about Stan is that that inference algorithm, Hamiltonian Monte Carlo,
    is, I mean, you can go in there and understand i
  name: Hamiltonian Monte Carlo
  position: 24664
- category: ai_organization
  confidence: high
  context: Organization hosting the Data Perf working group focused on benchmarking
    data-centric machine learning (DMLR). Also mentioned in relation to risk taxonomies.
  name: ML Commons
  source: llm_enhanced
- category: ai_data_source
  confidence: high
  context: Foundation that curates the data sets that most LLMs have been trained
    on. Partnering with ML Commons on a challenge for a low-resource language data
    set.
  name: Common Crawl
  source: llm_enhanced
- category: ai_figure/research
  confidence: high
  context: Coined the term 'data-centric AI' and was the keynote speaker at the inaugural
    DMLR workshop. A major figure in data science.
  name: Andrew Ng
  source: llm_enhanced
- category: ai_research_paper
  confidence: medium
  context: Mentioned as a paper that looked at weighting different domains of the
    pile for optimal LLM pre-training performance.
  name: Doremi
  source: llm_enhanced
- category: ai_research_lab
  confidence: high
  context: The lab at Berkeley that runs Chatbot Arena, associated with Joey Gonzalez.
  name: LMSys lab
  source: llm_enhanced
- category: ai_research_institution
  confidence: high
  context: The university hosting the LMSys lab that runs Chatbot Arena.
  name: Berkeley
  source: llm_enhanced
- category: ai_figure/research
  confidence: high
  context: Professor whose lab (LMSys) at Berkeley devised Chatbot Arena.
  name: Joey Gonzalez
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned in the context of relying too much on user thumbs up/down feedback
    in their arena-style evaluation, leading to issues.
  name: OpenAI
  source: llm_enhanced
- category: ai_user_domain
  confidence: high
  context: Mentioned as a financial services company, serving as the case study domain
    for the paper on Generative AI risks in finance.
  name: Bloomberg
  source: llm_enhanced
- category: ai_model/tool
  confidence: high
  context: A safety model/system mentioned as an example of out-of-the-box safeguards
    that might not be trained on finance-specific risks.
  name: Llama Guard
  source: llm_enhanced
- category: ai_model/tool
  confidence: high
  context: A safety model/system mentioned as an example of out-of-the-box safeguards
    that might not be trained on finance-specific risks.
  name: Chill Gemma
  source: llm_enhanced
- category: ai_model/tool
  confidence: high
  context: A safety model/system mentioned as an example of out-of-the-box safeguards
    that might not be trained on finance-specific risks.
  name: Aegis
  source: llm_enhanced
- category: ai_organization/standard
  confidence: high
  context: Mentioned for its risk management framework for AI, which can serve as
    a starting point for domain-specific risk taxonomies.
  name: NIST
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: General reference to companies providing LLMs (like OpenAI, Google, Anthropic)
    whose benchmark scores should be verified downstream.
  name: Large Language Model Providers
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Affiliation of Dr. Robert Ness, who discusses building causal AI models.
  name: Microsoft Research
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A deep learning library whose code examples are used to make causal AI
    intuitive; handles inference when differentiable loss functions are provided.
  name: PyTorch
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A probabilistic programming platform mentioned in relation to its inference
    algorithm (Hamiltonian Monte Carlo).
  name: Stan
  source: llm_enhanced
date: 2025-08-08 11:00:00 +0000
duration: 33
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD8804861026.mp3
processing_date: 2025-10-04 17:28:50 +0000
quotes:
- length: 143
  relevance_score: 4
  text: There's a really interesting paper, Doremi, that looked at weighting different
    domains of the pile to get the best LLM pre-training performance
  topics: []
- length: 172
  relevance_score: 4
  text: You pit two LLMs against each other and you as a human evaluator of the arena,
    you don't know which two you're seeing output from, but you pick one as better
    than the other
  topics: []
- length: 45
  relevance_score: 4
  text: For LLMs in general, but particularly for RAG
  topics: []
- length: 214
  relevance_score: 4
  text: How can we move forward with all the information that you provided in this
    episode in selecting an LLM for a particular use case, for a particular domain,
    particularly if we want to be applying it in RAG situations
  topics: []
- length: 122
  relevance_score: 4
  text: And that's why we've invested so much in connecting LLMs, you know, data and
    machine learning together in one nice package
  topics: []
- length: 231
  relevance_score: 3
  text: And so those acronyms that you were saying that earlier where this DMLR initiative
    was getting traction, so conferences like ICLR, ICML, NeurIPS, these are the biggest
    conferences that there are, academic conferences that there are
  topics: []
- length: 52
  relevance_score: 3
  text: He's one of the biggest names in data science period
  topics: []
- length: 86
  relevance_score: 3
  text: This was probably the biggest sleep deprivation I had because it's mind-blowing,
    right
  topics: []
- impact_reason: Shows a concrete, high-impact effort (in partnership with Common
    Crawl) to address data scarcity in low-resource languages, a major current challenge
    in global AI deployment.
  relevance_score: 10
  source: llm_enhanced
  text: We're working in partnership with Common Crawl, the foundation that curates
    the data sets that most LLMs have been trained on. We're partnering with them
    on a challenge that will result in a low resource language data set that will
    be publicly available.
  topic: technical/business
- impact_reason: 'The definitive, concise explanation distinguishing DMLR from traditional
    ML: focus on systematic data improvement while keeping the model constant.'
  relevance_score: 10
  source: llm_enhanced
  text: With data-centric machine learning, you're iterating on the data. So you're
    holding the model fixed and you're improving the data, you're systematically engineering
    better data.
  topic: technical
- impact_reason: Directly links human preference bias in arenas to the risk of models
    optimizing for sycophancy (telling users what they want to hear) rather than accuracy.
  relevance_score: 10
  source: llm_enhanced
  text: That might come down to which one's talking the way I like it to talk, which
    kind of leads to the whole 'sycophancy' thing, right?
  topic: safety/ethics
- impact_reason: Introduces a serious, albeit alleged, concern about model version
    control and potential manipulation of public benchmarks (training specifically
    for the arena).
  relevance_score: 10
  source: llm_enhanced
  text: One of the separate allegations for Llama 4 was they released a tested or
    a trained to test model specifically for the arena that was different than the
    Llama 4 we all got in the end.
  topic: safety/ethics
- impact_reason: Identifies a critical limitation of general-purpose LLMs when applied
    to specialized, high-stakes domains like finance.
  relevance_score: 10
  source: llm_enhanced
  text: You emphasize that most foundation models are not trained on finance-specific
    corporate bodies of knowledge.
  topic: business
- impact_reason: 'A crucial warning: Out-of-the-box safety guardrails (like Llama
    Guard) are insufficient for domain-specific risks because their safety definitions
    are too general.'
  relevance_score: 10
  source: llm_enhanced
  text: Even safeguards that are in dedicated models or systems to provide these kind
    of first paths like is the safe is this unsafe judgment, they're also not trained
    on financial services.
  topic: safety/ethics
- impact_reason: Directly critiques the false sense of security derived from using
    generic safety tools in specialized enterprise environments, contrasting broad
    safety categories with domain-specific obligations (law, healthcare, finance).
  relevance_score: 10
  source: llm_enhanced
  text: If you use them out of the box and say, look, I use Llama Guard, I use Chill
    Gemma, I use Aegis, I'm safe now, right? You're protected against a particular
    view of safety that is very much grounded in categories that are relevant to broad
    populations, to things like chatbots that help you do productivity day-to-day
    tasks.
  topic: safety/ethics
- impact_reason: 'This is the core, actionable strategic advice for deploying LLMs
    in specialized environments: context-specific evaluation is paramount.'
  relevance_score: 10
  source: llm_enhanced
  text: 'All I can do is spray my mantra: evaluate the system in the context that
    it''s deployed in.'
  topic: strategy/business
- impact_reason: A strong warning against relying solely on vendor benchmarks, stressing
    that performance must be validated in the actual deployment context.
  relevance_score: 10
  source: llm_enhanced
  text: Don't just take the word of the large language model providers that their
    benchmark scores are going to translate into all the downstream applications.
  topic: business/strategy
- impact_reason: 'Directly links neuroscience findings to business intelligence: if
    humans are predictable subconsciously, AI can predict consumer behavior long before
    the consumer is aware of their intent.'
  relevance_score: 10
  source: llm_enhanced
  text: it means that as a consumer, your behavior is also affected by many things
    that you're not aware of, and it means that as a business that sells to consumers,
    you can probably know much in advance of your specific customers' behaviors before
    the event takes place.
  topic: business/predictions
- impact_reason: 'Introduces the next frontier in ML: moving from correlation to causation,
    which is essential for robust decision-making.'
  relevance_score: 10
  source: llm_enhanced
  text: we can start to build causal AI models that go beyond this standard correlation-based
    patterns and probabilities that we're used to, machine learning models generally
    detecting.
  topic: technical/breakthroughs
- impact_reason: 'This is a perfect summary of the value proposition of probabilistic
    programming languages (PPLs): defining the model structure is sufficient; the
    library handles the complex inference machinery (like MCMC/HMC).'
  relevance_score: 10
  source: llm_enhanced
  text: And that was like, well, it kind of is rocket science. But you can, but you
    can still kind of just specify your model, specify what the parameters, what's
    the model. And as long as you satisfy a certain set of requirements, I think mainly
    that's all these things have to be continuous, and that's the inference will kind
    of just work for you without you having to go implement your own inference algorithm.
  topic: technical
- impact_reason: This bridges the gap between Causal Inference (represented by graphs)
    and automated inference engines, suggesting that explicit graphical representation
    enables automated computation, similar to how model specification works in PPLs.
  relevance_score: 10
  source: llm_enhanced
  text: It's the same thing here, right? Where like if you can, as long as you can
    kind of specify your causal assumptions in subcases in the form of a graph, for
    example, then you can rely on, say, graphical causal inference to do the inference
    algorithms from, say, probabilistic graphical models or to kind of handle the
    inference there for you.
  topic: technical
- impact_reason: 'This is a crucial insight for modern deep learning practitioners
    interested in causality: integrating causal structure directly into the model
    architecture (via graph constraints) allows standard deep learning optimization
    (differentiable loss) to perform the necessary causal inference.'
  relevance_score: 10
  source: llm_enhanced
  text: If you're, if you're implementing it in PyTorch, I have plenty of PyTorch
    examples in the book, as long as you can, you know, exactly, incorporate your
    causal assumptions in the structure of the model in your algorithm and write a
    kind of basic inference algorithm that has a differentiable loss function, then
    PyTorch is going to handle all of the inference for you, right?
  topic: technical
- impact_reason: Broad statement emphasizing that Data-Centric Machine Learning Research
    (DMLR) is no longer niche but fundamental to all data practitioners.
  relevance_score: 9
  source: llm_enhanced
  text: So this is now a topic that is relevant to every listener. Anybody who is
    working with data, this is relevant.
  topic: strategy
- impact_reason: 'A classic, relatable anecdote illustrating the core motivation behind
    DMLR: poor data quality renders even the best algorithms untrustworthy.'
  relevance_score: 9
  source: llm_enhanced
  text: At a certain point, I realized that the label data I was working with was
    so noisy, just had so many mislabeled instances and all of that, that it really
    curtailed my ability to evaluate the performance of the algorithm just because
    I couldn't necessarily trust my data.
  topic: technical
- impact_reason: Cites a specific, advanced DMLR technique (domain weighting) relevant
    to optimizing LLM pre-training data mixtures.
  relevance_score: 9
  source: llm_enhanced
  text: There's a really interesting paper, Doremi, that looked at weighting different
    domains of the pile to get the best LLM pre-training performance.
  topic: technical
- impact_reason: Describes a sophisticated, non-iterative data selection method using
    linear algebra, pushing the boundaries of data efficiency beyond standard active
    learning.
  relevance_score: 9
  source: llm_enhanced
  text: They looked at selecting the best data points for training a model, a priori.
    So not even active learning... but just with a data set from scratch using linear
    algebra to figure out which data points are worth labeling.
  topic: technical
- impact_reason: Identifies benchmark contamination as a critical, unresolved problem
    in AI evaluation.
  relevance_score: 9
  source: llm_enhanced
  text: You talked about contamination. And so what is the resolution there? This
    seems like a really tricky problem. How do we prevent leaks?
  topic: safety/ethics
- impact_reason: Strong endorsement for human preference evaluation (blind judging)
    as a key metric for real-world LLM usability.
  relevance_score: 9
  source: llm_enhanced
  text: Blind judging from a human for me is one of the best ways to really get a
    good sense of an LLM's usability.
  topic: strategy
- impact_reason: 'Highlights the fundamental limitation of preference-based evaluation:
    it measures subjective liking, not objective knowledge, unless paired with structured
    testing.'
  relevance_score: 9
  source: llm_enhanced
  text: We're just judging preference as opposed to knowledge. And again, without
    that structured data set.
  topic: technical
- impact_reason: 'Strong business advice: AI adoption in regulated or high-risk sectors
    requires domain-specific risk taxonomy development, not just generic safety layers.'
  relevance_score: 9
  source: llm_enhanced
  text: Finance here, financial services, and particular capital markets and asset
    management is the case study that we use to make the point that we really need
    to think about risk and risk taxonomies and risk management in our domain, in
    what we are trying to build.
  topic: business
- impact_reason: Points out the homogeneity of safety mechanisms in general productivity
    LLMs and contrasts their risk profiles with those in specialized, regulated industries.
  relevance_score: 9
  source: llm_enhanced
  text: The typical applications that you would see in those AI productivity tools,
    no matter which one you use, they all have similar mechanisms. But those are not
    necessarily the same risks that we are under in financial services.
  topic: safety/strategy
- impact_reason: Reinforces the mantra with specific domain examples, emphasizing
    the necessity of SME involvement in evaluation.
  relevance_score: 9
  source: llm_enhanced
  text: If you are building something for healthcare, well, you better evaluate in
    the context of healthcare. If you are building in the context of financial services,
    you better evaluate your subject matter experts in financial services.
  topic: strategy/business
- impact_reason: A concise, memorable summary of the necessity of continuous, rigorous
    evaluation.
  relevance_score: 9
  source: llm_enhanced
  text: The best way to know if you've got a system that's fit for purpose is to test,
    test, and test again.
  topic: strategy
- impact_reason: Introduces foundational neuroscience findings (Libet experiments)
    suggesting decisions are made subconsciously before conscious awareness, challenging
    the notion of free will.
  relevance_score: 9
  source: llm_enhanced
  text: Something very interesting that Benjamin Libet and countless others have shown
    is that you can have a neurological, a neuro—the neural basis of some conscious
    idea that you have happens hundreds of milliseconds before you have the conscious
    thought.
  topic: predictions/philosophy
- impact_reason: 'The core, potentially disturbing implication of predetermination
    research: conscious awareness lags behind subconscious decision-making.'
  relevance_score: 9
  source: llm_enhanced
  text: But in fact, what these experiments show is that you become aware of a decision
    after that decision has already been made subconsciously in your brain.
  topic: philosophy
- impact_reason: Positions predictive analytics based on historical (and potentially
    unconscious) behavior as the most significant driver for business transformation.
  relevance_score: 9
  source: llm_enhanced
  text: that ability to make those predictions based on their historical behavior
    is like I said earlier, the biggest lever we know in the industry for transforming
    businesses.
  topic: business
- impact_reason: 'Describes a key goal in making causal AI accessible: decoupling
    the theoretical causal structure from the computational/statistical implementation
    details.'
  relevance_score: 9
  source: llm_enhanced
  text: I was trying to do with my book was to separate out the abstractions that
    have to do with statistics and computing, right? Scale it up, you know, algorithmic
    complexity from the causality.
  topic: technical
- impact_reason: 'Explains the mechanism for implementing causal inference: using
    graphical models to encode assumptions so that established inference algorithms
    can execute the analysis.'
  relevance_score: 9
  source: llm_enhanced
  text: if you can kind of specify your causal assumptions in subcases in the form
    of a graph, for example, then you can rely on, say, graphical causal inference
    to do the inference algorithms from, say, probabilistic graphical mod[els].
  topic: technical
- impact_reason: 'This highlights a key strategic goal in modern ML/Causal Inference:
    decoupling complex statistical/computational scaling from the core causal modeling
    assumptions, allowing practitioners to focus on the latter.'
  relevance_score: 9
  source: llm_enhanced
  text: So one of the things that I was, I had mentioned that I was trying to do with
    my book was to separate out the abstractions that have to do with statistics and
    computing, right? Scale it up, you know, algorithmic complexity from the causality.
  topic: strategy
- impact_reason: This speaks directly to the power of modern ML frameworks (like PyTorch
    or specialized libraries) in abstracting away low-level complexity, enabling faster
    development and focus on domain-specific problems.
  relevance_score: 9
  source: llm_enhanced
  text: And what's cool about, you know, the libraries that we have today is that
    they can actually help us, if we're able to separate those abstractions, then
    we get to focus on one thing while leaving the nuts and bolts to be handled essentially
    by the library.
  topic: technical
- impact_reason: Highlights the long-standing, growing importance of the Data-Centric
    AI paradigm shift, moving beyond just model iteration.
  relevance_score: 8
  source: llm_enhanced
  text: The impetus for having an episode when we talked about it on the train already
    a year ago was this idea of data-centric machine learning.
  topic: strategy
- impact_reason: Directly links the speaker's journey to the widely recognized 'Data-Centric
    AI' movement spearheaded by Andrew Ng.
  relevance_score: 8
  source: llm_enhanced
  text: That led me to be very interested in what Andrew Ng coined data-centric AI.
  topic: strategy
- impact_reason: 'Explains the inherent paradox of creating verifiable benchmarks:
    the need for accessible answers conflicts directly with the need to keep them
    hidden from training data.'
  relevance_score: 8
  source: llm_enhanced
  text: If a benchmark literally comes with the answers, that's the whole point of
    the benchmark is you're supposed to know the right answer. So the same place where
    you get the questions for the benchmark also has the answers to the benchmarks
    where you can validate that it's correct. So it's impossible to not have the answers
    not on the internet.
  topic: safety/ethics
- impact_reason: Proposes the Chatbot Arena model (human preference ranking without
    ground truth answers) as an alternative paradigm for evaluation, especially for
    subjective tasks.
  relevance_score: 8
  source: llm_enhanced
  text: What about a solution like Chatbot Arena where in Chatbot Arena, there's no,
    there's no correct answer necessarily. It's so it's run by Berkeley, the LMSys
    lab...
  topic: technical
- impact_reason: Raises a profound philosophical and governance question regarding
    AI evaluation authority.
  relevance_score: 8
  source: llm_enhanced
  text: who has the right to judge whether or not the AI was correct or not? That's
    a big question.
  topic: safety/ethics
- impact_reason: A concluding, strategic statement on the enduring difficulty of establishing
    reliable AI evaluation methodologies.
  relevance_score: 8
  source: llm_enhanced
  text: Getting the right way to judge a system is a huge question and it's one that
    I am sure will continue to perplex and challenge us over the years.
  topic: strategy
- impact_reason: Provides practical starting points (NIST, ML Commons) for developing
    domain-specific risk taxonomies, balancing general frameworks with necessary customization.
  relevance_score: 8
  source: llm_enhanced
  text: There are very good starting points. There are taxonomies such as the NIST
    risk management framework for AI. There are other industry collaborations ongoing.
    There's ML Commons. Those all provide more general-purpose taxonomies, but just
    to take them as a starting point and then from there adjusting them to your domain
    can often save a lot of time.
  topic: safety/strategy
- impact_reason: Defines and advocates for red teaming as a crucial, proactive method
    for quantifying the unknown risk surface of an AI system.
  relevance_score: 8
  source: llm_enhanced
  text: Red teaming in this case is this practice that we had started in the Cold
    War where you have users trying to be malicious. So we get people in the same
    room and we say, look, for the next couple of hours, try and break the system,
    try and play evil.
  topic: safety/technical
- impact_reason: Connects rigorous evaluation directly to user trust, reliability,
    and long-term product adoption.
  relevance_score: 8
  source: llm_enhanced
  text: if you follow that advice, you're going to have a system that is in the end
    much more trustworthy, reliable, robust, and you're going to have users that are
    going to keep using it rather than trying it twice, getting really bad answers
    both times and never touching it again.
  topic: business
- impact_reason: Asserts the scientific consensus on the existence of unconscious
    causal drivers for human decisions.
  relevance_score: 8
  source: llm_enhanced
  text: the fact that there are brain processes that are directly causally related
    to decisions we make and that we don't have access, we don't have conscious access
    to those processes, I think is already completely agreed upon.
  topic: philosophy
- impact_reason: A forward-looking statement on the near-term capability of AI to
    exploit subconscious behavioral patterns derived from digital footprints.
  relevance_score: 8
  source: llm_enhanced
  text: Having AI knowing your next move might sound far-fetched, but given how much
    of our activities take place online, AI tools may soon be able to detect behavioral
    patterns we might not even consciously recognize ourselves.
  topic: predictions
- impact_reason: A crucial clarification on the current limitations of AI versus human
    cognition (pattern matching vs. intuition).
  relevance_score: 8
  source: llm_enhanced
  text: AI doesn't make intuitive decisions the way we do. Instead, it relies on patterns
    and probabilities.
  topic: limitations
- impact_reason: 'Illustrates the practical challenge in causal inference: the necessity
    of collecting relevant data to avoid making potentially flawed assumptions about
    confounding variables.'
  relevance_score: 8
  source: llm_enhanced
  text: if we weren't collecting that guild data, we'd have to have more assumptions.
    We'd have to basically make the assumption that being in a guild doesn't matter
    or not.
  topic: technical
- impact_reason: It provides a relatable perspective on complex inference algorithms
    like HMC, suggesting that while mathematically deep, they are accessible enough
    to be encapsulated in user-friendly tools like Stan.
  relevance_score: 8
  source: llm_enhanced
  text: To some extent, you saw this, you mentioned you interviewed somebody who talked
    about Stan. And what's cool about Stan is that that inference algorithm, Hamiltonian
    Monte Carlo, is, I mean, you can go in there and understand it's not, you know,
    well, it is physics, but it's not rocket science.
  topic: technical
- impact_reason: Provides a clear definition of 'low resource language' in the context
    of AI training data, emphasizing the internet representation gap.
  relevance_score: 7
  source: llm_enhanced
  text: When you say low resource language, this is languages for which there are
    not many data available online, there could be rarely spoken languages or for
    whatever reason, languages that even if they're spoken relatively commonly, they
    aren't represented on the internet.
  topic: safety/ethics
- impact_reason: 'Defines the current role of traditional benchmarks: serving as a
    proxy for agreed-upon general world knowledge.'
  relevance_score: 7
  source: llm_enhanced
  text: Benchmarks is that is our current proxy to that question, which is, well,
    if we all agree that Pablo Picasso painted this thing and that's one of the answers
    they can pick from, it's on the right track to knowing general world knowledge.
  topic: strategy
- impact_reason: Emphasizes that risk surface, often considered unknown, can and should
    be quantified through rigorous testing like red teaming.
  relevance_score: 7
  source: llm_enhanced
  text: We'll just measure it and then you have it. So that's kind of the main takeaway
    that we have.
  topic: strategy
- impact_reason: A strong statement on democratizing advanced predictive AI capabilities
    for SMBs.
  relevance_score: 7
  source: llm_enhanced
  text: my personal mission, I want to bring these capabilities to as many small and
    medium-sized businesses as possible because they also deserve quote unquote, that
    remarkable technology that basically tells you what people are going to do even
    before they know what they're going to do.
  topic: business
- impact_reason: A direct, albeit incomplete, statement about the foundational philosophy
    behind PyTorch—enabling flexible, custom algorithm implementation via automatic
    differentiation.
  relevance_score: 7
  source: llm_enhanced
  text: That's kind of why we invented PyTorch to say like,
  topic: technical
source: Unknown Source
summary: '## Podcast Episode Summary: 912: In Case You Missed It in July 2025


  This episode is a compilation highlighting the most significant discussions from
  the Super Data Science Podcast during July 2025, covering advancements and critical
  challenges in Data-Centric AI, LLM evaluation, domain-specific risk management,
  and the philosophical implications of human predictability.


  ---


  ### 1. Focus Area

  The episode focuses on four distinct, high-impact areas within modern AI and Data
  Science:

  1. **Data-Centric Machine Learning Research (DMLR):** Shifting focus from model
  iteration to systematic data improvement, particularly in complex fields like legal
  tech.

  2. **AI Benchmarking and Evaluation:** Addressing the inherent limitations of current
  benchmarks, including data contamination and the subjectivity of human preference
  in LLM assessment (e.g., Chatbot Arena).

  3. **Domain-Specific AI Risk Management:** Analyzing the risks of deploying general-purpose
  LLMs in highly regulated, knowledge-intensive sectors like finance, emphasizing
  the need for domain-specific safety taxonomies.

  4. **Causal AI and Human Predictability:** Exploring the neuroscience behind human
  decision-making (predetermination) and how this informs the development of Causal
  AI models that move beyond correlation.


  ### 2. Key Technical Insights

  *   **DMLR Paradigm:** The core technical shift is holding the model architecture
  fixed and systematically engineering better data (labeling, aggregation, selection)
  to improve performance, as championed by Andrew Ng''s data-centric AI movement.

  *   **Benchmark Contamination Challenge:** Traditional benchmarks are inherently
  flawed because the answers must be public for validation. Solutions lean toward
  controlled, blind evaluation methods, such as the preference-based judging seen
  in Chatbot Arena, though this introduces subjectivity (sycophancy).

  *   **Causal Modeling with Code:** Modern libraries (like PyTorch) allow data scientists
  to separate the theoretical concepts of causality from the computational complexity,
  enabling the practical implementation of causal inference models to determine cause-and-effect
  (e.g., does side quest engagement *cause* higher spending?).


  ### 3. Business/Investment Angle

  *   **Data Quality as Competitive Edge:** In specialized fields, the quality and
  relevance of proprietary data (not just model choice) will be the primary differentiator
  for AI accuracy and trustworthiness.

  *   **Domain-Specific Safety Investment:** Companies in regulated sectors (Finance,
  Healthcare) cannot rely on off-the-shelf safety guards (like Llama Guard). They
  must invest in adapting or building safety taxonomies specific to their regulatory
  environment, treating this as a critical risk management function.

  *   **Predictive Consumer Behavior:** Understanding that human behavior is often
  subconsciously predetermined offers businesses a massive lever. Collecting consumer
  data allows for highly accurate predictions of purchases, churn, and lifetime value,
  democratizing advanced predictive capabilities for SMEs.


  ### 4. Notable Companies/People

  *   **Lillith Batlia:** Discussed the rise of DMLR, its traction in academic conferences
  (ICLR, NeurIPS), and involvement with ML Commons'' Data Perf working group, including
  a partnership with **Common Crawl** to create low-resource language datasets.

  *   **Andrew Ng:** Mentioned as the originator of the "data-centric AI" concept
  and a key figure in the DMLR community.

  *   **Simon Osindero:** Discussed the limitations of AI benchmarks and the mechanics
  of preference-based evaluation systems like **Chatbot Arena** (run by **LMSys lab**
  at Berkeley).

  *   **Dr. Sebastian Gammon:** Highlighted research on the risks of Generative AI
  in **Financial Services**, noting that foundation models lack domain-specific training
  and that generic safety mechanisms fail in regulated contexts.

  *   **Dr. Zohar Brompton:** Presented research on **human predetermination** based
  on neuroscience (Libet experiments), suggesting that consumer behavior is often
  predictable long before the conscious decision is made.

  *   **Dr. Robert Ness (Microsoft Research):** Focused on building **Causal AI**
  models, emphasizing the need to move beyond correlation using tools that abstract
  statistical complexity (mentioning **Stan** as an example of a tool handling complex
  inference).


  ### 5. Future Implications

  The industry is moving toward **contextualized and rigorously evaluated AI systems**.
  Future success will depend less on finding the next groundbreaking algorithm and
  more on:

  1.  Systematically cleaning and engineering training/fine-tuning data (DMLR).

  2.  Developing domain-specific evaluation frameworks that test for real-world compliance
  and risk, rather than general benchmark scores.

  3.  Integrating causal inference to understand *why* systems behave as they do,
  moving beyond simple pattern matching, which aligns with the growing ability to
  predict human actions.


  ### 6. Target Audience

  This episode is highly valuable for **Senior Data Scientists, ML Engineers, AI Product
  Managers, and Technology Strategists** operating in specialized or regulated industries
  (e.g., FinTech, LegalTech, HealthTech) who are responsible for model deployment,
  risk assessment, and long-term AI strategy.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- investment
- openai
- microsoft
title: '912: In Case You Missed It in July 2025'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 94
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 11
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 2
  prominence: 0.2
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-04 17:28:50 UTC -->
