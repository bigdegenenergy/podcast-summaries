---
companies:
- category: unknown
  confidence: medium
  context: It's time for Pure Performance! Get your stopwatch ready! It's time for
    pure per
  name: Pure Performance
  position: 14
- category: unknown
  confidence: medium
  context: pwatch ready! It's time for pure performance with Andy Grabner and Brian
    Wilson. Hello everybody and welcome to
  name: Andy Grabner
  position: 94
- category: unknown
  confidence: medium
  context: s time for pure performance with Andy Grabner and Brian Wilson. Hello everybody
    and welcome to another episode o
  name: Brian Wilson
  position: 111
- category: unknown
  confidence: medium
  context: h me my co-host Andy Grabner. Who's here with the Davis AI figurine? And
    I just want to let people know beca
  name: Davis AI
  position: 291
- category: unknown
  confidence: medium
  context: y Grabner. Who's here with the Davis AI figurine? And I just want to let
    people know because this is audi
  name: And I
  position: 310
- category: unknown
  confidence: medium
  context: Andy, and I think we can all have some gratitude. But Andy's playing with
    toys today. Yeah, it looks like it
  name: But Andy
  position: 568
- category: unknown
  confidence: medium
  context: in my hand today? Because you're feeling playful? Maybe I feel playful.
    But really, because last week I had
  name: Maybe I
  position: 1102
- category: unknown
  confidence: medium
  context: sk me anything session. And these colleagues were Gabriela Hassan-Biegmann,
    Sophia Habib, who moderated the whole t
  name: Gabriela Hassan
  position: 1571
- category: unknown
  confidence: medium
  context: d these colleagues were Gabriela Hassan-Biegmann, Sophia Habib, who moderated
    the whole thing, and our guest fro
  name: Sophia Habib
  position: 1597
- category: unknown
  confidence: medium
  context: erated the whole thing, and our guest from today, Thomas Natchleger. And
    you talked about how we internally build a m
  name: Thomas Natchleger
  position: 1668
- category: unknown
  confidence: medium
  context: changes and how we in general deal with testing. And Brian, I thought because
    we both have a big history and
  name: And Brian
  position: 1923
- category: unknown
  confidence: medium
  context: fascinating to see you spent a lot of time at the Technical University
    in Graz, then at the Software Competence Center i
  name: Technical University
  position: 3188
- category: unknown
  confidence: medium
  context: at the Technical University in Graz, then at the Software Competence Center
    in Hagenberg. And then you were at Machine Learni
  name: Software Competence Center
  position: 3230
- category: unknown
  confidence: medium
  context: petence Center in Hagenberg. And then you were at Machine Learning Engineer
    at Blue Sky Weather. So I hope you had a lot of b
  name: Machine Learning Engineer
  position: 3292
- category: unknown
  confidence: medium
  context: And then you were at Machine Learning Engineer at Blue Sky Weather. So
    I hope you had a lot of blue sky predictions,
  name: Blue Sky Weather
  position: 3321
- category: unknown
  confidence: medium
  context: at Machine Learning Engineer at Blue Sky Weather. So I hope you had a lot
    of blue sky predictions, becau
  name: So I
  position: 3339
- category: unknown
  confidence: medium
  context: re pretty nice here. And now you're at Dynatrace. But Thomas, can you please
    introduce yourself to our audienc
  name: But Thomas
  position: 3480
- category: unknown
  confidence: medium
  context: ster thesis, because you can't remember bachelor. Because I went to high
    school, then started working, and af
  name: Because I
  position: 5380
- category: unknown
  confidence: medium
  context: early 2000s with somebody that you may also know, Ulrich Bodenhofer. He
    was one, he was my lecturer, and we did for t
  name: Ulrich Bodenhofer
  position: 5601
- category: tech
  confidence: high
  context: ime, but due to the lack of, you know, not having Nvidia back then, right,
    it didn't make it into the publ
  name: Nvidia
  position: 6966
- category: unknown
  confidence: medium
  context: ich mostly have been using kind of, I don't know, Hidden Markov Models
    and whatnot to model language back then. At some
  name: Hidden Markov Models
  position: 8622
- category: unknown
  confidence: medium
  context: 2018, there was this one big paper which called "Attention Is All You Need,"
    where researchers from OpenAI published the fir
  name: Attention Is All You Need
  position: 8980
- category: tech
  confidence: high
  context: ttention Is All You Need," where researchers from OpenAI published the
    first Transformer model. And then f
  name: Openai
  position: 9031
- category: tech
  confidence: high
  context: as like only three years on, and then, I mean, on Hugging Face, you could
    already have many of those Transformer
  name: Hugging Face
  position: 9152
- category: tech
  confidence: high
  context: inery which does all this learning and stochastic gradient descent, the
    real credit goes back to a student i
  name: Gradient
  position: 10927
- category: unknown
  confidence: medium
  context: whether the output of the system we built, of our Davis Co-Pilot, is giving
    what you're expecting, right? It
  name: Davis Co
  position: 14092
- category: unknown
  confidence: medium
  context: ur Davis Co-Pilot is what you typically call this Retrieval Augmented Generation
    approach, where under the hood, you use an LLM pl
  name: Retrieval Augmented Generation
  position: 14853
- category: unknown
  confidence: medium
  context: y of the measurement tools, I would argue, right? As I mentioned, let's
    say, let's do a time travel agai
  name: As I
  position: 16812
- category: unknown
  confidence: medium
  context: at model is very similar to what I would call the Stack Overflow model,
    where people say, this is the best answer,
  name: Stack Overflow
  position: 20541
- category: unknown
  confidence: medium
  context: re are some other ones that may or may not apply. And Watson, I remember
    when they were first talking about Wa
  name: And Watson
  position: 20824
- category: unknown
  confidence: medium
  context: e retrieval precision. But then comes to the LLM. The LLM gets to see this,
    right, and the expected answer.
  name: The LLM
  position: 23370
- category: tech
  confidence: high
  context: have the capability anymore to look up stuff via Google or find something
    or actually to understand, to h
  name: Google
  position: 24113
- category: unknown
  confidence: medium
  context: bout now again our problem domain, observability? If I think about observability,
    we have many different
  name: If I
  position: 24735
- category: unknown
  confidence: medium
  context: in 10 years, nobody knows today, that's my take. But I still trust the,
    that they come right at the divi
  name: But I
  position: 30981
- category: unknown
  confidence: medium
  context: ently as if you were trained in, I don't know, in San Francisco being a
    software developer, right? And so, the li
  name: San Francisco
  position: 32374
- category: unknown
  confidence: medium
  context: ext learning. The other thing is, if you go to an OpenAI API, you don't
    get this enterprise SLAs, you cannot o
  name: OpenAI API
  position: 35637
- category: unknown
  confidence: medium
  context: f a podcast we recorded the previous session with Pini Ressnick, because
    he talked about the transformation from
  name: Pini Ressnick
  position: 36835
- category: ai_application
  confidence: medium
  context: A figurine/character mentioned at the start, possibly related to the company's
    internal AI assistant or branding (Dynatrace).
  name: Davis AI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of an AI tool that sometimes produces hallucinations
    or incorrect outputs.
  name: CoPilot
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Where Thomas Natchleger spent time researching competition in neuroscience
    and simulating brains, foundational to understanding neural networks.
  name: Technical University in Graz
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A previous workplace of the guest speaker, Thomas Natchleger.
  name: Software Competence Center in Hagenberg
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Thomas Natchleger's former employer, where he worked as a Machine Learning
    Engineer predicting weather using ML models.
  name: Blue Sky Weather
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The current employer of the guest speaker, Thomas Natchleger, where they
    are building a managerial co-pilot using LLMs.
  name: Dynatrace
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as the publisher of the 2018 paper 'Attention Is All You Need'
    (introducing the Transformer model) and for commercializing LLMs via API.
  name: OpenAI
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a platform where many Transformer models were available shortly
    after the 2018 paper.
  name: Hugging Face
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A well-known researcher famous for being precise about credit attribution
    in AI history, specifically mentioning early work on automated gradient descent.
  name: JÃ¼rgen Schmidhuber
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in the context of modern neural network machinery, alongside
    PyTorch.
  name: TensorFlow
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in the context of modern neural network machinery, alongside
    TensorFlow.
  name: PyTorch
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Credited with inventing the first neural network in the 1950s.
  name: Minsky and Papert
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Credited with the fundamental paper on deep neural networks around 1986.
  name: Rumelhart and Hinton
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the provider of the computational power (GPUs) necessary for
    modern neural networks, which was lacking in earlier decades.
  name: Nvidia
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The internal managerial co-pilot being built at Dynatrace, utilizing RAG
    architecture.
  name: Davis Co-Pilot
  source: llm_enhanced
date: 2025-10-13 02:00:08 +0000
duration: 49
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/af5d5042fc3442b1b3c1e3593785d59e/
processing_date: 2025-10-13 08:11:33 +0000
quotes:
- length: 281
  relevance_score: 4
  text: He was one, he was my lecturer, and we did for the bachelor thesis, we worked
    also on some machine learning algorithms to predict and calculate how to best
    steer kind of like an autonomous driving car, and we were training the model with
    data and when to break and things like that
  topics: []
- length: 211
  relevance_score: 4
  text: And from my point of view, the major breakthrough was actually already earlier,
    in, let's say, in the late 2010s, when the first people started to think about
    reusing GPUs, not for games, but for neural networks
  topics: []
- length: 137
  relevance_score: 4
  text: And one is this, are more like a traditional, traditional, you know, since
    two years, traditional chatbot based on a large language model
  topics: []
- length: 192
  relevance_score: 4
  text: So, that's now a very common technique to use this, to use, let's say, an
    LLM which is more powerful than the LLM you're using to generate the output, as
    a judge to argue about, is it faithful
  topics: []
- length: 174
  relevance_score: 4
  text: Let's say two or three years ago, there were many, many people who were kind
    of fine-tuning these large language models to adhere to a task, to a domain, and
    things like that
  topics: []
- length: 146
  relevance_score: 4
  text: And so, the lineage and how the training actually takes place, and in which
    direction the large language model has been aligned to react to, right
  topics: []
- length: 241
  relevance_score: 4
  text: These are these major training steps in all these large language models where
    you have first the language training, then you have this reinforcement learning
    with human in the loop, then you have the next goal, assessment training, so to
    say
  topics: []
- length: 234
  relevance_score: 4
  text: And when you then skip code by that, for example, and you give it a thumbs
    down, or you don't accept the code proposal or whatnot, then all this information
    goes back into their service, and they will leverage it for the next training
  topics: []
- length: 300
  relevance_score: 4
  text: So depending on your settings, whether you turn that off or not, the information,
    the feedback you're getting from making it, and the LLM making a failure, can
    enter the improvement chain at various levels, really down to the next retraining
    or the in-context learning as we do it now for our product
  topics: []
- length: 184
  relevance_score: 3
  text: And then there was this, I think it was 2018, there was this one big paper
    which called "Attention Is All You Need," where researchers from OpenAI published
    the first Transformer model
  topics: []
- length: 154
  relevance_score: 3
  text: And then from there, it was like only three years on, and then, I mean, on
    Hugging Face, you could already have many of those Transformer models back then
  topics: []
- length: 193
  relevance_score: 3
  text: And yes, there is also, there's also companies or leaders in that space saying
    that then for exactly for narrowly focused agents, that the brain which controls
    the tools must not be the biggest
  topics: []
- length: 49
  relevance_score: 3
  text: It's not necessary that this is the biggest brain
  topics: []
- length: 149
  relevance_score: 3
  text: And as far as I see the situation, this pre-training is more like learning
    the general skills, but the factual knowledge you can Google always, right
  topics: []
- length: 117
  relevance_score: 3
  text: That's the training phase, and then if it's trained, it has Google and ping
    and whatnot, and it can pull in the facts
  topics: []
- impact_reason: Directly addresses the 'trust deficit' in AI outputs, comparing it
    to established concepts like observability trust, which resonates strongly with
    technical and business audiences.
  relevance_score: 10
  source: llm_enhanced
  text: Just like we have to trust what our observability is telling us and all that
    kind of stuff, there's this trust factor with is what AI is telling me true?
  topic: safety/business
- impact_reason: Uses an excellent analogy (baby learning to walk) to describe emergent
    behavior in AI capabilities, suggesting that progress isn't always linear but
    happens in sudden leaps once thresholds are met.
  relevance_score: 10
  source: llm_enhanced
  text: 'And then suddenly it walks, right? It''s not that it first can do two steps
    and three and four, but suddenly it can do all of them. And that''s the same in
    this evolution of neural network capabilities, when everything comes together:
    the hardware, the knowledge, the experience, and how to train them.'
  topic: technical/predictions
- impact_reason: Defines the rigorous testing requirements for structured output (like
    generated queries), emphasizing executability and correctness, which is much stricter
    than free text.
  relevance_score: 10
  source: llm_enhanced
  text: So the output is structured, is, let's say, testable, whether it's, for example,
    syntactically correct, whether it's semantically correct, whether it's actually
    executable by our database engine, by our data lake.
  topic: technical
- impact_reason: A crucial warning about the fragility of generated code/queries;
    a minor syntax error renders the entire output useless, necessitating specific
    prompting/guardrails.
  relevance_score: 10
  source: llm_enhanced
  text: When you look at the DQL generation, it's a, it's a different thing. When
    it forgets one comma, right, it's not executable. So, you need to make, you may
    want to help the LLM there in particular to get the syntax right, and then also
    to get the semantics right.
  topic: technical
- impact_reason: Provides a clear, concise definition of RAG architecture, which is
    foundational to most modern enterprise LLM applications.
  relevance_score: 10
  source: llm_enhanced
  text: our Davis Co-Pilot is what you typically call this Retrieval Augmented Generation
    approach, where under the hood, you use an LLM plus knowledge sources, knowledge
    bases, right... That's this classical Retrieval Augmented Generation.
  topic: technical
- impact_reason: Highlights the meta-use of LLMs as evaluators ('LLM-as-a-Judge'),
    a major contemporary trend in model evaluation.
  relevance_score: 10
  source: llm_enhanced
  text: Then later on, as of now, then the LLMs came alive, right? They produce the
    text now, but they are now also used as judges to judge whether the generated
    text is actually that what you want as an output.
  topic: technical
- impact_reason: Connects the LLM-as-a-Judge technique directly to the critical problem
    of hallucination mitigation in RAG systems.
  relevance_score: 10
  source: llm_enhanced
  text: So, that's now a very common technique to use this, to use, let's say, an
    LLM which is more powerful than the LLM you're using to generate the output, as
    a judge to argue about, is it faithful? Is the answer, is the output relevant
    to the found documents? So, that's the question whether it's faithful to avoid
    hallucinations, right?
  topic: safety
- impact_reason: 'Raises the fundamental data quality and trust issue in RAG systems:
    even if the LLM summarizes perfectly, bad source data leads to bad answers. This
    is the core challenge of grounding.'
  relevance_score: 10
  source: llm_enhanced
  text: If you have like, what's the data model that's being trained that's using
    the train, right? Because if you were to open it up to everything available on
    the internet, the deep internet, right, there's going to be a lot of wrong information
    out there... But the data you pulled from is bad, right? And how do we gatekeep
    that?
  topic: safety
- impact_reason: Details the practical necessity of creating high-quality, human-curated
    test sets specifically for evaluating the retrieval component (the 'R' in RAG)
    to ensure grounding.
  relevance_score: 10
  source: llm_enhanced
  text: The first, you need to make sure that you retrieve the right data. And to
    ensure that this is working, we have dedicated, built dedicated tests which measure
    the retrieval quality. So, in these kinds of tests, we have test sets in place...
    Somebody needs to sit down and say, "Okay, for that particular answer, I would
    like to get these sources."
  topic: technical
- impact_reason: 'Provides a clear, powerful metaphor for how agentic AI functions:
    a generic reasoning core that rapidly masters new capabilities by consuming tool
    documentation.'
  relevance_score: 10
  source: llm_enhanced
  text: When he reads the manual of five tools, he immediately becomes an expert in
    using these tools. And that's now how you nowadays are going to build these kind
    of experts. And this is what this whole agentic AI is about, that you have a generic
    brain where you can attach, where you easily can attach any kind of tool, and
    this generic brain immediately reads the manual of each individual tool and a
    small manual how they are best used together, and then it immediately knows how
    to work with these tools.
  topic: technical
- impact_reason: Advocates for mandatory data source transparency (data lineage) for
    smaller, specialized models as a prerequisite for building user trust.
  relevance_score: 10
  source: llm_enhanced
  text: So having those smaller models, but also publishing what the data we're making
    public, what the data sources are, I think would be critical to this trust of,
    is my answer a reliable answer if I can see the sources that are being used...
  topic: safety/ethics
- impact_reason: 'Identifies the critical gap in continuous learning: the lack of
    robust, automated feedback loops to incorporate real-world implementation results
    (especially minor tweaks) back into the model''s knowledge base.'
  relevance_score: 10
  source: llm_enhanced
  text: With how does that feedback go back into the model so the model can learn
    from that, right? And how critical is that? Because it seems like a lot of what
    the AI is learning is based on what exists, but then as we implement things, implement
    things, it's not getting those updates of what these changes are.
  topic: technical
- impact_reason: 'A crucial warning for enterprise users regarding third-party APIs:
    failure to opt-out of logging means user interaction data (including proprietary
    fixes/failures) feeds directly back into the provider''s next training cycle.'
  relevance_score: 10
  source: llm_enhanced
  text: if you go to an OpenAI API, you don't get this enterprise SLAs, you cannot
    opt out from prompt logging. So they will log everything... then all this information
    goes back into their service, and they will leverage it for the next training.
  topic: safety/ethics
- impact_reason: Provides a historical analogy to contextualize the current ambiguity
    and nascent stage of AI-native development, suggesting we are still in the early
    definition phase.
  relevance_score: 10
  source: llm_enhanced
  text: defining what AI native is, is like back in 2014 if you would have defined
    what cloud native is.
  topic: strategy/predictions
- impact_reason: This highlights the common pitfall of simply porting old paradigms
    onto new infrastructure (repackaging) rather than truly embracing the new paradigm
    (cloud native). This is highly relevant to avoiding the same mistake with AI.
  relevance_score: 10
  source: llm_enhanced
  text: back in the days, there was a new technology, containers, orchestration, Kubernetes,
    but people were just repackaging their apps and putting them on Kubernetes, but
    this is not cloud native.
  topic: strategy/business
- impact_reason: A profound strategic insight suggesting that AI's true potential
    lies not in optimizing current processes but in fundamentally redefining them,
    moving beyond mere digital modeling.
  relevance_score: 10
  source: llm_enhanced
  text: I feel like if you're just trying to model AI based on the world as we know
    it right now to help us, then maybe we missed an opportunity to really redefine
    everything, because we don't need to just model the world we live in right now
    digitally, but maybe it is now an easier and a different way, right?
  topic: strategy/predictions
- impact_reason: 'This sets the central theme of the discussion: the critical need
    for quality assurance and trust in AI/LLM outputs, a major concern for enterprise
    adoption.'
  relevance_score: 9
  source: llm_enhanced
  text: I want to invite Thomas and learn about how we can make sure that the LLMs
    and whatever AI models we use, and also anybody uses out there, how we can make
    sure the results produce the right quality.
  topic: safety/strategy
- impact_reason: Highlights the pervasive issue of LLM hallucinations across various
    tools, underscoring the practical risk of unverified AI output.
  relevance_score: 9
  source: llm_enhanced
  text: We've seen plenty of cases where, I'm not saying in the tooling, but in general,
    if you take a look at CoPilot, Rock, some of these others, sometimes they just
    come out with these crazy, crazy either hallucination, hallucination, or other
    things.
  topic: safety/limitations
- impact_reason: 'Pinpoints the critical inflection point for modern deep learning:
    the repurposing of GPU hardware for computation.'
  relevance_score: 9
  source: llm_enhanced
  text: The major breakthrough was actually already earlier, in, let's say, in the
    late 2010s, when the first people started to think about reusing GPUs, not for
    games, but for neural networks.
  topic: technical/trends
- impact_reason: Identifies the Transformer architecture paper as the key catalyst
    for the current LLM revolution, providing a specific historical marker.
  relevance_score: 9
  source: llm_enhanced
  text: And then there was this, I think it was 2018, there was this one big paper
    which called "Attention Is All You Need," where researchers from OpenAI published
    the first Transformer model.
  topic: technical/breakthroughs
- impact_reason: Articulates the core user frustration stemming from the stochastic
    nature of LLMs, directly leading into the need for quality control.
  relevance_score: 9
  source: llm_enhanced
  text: I ask the question and I get, you know, an answer that makes sense. I ask
    the same question again, and then I get an answer that is completely not what
    I was expecting. And then I wonder, why do you give me two different answers for
    a similar question?
  topic: limitations/user experience
- impact_reason: Highlights the widespread adoption of LLMs across the observability
    industry to democratize data analysis, moving away from requiring deep domain
    expertise.
  relevance_score: 9
  source: llm_enhanced
  text: And now, kind of to how we are using LLMs, and with "we," I mean the observability
    space, right? If you look at observability, a lot of, I think every vendor now
    is basically saying, we have the data and we put our models on top, and then we
    get better insights and better answers to make sense out of the data, and you
    don't have to become an expert in analyzing all of this.
  topic: business
- impact_reason: Emphasizes that testing LLM applications requires system-level integration
    testing, not just unit testing the model itself.
  relevance_score: 9
  source: llm_enhanced
  text: And so, what you need to test is not only the LLM, it's the whole system,
    right? It's more like an integration test at the end.
  topic: technical
- impact_reason: Draws a sharp contrast between the maturity of evaluation metrics
    in traditional ML (numerical prediction) versus the rapid, ongoing development
    of metrics for generative text outputs.
  relevance_score: 9
  source: llm_enhanced
  text: 'What are numerical values? You have well-established KPIs for the quality
    of a joint model: root mean squared error, mean absolute error, error squared
    measurement, what not. While for this text output thing, those kind of measures
    had to be established over the last two years...'
  topic: technical
- impact_reason: Highlights the importance of prompt engineering and guardrails in
    controlling the generative behavior of the LLM *after* relevant data has been
    retrieved.
  relevance_score: 9
  source: llm_enhanced
  text: But then comes to the LLM. The LLM gets to see this, right, and the expected
    answer. And now we want to make sure that the LLM, how we build the system prompt,
    how we build the guardrails and things like that, that it actually reads that
    stuff and summarizes it in a way that we want.
  topic: technical
- impact_reason: 'Poses a strategic question about the future of AI in enterprise:
    specialization via training distinct models for different user personas (Developer,
    SRE, Tester) versus a single general model.'
  relevance_score: 9
  source: llm_enhanced
  text: Do you think we will end up, what does it make sense to train different types
    of models for these different reps of roles we also currently have in our day-to-day
    life to really, and I'm using the word again, digital twin, which I know obviously
    is not a new word, but is this what we are ending, is this what we're heading
    to, that we're cre[ating specialized agents/models for every persona]?
  topic: predictions
- impact_reason: Proposes a future architecture of specialized, smaller 'expert' LLMs
    tailored to specific organizational roles (like developer vs. SRE) within a domain,
    suggesting a move away from monolithic general models.
  relevance_score: 9
  source: llm_enhanced
  text: Do we end up, will we maybe end up in a world, thinking about now again our
    problem domain, observability? If I think about observability, we have many different
    personas who can benefit from observability to make better decisions, right? On
    the one side, it's the developer... Then we have the SRE team, we have the deployment
    teams. Do you think we will end up, what does it make sense to train different
    types of models for these different reps of roles we also currently have in our
    day-to-day life to really, and I'm using the word again, digital twin, which I
    know obviously is not a new word, but is this what we are ending, is this what
    we're heading to, that we're creating smaller language models or smaller expert
    digital experts on a certain problem domain, and then they can also talk with
    each other and argue, like we humans do?
  topic: predictions
- impact_reason: Describes the shift from task-specific fine-tuning to leveraging
    general-purpose, high-reasoning models that become experts by learning to use
    external tools (Tool Use/Agentic AI).
  relevance_score: 9
  source: llm_enhanced
  text: That's now another way to build those experts. You don't necessarily fine-tune
    a model on a particular task, but what is happening now with these models which
    are general-purpose built with high reasoning capabilities and planning what tools
    to use in which situation is that you are trying a generic expert.
  topic: technical
- impact_reason: 'A fundamental statement on data governance and trust in AI systems:
    control over the training/contextual data equates to control over the model''s
    output and perspective.'
  relevance_score: 9
  source: llm_enhanced
  text: if you have bad, whoever's in control of the sources of data controls what
    the output is, right?
  topic: safety/ethics
- impact_reason: 'Offers a philosophical view on LLM training: pre-training establishes
    general skills/interpretation frameworks, while facts are external resources accessed
    via tools/search.'
  relevance_score: 9
  source: llm_enhanced
  text: this pre-training is more like learning the general skills, but the factual
    knowledge you can Google always, right? You can ping it always, you will find
    it, and then you need just to interpret it.
  topic: technical
- impact_reason: Points out the lack of transparency from major foundational model
    providers regarding their specific alignment and fine-tuning methodologies, creating
    trust barriers.
  relevance_score: 9
  source: llm_enhanced
  text: I also think that there is the danger with all these very big players which
    don't you don't get the details, right? They don't provide their data lineage,
    they don't provide all these fine-tuned, all these very narrow details how they
    actually do the fine-tuning.
  topic: safety/ethics
- impact_reason: Frames the current state of AI architecture as nascent ('AI Native'),
    drawing an analogy to the early, often misunderstood, definition phase of Cloud
    Native.
  relevance_score: 9
  source: llm_enhanced
  text: he talked about the transformation from cloud native to AI native. And he
    said, where we are right now defining what AI native is, is like back in 2014
    if you would have defined what cloud native is.
  topic: strategy
- impact_reason: This clearly outlines the feedback loop mechanism for LLM improvement,
    distinguishing between long-term retraining and immediate in-context learning,
    which is crucial for product development.
  relevance_score: 9
  source: llm_enhanced
  text: the information, the feedback you're getting from making it, and the LLM making
    a failure, can enter the improvement chain at various levels, really down to the
    next retraining or the in-context learning as we do it now for our product.
  topic: technical/product development
- impact_reason: This introduces a key strategic concept framing the current era of
    technological evolution, similar to the shift to cloud computing.
  relevance_score: 9
  source: llm_enhanced
  text: the transformation from cloud native to AI native.
  topic: strategy/predictions
- impact_reason: This challenges fundamental roles in software development (Developer,
    SRE) based on the potential capabilities of advanced AI, hinting at massive structural
    shifts in the tech industry.
  relevance_score: 9
  source: llm_enhanced
  text: why do we still need a developer and an SRE performance test? So maybe we
    need something completely different thanks to AI.
  topic: predictions/industry impact
- impact_reason: Draws a direct, insightful parallel between early neuroscience modeling
    and modern deep neural networks, offering a conceptual framework for understanding
    LLMs.
  relevance_score: 8
  source: llm_enhanced
  text: My station at the university in Graz was about competition in neuroscience
    and how the brain works. We tried to figure out some, let's say, theories about
    that. And that as of today comes in kind of hand, if you understand how all these
    layers in the brain work and when you compare that to, let's say, all this compilation
    neural networks and deep neural networks and stuff like that.
  topic: technical/theory
- impact_reason: Credits the combination of foundational research (Transformer) and
    commercialization/accessibility (OpenAI API) for the current widespread adoption.
  relevance_score: 8
  source: llm_enhanced
  text: And then OpenAI just made business out of it, so baked it with big money and
    easy-to-use API, and that's where we are now.
  topic: business/strategy
- impact_reason: Details the practical application architecture within an enterprise
    setting, distinguishing between generative chat/explanation skills and structured
    code/query generation skills (like NL2SQL).
  relevance_score: 8
  source: llm_enhanced
  text: Basically we have, I would say, what we call skills in place. And one is this,
    are more like a traditional, traditional, you know, since two years, traditional
    chatbot based on a large language model. And the other one is this translation
    of natural text into our query language, right?
  topic: business/deployment
- impact_reason: 'Clearly defines the two primary use cases for LLMs in an enterprise
    context: knowledge retrieval/summarization and structured output generation (code/queries).'
  relevance_score: 8
  source: llm_enhanced
  text: The one is more, 'Get me a summary, get me an explanation,' in easy-to-read
    text, such that I can navigate, for example, our documentation, that they can
    navigate another text. And the other one is more like code generation, you know.
  topic: business/use cases
- impact_reason: Contrasts the qualitative (summarization) vs. quantitative/structured
    (code generation) outputs, setting up the subsequent discussion on testing methodologies.
  relevance_score: 8
  source: llm_enhanced
  text: The one is more, "Get me a summary, get me an explanation," in easy-to-read
    text... And the other one is more like code generation, you know, you know all
    these code generation tools. And in our case, it's generation of our own invented
    query language.
  topic: technical
- impact_reason: Uses the classic Watson example to illustrate the need for probabilistic,
    multi-option outputs in high-stakes domains, rather than a single definitive answer,
    to build user trust.
  relevance_score: 8
  source: llm_enhanced
  text: And Watson, I remember when they were first talking about Watson back when
    it was on Jeopardy, the idea was to use it in a hospital scenario... 90% it's
    this, but these are some other conditions, right? So the doctor at least has a
    list to say, okay, let me check, let me go in order of what is most likely, but
    maybe it is going to be this thing way down here. And if we're only ever writing
    or returning an answer based on the top one and not considering the others, like
    how do you get that trust?
  topic: safety
- impact_reason: Separates the linguistic skill (summarization) from the knowledge
    retrieval skill, suggesting that for pure summarization of provided text, the
    LLM acts primarily as a highly skilled editor/linguist.
  relevance_score: 8
  source: llm_enhanced
  text: If you think about this classical summarization task, for that task, the LLM
    actually would not need factual knowledge, right? It would just need the capability
    of, I don't know, of a linguistically very well-trained person which is perfectly
    trained in doing excerpt and text summarization.
  topic: technical
- impact_reason: Highlights a distinction between specialized tasks (like summarization)
    and the broader reasoning/planning capabilities currently dominating generative
    AI frameworks, suggesting potential specialization paths for models.
  relevance_score: 8
  source: llm_enhanced
  text: It needs to be able to do this summarization task. And to some extent, that's
    a different task than what is needed nowadays in all these generative frameworks
    when it comes to reasoning and planning.
  topic: technical
- impact_reason: 'Suggests an efficiency principle in agent design: the central controller
    (the ''brain'') doesn''t always need to be a massive LLM; a smaller, specialized
    model might suffice for controlling a specific toolset.'
  relevance_score: 8
  source: llm_enhanced
  text: for exactly for narrowly focused agents, that the brain which controls the
    tools must not be the biggest. It's not necessary that this is the biggest brain.
    It can also be a smaller brain which is fine-tuned how to use this set of constraints
    tools, right?
  topic: technical
- impact_reason: 'Raises a critical organizational risk associated with specialized
    AI agents: recreating human silos in the AI layer, potentially hindering cross-functional
    communication.'
  relevance_score: 8
  source: llm_enhanced
  text: First of all, Andy, I think one of the dangers of the idea that you brought
    in would be, do we suddenly go back to siloed AI, which is what we've been trying
    to get away from within organizations with people, right? People who are completely
    siloed in their area of expertise, and then there's not the crosstalk.
  topic: safety/strategy
- impact_reason: 'Highlights the ongoing architectural debate: will prompt engineering/in-context
    learning remain sufficient, or will continuous, full retraining become the norm
    for factual updates?'
  relevance_score: 8
  source: llm_enhanced
  text: it's not yet decided whether this in-context learning, so how you prompt LLMs,
    or in the long term, the full, let's say, always need to train, so the need to
    train from scratch thing.
  topic: technical
- impact_reason: Expresses a desire for customizable, downloadable models where users
    can explicitly define and filter out unwanted biases or knowledge domains during
    initialization.
  relevance_score: 8
  source: llm_enhanced
  text: I would agree that it would be very cool to have an approach where you actually
    can download a model based on what background knowledge you like to have in that
    model, right? You say, 'Okay, I would like no bias in this direction. I don't
    like a background in astrology, just saying something. Please keep that out of
    my brain, right?'
  topic: safety/ethics
- impact_reason: Provides a concrete example of immediate, product-level feedback
    implementation using human input to correct errors via in-context learning, bypassing
    provider retraining.
  relevance_score: 8
  source: llm_enhanced
  text: On the small scale, for example, on our generation of our DQL queries from
    natural language, you have the feedback button in the product... And then actually,
    we look into the data, into the generated DQL, and take the mistakes... And then
    we place these improvements in our, so to say, in-context learning.
  topic: business/technical
- impact_reason: 'Actionable advice for users of AI tools: active feedback is necessary
    for improvement, especially when relying on local/in-context learning mechanisms.'
  relevance_score: 8
  source: llm_enhanced
  text: The lesson there is, everyone give feedback if it doesn't work, because if
    you're not giving that feedback, you're not going to get it to improve.
  topic: business
- impact_reason: A direct, actionable piece of advice for users and developers regarding
    the necessity of user feedback for iterative AI improvement.
  relevance_score: 8
  source: llm_enhanced
  text: everyone give feedback if it doesn't work, because if you're not giving that
    feedback, you're not going to get it to improve.
  topic: business/product development
- impact_reason: Provides historical context and establishes the deep expertise of
    the guest, contrasting decades of foundational work with the recent public explosion
    of AI.
  relevance_score: 7
  source: llm_enhanced
  text: I have written my first neural network in C, something like three decades
    ago.
  topic: technical/history
- impact_reason: Illustrates the historical limitations of early NLP evaluation metrics
    (like n-gram overlap) which failed to capture semantic meaning or word order correctly.
  relevance_score: 7
  source: llm_enhanced
  text: Then count how many tokens are equal, how large is the overlap of tokens,
    is the order somehow matching? So, the talk, I don't know if the output is by
    me, some butter, for example, or whatnot, and the word "butter" in the expected
    output is in the first place, while in the expected answer it's in the last place.
    Even the same word is there, it's not the perfect match, right?
  topic: technical
- impact_reason: Reinforces the idea that careful prompting can isolate and test specific
    capabilities of the LLM (like linguistic quality) separately from its knowledge
    base.
  relevance_score: 7
  source: llm_enhanced
  text: And if you prompt the judge properly, you can kind of turn it into this kind
    of linguistic person.
  topic: technical
- impact_reason: Draws a direct parallel between designing modular AI systems (agents/experts)
    and classical software engineering principles (system/module boundaries), suggesting
    established architectural wisdom applies.
  relevance_score: 7
  source: llm_enhanced
  text: This is stuff that's, I mean, in general, this question where to cut systems
    and put trust boundaries is like in the same thing in software engineering all
    over again. How do you, where do you put the system boundaries, where do you put
    your module boundaries, your package boundaries, how do you structure, how do
    you structure the overall system, right?
  topic: strategy
- impact_reason: Summarizes the multi-stage alignment process of modern LLMs (pre-training,
    RLHF, assessment training), emphasizing that alignment dictates reaction patterns.
  relevance_score: 7
  source: llm_enhanced
  text: And so, the lineage and how the training actually takes place, and in which
    direction the large language model has been aligned to react to, right? These
    are these major training steps in all these large language models where you have
    first the language training, then you have this reinforcement learning with human
    in the loop, then you have the next goal, assessment training, so to say.
  topic: technical
- impact_reason: A cautionary closing thought suggesting that modeling AI purely on
    existing paradigms (the world as we know it) might miss the true potential or
    nature of 'AI Native' systems.
  relevance_score: 7
  source: llm_enhanced
  text: if you're just trying to model AI based on the world as we know it right n
  topic: strategy
- impact_reason: Provides an interesting historical footnote regarding the deep roots
    of optimization techniques like automated gradient descent, emphasizing that modern
    frameworks build on decades-old concepts.
  relevance_score: 6
  source: llm_enhanced
  text: And he's always trying to find out what what the earliest citation of some
    new invention and something like that, right? And in his opinion, all these kind
    of TensorFlow, PyTorch neural network machinery which does all this learning and
    stochastic gradient descent, the real credit goes back to a student in Finland
    in 1970 or something like that, who first in time, according to his opinion, invented
    first time automated gradient descent.
  topic: history/technical
source: Unknown Source
summary: '## Podcast Summary: How to Test, Optimize, and Reduce Hallucinations of
  AIs with Thomas Natschlaeger


  This 49-minute episode of "Pure Performance" features Thomas Natschlaeger, Principal
  Data Scientist at Dynatrace, discussing the critical challenges of ensuring quality,
  reliability, and trustworthiness in Large Language Models (LLMs), particularly within
  enterprise applications like Dynatraceâs internal managerial co-pilot. The discussion
  bridges the long history of AI research with the modern imperative of rigorous testing
  for generative models.


  ---


  ### 1. Focus Area

  The primary focus is on **AI/ML Quality Assurance and Reliability Engineering**,
  specifically addressing the testing methodologies, hallucination mitigation, and
  system integration challenges associated with deploying LLMs in production environments.
  Key applications discussed include building managerial co-pilots and translating
  natural language into proprietary query languages (DQL).


  ### 2. Key Technical Insights

  *   **RAG Architecture Testing:** Testing LLM applications requires treating the
  entire systemâincluding the **Retrieval Augmented Generation (RAG)** pipeline (retrieval
  mechanism + LLM)âas an integrated unit, not just testing the LLM in isolation.

  *   **Dual Testing Requirements:** Testing must be bifurcated: one approach for
  **free-text/summary outputs** (checking for linguistic quality and relevance) and
  a stricter approach for **structured outputs** (like DQL generation), which demands
  perfect syntactic and semantic correctness for executability.

  *   **LLMs as Judges:** A common and effective technique for measuring output quality
  (faithfulness, relevance) is using a more powerful, external LLM as a "judge" to
  evaluate the output of the primary generative model, moving beyond older statistical
  metrics like n-gram overlap.


  ### 3. Business/Investment Angle

  *   **Trust as the New KPI:** As AI moves into core business functions (like observability
  analysis), the "trust factor"âensuring AI outputs are true and reliableâbecomes
  paramount, mirroring the need for trust in traditional monitoring data.

  *   **Enterprise Adoption Requires Rigor:** The shift from experimental LLMs to
  reliable enterprise tools necessitates establishing well-defined Key Performance
  Indicators (KPIs) for text generation, similar to the established RMSE/MAE metrics
  used in traditional numerical ML forecasting.

  *   **Data Gating is Crucial:** A major business risk is the quality of the underlying
  knowledge base. Ensuring the retrieved data sources are accurate and relevant is
  the first line of defense against propagating misinformation or "mass hallucinations"
  from the training data.


  ### 4. Notable Companies/People

  *   **Thomas Natschlaeger (Dynatrace):** Guest expert, Principal Data Scientist,
  with a deep background in neural networks dating back to the 1990s and experience
  in weather prediction ML.

  *   **Dynatrace:** The company where Natschlaeger is building internal AI tools,
  specifically a managerial co-pilot that translates natural language into their proprietary
  DQL.

  *   **JÃ¼rgen Schmidhuber:** Mentioned in an anecdote regarding the historical credit
  for AI innovations, particularly automated gradient descent.

  *   **OpenAI:** Referenced as the entity that commercialized the Transformer model,
  leading to the current LLM boom.


  ### 5. Future Implications

  The industry is moving toward a future where AI quality assurance is formalized
  through dedicated testing frameworks that account for the unique non-deterministic
  nature of generative models. The focus will remain on **grounding** LLMs in proprietary,
  verified knowledge bases (RAG) and using sophisticated LLM-based evaluation systems
  to maintain high standards of factual accuracy and relevance, especially as AI is
  integrated across diverse enterprise personas (developers, testers, managers).


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Software Quality Assurance
  Professionals, Data Scientists, and Technology Leaders** involved in building, deploying,
  or integrating LLM-powered features into enterprise software, particularly those
  concerned with model drift, hallucination, and production reliability.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- nvidia
- openai
- google
title: How to test, optimize, and reduce hallucinations of AIs with Thomas Natschlaeger
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 132
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 11
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 2
  prominence: 0.2
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-13 08:11:33 UTC -->
