---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yoshua Bengio. With nearly
    a million annual listeners, AI and B
  name: Yoshua Bengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit emerj.com
    and fill out our Thought Leaders
  name: AI ROI
  position: 1864
- category: unknown
  confidence: medium
  context: eedle on AI ROI, visit emerj.com and fill out our Thought Leaders submission
    form. That's emerj.com and click on be
  name: Thought Leaders
  position: 1905
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of NotebookLM. As an author, I'v
  name: Steven Johnson
  position: 2155
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2553
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5295
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 17985
- category: unknown
  confidence: medium
  context: generative AI coming down the horizon right now. Generative AI seems to
    have just found its place, but it's also
  name: Generative AI
  position: 23945
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24105
- category: ai_research
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    indicating enterprise adoption of AI.
  name: Goldman Sachs
  source: llm_enhanced
- category: enterprise_user
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, indicating enterprise adoption of AI.
  name: Raytheon
  source: llm_enhanced
- category: ai_researcher
  confidence: high
  context: Referenced as an AI pioneer featured on the podcast.
  name: Yoshua Bengio
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool for organizing ideas and making sense of complex information,
    co-founded by Steven Johnson.
  name: NotebookLM
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the proposer of the transformer architecture, which underpins
    modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: NotebookLM is a Google product, implied by the URL notebooklm.google.com.
  name: NotebookLM (Google)
  source: llm_enhanced
- category: consulting/research
  confidence: high
  context: Mentioned for conducting a recent survey on generative AI investment in
    the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a reference point for the explosion of LLMs, contrasting with
    earlier models like BERT.
  name: ChatGPT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an earlier language model architecture used for information
    extraction before the GPT-like models became prevalent.
  name: BERT
  source: llm_enhanced
- category: ai_application_concept
  confidence: high
  context: Mentioned as an example of a model leveraging the decoding part of the
    transformer architecture for sequence generation (implying generative AI models).
  name: GPT
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 21:33:59 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 100
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs, foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 71
  relevance_score: 3
  text: So we can use the data you have to train, to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: 'This is the core technical distinction between traditional ML and
    the foundational model approach: leveraging massive, broad pre-training data rather
    than solely relying on small, task-specific labeled datasets.'
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs, foundation models. So
    the idea is that yes, you still care about the specific data you are working on,
    but at the same time, we want to collect all the available data, relevant data,
    or irrelevant data, but as long as they fall into the same domain, and feed those
    data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical
- impact_reason: 'Explains the primary value proposition of foundation models: knowledge
    transfer and utility even with limited domain-specific data (few-shot/zero-shot
    learning potential).'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models.
  topic: technical
- impact_reason: 'A powerful philosophical framing: treating genetics as a language
    that LLMs can process, opening doors for simulation and reducing heavy experimental
    work.'
  relevance_score: 10
  source: llm_enhanced
  text: Genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process...
  topic: predictions/industry impact
- impact_reason: Provides a clear, concise technical distinction between BERT (encoder-only,
    good for representation/embeddings) and generative models.
  relevance_score: 10
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they use the decoder
    part of the transformer. So you can think it's as a very good representation model.
  topic: technical
- impact_reason: Explains the key architectural shift (leveraging the decoder) that
    enables generative capabilities (like GPT) compared to earlier models like BERT.
  relevance_score: 10
  source: llm_enhanced
  text: but now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: Details the power of constrained generative modeling in chemistry/biologyâ€”generating
    novel entities while optimizing for multiple, complex, domain-specific properties
    simultaneously.
  relevance_score: 10
  source: llm_enhanced
  text: And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/business
- impact_reason: 'Crucial insight into the current bottleneck in biotech AI: AI concepts
    are ahead of the experimental capacity to generate the necessary high-fidelity,
    high-throughput data.'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
    So, there's, you know, where AI, the architecture, the concept there, but the,
    sometimes we are limited by the, you know, experimental side.
  topic: limitations/strategy
- impact_reason: Directly addresses the high-stakes nature of hallucination in scientific
    domains, where validation requires expensive, time-consuming physical experiments,
    unlike text or code generation.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: Highlights the significant gap between investment interest and successful,
    scaled implementation of GenAI in life sciences, a critical business challenge.
  relevance_score: 9
  source: llm_enhanced
  text: 73% of life sciences leaders are investing in generative AI initiatives, fewer
    than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: Provides a concise historical overview of the evolution of AI/ML,
    clearly marking the shift to foundational models as the current paradigm.
  relevance_score: 9
  source: llm_enhanced
  text: We have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks. And more recently, we have a paradigm shift into those language models,
    foundation models.
  topic: technical
- impact_reason: Provides a concrete, high-value example of foundation models benefiting
    life sciences R&D by leveraging public biological data (like cell atlases) for
    specific indication studies.
  relevance_score: 9
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data. So for example, cell atlases, etc.
  topic: predictions/industry impact
- impact_reason: Offers a clear, accessible definition of fine-tuning in the context
    of foundation models, distinguishing it from full retraining.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right? So we can use the data you have to train, to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems...
  topic: technical
- impact_reason: Connects foundational models directly to the concept of 'digital
    twins' in personalized medicine, suggesting models can simulate patient responses
    based on genetic code/language.
  relevance_score: 9
  source: llm_enhanced
  text: The folks, the professionals running the clinical trials can run that foundational
    model based on what they're seeing with this specific patient and that kind of
    acts like the digital twin.
  topic: predictions/industry impact
- impact_reason: 'Highlights a specific, advanced application: using GenAI to synthesize
    biological data (cell types, gene expressions) for in-silico testing.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: 'Highlights the primary value proposition of using foundation models
    in drug discovery: accelerating the identification of novel biological targets.'
  relevance_score: 9
  source: llm_enhanced
  text: So that way can allow us to better focus on discovering, you know, novel targets,
    novel gene pathways, etc.
  topic: predictions/business
- impact_reason: Confirms the foundational role of the Transformer architecture across
    diverse applications, including specialized domains like drug discovery.
  relevance_score: 9
  source: llm_enhanced
  text: essentially they have the same, I mean, by nature, they have the same setup.
    So that is based on the breakthrough of the transformer architecture that Google
    proposed a long time ago.
  topic: technical
- impact_reason: Illustrates the concept of applying large-scale pre-training (like
    LLMs) to non-textual data (molecules represented as SMILES or graphs) to learn
    fundamental 'signatures'.
  relevance_score: 9
  source: llm_enhanced
  text: So all those information publicly available data are from different databases.
    They could all fit into those kind of transformer architecture to let you learn
    the hidden embedding signatures of those molecules, atoms, etc.
  topic: technical/predictions
- impact_reason: Emphasizes the indispensable role of domain experts (SMEs) in data
    curation, model guidance, and, crucially, benchmarking, countering the idea that
    models can be built purely in isolation.
  relevance_score: 9
  source: llm_enhanced
  text: But, you know, we still need the domain knowledge people, you know, people
    have those kind of expertise to guide the, you know, the selection of the data
    and also more importantly, then these models are, you build, now how do we benchmark
    with them?
  topic: strategy/safety
- impact_reason: 'Proposes a necessary paradigm shift in evaluation: moving beyond
    generic metrics to hybrid metrics that incorporate deep domain expertise to score
    model outputs.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is key here,
    because there could be so many versions of found data models.
  topic: safety/strategy
- impact_reason: Identifies organizational alignment and communication across diverse
    stakeholders (scientists, modelers, leadership) as a critical barrier to effective
    AI adoption.
  relevance_score: 9
  source: llm_enhanced
  text: So, there's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: 'Describes a common enterprise architecture pattern: using a large
    central model/data repository to feed specialized, smaller models for specific
    organizational tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: We're also describing the relationship as you'll have LLMs, a foundational
    model, and then break off into smaller models for smaller tasks, but this larger
    data repository is going to be the method of driving efficiencies across the organization
    in ways that only you'll be able to do as an example with coding automation through
    digital technology.
  topic: strategy
- impact_reason: Reiterates that the underlying technical commonality across diverse
    modern AI models (text, image, molecular) is the Transformer architecture.
  relevance_score: 8
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: 'A core strategic takeaway for implementing AI in specialized fields:
    success requires mandatory, deep collaboration between AI experts and domain scientists.'
  relevance_score: 8
  source: llm_enhanced
  text: So, yeah, so it's basically a cross-discipline collaboration, whether in house
    or externally, yeah.
  topic: strategy
- impact_reason: 'Provides a strategic observation on why NLP/genetics benefited early
    from GenAI: the inherent ''language-like'' structure of the data (sequences, text).'
  relevance_score: 8
  source: llm_enhanced
  text: advantage that natural language processing as a domain became one of the first
    gigantic generative AI use cases, right? That builds in certain advantage to genetics,
    which is more language-based than anything else versus, you know, other domains
    of health and life sciences, the services involved.
  topic: strategy
- impact_reason: 'Reiterates the persistent challenge: even foundation models require
    high-quality, domain-specific data for specialized tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: business/technical
- impact_reason: 'Actionable advice: Due to inherent model uncertainty, rigorous,
    domain-specific benchmarking is mandatory for model selection and deployment.'
  relevance_score: 8
  source: llm_enhanced
  text: So, it's quite important that we have to do those benchmarking and select
    models accordingly.
  topic: strategy
- impact_reason: 'Highlights the ''digestibility'' problem: technical teams must translate
    model capabilities into terms that scientists can effectively use and validate.'
  relevance_score: 8
  source: llm_enhanced
  text: how do we digest the models, the capabilities? And again, we have to plug
    in, you know, the right scientists to help us together validate those.
  topic: strategy
- impact_reason: 'Strategic advice for SMEs: they must understand the evolving AI
    landscape (new vs. old paradigms) rather than expecting a single, monolithic AI
    solution to replace all existing methods.'
  relevance_score: 8
  source: llm_enhanced
  text: It behooves them to kind of have some awareness of the new and old paradigm
    that we were talking about before, how both are still present, but how we thought
    that AI would impact life sciences organization or that they'd just be this one
    school way of doing things is not the case.
  topic: strategy
- impact_reason: Raises a critical business/strategy question regarding the build
    vs. buy decision for foundational models in specialized verticals like healthcare.
  relevance_score: 7
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
source: Unknown Source
summary: '## Comprehensive Summary: Building AI-Ready Cultures in Life Sciences R&D


  This 30-minute podcast episode, featuring **Jung Liu, Director of Data Science in
  AI at Novartis**, focuses on the paradigm shift in Life Sciences R&D driven by **Generative
  AI (GenAI)** and **Foundation Models (FMs)**, and the cultural and technical challenges
  of scaling these technologies beyond pilot projects.


  ### 1. Focus Area

  The discussion centers on the transition from traditional, task-specific Machine
  Learning (ML) to leveraging large-scale Foundation Models in drug discovery, clinical
  workflows, and protocol design. Key themes include the architectural differences
  between older models (like BERT) and generative models (like GPT), the role of massive,
  domain-specific data repositories, and the necessity of building "AI-ready cultures"
  to bridge the gap between AI practitioners and domain scientists.


  ### 2. Key Technical Insights

  *   **Paradigm Shift to Foundation Models:** The core technical evolution is moving
  from collecting, labeling, and training models on small, specific datasets (traditional
  ML) to utilizing massive, pre-trained FMs (based on Transformer architecture) that
  capture generalized domain knowledge. This allows for effective downstream tasks
  even with limited proprietary data via **fine-tuning**.

  *   **Generative Capabilities (GPT vs. BERT):** The shift from models leveraging
  the decoder part of the Transformer (like BERT, which excels at generating text
  embeddings/features) to models leveraging the encoder part (like GPT, which can
  generate new data sequences, such as novel molecules represented by SMILES strings
  or graphs) unlocks powerful capabilities in *de novo* design.

  *   **Domain-Specific FM Development:** While public FMs exist, the industry is
  moving toward building domain-specific FMs by feeding them vast amounts of chemical,
  clinical trial, and biological data (e.g., cell atlases) to enable specialized tasks
  like target identification and molecule generation under specific constraints (e.g.,
  toxicity, solubility).


  ### 3. Business/Investment Angle

  *   **Scaling Challenge:** Despite high investment (73% of leaders investing), scaling
  GenAI beyond pilots remains difficult (fewer than 20% successful), highlighting
  organizational and workflow hurdles over pure technological capability.

  *   **Data Dependency vs. Model Sophistication:** While model architecture is advancing
  rapidly (thanks to public research), the bottleneck often lies in the availability
  and quality of high-throughput, experimental data (like single-cell RNA-seq) needed
  to fully train or fine-tune these models for specific biological realities.

  *   **ROI and Evaluation:** Measuring the Return on Investment (ROI) for AI-driven
  research requires moving beyond simple technical metrics. Developing **domain-driven
  evaluation standards and knowledge-checking benchmarks** is crucial to validate
  outputs (like generated molecules) that cannot be instantly verified.


  ### 4. Notable Companies/People

  *   **Jung Liu (Novartis):** The expert guest, providing insights from a major pharmaceutical
  company on practical implementation and cultural alignment.

  *   **Google (Transformer Architecture):** Mentioned as the source of the foundational
  architectural breakthrough enabling modern FMs.

  *   **Deloitte:** Cited for a recent survey highlighting the gap between GenAI investment
  and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is heading toward a hybrid development model, leveraging publicly available
  FMs while building proprietary, domain-specific versions tailored to internal data
  and specific R&D needs. The success of this integration hinges on **cross-discipline
  collaboration** between AI professionals, domain experts (biologists, chemists),
  and leadership. Furthermore, the inherent language-like structure of genetics gives
  life sciences a unique advantage in adopting NLP-derived FM technology, potentially
  accelerating areas like digital twin creation and personalized medicine.


  ### 6. Target Audience

  This conversation is most valuable for **AI/Tech Leaders** and **Life Sciences R&D
  Executives** involved in strategy, investment, and deployment decisions. It is highly
  relevant for professionals navigating the practical challenges of integrating advanced
  ML/GenAI into established scientific workflows.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 121
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 16
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 21:33:59 UTC -->
