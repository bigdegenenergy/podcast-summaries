---
companies:
- category: unknown
  confidence: medium
  context: how to have this discussion in a way that for the United States interests
    balances these two things. And if we're
  name: United States
  position: 115
- category: unknown
  confidence: medium
  context: and the future of open source? I'm joined by a16z Journal Partners Martín
    Casado and Anjane Midha to unpack the new
  name: Journal Partners
  position: 771
- category: unknown
  confidence: medium
  context: joined by a16z Journal Partners Martín Casado and Anjane Midha to unpack
    the new AI action plan, the politics be
  name: Anjane Midha
  position: 806
- category: unknown
  confidence: medium
  context: opposition, and then nobody was going against it. So I think today we should
    definitely talk about the a
  name: So I
  position: 2540
- category: unknown
  confidence: medium
  context: his petition? Oh, yeah. I think that was the last AI Action Summit, right?
    I wanted to prepare it to everybody here.
  name: AI Action Summit
  position: 2907
- category: unknown
  confidence: medium
  context: that's right. I don't remember. Remember what was Dan Hendrycks' CAS? What
    was the Center for AI Safety? Center f
  name: Dan Hendrycks
  position: 3067
- category: unknown
  confidence: medium
  context: t was Dan Hendrycks' CAS? What was the Center for AI Safety? Center for
    AI Safety, that's it. That's it. The
  name: AI Safety
  position: 3111
- category: unknown
  confidence: medium
  context: had examples of the stuff being dangerous, right? Like Robert Morris went
    out, the Morris Worm, it took down critical
  name: Like Robert Morris
  position: 3566
- category: unknown
  confidence: medium
  context: angerous, right? Like Robert Morris went out, the Morris Worm, it took
    down critical infrastructure. We had new
  name: Morris Worm
  position: 3599
- category: tech
  confidence: high
  context: tead of mutually assured destruction, we have the notion of asymmetry.
    So there are all these great exampl
  name: Notion
  position: 3932
- category: unknown
  confidence: medium
  context: ebody sent you and me a link to the SB 1047 bill. And I remember Martin
    and I reacting, "There's no way t
  name: And I
  position: 4732
- category: unknown
  confidence: medium
  context: ng in tech—talking against open source. You know, Founders Fund, they're
    like, "Open source AI is dangerous. It g
  name: Founders Fund
  position: 6211
- category: unknown
  confidence: medium
  context: not dual-use. Nuclear energy is dual-use, right? An F-16 is not dual-use.
    A jet engine is dual-use, but
  name: An F
  position: 7691
- category: tech
  confidence: high
  context: t know, really. The difference between a model in Google is almost nothing."
    But, you know, that was used
  name: Google
  position: 9513
- category: unknown
  confidence: medium
  context: t of Congress and literally in a testimony said, "The U.S. is years ahead
    of China." And so since these a
  name: The U
  position: 9941
- category: unknown
  confidence: medium
  context: t identified at the time. I mean, you would go to Dawn Song, who is like
    a safety researcher, McCarthy Fellow
  name: Dawn Song
  position: 10450
- category: unknown
  confidence: medium
  context: go to Dawn Song, who is like a safety researcher, McCarthy Fellow at Berkeley,
    and you'd say, "What are the margina
  name: McCarthy Fellow
  position: 10494
- category: unknown
  confidence: medium
  context: t to publish a fantastic set of papers, including DeepSeek Math V2, which
    came out last summer. And you're like,
  name: DeepSeek Math
  position: 11039
- category: unknown
  confidence: medium
  context: They're not years behind." And so when their R1, DeepSeek Car 1, came out
    earlier this year, a lot of Washingto
  name: DeepSeek Car
  position: 11214
- category: unknown
  confidence: medium
  context: with compute. Remember when we were like, "Okay, Saddam Hussein shouldn't
    have PlayStations because you can use G
  name: Saddam Hussein
  position: 12398
- category: unknown
  confidence: medium
  context: Actually, I don't know which version it was. Yes. But I remember we actually
    looked it up. The legal defi
  name: But I
  position: 15237
- category: unknown
  confidence: medium
  context: our best talent is considered, "I could be sued." Like I'm a random kid
    in Arkansas developing something.
  name: Like I
  position: 16313
- category: unknown
  confidence: medium
  context: ogy realized they would do that. In fact, I think Jack Clark, who runs
    policy for Anthropic, literally tweeted
  name: Jack Clark
  position: 19715
- category: tech
  confidence: high
  context: In fact, I think Jack Clark, who runs policy for Anthropic, literally tweeted
    toward the end of the SB 1047
  name: Anthropic
  position: 19747
- category: unknown
  confidence: medium
  context: h ecosystem anymore. We have different interests. And D.C. had an update
    at that. And I think what's amaz
  name: And D
  position: 23524
- category: tech
  confidence: high
  context: rom a business strategy perspective? Maybe we saw Meta maybe the first
    big open source push. OpenAI has
  name: Meta
  position: 24001
- category: tech
  confidence: high
  context: we saw Meta maybe the first big open source push. OpenAI has evolved their
    tune. I've seen even the topic
  name: Openai
  position: 24044
- category: unknown
  confidence: medium
  context: ', faster, more control, they need somebody like a Red Hat to then introduce
    them and provide solutions and'
  name: Red Hat
  position: 24729
- category: unknown
  confidence: medium
  context: n models and American post-training pipelines and American RL techniques,
    then that ecosystem win is orders of
  name: American RL
  position: 30578
- category: unknown
  confidence: medium
  context: e more sophisticated than policy experts, even in Silicon Valley, would
    recommend because building an AI evaluatio
  name: Silicon Valley
  position: 36230
- category: investment_firm
  confidence: high
  context: Venture capital firm (Andreessen Horowitz) hosting the podcast and having
    investments in the companies discussed. They are actively involved in the discourse
    around AI innovation and policy.
  name: a16z
  source: llm_enhanced
- category: personnel_in_ai_ecosystem
  confidence: high
  context: Partner at a16z Journal Partners, participating in the discussion on AI
    action plans and policy.
  name: Martín Casado
  source: llm_enhanced
- category: personnel_in_ai_ecosystem
  confidence: high
  context: Partner at a16z Journal Partners, participating in the discussion on AI
    action plans and policy.
  name: Anjane Midha
  source: llm_enhanced
- category: ai_advocacy_nonprofit
  confidence: high
  context: A nonprofit organization, led by Dan Hendrycks, that was prominent in advocating
    for pausing AI development due to existential risk concerns.
  name: Center for AI Safety (CAS)
  source: llm_enhanced
- category: personnel_in_ai_ecosystem
  confidence: high
  context: Associated with the Center for AI Safety (CAS), mentioned as being involved
    in the petition to pause AI.
  name: Dan Hendrycks
  source: llm_enhanced
- category: investment_firm
  confidence: high
  context: Venture capital firm mentioned for having taken a stance against open source
    AI, arguing it gives China an advantage.
  name: Founders Fund
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as a company publishing fantastic papers (including DeepSeek
    Math V2 and DeepSeek Car 1), indicating they are close to the AI frontier, likely
    Chinese.
  name: DeepSeek
  source: llm_enhanced
- category: ai_researcher
  confidence: high
  context: Mentioned as a safety researcher and McCarthy Fellow at Berkeley, consulted
    regarding marginal risks of AI.
  name: Dawn Song
  source: llm_enhanced
- category: research_institution
  confidence: medium
  context: University where Dawn Song is a McCarthy Fellow, implying AI research activity.
  name: Berkeley
  source: llm_enhanced
- category: technology_company
  confidence: medium
  context: Mentioned in the context of national government restrictions (alongside
    Cisco) related to sensitive technology, drawing a parallel to current AI export
    concerns.
  name: Huawei
  source: llm_enhanced
- category: technology_company
  confidence: medium
  context: Mentioned in the context of national government restrictions (alongside
    Huawei) related to sensitive technology.
  name: Cisco
  source: llm_enhanced
- category: personnel_in_ai_ecosystem
  confidence: low
  context: Mentioned in the context of tweeting about open source, likely referring
    to Vinod Khosla or another prominent figure in the tech/VC space.
  name: Vinod
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned in relation to Jack Clark, who runs policy there, and the SB
    1047 saga.
  name: Anthropic
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as having evolved its tune regarding open source.
  name: OpenAI
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as perhaps having the first big open source push in the AI space.
  name: Meta
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Referenced as an example of previous computing infrastructure where open
    source eventually thrived in the infrastructure layer.
  name: Linux
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced as an example of a company that provides solutions and services
    around open source infrastructure (like Linux) for enterprises.
  name: Red Hat
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Refers to the hardware infrastructure underpinning the US AI ecosystem
    (strongly implying NVIDIA or similar chip manufacturers).
  name: American chips
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Refers to foundational AI models developed in the US, likely referencing
    major players like OpenAI, Google, or Meta.
  name: American models
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Refers to the techniques and infrastructure used for fine-tuning and deploying
    models.
  name: American post-training pipelines
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Refers to Reinforcement Learning techniques, a core area of AI research
    and application.
  name: American RL techniques
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Discussed as a mainstay of innovation in computer science that is currently
    under-invested in by the discussed policy action plan.
  name: Academia
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Synonym for academia, mentioned in the context of a standoff with the administration
    regarding tech initiatives.
  name: Higher ed
  source: llm_enhanced
- category: organization
  confidence: low
  context: Refers to Washington D.C., the location where the speakers need to help
    implement the policy details, implying engagement with government AI initiatives.
  name: D.C.
  source: llm_enhanced
- category: organization
  confidence: low
  context: Mentioned as the location where policy experts exist, often contrasted
    with the sophistication of government policy documents regarding AI evaluation.
  name: Silicon Valley
  source: llm_enhanced
- category: technology_analogy
  confidence: low
  context: Mentioned as an example of a complex general-purpose technology whose underlying
    mechanism was not fully understood when deployed, drawing an analogy to AI models.
  name: Nuclear fusion
  source: llm_enhanced
date: 2025-10-04 14:41:45 +0000
duration: 42
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: just regulate, we should pause it
  text: we should just regulate, we should pause it.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: definitely talk about the action plan—it's great
  text: we should definitely talk about the action plan—it's great.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: also talk about how the entire industry has kind of come around to say,
    "We need to keep these things in check
  text: We should also talk about how the entire industry has kind of come around
    to say, "We need to keep these things in check.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: research
  text: We should research.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: probably learn from that experience
  text: we should probably learn from that experience.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: have realized the impact of how far this could have gone
  text: we should have realized the impact of how far this could have gone.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: be working backward from
  text: we should be working backward from.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: just kind of assume that open source is part of it, and every country
    is going to do it
  text: we should just kind of assume that open source is part of it, and every country
    is going to do it.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: actually dive into some of the bullets
  text: we should actually dive into some of the bullets.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: all understand
  text: we should all understand.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: open source? I'm joined by a16z Journal Partners Martín Casado and Anjane
    Midha to unpack the new AI action plan, the politics behind it, and the implications
    for builders and policymakers alike. Let's get into it. As a reminder, the content
    here
  text: the future of open source? I'm joined by a16z Journal Partners Martín Casado
    and Anjane Midha to unpack the new AI action plan, the politics behind it, and
    the implications for builders and policymakers alike. Let's get into it. As a
    reminder, the content here is for informational purposes only.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/3f86df7b-51c6-4101-88a2-550dba782de8/episodes/7b0c14a0-e574-47cc-8383-98719c2f1e6a/audio/128/default.mp3?aid=rss_feed&awCollectionId=3f86df7b-51c6-4101-88a2-550dba782de8&awEpisodeId=7b0c14a0-e574-47cc-8383-98719c2f1e6a&feed=JGE3yC0V
processing_date: 2025-10-04 14:41:45 +0000
quotes:
- length: 245
  relevance_score: 5
  text: And so I think the biggest mistake is to confuse these two markets as one
    and to do the classic, "Oh, let's wait to see how they evolve," because the pace
    at which a new entrant is able to actually create a lead in the category is quite
    stunning
  topics:
  - market
- length: 129
  relevance_score: 4
  text: And what happens is a few bad actors essentially use that arbitrage opportunity
    to represent Silicon values used incorrectly in D
  topics:
  - opportunity
- length: 268
  relevance_score: 4
  text: We both have the chance to work with founders who are, I mean, literally,
    twenty-somethings out of college, two years out of college, building revenue-run-rate
    businesses in the tens to hundreds of millions of dollars serving both of these
    markets, expanding like this
  topics:
  - revenue
  - market
- length: 84
  relevance_score: 3
  text: But the reality is, I think it was driven—I think the majority of people are
    neither
  topics: []
- length: 59
  relevance_score: 3
  text: Maybe the biggest change now is like those people are there
  topics: []
- length: 89
  relevance_score: 3
  text: You have to literally hook up wet labs and start doing experiments in an entirely
    new way
  topics: []
- length: 46
  relevance_score: 3
  text: And look, the reality is America leads the way
  topics: []
- impact_reason: Highlights the massive, rapid shift in US policy sentiment regarding
    AI, moving from calls for pausing innovation to actively leading the global race,
    which is crucial for understanding the current regulatory and investment landscape.
  relevance_score: 10
  source: llm_enhanced
  text: The conversation around AI regulation in the U.S. has changed dramatically.
    Just a year ago, the loudest voices were calling to pause or shut down open source
    AI. Today, the U.S. is pushing to lead the global race.
  topic: strategy/predictions
- impact_reason: Clearly defines the fundamental regulatory philosophy shift that
    occurred regarding AI—from reactive to preemptive regulation.
  relevance_score: 10
  source: llm_enhanced
  text: That culture shift was one from, 'Let's let the tech mature and then decide
    how to regulate it later,' to, 'Before, let's try to regulate it in its infancy.'
  topic: regulation/strategy
- impact_reason: Uses the rapid advancement of Chinese models (like DeepSeek) as empirical
    evidence against the 'slow down to maintain lead' argument, suggesting the initial
    fears were misplaced or factually incorrect.
  relevance_score: 10
  source: llm_enhanced
  text: And then you forward by a year, and they've got the best models by far, and
    we're way behind. So it was like the people that are supposed to be protecting
    the U.S. innovation brain trust were somehow on the side of, 'Let's slow it down.'
  topic: predictions/business
- impact_reason: Demonstrates that leading safety researchers at the time acknowledged
    that the marginal risks of AI were an open, unanswered research question, not
    established facts justifying immediate lockdown.
  relevance_score: 10
  source: llm_enhanced
  text: You would go to Dawn Song, who is like a safety researcher, McCarthy Fellow
    at Berkeley, and you'd say, 'What are the marginal risks?' And she'd say, 'Great
    question. We should research.' The world expert on this question was like, 'This
    is very important, but it's an open research statement.'
  topic: safety/technical
- impact_reason: 'A strong cautionary principle for policymakers: significant deviations
    from established tech policy norms require exceptionally high justification.'
  relevance_score: 10
  source: llm_enhanced
  text: If we're going to make a departure from a posture that was developed from
    40 years, we'd better have a pretty damn good reason.
  topic: strategy
- impact_reason: A core principle applied to AI regulation debates, specifically challenging
    those who claim existential risk warrants immediate, sweeping policy changes.
  relevance_score: 10
  source: llm_enhanced
  text: Extraordinary claims require extraordinary evidence. And so the burden of
    proof should be on the party making the extraordinary claims.
  topic: safety/strategy
- impact_reason: A strong defense of the current liability framework for researchers,
    warning that imposing liability for open-sourced weights would destroy innovation.
  relevance_score: 10
  source: llm_enhanced
  text: The status quo is you do not hold scientists liable for downstream uses of
    their technology. That's absurd. That's a great way to shut down the entire innovation
    ecosystem and start throwing literally researchers in jail.
  topic: safety/business
- impact_reason: A sharp critique of early AI safety advocates, arguing they failed
    to consider the real-world, second-order consequences of their rhetoric when interacting
    with non-expert policymakers.
  relevance_score: 10
  source: llm_enhanced
  text: The ungenerous approach would be that there was a lot of discourse, awesome,
    but a lot of the people pushing the discourse were first-order thinkers. They
    weren't doing the math on, "Wait a minute, if policymakers who have no background
    in frontier AI... start to take discourse as canon... then what happens? What
    are the second and third-order effects?"
  topic: safety/strategy
- impact_reason: Signals a fundamental difference in how open source/open weights
    strategies apply to AI compared to traditional software, suggesting novel and
    potentially more sustainable business models.
  relevance_score: 10
  source: llm_enhanced
  text: There's something that's unique about AI that software doesn't have, and like
    we're seeing very viable business models come out of it that don't have the limitations
    of traditional software.
  topic: technical/business
- impact_reason: 'Crucial technical distinction: Open weights do not equate to open
    source software because the proprietary data pipeline/training process remains
    protected, limiting a competitor''s ability to fully replicate the model.'
  relevance_score: 10
  source: llm_enhanced
  text: One of them is like open weights is not the ability to produce the weights,
    right? But open software is the ability to produce the software. If you give me
    open software, I can compile it, I can modify it, whatever. But giving open weights,
    you don't have it. You don't have the data pipeline when you're talking about
    open weights.
  topic: technical
- impact_reason: 'Describes a specific, viable ''open core'' strategy for AI: releasing
    smaller, open models for distribution/brand while retaining the largest, most
    complex model for core revenue, leveraging the difficulty of inference scaling.'
  relevance_score: 10
  source: llm_enhanced
  text: The second one is this is a very nice business model. It's kind of a piece
    dividend to the rest of the industry, which is you open weights your smaller models
    that anybody can use, but the larger model you keep internally, which is actually
    also more difficult to operationalize for inference, right? There's kind of good
    reasons to do this.
  topic: business
- impact_reason: 'This is a critical strategic insight: Open vs. Closed AI is not
    a spectrum but two distinct market segments serving fundamentally different customer
    needs (frontier capability vs. sovereign deployment).'
  relevance_score: 10
  source: llm_enhanced
  text: Well, I think these are two different markets. Literally, the requirements
    of the customers are completely different, right? So if you're a developer, you're
    building an application, and you happen to need the latest and greatest frontier
    capabilities today, you have a different set of requirements than if you're a
    nation-state deploying like a chat companion for your entire employee base...
    closed source and open source are not just differences in technology, but completely
    different markets altogether.
  topic: strategy
- impact_reason: 'Praises the policy''s sophisticated approach to regulation: prioritizing
    the development of measurement frameworks (evaluations ecosystem) before imposing
    restrictive rules on model risk.'
  relevance_score: 10
  source: llm_enhanced
  text: I think I do think directionally it was great that they said, "We need, let's
    read this bullet point on build an AI evaluations ecosystem." I loved that because
    it acknowledges that, hey, before we start actually passing grant proclamations
    of what these models are risky or whether these models are dangerous or not, let's
    first even agree on how to measure the risk in these models before jumping the
    gun.
  topic: safety/policy
- impact_reason: Indicates a positive, pragmatic shift in policy discourse towards
    scientific assessment over premature alarmism regarding AI risk.
  relevance_score: 10
  source: llm_enhanced
  text: the vibe shift going from, 'Let's not jump the gun on saying these models
    are dangerous. Let's first talk about building a scientific grounded framework
    on how to assess the risk in these models,' to me was not at all a given, and
    I was really excited about that.
  topic: safety/strategy
- impact_reason: Raises a critical ethical and philosophical concern about alignment
    efforts potentially becoming tools for ideological control rather than pure safety
    measures.
  relevance_score: 10
  source: llm_enhanced
  text: The subtext that certainly I bristle to is that the people doing the alignments
    are somehow protecting the rest of us from whatever they think their ideal is
    as far as dangers to me or thoughts I shouldn't have or information I shouldn't
    be exposed to.
  topic: safety/ethics
- impact_reason: A powerful analogy for understanding modern deep learning models,
    emphasizing emergent behavior over explicit programming.
  relevance_score: 10
  source: llm_enhanced
  text: you've got to think about these AI systems as almost biological systems that
    are grown, not coded up.
  topic: technical
- impact_reason: A key argument against stalling deployment based solely on the lack
    of full mechanistic interpretability, drawing parallels to historical technologies.
  relevance_score: 10
  source: llm_enhanced
  text: that doesn't mean just because you don't understand the true mechanism of
    the system doesn't mean you don't unlock its useful value.
  topic: strategy/business
- impact_reason: Provides a stark contrast between recent regulatory actions and the
    previous administration's stance, framing the current environment as a significant
    pivot away from limiting innovation.
  relevance_score: 9
  source: llm_enhanced
  text: Under the Biden administration, we had the executive order, which was basically
    the opposite of what we're seeing today. It was trying to limit innovation. It
    was doing a bunch of fear-mongering.
  topic: strategy/regulation
- impact_reason: Reveals a surprising initial alignment among technologists and academics
    favoring caution or pausing AI, which contrasts sharply with the current pro-innovation
    stance of the industry.
  relevance_score: 9
  source: llm_enhanced
  text: Academia was silent. The startups were silent. And if anything, the technologists
    were supporting it [regulation/pausing AI]. We were in the super back-the-world
    where innovation is bad or dangerous, and we should just regulate, we should pause
    it.
  topic: safety/strategy
- impact_reason: Draws a critical historical parallel between the internet's early
    security crises and the current AI debate, noting that past tangible threats led
    to accelerated adoption, not deceleration.
  relevance_score: 9
  source: llm_enhanced
  text: At that time [early internet], we actually had examples of the stuff being
    dangerous, right? Like Robert Morris went out, the Morris Worm, it took down critical
    infrastructure... And what did everybody else do? Pedal to the metal, all invest
    more technology. This is great.
  topic: strategy/predictions
- impact_reason: Highlights the near-miss of highly restrictive legislation (SB 1047),
    indicating how close the US came to severely limiting AI development via policy.
  relevance_score: 9
  source: llm_enhanced
  text: What was absurd to us, I think, was that it made it through the House and
    the Senate, and it was on its way to a final vote [SB 1047], and it would have
    become law one signature from the governor later.
  topic: regulation/strategy
- impact_reason: Offers a strategic insight into the historical separation between
    tech building and policy-making, and identifies the moment this separation broke
    down regarding AI.
  relevance_score: 9
  source: llm_enhanced
  text: I think my view is that technologists like technology, and politicians like
    policy, and they pretend like these two things are in different worlds. And as
    long as these two worlds don't collide... that changed completely, I think, last
    summer.
  topic: strategy
- impact_reason: Documents the surprising internal conflict where key financial backers
    of tech actively argued against open source AI, based on geopolitical fears.
  relevance_score: 9
  source: llm_enhanced
  text: You had VCs—their entire job is investing in tech—talking against open source.
    You know, Founders Fund, they're like, 'Open source AI is dangerous. It gives
    China the advantage.'
  topic: business/strategy
- impact_reason: 'Provides the counter-argument: distinguishing between the final
    weaponized application and the fundamental, dual-use underlying technology, advocating
    for leadership through open development.'
  relevance_score: 9
  source: llm_enhanced
  text: No, you would not do this for an F-16 because that is a fighter jet. However,
    a lot of the technologies used to build it, yes, this is fundamental. It's not
    like people aren't going to figure out anyways, and we need to be the leader,
    just like we were the leader in nuclear.
  topic: strategy/technical
- impact_reason: 'Offers a clear framework for analyzing the anti-open-source arguments:
    confusing the base technology with its worst-case theoretical applications.'
  relevance_score: 9
  source: llm_enhanced
  text: 'They were basically like there was a substantive argument against open source,
    and there was an atmospheric one. A substantive one was like the one Martin mentioned:
    that the technology was being confused for the applications, right? And all the
    worst-case outcomes of the applications or misuses were then being confused.'
  topic: safety/strategy
- impact_reason: Exposes factual inaccuracies (US lead over China) that were used
    to justify restrictive policy, linking flawed premises to poor strategic outcomes.
  relevance_score: 9
  source: llm_enhanced
  text: There was a famous former CEO who went up in front of Congress and literally
    in a testimony said, 'The U.S. is years ahead of China.' And so since these are
    nuclear weapons... let's lock it down so we can maintain that lead, and therefore
    our adversaries will never get their hands on it. Which were both just fundamentally
    wrong.
  topic: strategy/regulation
- impact_reason: 'Perfectly frames the central philosophical dilemma in AI governance:
    the risk trade-off between accelerating innovation versus pausing for safety/interpretability
    research.'
  relevance_score: 9
  source: llm_enhanced
  text: Is it riskier to just go full steam ahead, or is it riskier to kind of slow
    down until we better understand these models, interpretability, et cetera?
  topic: safety/strategy
- impact_reason: Provides historical context showing that past fears about dual-use
    technology (GPUs/compute) were managed without halting the underlying technological
    progress.
  relevance_score: 9
  source: llm_enhanced
  text: The discourse around tech safety has been around for 40 years, and we went
    through it with compute. Remember when we were like, 'Okay, Saddam Hussein shouldn't
    have PlayStations because you can use GPUs to simulate nuclear weapons'? That
    was actually a pretty robust and real discussion, but that did not stop us from
    having other people create chips or video games, right?
  topic: strategy/safety
- impact_reason: Specifically calls out a proposed regulation (SB 1047) as fundamentally
    flawed by imposing retroactive liability on open-source contributors.
  relevance_score: 9
  source: llm_enhanced
  text: The SB 1047 regime proposed that the original developer of the weights that
    they put out basically as free information should be held liable, which was absurd.
  topic: safety/regulation
- impact_reason: 'Clearly articulates the negative impact of potential liability laws:
    discouraging individual researchers and innovators due to legal uncertainty.'
  relevance_score: 9
  source: llm_enhanced
  text: The point is that creates a chilling effect. The chilling effect is the idea
    that when our best talent is considered, "I could be sued." Like I'm a random
    kid in Arkansas developing something. I don't want to be in a world where it can
    be resolved in the courts.
  topic: safety/business
- impact_reason: 'Highlights the geopolitical risk: unilateral self-restriction (chilling
    effect) in the US plays directly into the hands of state-backed competitors.'
  relevance_score: 9
  source: llm_enhanced
  text: In a situation where you have an entire nation-state-backed entity like China,
    they are actually doing the opposite of a chilling effect, encouraging a race
    to the frontier.
  topic: strategy
- impact_reason: Distinguishes between acknowledging non-zero existential risk (which
    is intellectually honest) and using that acknowledgment to justify immediate,
    sweeping regulatory changes, highlighting the danger of that leap.
  relevance_score: 9
  source: llm_enhanced
  text: Law is like impossible to refactor. And so I think the second and third-order
    effects were that there were a lot of well-intentioned folks... saying, "Look,
    if you're intellectually honest about the rate of progress of AI, it's not crazy
    to say that there are some existential risks to the technology. It's non-zero.
    Sure. Yes, that is true." But then to then say that that threshold is high enough
    to start introducing sweeping changes in regulation to the way we create technology—that
    leap, I don't think a lot of the early proponents of that technology realized
    they would do that.
  topic: safety/regulation
- impact_reason: Identifies the fundamental disconnect between D.C. policy circles
    and the technical reality of AI development, emphasizing the necessity of technologist
    involvement in drafting policy.
  relevance_score: 9
  source: llm_enhanced
  text: 'The fact that the co-authors include technologists. Right. And I think that
    was the core problem: D.C. is a system, like a self-contained system, and the
    values are a self-contained system.'
  topic: strategy/regulation
- impact_reason: 'Describes the classic enterprise adoption model: closed-source pioneers
    drive capability, and open-source/service providers capture the enterprise infrastructure
    market by offering control and support.'
  relevance_score: 9
  source: llm_enhanced
  text: The way it works is the closed-source pioneers, the frontier of capabilities,
    it introduces new use cases. And then the enterprises never know how to consume
    that technology. And when they do figure out eventually that they want cheaper,
    faster, more control, they need somebody like a Red Hat to then introduce them
    and provide solutions and services and packaging and forward-deployed engineering
    and all of that around it.
  topic: business/strategy
- impact_reason: 'Provides a clear strategic division for AI/ML deployment: proprietary
    models for cutting-edge applications, and open source for regulated/sovereignty-focused
    infrastructure needs.'
  relevance_score: 9
  source: llm_enhanced
  text: Closed source wins applications. And open source tends to do really well in
    infrastructure, especially in large government customers, regulated industries
    where there's a bunch of security requirements, things need to run on-prem. The
    customer needs control over it. Broadly, you could call that the sovereignty I
    market right now.
  topic: business/strategy
- impact_reason: 'Uses a classic political economy concept to categorize actors in
    the AI safety debate: those genuinely concerned (''true believers'') and those
    opportunistically leveraging the fear for business/regulatory advantage (''bootleggers'').'
  relevance_score: 9
  source: llm_enhanced
  text: 'The Baptist bootleggers. Yes, I was just going to say exactly: true believers
    and then sort of people who use that thinking to support their own ends.'
  topic: strategy
- impact_reason: Highlights the growing strategic importance of open source, particularly
    for regulated industries and governments needing data sovereignty and on-premise
    control, shifting its perception from purely philosophical to critical business
    necessity.
  relevance_score: 9
  source: llm_enhanced
  text: Broadly, you could call that the sovereignty I market right now. Lots of governments
    and lots of legacy industries are going, "Wait, this open source thing is really
    critical to us."
  topic: business/strategy
- impact_reason: Directly links the demand for on-premise/controlled deployment (driven
    by large enterprises/governments) to the resurgence and strategic pivot towards
    open source models in AI.
  relevance_score: 9
  source: llm_enhanced
  text: But now there's an extraordinary business case for it, which is why I think
    you're seeing a lot of startups and companies also changing their posture because
    they're going, "Wait a minute, some of the largest customers in the world, enterprise
    customers, happen to be governments and happen to be legacy industries, Fortune
    50 companies, and they want stuff on-prem."
  topic: business/strategy
- impact_reason: Positions the 'open weights' strategy as an evolution of the traditional,
    often difficult-to-sustain, open-core model, made viable in AI because IP (the
    data pipeline/training) is inherently better protected.
  relevance_score: 9
  source: llm_enhanced
  text: I feel like it's actually almost an evolved from a business strategy and an
    industry perspective version of open core, which was historically sort of sustainable
    model for open source software development, which was really hard to implement
    because of the reasons Martin said, where once you gave away the code, it was
    really hard for you to protect your IP.
  topic: business/strategy
- impact_reason: 'Highlights the dual benefit of open weights: enabling community
    security auditing (red teaming) while retaining proprietary advantage through
    techniques like distillation and post-training.'
  relevance_score: 9
  source: llm_enhanced
  text: But with weights, you can contribute something to the research community.
    You can give developers control. You can allow the world to red team it and make
    it more secure while you're still able to actually, because of the way distillation
    works and some of the ways like post-training works, you can still actually hold
    on to some of the core IP...
  topic: technical/safety
- impact_reason: Reinforces the positive shift in regulatory mindset from reactive
    prohibition to proactive, scientifically grounded risk assessment.
  relevance_score: 9
  source: llm_enhanced
  text: The vibe shift going from, "Let's not jump the gun on saying these models
    are dangerous. Let's first talk about building a scientific grounded framework
    on how to assess the risk in these models," to me was not at all a given, and
    I was really excited about that.
  topic: safety/policy
- impact_reason: 'Articulates a long-term geopolitical strategy: maintaining control
    over the foundational infrastructure (chips, training techniques, pipelines) yields
    greater strategic advantage than hoarding incremental model IP.'
  relevance_score: 9
  source: llm_enhanced
  text: And in the long term, if every other major nation is running their entire
    AI ecosystem on the back of American chips and American models and American post-training
    pipelines and American RL techniques, then that ecosystem win is orders of magnitude
    more valuable than any short-term sort of give of IP...
  topic: strategy
- impact_reason: Highlights the significant, non-trivial challenge of creating robust
    frameworks for assessing AI risk, moving beyond simple declarations of danger.
  relevance_score: 9
  source: llm_enhanced
  text: building an AI evaluations ecosystem is not easy.
  topic: safety/strategy
- impact_reason: A strong cautionary note for policymakers against embedding specific
    worldviews into AI governance frameworks.
  relevance_score: 9
  source: llm_enhanced
  text: Which is why I think we need to be very careful, even when we come up with
    policy, not to impose a different set of ideological rules on top of these.
  topic: safety/regulation
- impact_reason: Explains the empirical, emergent nature of model capabilities, linking
    directly to the difficulty of pre-deployment risk assessment.
  relevance_score: 9
  source: llm_enhanced
  text: when you're training a model, you are actually growing it in this environment
    of a bunch of prior history and training data, et cetera. And often empirically,
    you actually don't know what the capabilities of the model are until it's actually
    done training.
  topic: technical
- impact_reason: Articulates the core tension between the 'grown' nature of AI and
    the requirement for interpretability before deployment in high-stakes scenarios.
  relevance_score: 9
  source: llm_enhanced
  text: if we can't align it because we actually don't know it's a biological mechanism
    until it's grown up, you don't know what its risks are... then we can't deploy
    these AI models in mission-critical places until we've solved, let's say, the
    black box problem, the mechanistic interpretability problem.
  topic: safety/technical
- impact_reason: Provides historical context to argue for pragmatic adoption of powerful,
    complex technologies even before full theoretical understanding is achieved.
  relevance_score: 9
  source: llm_enhanced
  text: If you look at most general-purpose technologies in history—electricity, nuclear
    fusion—like we there are many examples of technologies where we knew there were
    complex systems and we didn't truly understand at an atomistic level or mechanistic
    level how they work, but we still used them.
  topic: strategy
- impact_reason: Pinpoints the key moment/organization (Center for AI Safety letter)
    that defined the initial 'existential risk' mood dominating the AI discourse.
  relevance_score: 8
  source: llm_enhanced
  text: I wanted to prepare it to everybody here. Guys, there's been so many of these.
    Yeah, that's right. I don't remember. Remember what was Dan Hendrycks' CAS? What
    was the Center for AI Safety. Center for AI Safety, that's it. That's it. The
    nonprofit, yeah. Yeah. And then they got all these people to sign this list, you
    know, like, 'We need to worry about the existential risk of AI,' and that was
    the mood.
  topic: safety/strategy
- impact_reason: Contrasts the AI discourse with the internet's, noting a lack of
    strong pro-investment voices early on, suggesting a cultural hesitation around
    AI adoption.
  relevance_score: 8
  source: llm_enhanced
  text: And coming into this AI stuff two years ago, it was the opposite, which is
    like there were the concerns with the new technology, which you always have, but
    there were very few voices that were actually saying it's really important we
    invest in this stuff.
  topic: strategy
- impact_reason: Articulates the core analogy used by AI restrictionists—comparing
    foundational models to weapons—which is central to the dual-use debate.
  relevance_score: 8
  source: llm_enhanced
  text: Would you open source your new weapon plans? Would you open source your F-16
    plan? So the idea was that somehow this was like, you know, nuclear weapons are
    not dual-use.
  topic: safety/technical
- impact_reason: Provides an anecdote suggesting that even domain experts in high-risk
    areas (bioweapons) found the claims about immediate, model-derived catastrophic
    risk to be exaggerated or unfounded at the time.
  relevance_score: 8
  source: llm_enhanced
  text: It was funny; we got a bioweapon expert, and he's like, 'I don't know, really.
    The difference between a model in Google is almost nothing.'
  topic: safety/technical
- impact_reason: A strong statement suggesting that the narrative surrounding AI risk
    and US competitiveness was deliberately misleading or based on falsehoods.
  relevance_score: 8
  source: llm_enhanced
  text: I feel like when you think these people come from, right? So I think those
    two things were—I felt like we were being gaslit constantly because both the content
    and the atmosphere were just wrong.
  topic: strategy
- impact_reason: Indicates a positive shift in the policy landscape where pragmatic,
    technically informed voices are now present, leading to more sensible discourse
    compared to the earlier period.
  relevance_score: 8
  source: llm_enhanced
  text: I'm not calling on at night thinking we have to do something now because I
    feel like, okay, there's actually representation. Yeah. That's sensible. At the
    time there was not.
  topic: strategy
- impact_reason: Highlights the fragmentation of the 'tech' industry into distinct
    subcultures (big tech, little tech, academia) with divergent interests, complicating
    unified lobbying or policy advocacy.
  relevance_score: 8
  source: llm_enhanced
  text: We used to be one shared culture, and then when tech grew, we actually have
    some major differences in the values, at least between parties. We're not one
    tech ecosystem anymore. We have different interests.
  topic: business/strategy
- impact_reason: A call for caution against abandoning established norms (like researcher
    liability protection) without overwhelming justification.
  relevance_score: 8
  source: llm_enhanced
  text: If we don't have a good reason, then I think we should probably learn from
    that experience [of deviating from the status quo].
  topic: strategy
- impact_reason: Pinpoints a specific event (DeepSeek release) as the trigger that
    made latent policy risks visible to everyone, rather than the cause of those risks.
  relevance_score: 8
  source: llm_enhanced
  text: I think DeepSeek was the catalyst, but it didn't change the reality that the
    second and third-order effects of policymakers confusing discourse for fact were
    always going to be terrible.
  topic: safety/regulation
- impact_reason: Describes the 'silent majority' of industry players who favor self-regulation
    and dialogue over extreme positions, suggesting their recent entry into the policy
    debate is a positive development.
  relevance_score: 8
  source: llm_enhanced
  text: I think the majority of people are pragmatists that are not trying to take
    advantage of the system, that think, "Well, maybe if we have this discourse, it's
    an on-diss course, and then we'll self-police."
  topic: strategy
- impact_reason: A strong prediction that open-weight strategies will dominate the
    AI landscape because they are inherently more advantageous to the originating
    company than traditional open-source software strategies.
  relevance_score: 8
  source: llm_enhanced
  text: I actually think that the AI one is more beneficial to the companies doing
    it, for sure. But as a result of that, we're going to continue to see a lot of
    it. And so I think we should just kind of assume that open source is part of it,
    and every country is going to do it.
  topic: predictions
- impact_reason: Emphasizes the extreme velocity required for startups in the current
    AI platform shift; hesitation means immediate obsolescence.
  relevance_score: 8
  source: llm_enhanced
  text: I do think what we need to contend with is that it seems like it's getting
    harder and harder to be a category leader if you don't enter fast. Like the speed
    at which a new startup is able to enter the open source or the closed source market
    and create a lead is absurd, right?
  topic: business/strategy
- impact_reason: Focuses on the high-value, difficult application of AI in the physical
    sciences, stressing the need for interdisciplinary talent (ML + lab work).
  relevance_score: 8
  source: llm_enhanced
  text: I personally, I just love the fact that we are just starting to explore what
    these frontier models mean for scientific discovery in physics, in chemistry,
    in material science. And we need to inspire the next generation to want to go
    into those areas because it's hard. It's really hard to do AI in the physical
    world.
  topic: predictions/strategy
- impact_reason: A pointed critique of policy that overlooks the historical and ongoing
    critical role of academic research in driving foundational CS/AI innovation.
  relevance_score: 8
  source: llm_enhanced
  text: The other one that I thought was a huge omission is there's basically no real
    mention of academia investing in academia. Like there's some oblique references
    to it, but it's just been such a mainstay of innovation in computer science for
    the last 40 years...
  topic: strategy/policy
- impact_reason: 'Provides a concise, high-level technical framing of the alignment
    problem: imposing a desired purpose onto inherently complex and unpredictable
    model behavior.'
  relevance_score: 8
  source: llm_enhanced
  text: At an almost technological level, alignment is obviously you'd want to do.
    I have a purpose. I want to align the AI to this purpose. It turns out these models
    are problematic, generally unruly, chaotic, whatever adjective you want to use.
  topic: safety/technical
- impact_reason: Uses the human brain as the ultimate counter-example to the demand
    for perfect mechanistic understanding before utilizing intelligence.
  relevance_score: 8
  source: llm_enhanced
  text: We don't even know how our own brain works. No, we don't think. We're conscious
    of this. Yeah. And we don't stop working without a human being.
  topic: strategy/safety
- impact_reason: 'Summarizes the core trade-off: maximizing the benefits of intelligence
    (human or artificial) while developing parallel management strategies for inherent
    risks.'
  relevance_score: 8
  source: llm_enhanced
  text: I still want to unlock all the beautiful benefits of the big, beautiful brains
    that humans have. And so you develop [mechanisms to manage the risks].
  topic: strategy
- impact_reason: Asserts that the US has established institutional knowledge for balancing
    innovation and risk across previous technology waves.
  relevance_score: 7
  source: llm_enhanced
  text: And so we've been through all of these tech waves, and we've learned how to
    have this discussion in a way that for the United States interests balances these
    two things.
  topic: strategy
- impact_reason: Suggests that current discourse around AI risk is partially inherited
    from earlier, perhaps less relevant, intellectual movements (like those associated
    with Sturm/existential risk thinkers).
  relevance_score: 7
  source: llm_enhanced
  text: I think part of it is just a path dependency on where we came from, which
    is kind of the legacy of both Strum. I think that was part of it.
  topic: strategy
- impact_reason: Sets up a comparison between AI/ML deployment and historical infrastructure
    adoption cycles, suggesting AI might follow established patterns.
  relevance_score: 7
  source: llm_enhanced
  text: 'I don''t think this part is actually playing out beautifully along the same
    trend lines of all previous computing infrastructure: databases, analytics, operating
    systems, like Linux.'
  topic: business/predictions
- impact_reason: Reinforces the idea that the US has established mechanisms for balancing
    innovation and national interest during major technological shifts.
  relevance_score: 7
  source: llm_enhanced
  text: We went through the internet. We went through the cloud. We went through mobile.
    And so we've been through all of these tech waves, and we've learned how to have
    this discussion in a way that for the United States interests balances these two
    things.
  topic: strategy
- impact_reason: Uses specific historical examples (Huawei/Cisco) to justify the potential
    for targeted, effective national security restrictions in the AI domain.
  relevance_score: 7
  source: llm_enhanced
  text: We've had kind of areas that were very sensitive to national governments,
    think about Huawei and Cisco, for example. And we as a nation did start to put
    in kind of important export restrictions as a result.
  topic: strategy
- impact_reason: Criticizes abstract, purely theoretical safety debates for lacking
    grounding in historical precedent from previous tech waves.
  relevance_score: 7
  source: llm_enhanced
  text: I just feel these almost platonic, polemic questions like the one that you
    just posed aren't rooted in 40 years of learning.
  topic: safety/strategy
- impact_reason: Highlights the inspirational framing of the new AI action plan, focusing
    on scientific benefit rather than just economic competition or risk.
  relevance_score: 7
  source: llm_enhanced
  text: Today a new frontier of scientific discovery lies before us.
  topic: strategy/general
- impact_reason: A philosophical point about the inherent uncertainty in complex systems,
    including AI, software, and even biology.
  relevance_score: 7
  source: llm_enhanced
  text: I mean, any complex system has states that you just don't understand.
  topic: strategy
source: Unknown Source
summary: '## Podcast Summary: The State of American AI Policy: From ‘Pause AI’ to
  ‘Build’


  This 42-minute podcast episode analyzes the dramatic shift in U.S. policy discourse
  regarding Artificial Intelligence, moving from calls to "Pause AI" and existential
  risk warnings to a proactive stance focused on leading global innovation, as exemplified
  by the recent AI Action Plan. The discussion centers on the political dynamics,
  the role of open source, and the need for a more empirically grounded policy framework.


  ---


  ### 1. Focus Area

  The primary focus is the **evolution of U.S. AI policy and regulation**, contrasting
  the previous fear-driven regulatory posture (like the proposed SB 1047 bill) with
  the current administration''s emphasis on innovation and global leadership. Key
  sub-themes include the debate surrounding **open-source AI models (weights)**, the
  influence of technologists versus policymakers, and historical parallels with previous
  technology waves (Internet, nuclear energy).


  ### 2. Key Technical Insights

  *   **Confusion of Technology vs. Application:** A major critique of early regulation
  was the conflation of fundamental AI technology (the model weights) with theoretical,
  worst-case applications (e.g., bioweapons), leading to overly broad and potentially
  damaging regulatory proposals.

  *   **Empirical Basis for Risk:** Experts noted a lack of empirical evidence supporting
  the marginal risks claimed by those advocating for immediate, sweeping regulation.
  The burden of proof, they argue, should lie with those making "extraordinary claims"
  that threaten the status quo of open research.

  *   **Open Source Infrastructure Parallel:** The discussion drew parallels between
  the AI ecosystem and historical infrastructure shifts (databases, OS), suggesting
  a pattern where closed-source pioneers establish the frontier, followed by open-source
  providers offering enterprise solutions, control, and cost efficiency later on.


  ### 3. Business/Investment Angle

  *   **Chilling Effect on Innovation:** Proposals like SB 1047, which threatened
  downstream liability for open-sourcing model weights, were seen as creating a severe
  chilling effect, discouraging top talent from contributing to open research while
  adversaries (like China) accelerate their efforts.

  *   **China''s Rapid Advancement:** The failure of the initial premise—that slowing
  down U.S. open source would keep China behind—was highlighted. The rapid emergence
  of powerful models from Chinese entities like DeepSeek demonstrated that adversaries
  are not easily hampered and are actively competing at the frontier.

  *   **Strategic Value of Open Source:** Companies like Meta have leveraged open-sourcing
  as a business strategy, mirroring historical infrastructure plays where open models
  drive adoption and create demand for enterprise services and support.


  ### 4. Notable Companies/People

  *   **Martín Casado & Anjane Midha (a16z Partners):** Hosts and primary voices driving
  the argument for pro-innovation policy, tracing the industry''s response to early
  regulatory threats.

  *   **Center for AI Safety (CAIS):** Mentioned as the organization behind the influential
  "Pause AI" petition signed by many CEOs, representing the initial existential risk
  focus.

  *   **SB 1047 Bill:** Cited as the most egregious example of premature, ill-informed
  regulation that galvanized the pro-innovation tech community into action.

  *   **DeepSeek:** Highlighted as a concrete example proving that Chinese AI capabilities
  are already near the frontier, invalidating the premise that locking down U.S. models
  would maintain a multi-year lead.

  *   **Anthropic (Jack Clark):** Mentioned as an example of a company initially involved
  in the risk discourse that later acknowledged the potential negative policy implications
  of those discussions.


  ### 5. Future Implications

  The conversation suggests the industry is moving toward a **pragmatic, innovation-first
  policy framework**, exemplified by the new AI Action Plan which explicitly includes
  technologists in its authorship. The future hinges on learning from the past 40
  years of technology policy, where innovation was generally favored unless there
  was "pretty damn good reason" (i.e., extraordinary evidence) to slow down. The industry
  now has better representation in D.C., ensuring that policy discussions are more
  "fulsome" and grounded in technical reality rather than theoretical alarmism.


  ### 6. Target Audience

  This podcast is highly valuable for **AI/ML professionals, venture capitalists,
  policy advisors, and technology executives** who need to understand the current
  political climate surrounding AI development, the strategic implications of open
  source, and the shift in regulatory philosophy in the United States.'
tags:
- artificial-intelligence
- startup
- ai-infrastructure
- investment
- generative-ai
- google
- anthropic
- meta
title: 'The State of American AI Policy: From ‘Pause AI’ to ‘Build’'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 82
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 9
  prominence: 0.9
  topic: startup
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 7
  prominence: 0.7
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 6
  prominence: 0.6
  topic: investment
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 2
  prominence: 0.2
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-04 14:41:45 UTC -->
