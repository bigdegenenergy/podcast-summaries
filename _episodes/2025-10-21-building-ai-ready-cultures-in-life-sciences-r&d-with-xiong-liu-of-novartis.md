---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit emerj.com
    and fill out our Thought Leaders
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: eedle on AI ROI, visit emerj.com and fill out our Thought Leaders submission
    form. That's emerj.com and click on be
  name: Thought Leaders
  position: 1906
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2156
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2191
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2556
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2618
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5299
- category: unknown
  confidence: medium
  context: he the paradigms has a lot of implications in the Certicle Health Care.
    We're talking about generic AI versus the domain
  name: Certicle Health Care
  position: 6950
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 18032
- category: unknown
  confidence: medium
  context: rganizations, there are different players in this Gen AI paradigm, right?
    Then how do we ensure, you know,
  name: Gen AI
  position: 22887
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think
  name: And I
  position: 24187
- category: ai_media_and_analysis
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an example of a company whose CIO has been featured on the
    podcast, indicating their involvement in enterprise AI.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_enterprise
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, indicating their involvement in enterprise AI.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    sense of complex information by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the originator of the transformer architecture, which underpins
    modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The specific URL provided for the Notebook LM tool, which is a Google product.
  name: NotebookLM.google.com
  source: llm_enhanced
- category: ai_consulting_and_analysis
  confidence: high
  context: Mentioned for conducting a recent survey on generative AI investment and
    scaling in the life sciences industry.
  name: Deloitte
  source: llm_enhanced
- category: ai_data_and_research
  confidence: high
  context: Mentioned as a source of publicly available molecular data (gene expression
    data) used to build foundation models.
  name: Cell Atlas
  source: llm_enhanced
- category: ai_model_architecture
  confidence: high
  context: An older language model architecture (pre-GPT) that was applied to clinical
    trial documents for information extraction.
  name: BERT
  source: llm_enhanced
- category: ai_model_architecture
  confidence: high
  context: Referenced generally (and implicitly GPT-like models) as the current generation
    of generative models leveraging the decoder part of the transformer architecture.
  name: GPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a benchmark for the timeline, noting that the speaker's team
    was developing language models for clinical trials before its release.
  name: Chat GPT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as being quite good at building foundation models and publishing
    results.
  name: Academics/Research Institutions
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as providers of open public architectures that can be leveraged.
  name: Tech Companies
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as entities that leverage published models or build their own
    versions, requiring in-house AI/domain experts.
  name: Healthcare Companies
  source: llm_enhanced
- category: ai_field
  confidence: high
  context: Mentioned as one of the first gigantic generative AI use cases, which provides
    an advantage to language-based fields like genetics.
  name: Natural Language Processing (NLP) Domain
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 19:24:35 +0000
quotes:
- length: 209
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 160
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpora,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 103
  relevance_score: 4
  text: A fine-tuning meaning yes, we take the foundation models but we can still
    take the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 70
  relevance_score: 3
  text: So we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: 'This is a core explanation differentiating traditional ML from the
    foundation model approach: moving from small, task-specific, labeled datasets
    to leveraging massive, domain-wide pre-training.'
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs foundation models. So
    the idea is that yes, you still care about the specific data you are working on,
    but at the same time, we want to collect all the available data, relevant data,
    or irrelevant data, but as long as they fall into the same domain, and fill those
    data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical
- impact_reason: Provides a concrete, high-impact example of foundation models accelerating
    early R&D by encoding vast amounts of public biological knowledge (e.g., gene
    expression data), offering 'hints' even with limited proprietary data.
  relevance_score: 10
  source: llm_enhanced
  text: For example, when we study the disease pathways and identify new targets for
    specific indications... But now with foundation models, it has a lot of pre-point
    models based on a publicly available molecular data. So for example, cell applers,
    etc. So there's a lot of different cell types that gene expression data already
    being built into those foundation models.
  topic: predictions/technical
- impact_reason: 'This is a powerful conceptual link: treating genetics/molecular
    biology as a ''language'' that LLMs can process, suggesting a massive potential
    for in-silico testing and reducing wet-lab work.'
  relevance_score: 10
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process.
  topic: predictions
- impact_reason: Highlights the cutting-edge application of generative AI in creating
    synthetic biological data (cell types, gene expressions) for in-silico testing,
    drastically reducing physical experimentation time.
  relevance_score: 10
  source: llm_enhanced
  text: using foundation models, gen AI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: 'Highlights the key strategy for domain adaptation: applying the
    large-scale pre-training paradigm to specialized scientific datasets to create
    domain-specific foundation models.'
  relevance_score: 10
  source: llm_enhanced
  text: So people in the healthcare have been thinking similarly, can we put all the
    available, you know, data, chemical data, clinical trial, data, etc. into those,
    you know, transformer architecture. So that way we can also build our domain-specific
    foundation models to enable a lot of downstream tasks.
  topic: technical/strategy
- impact_reason: 'Clearly articulates the key difference between encoder-only models
    (like BERT) and decoder-based models (like GPT): the capability to generate novel
    data sequences.'
  relevance_score: 10
  source: llm_enhanced
  text: But now the the GPT they also leverage the decoding part of the transformer
    architecture. So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: 'Highlights a critical capability of generative models in science:
    constrained generation, allowing simultaneous optimization against multiple desirable
    properties (e.g., efficacy AND low toxicity).'
  relevance_score: 10
  source: llm_enhanced
  text: And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: predictions/applications
- impact_reason: 'Identifies a critical limitation: the pace of AI model development
    is currently outpacing the ability of experimental science to generate the necessary
    high-throughput, high-fidelity data.'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, et
    cetera.
  topic: limitations/technical
- impact_reason: 'Pinpoints the danger of hallucination in high-stakes domains: unlike
    code or text, biomedical outputs cannot be instantly verified, requiring costly,
    slow experimental validation.'
  relevance_score: 10
  source: llm_enhanced
  text: But a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... when it goes to the bio-medical biochemistry, et cetera...
    is this something that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: 'Proposes the solution to the hallucination/validation problem: evaluation
    metrics must be jointly driven by domain expertise and AI performance measures.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven those evaluation is a
    key here, because there could be so many versions of found data models.
  topic: safety/strategy
- impact_reason: Clearly outlines the historical evolution of AI/ML, highlighting
    the current 'paradigm shift' towards foundation models, which is crucial context
    for understanding modern AI strategy.
  relevance_score: 9
  source: llm_enhanced
  text: we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks. And more recently, we have a paradigm shift into those language models,
    foundation models.
  topic: technical
- impact_reason: 'Explains the primary value proposition of foundation models: transfer
    learning and enabling performance on low-data tasks, a major breakthrough for
    specialized industries like life sciences.'
  relevance_score: 9
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: A clear, concise definition of fine-tuning in the context of foundation
    models, clarifying that it involves weight adjustment rather than full retraining,
    which is vital for efficient deployment.
  relevance_score: 9
  source: llm_enhanced
  text: A fine-tuning meaning yes, we take the foundation models but we can still
    take the data you have, right? So we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems.
  topic: technical
- impact_reason: 'Describes the ultimate goal of personalized medicine enabled by
    AI: using population-scale models to inform individual patient treatment plans,
    linking population data to personalization.'
  relevance_score: 9
  source: llm_enhanced
  text: foundational models will be able to, you know, track what these normal human
    bodily functions are like across populations and then curtail that data to very
    personalized circumstances among patients.
  topic: predictions
- impact_reason: Provides a critical, quantifiable metric illustrating the 'AI adoption
    gap' or 'pilot purgatory' prevalent in the life sciences industry.
  relevance_score: 9
  source: llm_enhanced
  text: 73% of life sciences leaders are investing in generative AI initiatives, fewer
    than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: 'Identifies the practical, non-technical challenges organizations
    face when moving from experimentation to production: evaluation, alignment, and
    ROI measurement.'
  relevance_score: 9
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: 'Provides a concrete, high-impact application of GenAI in life sciences:
    generating biological data for in silico testing, accelerating discovery.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, gen AI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: predictions/applications
- impact_reason: Reiterates the foundational role of the Transformer architecture
    across diverse GenAI applications, confirming its dominance in modern AI.
  relevance_score: 9
  source: llm_enhanced
  text: So all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Applies the Transformer/embedding concept directly to molecular data
    (SMILES strings, graphs), showing how it learns fundamental chemical signatures.
  relevance_score: 9
  source: llm_enhanced
  text: So all those information publicly available data are from different databases.
    They could all fit into those kind of transformer architecture to let you learn
    the hidden embedding signatures of those molecules, atoms, etc.
  topic: technical/applications
- impact_reason: Emphasizes the indispensable role of domain experts (SMEs) in the
    entire ML lifecycle, especially in data curation and crucial model validation/benchmarking.
  relevance_score: 9
  source: llm_enhanced
  text: But, you know, we still need the domain knowledge people, you know, people
    have those kind of expertise to guide the, you know, the selection of the data
    and also more importantly, then these models are, you build, now how do we benchmark
    with them?
  topic: strategy/business
- impact_reason: Reiterates that data quality remains a primary challenge, even when
    using massive pre-trained models, especially when moving to domain-specific tasks.
  relevance_score: 9
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: business/technical
- impact_reason: 'A concise statement on the current bottleneck: AI concepts are ready,
    but experimental data collection limits progress.'
  relevance_score: 9
  source: llm_enhanced
  text: So, there's, you know, where AI, the architecture, the concept there, but
    the, sometimes we are limited by the, you know, experimental side.
  topic: limitations
- impact_reason: 'A crucial cautionary note: current AI is probabilistic, not deterministic,
    necessitating rigorous, ongoing benchmarking and model selection rather than blind
    trust.'
  relevance_score: 9
  source: llm_enhanced
  text: So, right now the technology cannot give you 100% answers, how those nations
    are always there, right? So, it's quite important that we have to do those benchmarking
    and select models accordingly.
  topic: safety/strategy
- impact_reason: 'Offers strategic advice to domain experts: they must understand
    the evolution of AI methods (old vs. new paradigms) because the future involves
    hybrid approaches, not total replacement.'
  relevance_score: 9
  source: llm_enhanced
  text: It behooves them [subject matter experts] to kind of have some awareness of
    the new and old paradigm that we were talking about before, how both are still
    present, but how we thought that AI would impact life sciences organization or
    that they'd just be this one school way of doing things is not the case.
  topic: strategy
- impact_reason: Highlights a critical need for domain expertise combined with AI
    literacy, suggesting that technical AI knowledge alone is insufficient for successful
    implementation.
  relevance_score: 9
  source: llm_enhanced
  text: it's also just going to be who the subject matter experts to have some awareness
    of this history as well.
  topic: Strategy/Adoption
- impact_reason: Describes a strategic shift in data management for AI adoptionâ€”centralizing
    data first, then applying advanced algorithms, moving beyond simple task automation
    to broad strategic impact.
  relevance_score: 8
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization that need that kind
    of whole-scale approach.
  topic: strategy
- impact_reason: Highlights the critical industry tension between using general-purpose
    models and the necessity of domain-specific fine-tuning or development in regulated
    fields like healthcare.
  relevance_score: 8
  source: llm_enhanced
  text: We're talking about generic AI versus the domain-specific gen AI. When we
    delve into those domain-specific use cases, we probably can better understand
    the benefit of foundation models.
  topic: strategy
- impact_reason: Lists the key high-value application areas where foundation models
    are expected to revolutionize R&D workflows.
  relevance_score: 8
  source: llm_enhanced
  text: enabling teams to accelerate molecule design, digital twin creation, and protocol
    optimization through deeper automation and improved data quality.
  topic: predictions
- impact_reason: 'Explains the core mechanism driving current AI success: leveraging
    massive, diverse, generic pre-training data within the Transformer framework.'
  relevance_score: 8
  source: llm_enhanced
  text: So the difference is that we see the benefits of putting a lot of the generic
    popping data like text images into those transformers so we can do, you know,
    so many downstream tasks.
  topic: technical
- impact_reason: 'Offers a clear, accessible explanation of what models like BERT
    produce: high-quality, abstracted feature representations (embeddings) that boost
    traditional ML models.'
  relevance_score: 8
  source: llm_enhanced
  text: So the outcome is some representations or embeddings of those text tokens,
    etc. So those embeddings are token representations they have predictive power.
    So you can think them as more advanced abstracted features to your machine learning
    models.
  topic: technical
- impact_reason: 'Summarizes the organizational requirement for successful AI implementation
    in complex fields: mandatory cross-disciplinary teamwork (AI experts + domain
    experts).'
  relevance_score: 8
  source: llm_enhanced
  text: So, yeah, so it's basically a cross-discipline collaboration, whether in house
    or externally, yeah.
  topic: strategy
- impact_reason: Points out the inherent advantage NLP/language-based domains have
    in adopting current GenAI paradigms compared to domains requiring more complex,
    non-sequential data representations.
  relevance_score: 8
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, bait, which is more language-based than anything else versus, you
    know, other domains of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: Identifies organizational culture and alignment across diverse stakeholders
    (scientists, modelers, leadership) as a major hurdle to effective AI adoption.
  relevance_score: 8
  source: llm_enhanced
  text: So, there's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: 'Highlights the communication gap: technical teams must translate
    model capabilities into actionable insights that domain scientists can validate,
    requiring integration, not just deployment.'
  relevance_score: 8
  source: llm_enhanced
  text: how do we digest the models, the capabilities? And again, we have to plug
    in, you know, the right scientists to help us together validate those.
  topic: business
- impact_reason: This suggests a consensus or inflection point where Generative AI
    has moved from experimental to being integrated or recognized as a standard technology.
  relevance_score: 8
  source: llm_enhanced
  text: A generative AI seems to have just found its place
  topic: Technology Trend
- impact_reason: Reiterates that the underlying technical commonality across diverse
    foundation models (text, image, molecular) is the Transformer architecture, emphasizing
    its foundational importance.
  relevance_score: 7
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Sets the agenda to discuss the mechanics and deployment of GenAI/Foundation
    Models, signaling a move from high-level concepts to practical application.
  relevance_score: 7
  source: llm_enhanced
  text: Maybe we can give some more detailed examples about how gen AI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right?
  topic: strategy
- impact_reason: Provides historical context, showing that domain-specific NLP applications
    (using BERT) predated the current GenAI wave (GPT) and already delivered significant
    efficiency gains in specific tasks like information extraction.
  relevance_score: 7
  source: llm_enhanced
  text: So as the time that was before the chat GPT. Right. Yeah, before that, there's
    also another language model called BERT. So we, you know, applied those kind of
    models into a clinical track documents so that we found a lot of efficiency gain,
    accuracy gain in extracting information.
  topic: technical/history
- impact_reason: Identifies data availability and quality as the next critical bottleneck/focus
    area after establishing the architecture.
  relevance_score: 7
  source: llm_enhanced
  text: Now the next question is about the data, right? What are the data? I gave
    some example, like all the sales from the, you know, the cell atlas. So those
    things, they could be fed into those models.
  topic: business/strategy
- impact_reason: Emphasizes the importance of historical context in understanding
    current AI capabilities and limitations, which is crucial for realistic deployment.
  relevance_score: 7
  source: llm_enhanced
  text: who the subject matter experts to have some awareness of this history as well.
  topic: Strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Building AI-Ready Cultures in Life Sciences
  R&D


  This 30-minute episode of the AI and Business Podcast, featuring **Zhong Liu, Director
  of Data Science in AI at Novartis**, focuses on the paradigm shift in Life Sciences
  R&D driven by **Generative AI (GenAI)** and **Foundation Models (FMs)**, and the
  cultural and technical challenges of scaling these technologies beyond pilot projects.


  ---


  ### 1. Focus Area

  The discussion centers on the transition from traditional, task-specific Machine
  Learning (ML) to leveraging large-scale Foundation Models in drug discovery, clinical
  workflows, and protocol design within Life Sciences R&D. Key themes include the
  architectural shift in AI, data infrastructure requirements, domain-specific application
  of FMs (e.g., molecular design, digital twins), and the critical need for cultural
  alignment between AI teams and domain scientists.


  ### 2. Key Technical Insights

  *   **Paradigm Shift to Foundation Models:** The evolution moved from supervised/deep
  learning requiring proprietary, labeled datasets to leveraging massive, pre-trained
  transformer architectures (FMs). These FMs capture broad domain knowledge, allowing
  for effective downstream tasks even with limited, specific data (transfer learning/fine-tuning).

  *   **Generative Capabilities vs. Representation:** The shift from models like BERT
  (focused on text representation/embeddings) to GPT-like models (leveraging the decoder
  architecture) enables the *generation* of new data types, such as novel molecular
  structures (represented by SMILES strings or graphs) constrained by desired properties
  (e.g., toxicity, solubility).

  *   **Domain-Specific Fine-Tuning:** While FMs provide general biological context
  (e.g., gene expression data from cell atlases), achieving high accuracy for specific
  indications (like lung cancer pathways) requires **fine-tuning** the FM using proprietary,
  high-quality domain data to adjust weights for personalized prediction tasks.


  ### 3. Business/Investment Angle

  *   **Scaling Bottleneck:** Despite high investment interest (73% of leaders investing),
  scaling GenAI beyond pilots remains a major hurdle (<20% success), often due to
  cultural misalignment and evaluation difficulties.

  *   **Data Dependency vs. Innovation Pace:** While AI architectures advance rapidly,
  the pace of generating high-quality, experimental data (like single-cell RNA-seq)
  remains a limiting factor for fully realizing FM potential in R&D.

  *   **ROI and Evaluation:** A critical business challenge is establishing domain-driven
  evaluation standards and objective metrics to measure the ROI of AI-driven research,
  especially given the risk of model **hallucination** in complex biochemical outputs
  that cannot be instantly validated.


  ### 4. Notable Companies/People

  *   **Zhong Liu (Novartis):** Guest expert, Director of Data Science in AI, providing
  insights from a major pharmaceutical company on practical implementation and cultural
  readiness.

  *   **Novartis:** The context for discussing real-world application of GenAI in
  drug development workflows.

  *   **Google (Transformer Architecture):** Mentioned as the originator of the foundational
  architecture underpinning modern FMs.

  *   **Deloitte:** Referenced for a survey highlighting the gap between GenAI investment
  and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is moving toward a hybrid development model, leveraging public FM advancements
  while building proprietary, domain-specific versions. The future success of AI adoption
  hinges less on the availability of the architecture and more on **cross-disciplinary
  collaboration** to ensure effective communication, data quality, and the development
  of robust, domain-specific **benchmarks** to validate model outputs before costly
  experimental testing. The concept of patient data interacting with FMs is likened
  to creating **digital twins** for in-silico testing.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Leaders** and **Life Sciences R&D
  Executives** responsible for strategy, investment, and deployment of advanced analytics.
  It is also relevant for **Data Scientists** and **Domain Experts (Biologists/Chemists)**
  who need to understand the new paradigm and foster alignment with AI development
  teams.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 119
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 10
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 19:24:35 UTC -->
