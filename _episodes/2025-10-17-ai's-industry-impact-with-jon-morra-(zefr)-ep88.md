---
companies:
- category: unknown
  confidence: medium
  context: lp understand this content better. Hi, my name is Dmitri Benicci, and I'm
    a content creator, agency owner, and AI
  name: Dmitri Benicci
  position: 367
- category: unknown
  confidence: medium
  context: owner, and AI enthusiast. You're listening to the AI Agents podcast brought
    to you by Jotform and featuring o
  name: AI Agents
  position: 467
- category: unknown
  confidence: medium
  context: tform and featuring our very own CEO and founder, Ida Kintank. This is
    the show where artificial intelligence m
  name: Ida Kintank
  position: 555
- category: unknown
  confidence: medium
  context: is episode, we have a very interesting guest, the Chief AI Officer, John
    Mora, who's coming today here from Zephyr.
  name: Chief AI Officer
  position: 829
- category: unknown
  confidence: medium
  context: e a very interesting guest, the Chief AI Officer, John Mora, who's coming
    today here from Zephyr. How are you
  name: John Mora
  position: 847
- category: unknown
  confidence: medium
  context: before this role, I was actually the Director of Data Science at eHarmony,
    which back in 2010 was a very popula
  name: Data Science
  position: 1865
- category: unknown
  confidence: medium
  context: ess doing machine learning in radiation oncology. So I was very early to
    the game. And then I studied ma
  name: So I
  position: 2177
- category: unknown
  confidence: medium
  context: n around since 2009. It was originally founded as Movie Clips, which was
    one of the biggest YouTube channels in
  name: Movie Clips
  position: 2447
- category: unknown
  confidence: medium
  context: take it down?" Right? So there's a system called Content ID on YouTube
    that they were formative in using a lo
  name: Content ID
  position: 3337
- category: tech
  confidence: high
  context: re the first people to do brand safety on TikTok. Meta came to us and asked
    us the same thing shortly th
  name: Meta
  position: 4758
- category: unknown
  confidence: medium
  context: right content, primarily on YouTube. Yeah, okay. And I think that's going
    to be a continuously issue for
  name: And I
  position: 5300
- category: unknown
  confidence: medium
  context: saw that as of this morning, maybe I'm not wrong. GPT Sora 2 just got announced,
    the new—did you hear about
  name: GPT Sora
  position: 5584
- category: unknown
  confidence: medium
  context: s like in comparison to what you're doing, right? Because I don't know.
    It's weird to me that people can make
  name: Because I
  position: 6017
- category: unknown
  confidence: medium
  context: h IP, you know, of like, like, people making like Star Wars stuff. And
    what do you think about that whole sit
  name: Star Wars
  position: 6155
- category: tech
  confidence: high
  context: very, very large models of anything—Gemini, GPT, Anthropic, that kind of
    stuff—and how do we distill down wh
  name: Anthropic
  position: 9405
- category: unknown
  confidence: medium
  context: at. But what about somebody committing a crime in Grand Theft Auto? Is
    that crime content?" Well, maybe, exactly. Ma
  name: Grand Theft Auto
  position: 12427
- category: unknown
  confidence: medium
  context: the training data necessary to train our models. But I was used to joke
    with my CTO, you know, years ago
  name: But I
  position: 13016
- category: tech
  confidence: high
  context: ally, YouTube would just come and say—and this is Google, and this is not
    only just Google, this is all pl
  name: Google
  position: 14775
- category: unknown
  confidence: medium
  context: cognize is that social media is different, right? A TikTok video is very
    different than a web page, you know
  name: A TikTok
  position: 18818
- category: unknown
  confidence: medium
  context: ou know, a YouTube video is different, and then a Meta Reels or whatever
    it is also different. So we built dif
  name: Meta Reels
  position: 18919
- category: unknown
  confidence: medium
  context: as an organization that is now defined called the Global Alliance for Responsible
    Media, where set out different ca
  name: Global Alliance
  position: 20161
- category: unknown
  confidence: medium
  context: hat is now defined called the Global Alliance for Responsible Media, where
    set out different categorizations of ways
  name: Responsible Media
  position: 20181
- category: unknown
  confidence: medium
  context: instance, Meta has, like, photo galleries, right? That TikTok—well, TikTok
    has photo galleries as well—that You
  name: That TikTok
  position: 23388
- category: unknown
  confidence: medium
  context: far as the type of content, it is quite similar. So YouTube has Shorts
    that competes with TikTok. Meta has Re
  name: So YouTube
  position: 23583
- category: unknown
  confidence: medium
  context: t being posted on different platforms, with it be Instagram Reels and,
    you know, I guess Facebook has its own Short
  name: Instagram Reels
  position: 24046
- category: tech
  confidence: high
  context: with it be Instagram Reels and, you know, I guess Facebook has its own
    Shorts kind of function out. TikTok i
  name: Facebook
  position: 24085
- category: unknown
  confidence: medium
  context: never work." So they integrated with the IFCN, or International Fact-Checking
    Network, and pulled down a whole bunch o
  name: International Fact
  position: 26519
- category: unknown
  confidence: medium
  context: y integrated with the IFCN, or International Fact-Checking Network, and
    pulled down a whole bunch of facts. And so t
  name: Checking Network
  position: 26538
- category: unknown
  confidence: medium
  context: o, you know, we do some—we have some customers in Southeast Asia, and unfortunately,
    Thailand went through a war r
  name: Southeast Asia
  position: 27655
- category: unknown
  confidence: medium
  context: this, and I heard of a burgeoning industry, like AI SEO or whatever you
    call it, where brands are trying
  name: AI SEO
  position: 31381
- category: unknown
  confidence: medium
  context: Yes. So, the architecture we're talking about is Retrieval Augmented Generation.
    Absolutely. LLMs have a knowledge cutoff date wh
  name: Retrieval Augmented Generation
  position: 32067
- category: unknown
  confidence: medium
  context: do not—like, that kind of stuff, do not trust the Google AI summary overview
    because it's just like, okay, yo
  name: Google AI
  position: 33300
- category: unknown
  confidence: medium
  context: ly. I was actually reading an interesting post by Andrew Ng recently, and
    something I totally agree with, and
  name: Andrew Ng
  position: 35233
- category: ai_application
  confidence: high
  context: The podcast, 'AI Agents podcast,' is brought to you by Jotform, suggesting
    they are a company utilizing or promoting AI tools for productivity/work.
  name: Jotform
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The guest's company, where John Mora is the Chief AI Officer. They specialize
    in large-scale content classification, brand safety verification, and provenance
    detection using ML/AI.
  name: Zephyr
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The previous employer of the guest, where he led data science teams focusing
    on algorithmic matchmaking, fraud modeling, and churn modeling (early ML application).
  name: eHarmony
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: A system on YouTube (owned by Google) used by Zephyr's predecessor to identify
    and manage copyrighted content, involving early classification/matching technology.
  name: Content ID on YouTube
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A platform that contracted Zephyr for brand safety verification services,
    requiring large-scale content classification.
  name: TikTok
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: A platform that contracted Zephyr for brand safety verification services
    shortly after TikTok.
  name: Meta
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: A platform where Zephyr provides brand safety services and which was the
    subject of the 2017 'ad apocalypse' regarding content monetization.
  name: YouTube
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Mentioned as breaking news, this refers to a new generative model (likely
    OpenAI's Sora successor, though the speaker is unsure of the exact developer),
    indicating cutting-edge generative AI.
  name: GPT Sora 2
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as an example of a very large model (developed by Google) whose
    knowledge Zephyr is attempting to distill for cost-effective inference.
  name: Gemini
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an example of a very large model (likely referring to OpenAI's
    GPT series) whose knowledge Zephyr is attempting to distill.
  name: GPT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an example of a very large model developer whose models Zephyr
    is analyzing for distillation purposes.
  name: Anthropic
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The owner of YouTube, referenced in the context of the 2017 ad apocalypse
    and the operation of platforms generally.
  name: Google
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: A small Israeli startup acquired by the speaker's company in '22 that specialized
    in misinformation detection.
  name: AdVerify
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced indirectly regarding general LLM search functions and the percentage
    of source data it relies on.
  name: ChatGPT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The network that AdVerify integrated with to pull down facts used for training
    misinformation models.
  name: IFCN (International Fact-Checking Network)
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned because Google struck a lucrative deal for training data from
    the platform to feed LLMs.
  name: Reddit
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned specifically as a social media platform the speaker's company
    does not currently work with.
  name: X
  source: llm_enhanced
- category: ai_researcher
  confidence: high
  context: Mentioned as the source of a recent post/opinion regarding the shift in
    AI development towards agentic systems rather than just larger models.
  name: Andrew Ng
  source: llm_enhanced
date: 2025-10-17 13:27:00 +0000
duration: 42
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: 'work. Enjoy the show. Hello, and welcome back to another episode of
    the AI Agents podcast. In this episode, we have a very interesting guest, the
    Chief AI Officer, John Mora, who''s coming today here from Zephyr. How are you
    doing, John? I''m great. Thanks for having me. We''re excited to chat. Usually,
    it''s interesting. Sometimes there are companies that, oh, they are an AI agent-first
    company. There are a lot of different ways that people can get into AI as companies,
    and I think with Zephyr, it''s going to be a pretty interesting and unique instance
    that we haven''t quite had on the show before. So just to kick things off, tell
    us a little bit about yourself, how you got into AI yourself, and then get into
    the journey of Zephyr''s forte, not forte, Zephyr getting into AI. Sure. So as
    you said, I''m the Chief AI Officer here at Zephyr. I''ve been here about nine
    years. In my time here, I''ve led machine learning teams, data science teams,
    content policy teams, product teams. I''ve written a lot of machine learning and
    AI software myself, so I''ve been through the gamut here at the company. Just
    a little bit about myself: so before this role, I was actually the Director of
    Data Science at eHarmony, which back in 2010 was a very popular internet dating
    brand, I think less so now. And in my time there, I was in charge of algorithmic
    matchmaking, dynamic pricing, fraud modeling, and churn modeling. And before that,
    I had my own business doing machine learning in radiation oncology. So I was very
    early to the game. And then I studied machine learning and brain MRI for my dissertation
    work. I''ve been doing machine learning for a number of decades now. A little
    bit about the company: so Zephyr has been around since 2009. It was originally
    founded as Movie Clips, which was one of the biggest YouTube channels in the day.
    And if you watch clips of movies, you might have watched Movie Clips. It''s still
    a channel right now. And so the founders were these two guys, Rich and Zach. And
    what they realized'
  text: 'the future of work. Enjoy the show. Hello, and welcome back to another episode
    of the AI Agents podcast. In this episode, we have a very interesting guest, the
    Chief AI Officer, John Mora, who''s coming today here from Zephyr. How are you
    doing, John? I''m great. Thanks for having me. We''re excited to chat. Usually,
    it''s interesting. Sometimes there are companies that, oh, they are an AI agent-first
    company. There are a lot of different ways that people can get into AI as companies,
    and I think with Zephyr, it''s going to be a pretty interesting and unique instance
    that we haven''t quite had on the show before. So just to kick things off, tell
    us a little bit about yourself, how you got into AI yourself, and then get into
    the journey of Zephyr''s forte, not forte, Zephyr getting into AI. Sure. So as
    you said, I''m the Chief AI Officer here at Zephyr. I''ve been here about nine
    years. In my time here, I''ve led machine learning teams, data science teams,
    content policy teams, product teams. I''ve written a lot of machine learning and
    AI software myself, so I''ve been through the gamut here at the company. Just
    a little bit about myself: so before this role, I was actually the Director of
    Data Science at eHarmony, which back in 2010 was a very popular internet dating
    brand, I think less so now. And in my time there, I was in charge of algorithmic
    matchmaking, dynamic pricing, fraud modeling, and churn modeling. And before that,
    I had my own business doing machine learning in radiation oncology. So I was very
    early to the game. And then I studied machine learning and brain MRI for my dissertation
    work. I''ve been doing machine learning for a number of decades now. A little
    bit about the company: so Zephyr has been around since 2009. It was originally
    founded as Movie Clips, which was one of the biggest YouTube channels in the day.
    And if you watch clips of movies, you might have watched Movie Clips. It''s still
    a channel right now. And so the founders were these two guys, Rich and Zach. And
    what they realized is that if they worked with movie studios, that they could
    license their official content and make official movie clips.'
  type: prediction
- actionable: false
  confidence: medium
  extracted: that
  text: the problem with that is that social media content is always weirder than
    you think.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/a780c36c74cd47ad9c105662f882c74f/
processing_date: 2025-10-17 23:12:36 +0000
quotes:
- length: 143
  relevance_score: 5
  text: So from a very high level, you start with both how much you know, what's your
    cost envelope is, and how many inferences you have to run per day
  topics: []
- length: 119
  relevance_score: 3
  text: This is the show where artificial intelligence meets innovation, productivity,
    and the tools shaping the future of work
  topics: []
- length: 98
  relevance_score: 3
  text: It was originally founded as Movie Clips, which was one of the biggest YouTube
    channels in the day
  topics: []
- length: 229
  relevance_score: 3
  text: And what we found with our customers is that they tend to agree that yes,
    brand safety is super important in social media, and they work with us on social
    media platforms, even if they work with competitors on all their platforms
  topics: []
- length: 291
  relevance_score: 3
  text: And so what we found that our value add is being able to come to the brands
    with a more well-defined recipe and say, like, "Hey, when you're talking about
    crime or you're talking about whatever, here are the things that actually matter,"
    and they matter because we're experts in social media
  topics: []
- length: 91
  relevance_score: 3
  text: '" You have to actually do work and find facts that support whether something
    is true or not'
  topics: []
- length: 72
  relevance_score: 3
  text: I know that Google struck a lucrative deal for training data from Reddit
  topics: []
- impact_reason: 'Defines the core operational challenge for modern AI companies:
    balancing massive scale (hundreds of millions of inferences) with cost efficiency
    and accuracy.'
  relevance_score: 10
  source: llm_enhanced
  text: Our bread and butter is very, very large-scale classification, right? So we
    run hundreds of millions of inferences a day, and we have to do that cost-effectively.
    We have to do that accurately for our customers.
  topic: technical/business
- impact_reason: 'This is a key technical strategy: leveraging the power of frontier
    LLMs (GPT, Gemini) and then distilling their knowledge into smaller, cost-effective
    models for high-volume inference.'
  relevance_score: 10
  source: llm_enhanced
  text: A lot of what our cutting-edge research is focused on is the using of very,
    very large models of anything—Gemini, GPT, Anthropic, that kind of stuff—and how
    do we distill down what they know to be most effective for our customers so that
    it meets all abilities?
  topic: technical/AI trends
- impact_reason: 'Explains the practical reality of MLOps at scale: using a heterogeneous
    fleet of models (different sizes/parameter counts) matched to specific GPU hardware
    for optimal cost/performance.'
  relevance_score: 10
  source: llm_enhanced
  text: And then what you do is you right-size your model in order to do that. So
    we have a variety of different models that we have deployed of all different parameter
    counts amongst a variety of different types of GPUs.
  topic: technical/deployment
- impact_reason: 'Describes a sophisticated, cost-saving inference pipeline architecture:
    a tiered system where cheap models filter and only pass uncertain or high-value
    cases to expensive models or human reviewers (cascading/promotion strategy).'
  relevance_score: 10
  source: llm_enhanced
  text: And then what the smaller models are able to do sometimes is they're able
    to promote. So if a smaller model says there's something interesting in this content,
    but I'm not really sure what it is, it can promote it to a higher tier where those
    higher tiers can be more expensive models. They can include human review.
  topic: technical/deployment
- impact_reason: A strong argument for the necessity of AI/ML in content moderation,
    positioning it as the only viable solution balancing cost and necessary quality/nuance
    where human review fails on scale and keyword matching fails on accuracy.
  relevance_score: 10
  source: llm_enhanced
  text: So it just turns out that human review is not scalable. There are other much
    cheaper options you could do. You could do all keyword matches and just say, "Hey,
    here's all the keywords that matter," and if it has one of these keywords, then
    great. And if that worked, which it doesn't, then we do that because it's cheaper.
    So it turns out that AI is the only solution that has both the cost and the quality
    that you need in order to deliver the solution.
  topic: AI necessity/business case
- impact_reason: 'Crucially differentiates misinformation from standard safety/suitability
    issues: Misinformation detection often breaks the traditional model of clear,
    rule-based policy application.'
  relevance_score: 10
  source: llm_enhanced
  text: we kind of look at all other classifications as when you have a well-written
    policy and a piece of content, a person who knows the policy can apply it to the
    content and say, "Yes, you know, it matches." That's not true. Misinformation.
  topic: safety/AI limitation
- impact_reason: Describes a practical, scalable architecture for dynamic misinformation
    detection using continuous fact ingestion and model retraining based on external
    consensus (IFCN).
  relevance_score: 10
  source: llm_enhanced
  text: So today, that's still how this process works is we have a—we have facts that
    are pulled in all the time, and then we retrain our models given new facts regularly,
    and we deploy those models in order to find posts that are not true according
    to the consensus amongst our fact-checking partners.
  topic: technical/safety
- impact_reason: Provides a clear, concise explanation of RAG architecture and its
    necessity due to the inherent knowledge cutoff limitation of foundational LLMs.
  relevance_score: 10
  source: llm_enhanced
  text: The architecture we're talking about is Retrieval Augmented Generation. Absolutely.
    LLMs have a knowledge cutoff date where they don't know anything about the future
    relative to whenever they were trained, and in order to know anything about the
    future, they need to reach out to some third-party source and get them.
  topic: technical
- impact_reason: 'Echoes a major industry trend: the immediate focus is shifting from
    scaling model size (compute) to building agentic workflows that utilize existing
    models effectively.'
  relevance_score: 10
  source: llm_enhanced
  text: He says that while a lot of the short-term development today in AI is not
    going to be as much larger and larger models, but it's all agentic. So it's how
    do we take this AI and get access to this information, these tools to perform
    this task?
  topic: predictions/technical
- impact_reason: 'Identifies the next major research frontier beyond current capabilities:
    overcoming the static knowledge cutoff via ''forgetfulness and lifelong learning''
    to achieve true human-like continuous adaptation.'
  relevance_score: 10
  source: llm_enhanced
  text: I think some of the long-term stuff is where it gets really interesting, right?
    As these models do get bigger, they do get better, they can accomplish more, and
    they get more human-like. Because right now, I don't—like, yes, they are very,
    very, very good at emulating humans, but I think the next frontier for research
    is actually forgetfulness and lifelong learning.
  topic: technical/predictions
- impact_reason: Identifies a critical current limitation of LLMs (static knowledge
    cutoff) and points to the next major research hurdle.
  relevance_score: 10
  source: llm_enhanced
  text: I think the next frontier for research is actually forgetfulness and lifelong
    learning.
  topic: Technical/Research Frontier
- impact_reason: 'Highlights a critical, unresolved business and policy challenge
    in the age of generative AI: differentiating between benign and harmful AI content
    and understanding customer tolerance for it.'
  relevance_score: 9
  source: llm_enhanced
  text: What's still an open question for us is how do we talk about AI-generated
    content that's more innocuous or more spammy, and what do our customers think
    about that?
  topic: safety/business
- impact_reason: Details specific technical approaches (provenance detection, unsupervised
    clustering) being used to tackle the emerging problem of AI-generated content
    identification.
  relevance_score: 9
  source: llm_enhanced
  text: So we're working a lot on provenance detection right now and then unsupervised
    clustering problems to help understand this content better.
  topic: technical/AI trends
- impact_reason: 'Pinpoints a major inflection point in social media monetization:
    the necessity of third-party verification services to secure large brand advertising
    budgets.'
  relevance_score: 9
  source: llm_enhanced
  text: TikTok came to us and said, 'Hey, we're having a problem. A lot of big brands
    don't trust our content for a bunch of reasons. Can you come in and perform a
    service called verification for brand safety?'
  topic: business/predictions
- impact_reason: 'Summarizes the constant pressure points in the AI service industry:
    the ''Iron Triangle'' of AI service delivery—cost, accuracy (fidelity), and scope
    (classes).'
  relevance_score: 9
  source: llm_enhanced
  text: What our customers are demanding is they are demanding lower costs, they're
    demanding higher fidelity, they're demanding more classes, right?
  topic: business
- impact_reason: 'Articulates the strategic benefit of the tiered model approach:
    maximizing the efficiency of both human expertise and expensive compute resources.'
  relevance_score: 9
  source: llm_enhanced
  text: This tier promotion strategy allows us to make sure that we focus our resources—and
    I mean that in terms of both human capital and compute capital—most effectively.
  topic: strategy/business
- impact_reason: A powerful statement summarizing the necessity of automation/AI in
    handling complex, nuanced policy enforcement that manual labor cannot scale to
    meet.
  relevance_score: 9
  source: llm_enhanced
  text: We don't do it because we want to, we do it because we have to.
  topic: strategy/business
- impact_reason: Provides a perfect, relatable example of the ambiguity inherent in
    content policy, demonstrating why simple keyword matching fails and advanced classification
    is required.
  relevance_score: 9
  source: llm_enhanced
  text: We have this nuanced policy... 'We don't want to be around crime content.'
    And you say, 'Okay, that's great. But what about somebody committing a crime in
    Grand Theft Auto? Is that crime content?'
  topic: safety/strategy
- impact_reason: A definitive statement on the fundamental limitation of human labor
    in the face of modern internet scale, necessitating AI solutions.
  relevance_score: 9
  source: llm_enhanced
  text: And so it just turns out that human review is not scalable.
  topic: strategy/predictions
- impact_reason: Perfectly illustrates the core challenge of nuanced policy definition
    in content moderation, where context (like a video game) drastically changes interpretation.
  relevance_score: 9
  source: llm_enhanced
  text: A customer says, "We don't want to be around crime content." And you say,
    "Okay, that's great. I understand that. But what about somebody committing a crime
    in Grand Theft Auto? Is that crime content?"
  topic: safety/policy
- impact_reason: Details the multimodal approach required for modern brand suitability
    solutions, acknowledging that text alone is insufficient on visual platforms.
  relevance_score: 9
  source: llm_enhanced
  text: we built differentiating technology that was focused a lot on understanding
    imagery, audio, video, and text as well, because that's also important in order
    to deliver brand suitability as a product on social media.
  topic: technical/AI application
- impact_reason: Clearly distinguishes between 'brand safety' (content that shouldn't
    exist) and 'brand suitability' (contextual appropriateness for a specific advertiser).
  relevance_score: 9
  source: llm_enhanced
  text: But the brand suitability framework is all about what's right for brand. This
    goes back to that crime example we talked about before, right? Is somebody committing
    a crime in Grand Theft Auto considered unsuitable for a brand? Maybe, maybe not,
    right?
  topic: safety/policy
- impact_reason: A powerful, relatable summary of the complexity and unpredictability
    of user-generated content on modern social platforms.
  relevance_score: 9
  source: llm_enhanced
  text: Social media content is always weirder than you think. No matter what you
    think it is, it's way, way weirder and in different languages and different subtext
    and like everything you can imagine, right? Emojis and whatever.
  topic: strategy/content analysis
- impact_reason: 'Highlights the fundamental challenge of misinformation detection:
    it requires external factual grounding, not just binary classification, which
    is a key insight for building reliable content moderation systems.'
  relevance_score: 9
  source: llm_enhanced
  text: So, in order to get misinformation right, you can't just say, "Is this true
    or false?" You have to actually do work and find facts that support whether something
    is true or not.
  topic: safety/technical
- impact_reason: A critical statement on the scalability limitations of human moderation
    for massive platforms, justifying the necessity of AI/ML solutions in content
    safety.
  relevance_score: 9
  source: llm_enhanced
  text: We cannot scale journalists at social media scale. We cannot have a person
    review every piece of content to say whether it's true or not. That just will
    never work.
  topic: business/strategy
- impact_reason: Directly links the need for real-time situational awareness (e.g.,
    during breaking news) to the emerging capability of AI agents.
  relevance_score: 9
  source: llm_enhanced
  text: Enter agents. This is exactly where agents should enter the conversation because
    this is what our customers are asking.
  topic: technical/predictions
- impact_reason: 'A crucial business and product insight: the value of AI systems
    lies not just in data absorption, but in intelligent filtering and prioritization
    (signal vs. noise).'
  relevance_score: 9
  source: llm_enhanced
  text: What's actually tough is differentiating the signal from the noise, like what
    really matters to our customers, what really matters online. That's what's important
    because if we just bombard everyone with, "Oh, do you know this happened and this
    happened and this happened," we're not providing value.
  topic: business/strategy
- impact_reason: 'Identifies a new, emerging business focus area: optimizing content
    strategy based on how specific LLMs index and prioritize information (AI SEO).'
  relevance_score: 9
  source: llm_enhanced
  text: I have heard of a burgeoning industry, like AI SEO or whatever you call it,
    where brands are trying to figure out how different models think about them relative
    to their competition.
  topic: business/strategy
- impact_reason: A strong cautionary warning about relying on LLM summaries for deep,
    nuanced, or historical knowledge, emphasizing the need for deep context that RAG
    summaries often miss.
  relevance_score: 9
  source: llm_enhanced
  text: If you're looking for the truth about what happened around some event, yeah.
    An ideal. Yeah, if it requires, yeah, the large levels of journalism and or historical
    accounts. I've read a fair amount of heavy literature on history, philosophy,
    whatever. It's so funny. Like, do not—like, that kind of stuff, do not trust the
    Google AI summary overview because it's just like, okay, you've got to have read
    hundreds of pages of context of what people mean by words sometimes for stuff.
  topic: safety/limitations
- impact_reason: Draws a clear line between the current state (augmentation) and the
    future state (replacement) of knowledge work, contingent on solving the lifelong
    learning problem.
  relevance_score: 9
  source: llm_enhanced
  text: So once we discover that [lifelong learning], then I think it'll open a whole
    new can of actually replacing knowledge workers and not augmenting knowledge workers,
    but replacing them. But that still could be years off.
  topic: predictions/safety
- impact_reason: Provides a specific, actionable assessment of which job roles (repeatable
    tasks, tool-based knowledge work) are most vulnerable to near-term displacement
    by AI agents.
  relevance_score: 9
  source: llm_enhanced
  text: In the interim, what I think is the most at risk is those activities which
    are repeatable, which have just a set of tools that people can do, and they are
    very much, you know, knowledge workers, whether that's call center, whether that's
    software development, whether that's UI, whatever the case may be. Those are the
    ones that are most at risk because AI agents are going to be efficient at it.
  topic: predictions/strategy
- impact_reason: Clearly articulates the fundamental difference between current AI
    and human cognition regarding continuous learning and adaptation.
  relevance_score: 9
  source: llm_enhanced
  text: So these models have a knowledge cutoff date. People don't have a knowledge
    cutoff date. I'm always learning, you're always learning, and we're always changing
    our representation of the world. We don't know how to do that yet with LLMs.
  topic: AI Limitations
- impact_reason: Offers a concrete, near-term prediction on which job categories (repeatable,
    tool-based knowledge work) are most vulnerable to current AI capabilities.
  relevance_score: 9
  source: llm_enhanced
  text: So in the interim, what I think is the most at risk is those activities which
    are repeatable, which have just a set of tools that people can do, and they are
    very much, you know, knowledge workers, whether that's call center, whether that's
    software development, whether that's UI, whatever the case may be.
  topic: Business Impact/Job Displacement
- impact_reason: A cautionary warning specifically aimed at entry-level knowledge
    workers regarding near-term job market disruption.
  relevance_score: 9
  source: llm_enhanced
  text: I think, you know, there's a lot of people right now that are working at jobs,
    especially from an associate and entry-level standpoint, that, um, I think are
    fortunate to be at that position now because I do think in a couple of years,
    five, probably maybe less, I don't know, there will be some issues, I think.
  topic: Safety/Societal Concern
- impact_reason: Illustrates a successful business model pivot from transactional
    fees to performance-based revenue sharing, a key strategic insight for service
    providers.
  relevance_score: 8
  source: llm_enhanced
  text: We ended up making more money by doing a revenue share... and then all future
    advertising revenue on that video would be split between Zephyr and the movie
    studio, and we ended up making more money on that.
  topic: business/strategy
- impact_reason: Demonstrates the rapid adoption and critical need for brand safety
    solutions across major platforms, establishing a high barrier to entry for competitors.
  relevance_score: 8
  source: llm_enhanced
  text: We won. We were the first people to do brand safety on TikTok. Meta came to
    us and asked us the same thing shortly thereafter...
  topic: business/strategy
- impact_reason: Frames human review not as a simple labor pool, but as a complex
    queuing problem where prioritization must be based on business impact, a key operational
    insight.
  relevance_score: 8
  source: llm_enhanced
  text: How do we enqueue the work for them to do that's most impactful for the business,
    where impactful has a lot of different meanings?
  topic: strategy/business
- impact_reason: 'Explains the historical necessity of human labeling/outsourcing:
    it was the primary method for generating the ground truth data required for supervised
    machine learning models before synthetic data or LLM distillation became viable.'
  relevance_score: 8
  source: llm_enhanced
  text: And in fact, we used to do a lot more human review via outsourcing, right?
    And this was all pre-LLMs. And the reason we did it is because that's how we gathered
    the training data necessary to train our models.
  topic: technical/history
- impact_reason: 'A pragmatic, results-oriented view on model deployment: accuracy
    and provability trump the specific underlying technology, provided the performance
    threshold is met.'
  relevance_score: 8
  source: llm_enhanced
  text: I was used to joke with my CTO, you know, years ago, that it doesn't really
    matter how we do it, as long as we can prove to ourselves and our customers we
    get the right answer enough of the time.
  topic: strategy/business
- impact_reason: References a major historical event in digital advertising and content
    monetization that fundamentally shifted platform policies and created opportunities
    for verification vendors.
  relevance_score: 8
  source: llm_enhanced
  text: The ad apocalypse from 2016 on YouTube. Remember that? 2017?
  topic: business/history
- impact_reason: Defines the fundamental 'three-legged stool' balancing act required
    for success on major content platforms (viewers, creators, advertisers).
  relevance_score: 8
  source: llm_enhanced
  text: 'platforms want to be able to say, "Hey, we have three things that matter
    to us: we have the viewers, the people that watch the content; the creators that
    make the content; and the brands that pay for the content."'
  topic: strategy
- impact_reason: Emphasizes the long-term, often immeasurable, reputational risk advertisers
    face from adjacency issues, justifying the need for sophisticated safety tools.
  relevance_score: 8
  source: llm_enhanced
  text: I'm a brand that's 100 years old, 150 years old. I care a lot about my reputation,
    and I know that if my ad is next to unsavory content, that has brand effects that
    won't be measured over the course of a campaign or, you know, a year. It could
    take decades for it to measure.
  topic: business/safety
- impact_reason: 'Explains the value proposition of expert guidance: preventing brands
    from creating unmanageable, infinitely complex policies due to a lack of platform-specific
    knowledge.'
  relevance_score: 8
  source: llm_enhanced
  text: we guide them because if they describe their policy however they want, they're
    not experts in social media, and there will be an infinite number of corner cases.
  topic: business/strategy
- impact_reason: Emphasizes the critical requirement for localization and real-time
    context in AI monitoring systems, moving beyond global consensus to regional relevance.
  relevance_score: 8
  source: llm_enhanced
  text: Our customers are saying, "You know, something just happened, and not only
    something just happened, but something just happened in my part of the world."
  topic: business/strategy
- impact_reason: Distinguishes between questions where crowd consensus is valuable
    (sentiment, innocuous queries) and those requiring objective truth, providing
    a framework for evaluating LLM outputs.
  relevance_score: 8
  source: llm_enhanced
  text: There is all this study around the wisdom of the crowd. In fact, an academic
    paper I used to love to cite is that if you ask a bunch of people to answer kind
    of an innocuous question, the wisdom of the crowd really does hold. And I think
    that's true for a lot of questions that people interact with LLMs with. But again,
    when you're looking for truth, when you're looking for did something happen, it's
    not about the wisdom of the crowd.
  topic: strategy/limitations
- impact_reason: 'Provides clear entrepreneurial advice: target ''boring'' industries
    for high-impact automation opportunities using current AI capabilities.'
  relevance_score: 8
  source: llm_enhanced
  text: And how do we do that in all the boring industries, right? And that's where
    I think a lot of opportunities, if you're an entrepreneur, is bringing AI into
    a boring industry and automating away different parts of the jobs that don't need
    to be done anymore. There's data entry or QA or whatever, the customer service,
    whatever the case may be.
  topic: business/strategy
- impact_reason: Summarizes the current trend driving excitement and progress in LLMs.
  relevance_score: 8
  source: llm_enhanced
  text: As these models do get bigger, they do get better, they can accomplish more,
    and they get more human-like.
  topic: AI Technology Trends
- impact_reason: Directly links efficiency gains from AI agents to job risk.
  relevance_score: 8
  source: llm_enhanced
  text: Those are the ones that are most at risk because AI agents are going to be
    efficient at it.
  topic: Business Impact
- impact_reason: Provides a concrete overview of the diverse, high-value applications
    of classical machine learning in a consumer-facing business (dating) prior to
    the current LLM boom.
  relevance_score: 7
  source: llm_enhanced
  text: I was in charge of algorithmic matchmaking, dynamic pricing, fraud modeling,
    and churn modeling.
  topic: technical/business
- impact_reason: Historical context on how early digital rights management (DRM) systems,
    like Content ID, laid the groundwork for large-scale content classification and
    monetization.
  relevance_score: 7
  source: llm_enhanced
  text: So there's a system called Content ID on YouTube that they were formative
    in using a lot in the early days.
  topic: strategy/history
- impact_reason: Defines the strategic relationship between market demand, policy
    definition, and technical implementation in a regulatory/safety-focused business.
  relevance_score: 7
  source: llm_enhanced
  text: The ideas of policy flows from what the market needs. And once we've defined
    the policy, then we go and say, 'We have to implement that policy.'
  topic: strategy
- impact_reason: 'Provides a clear business strategy: specializing in the unique challenges
    of social media safety rather than trying to cover all digital advertising.'
  relevance_score: 7
  source: llm_enhanced
  text: we believe, at least in social media, that focusing on brand safety is a key
    differentiator.
  topic: business/strategy
- impact_reason: References the industry standard framework (GARM) used to define
    and categorize content risks for advertisers.
  relevance_score: 7
  source: llm_enhanced
  text: The Global Alliance for Responsible Media, where set out different categorizations
    of ways that brands can think about content.
  topic: safety/policy
- impact_reason: Identifies the 2024 election cycle as a major inflection point driving
    increased brand focus and investment in combating misinformation.
  relevance_score: 7
  source: llm_enhanced
  text: misinformation became a topic—I mean, it's always been a topic brands care
    about—but before the 2024 election, it really came into the forefront, and brands
    very much cared about this information.
  topic: safety/business
- impact_reason: Sets realistic expectations for the latency of knowledge acquisition
    in high-stakes, breaking news scenarios, even with advanced systems.
  relevance_score: 7
  source: llm_enhanced
  text: It's not possible that we would know the name of a shooter or that our model
    would know the name of a shooter the instant it happens. This is just infeasible.
  topic: safety/limitations
- impact_reason: Provides a necessary tempering of the previous prediction, suggesting
    the most transformative replacement phase is not immediate.
  relevance_score: 7
  source: llm_enhanced
  text: But that still could be years off.
  topic: Predictions/Timeline
- impact_reason: A candid observation about the counter-intuitive naming convention
    ('safety floor' implying the lowest acceptable standard) within the GARM framework.
  relevance_score: 6
  source: llm_enhanced
  text: And the GARM also came out with this idea of the brand safety floor, which
    don't ask me why that's the most degree just because you would think it would
    be the least, but never mind.
  topic: safety/policy
- impact_reason: Points out specific technical/format idiosyncrasies between platforms
    (e.g., photo galleries) that require platform-specific model adaptation.
  relevance_score: 6
  source: llm_enhanced
  text: So, for instance, Meta has, like, photo galleries, right? That TikTok—well,
    TikTok has photo galleries as well—that YouTube doesn't have, for instance, right?
  topic: technical/deployment
source: Unknown Source
summary: '## Podcast Episode Summary: AI''s Industry Impact with Jon Morra (Zefr)
  | EP88


  This episode of the AI Agents podcast features Jon Morra, Chief AI Officer at Zefr,
  discussing the company''s evolution from digital rights management to a leader in
  large-scale content classification, brand safety, and suitability, particularly
  in the context of rapidly advancing generative AI.


  The main narrative arc traces Zefr''s journey: starting with licensing official
  movie content (Movie Clips), moving into digital rights management (Content ID utilization),
  pivoting to advertising revenue sharing, and finally establishing its core business
  in **brand safety and suitability verification** for major social platforms (TikTok,
  Meta, YouTube). The conversation then pivots to the immediate challenges posed by
  generative AI content, such as deepfakes and unauthorized IP usage.


  ### 1. Focus Area

  The primary focus is the application of **large-scale machine learning and AI for
  content classification, brand safety, and suitability** across social media platforms.
  Secondary focus areas include the impact of generative AI (like Sora) on content
  provenance and the technical challenges of deploying cost-effective, high-fidelity
  classification models at massive scale.


  ### 2. Key Technical Insights

  *   **Model Distillation for Scale:** Zefr leverages massive LLMs (Gemini, GPT,
  Anthropic) but focuses on distilling their knowledge into smaller, more cost-effective
  models for high-volume, daily inferences, balancing cost envelopes with required
  accuracy.

  *   **Tiered Inference Strategy:** They employ a tiered promotion strategy where
  smaller, cheaper models handle initial classification. If uncertainty is high, content
  is promoted to more expensive models or human review, optimizing resource allocation
  (compute and human capital).

  *   **Provenance Detection & Unsupervised Clustering:** Due to the rise of AI-generated
  content, Zefr is actively researching provenance detection methods and utilizing
  unsupervised clustering to better categorize and understand the nature (spammy vs.
  innocuous) of synthetic media.


  ### 3. Business/Investment Angle

  *   **Necessity of AI in Content Moderation:** AI is deemed the *only* scalable
  solution that meets both the required quality and cost thresholds for complex, nuanced
  content policies (e.g., distinguishing between legitimate crime content and in-game
  violence).

  *   **Social Media Specialization:** Zefr differentiates itself by focusing deeply
  on the unique complexities of social media content (imagery, audio, video, text)
  rather than trying to verify every ad dollar across all platforms, which competitors
  often attempt.

  *   **Brand Suitability vs. Safety Floor:** The business revolves around defining
  brand suitability—moving beyond the "brand safety floor" (content that shouldn''t
  exist) to nuanced policies like whether an ad should appear next to specific types
  of contextual content (e.g., Grand Theft Auto gameplay).


  ### 4. Notable Companies/People

  *   **Jon Morra (Zefr):** Chief AI Officer, with a background spanning algorithmic
  matchmaking (eHarmony) and early machine learning in radiation oncology.

  *   **Zefr:** Company history detailed from Movie Clips to digital rights management
  to brand verification, notably being the first to provide brand safety verification
  on **TikTok** and subsequently **Meta**.

  *   **GARM (Global Alliance for Responsible Media):** Mentioned as the body that
  established standardized frameworks (taxonomy, low/medium/high risk) for brands
  to define content suitability.


  ### 5. Future Implications

  The industry is moving toward greater scrutiny of **content provenance** as generative
  AI floods platforms. Furthermore, there is a recognized trend among brands to migrate
  away from rigid, pre-defined taxonomies toward more expressive, context-aware suitability
  policies, which requires expert guidance (like Zefr’s) due to the "weirdness" and
  corner cases inherent in social media content.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML professionals, AdTech/MarTech strategists,
  content policy executives, and investors** focused on digital advertising infrastructure
  and content moderation technologies.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- startup
- meta
- anthropic
- google
title: AI's Industry Impact with Jon Morra (Zefr) | EP88
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 88
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 6
  prominence: 0.6
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 6
  prominence: 0.6
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 5
  prominence: 0.5
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-17 23:12:36 UTC -->
