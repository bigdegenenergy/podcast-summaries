---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit emerj.com
    and fill out our Thought Leaders
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: eedle on AI ROI, visit emerj.com and fill out our Thought Leaders submission
    form. That's emerj.com and click on be
  name: Thought Leaders
  position: 1906
- category: unknown
  confidence: medium
  context: ert1. Again, that's emerj.com/expert1. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2152
- category: unknown
  confidence: medium
  context: t1. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2187
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2552
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2614
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5294
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7016
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 17981
- category: unknown
  confidence: medium
  context: generative AI coming down the horizon right now. Generative AI seems to
    have just found its place, but it's also
  name: Generative AI
  position: 23955
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24115
- category: ai_media_and_research
  confidence: high
  context: The organization hosting the AI and Business Podcast and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_finance
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    indicating enterprise AI adoption.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_defense
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, indicating enterprise AI adoption.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    sense of complex information by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The company behind Notebook LM (notebooklm.google.com) and the originator
    of the transformer architecture.
  name: Google
  source: llm_enhanced
- category: consulting_and_advisory
  confidence: medium
  context: Mentioned for conducting a recent survey on generative AI investment in
    the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a reference point for the explosion of LLMs, contrasting with
    earlier models like BERT.
  name: ChatGPT
  source: llm_enhanced
- category: ai_research_model
  confidence: high
  context: An earlier language model architecture (pre-GPT) applied to clinical trial
    documents for information extraction.
  name: BERT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a model leveraging the transformer architecture
    for generating new sequences (language, molecules). Refers generally to OpenAI's
    Generative Pre-trained Transformer series.
  name: GPT
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Implied reference through the mention of 'GPT' and the general context
    of generative AI advancements.
  name: OpenAI
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 16:08:28 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 70
  relevance_score: 3
  text: So we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: This is a concise explanation of the fundamental difference between
    traditional task-specific ML and the foundation model approach (pre-training on
    massive, diverse data).
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs foundation models.
  topic: technical
- impact_reason: Provides a clear, accessible definition of fine-tuning in the context
    of foundation models, emphasizing weight adjustment rather than full retraining.
  relevance_score: 10
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right? So we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems.
  topic: technical
- impact_reason: A powerful conceptual link between LLMs and biological systems (genetics
    as code), suggesting LLMs can simulate or test molecular functions, accelerating
    R&D.
  relevance_score: 10
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process.
  topic: predictions
- impact_reason: Highlights the fundamental reliance of modern foundation models (even
    domain-specific ones) on the Transformer architecture, emphasizing its foundational
    importance.
  relevance_score: 10
  source: llm_enhanced
  text: So all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Crucially differentiates BERT-style models (representation/encoding
    focus) from GPT-style models (decoding focus), explaining the shift from feature
    extraction to content generation.
  relevance_score: 10
  source: llm_enhanced
  text: But now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data. So for example, a
    new types of sequence that represent your language.
  topic: technical
- impact_reason: 'Highlights a major breakthrough: generative models can incorporate
    complex, multi-objective constraints (like toxicity and solubility) simultaneously
    during the generation process, a huge leap for molecular design.'
  relevance_score: 10
  source: llm_enhanced
  text: So now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/predictions
- impact_reason: 'Crucial limitation identified: AI progress in biology is currently
    bottlenecked by the speed and availability of experimental, real-world data collection
    (wet lab limitations).'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
    So, there's, you know, where AI, the architecture, the concept there, but the,
    sometimes we are limited by the, you know, experimental side.
  topic: limitations/technical
- impact_reason: Directly addresses the hallucination problem, emphasizing that in
    high-stakes domains like biology, validation is not immediate and requires costly,
    time-consuming experimentation.
  relevance_score: 10
  source: llm_enhanced
  text: But a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/challenges
- impact_reason: 'Proposes the solution to the validation problem: creating hybrid
    evaluation metrics that combine domain expertise with AI performance measures
    to compare models effectively.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven those evaluation is a
    key here, because there could be so many versions of found data models. Now, if
    we, you know, have those kind of domain plus AI-driven those evaluation metrics,
    so we can score across the different models, right?
  topic: safety/strategy
- impact_reason: This highlights the massive gap between investment interest and successful,
    scaled implementation of GenAI in the life sciences sector, a critical challenge
    for business leaders.
  relevance_score: 9
  source: llm_enhanced
  text: 73% of life sciences leaders are investing in generative AI initiatives, fewer
    than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: 'Explains the core value proposition of foundation models: enabling
    success even with limited domain-specific data through transfer learning/pre-training
    knowledge.'
  relevance_score: 9
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data.
  topic: technical
- impact_reason: Describes a strategic shift in data management—centralizing and leveraging
    massive, diverse data lakes to power organization-wide strategic AI applications,
    moving beyond simple task automation.
  relevance_score: 9
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization.
  topic: strategy
- impact_reason: Illustrates a concrete, high-value application of foundation models
    in drug discovery (understanding disease pathways) even when domain-specific data
    is scarce.
  relevance_score: 9
  source: llm_enhanced
  text: You could still look into these foundation models to look at the specific
    gene interactions among specific cell types. You can still get some hints about
    the disease pathways, etc.
  topic: predictions
- impact_reason: Summarizes the primary areas where GenAI is expected to revolutionize
    life sciences R&D.
  relevance_score: 9
  source: llm_enhanced
  text: accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality.
  topic: predictions
- impact_reason: Provides a concrete, high-impact application of GenAI in early drug
    discovery and biological research (in silico testing).
  relevance_score: 9
  source: llm_enhanced
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc.
  topic: predictions/applications
- impact_reason: 'Explains the core mechanism behind the success of large foundation
    models: leveraging massive, diverse public data via the Transformer structure
    for general utility.'
  relevance_score: 9
  source: llm_enhanced
  text: So now the difference is that we see the benefits of putting a lot of the
    generic public data like text, images into those transformers so we can do, you
    know, so many downstream tasks.
  topic: technical
- impact_reason: Illustrates the strategic shift in specialized domains (like healthcare)
    towards creating domain-specific foundation models by adapting the general Transformer
    paradigm to proprietary/domain data.
  relevance_score: 9
  source: llm_enhanced
  text: So people in the healthcare have been thinking similarly, can we put all the
    available, you know, data, chemical data, clinical trial data, etc. into those,
    you know, transformer architecture? So that way we can also build our domain-specific
    foundation models to enable a lot of downstream tasks.
  topic: strategy/technical
- impact_reason: 'Offers a clear, accessible explanation of what models like BERT
    produce: high-quality, abstracted features (embeddings) that boost traditional
    ML models.'
  relevance_score: 9
  source: llm_enhanced
  text: So the outcome is some representations or embeddings of those text tokens,
    etc. So those embeddings are token representations they have predictive power.
    So you can think them as more advanced abstracted features to your machine learning
    models.
  topic: technical
- impact_reason: Strong assertion on the indispensable role of domain experts (not
    just data scientists) in data curation, model development, and, critically, benchmarking.
  relevance_score: 9
  source: llm_enhanced
  text: But, you know, we still need the domain knowledge people, you know, people
    have those kind of expertise to guide the, you know, the selection of the data
    and also more importantly, then these models are, you build, now how do we benchmark
    with them? So we would definitely need those, the domain experts to be part of
    this model development process.
  topic: strategy/business
- impact_reason: 'Provides a strategic insight: NLP''s early success provides a ''head
    start'' advantage to genetics/biology because they are inherently language-like
    domains, suggesting where AI adoption might accelerate fastest.'
  relevance_score: 9
  source: llm_enhanced
  text: advantage that natural language processing as a domain became one of the first
    gigantic generative AI use cases, right? That builds in certain advantage to genetics,
    which is more language-based than anything else versus, you know, other domains
    of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: A pragmatic warning that current AI technology is inherently probabilistic,
    necessitating rigorous benchmarking and selection rather than blind trust.
  relevance_score: 9
  source: llm_enhanced
  text: So, right now the technology cannot give you 100% answers, how those nations
    are always there, right? So, it's quite important that we have to do those benchmarking
    and select models accordingly.
  topic: safety/business
- impact_reason: Clearly articulates the historical evolution of ML techniques leading
    up to the current foundational model paradigm shift, providing context for the
    audience.
  relevance_score: 8
  source: llm_enhanced
  text: we move to the deep learning techniques like convolutional neural networks,
    RNNs, and graph neural networks. And more recently, we have a paradigm shift into
    those language models, foundation models.
  topic: technical
- impact_reason: Attributes the current AI revolution directly back to the foundational
    importance of the Transformer architecture.
  relevance_score: 8
  source: llm_enhanced
  text: foundation models have been built. So all those models, you know, essentially
    they have the same, I mean, by nature, they have the same setup. So that is based
    on the breakthrough of the transformer architecture that Google proposed a long
    time ago.
  topic: technical
- impact_reason: 'Lists three key practical challenges for adopting AI in life sciences:
    evaluation, alignment, and benchmarking.'
  relevance_score: 8
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: Sets the agenda to move from general discussion to specific application
    and development of GenAI/foundation models, which is crucial for the audience
    interested in practical implementation.
  relevance_score: 8
  source: llm_enhanced
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right?
  topic: strategy
- impact_reason: Provides historical context, showing that specialized NLP/ML applications
    in regulated fields predate the current GenAI boom (BERT era), indicating maturity
    in certain sectors.
  relevance_score: 8
  source: llm_enhanced
  text: We were one of the first to develop language models for clinical trials, information
    extraction and the protocol design. So at that time that was before ChatGPT, right?
    Yeah, before that, there's also another language model called BERT.
  topic: technical/history
- impact_reason: Outlines the necessary data landscape for domain-specific foundation
    models, emphasizing the blend of public datasets (atlases) and proprietary internal
    data.
  relevance_score: 8
  source: llm_enhanced
  text: Now the next question is about the data, right? What are the data? I gave
    some example like all the cell atlases. So those things, they could be fed into
    those models. So they are available. And also, like for domain-specific data,
    you know, molecules, chemicals, right? So those kinds of things, you could also
    leverage public data and also your protocols, which may be from publicly, there's
    a lot of, like, clinical trials registry, but also your own company owned those
    kinds of proprietary information, etc. So those things could potentially are building
    into those models.
  topic: business/data strategy
- impact_reason: Identifies data quality, not just quantity, as the primary challenge,
    even when leveraging massive foundation models for domain-specific tasks.
  relevance_score: 8
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: business/challenges
- impact_reason: Identifies organizational alignment and communication between technical
    builders (modelers) and domain experts (scientists/leadership) as a major execution
    hurdle.
  relevance_score: 8
  source: llm_enhanced
  text: There's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: 'Advice for domain experts: they must understand the evolving AI
    landscape (new vs. old paradigms) because the future is hybrid, not a complete
    replacement of existing methods.'
  relevance_score: 8
  source: llm_enhanced
  text: It behooves them [subject matter experts] to kind of have some awareness of
    the new and old paradigm that we were talking about before, how both are still
    present, but how we thought that AI would impact life sciences organization or
    that they'd just be this one school way of doing things is not the case.
  topic: strategy
- impact_reason: Defines the term 'foundation model' based on its multi-utility nature.
  relevance_score: 7
  source: llm_enhanced
  text: So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Raises a crucial business/strategy question regarding the build vs.
    buy decision for foundational models in specialized industries like healthcare.
  relevance_score: 7
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
- impact_reason: 'Summarizes the organizational requirement for successful AI implementation
    in specialized fields: mandatory cross-disciplinary teamwork.'
  relevance_score: 7
  source: llm_enhanced
  text: So, yeah, so it's basically a cross-discipline collaboration, whether in house
    or externally, yeah.
  topic: strategy
source: Unknown Source
summary: '## Comprehensive Summary: Building AI-Ready Cultures in Life Sciences R&D


  This 30-minute podcast episode, featuring **Xiong Liu, Director of Data Science
  in AI at Novartis**, focuses on the paradigm shift in Life Sciences R&D driven by
  **Generative AI (GenAI)** and **Foundation Models (FMs)**, and the cultural and
  technical challenges of scaling these technologies beyond pilot phases.


  ### 1. Focus Area

  The discussion centers on the transition from traditional, task-specific Machine
  Learning (ML) to leveraging large-scale Foundation Models (like LLMs) in drug discovery,
  molecular design, clinical protocol optimization, and digital twin creation within
  life sciences R&D. A major theme is the necessity of building "AI-ready cultures"
  to bridge the gap between advanced AI capabilities and practical scientific application.


  ### 2. Key Technical Insights

  *   **Shift to Foundation Models:** The paradigm has moved from training models
  solely on proprietary, labeled data (supervised learning) to utilizing massive,
  pre-trained FMs that capture broad domain knowledge (e.g., public molecular data,
  human language corpus). This allows for better performance even with small, specific
  datasets via **fine-tuning**.

  *   **Transformer Architecture Dominance:** The success of GenAI stems from the
  **Transformer architecture**. In life sciences, this architecture is being adapted
  to process chemical data (SMILES strings, graphs) to generate novel molecules with
  specified constraints (e.g., toxicity, solubility).

  *   **BERT vs. GPT Analogy:** The evolution from earlier models like BERT (focused
  on representation/embeddings) to GPT-like models (focused on generation) mirrors
  the potential for AI in R&D—moving from advanced feature extraction to generating
  novel biological sequences or molecular structures.


  ### 3. Business/Investment Angle

  *   **Scaling Challenge:** Despite high investment (Deloitte survey noted 73% investing,
  but <20% scaling), organizations struggle to move GenAI past pilots, highlighting
  a critical need for strategic deployment rather than just experimentation.

  *   **Domain-Specific Value:** The real benefit of FMs in life sciences lies in
  creating **domain-specific FMs** trained on chemical, clinical, and biological data
  (like cell atlases), enabling better target identification and pathway analysis
  even when specific indication data is limited.

  *   **ROI and Evaluation:** A major hurdle is establishing **domain-driven evaluation
  standards** and knowledge-checking benchmarks to accurately measure the ROI and
  reliability of AI-generated outputs (e.g., novel molecules), especially given the
  risk of "hallucination" in complex biological contexts.


  ### 4. Notable Companies/People

  *   **Xiong Liu (Novartis):** The expert guest, providing insights from his role
  as Director of Data Science in AI, emphasizing practical implementation and cultural
  alignment.

  *   **Google/OpenAI (Implied):** Mention of the foundational work on Transformer
  architecture and GPT-like models, which underpin the current GenAI wave.

  *   **Deloitte:** Cited for survey data illustrating the gap between GenAI investment
  and successful scaling in the life sciences sector.


  ### 5. Future Implications

  The industry is moving toward a hybrid model where internal teams leverage public
  FM breakthroughs while simultaneously building proprietary, domain-specific FMs.
  Success hinges on **cross-disciplinary collaboration** between AI experts, computer
  scientists, and domain experts (biologists, chemists) to guide data selection, model
  fine-tuning, and rigorous, biologically relevant validation. The future requires
  subject matter experts to be aware of AI history and paradigms to effectively guide
  adoption.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Professionals** working in R&D, **Life
  Sciences Executives** responsible for digital transformation and investment strategy,
  and **Data Scientists** needing to understand the shift from traditional ML to foundation
  models in a highly regulated, complex domain.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
- openai
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 121
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 14
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 16:08:28 UTC -->
