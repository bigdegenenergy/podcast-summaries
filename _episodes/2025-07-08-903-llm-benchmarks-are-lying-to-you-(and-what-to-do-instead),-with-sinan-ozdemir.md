---
companies:
- category: unknown
  confidence: medium
  context: This is episode number 903 with Sinan Osdimer, author of the Quick Start
    Guide to LLMs. Today's
  name: Sinan Osdimer
  position: 32
- category: unknown
  confidence: medium
  context: sode number 903 with Sinan Osdimer, author of the Quick Start Guide to
    LLMs. Today's episode is brought to you by Tra
  name: Quick Start Guide
  position: 61
- category: unknown
  confidence: medium
  context: the conversational analytics platform, and by the Dell AI Factory with
    NVIDIA. Welcome to the Super Data Science Po
  name: Dell AI Factory
  position: 229
- category: tech
  confidence: high
  context: alytics platform, and by the Dell AI Factory with NVIDIA. Welcome to the
    Super Data Science Podcast, the m
  name: Nvidia
  position: 250
- category: unknown
  confidence: medium
  context: y the Dell AI Factory with NVIDIA. Welcome to the Super Data Science Podcast,
    the most listened-to podcast in the data science
  name: Super Data Science Podcast
  position: 273
- category: unknown
  confidence: medium
  context: sforming our world for the better. I'm your host, John Cron. Thanks for
    joining me today. And now, let's make
  name: John Cron
  position: 560
- category: unknown
  confidence: medium
  context: w for the sixth time. Sinan is founder and CTO of Loop Genius, a generative
    AI startup. He's authored several e
  name: Loop Genius
  position: 829
- category: unknown
  confidence: medium
  context: several excellent books, including most recently, The Best Selling Quick
    Start Guide to Large Language Models. He hosts the Practicall
  name: The Best Selling Quick Start Guide
  position: 931
- category: unknown
  confidence: medium
  context: t recently, The Best Selling Quick Start Guide to Large Language Models.
    He hosts the Practically Intelligent Podcast. He
  name: Large Language Models
  position: 969
- category: unknown
  confidence: medium
  context: tart Guide to Large Language Models. He hosts the Practically Intelligent
    Podcast. He was previously adjunct faculty at Johns Hopki
  name: Practically Intelligent Podcast
  position: 1005
- category: unknown
  confidence: medium
  context: ent Podcast. He was previously adjunct faculty at Johns Hopkins University
    and now teaches several times a month within the
  name: Johns Hopkins University
  position: 1075
- category: unknown
  confidence: medium
  context: preneur, including founding a Y-combinator-backed Gen AI startup way back
    in 2015 that was later acquired.
  name: Gen AI
  position: 1240
- category: unknown
  confidence: medium
  context: r acquired. He holds a master's in pure math from Johns Hopkins. Today's
    episode skews slightly toward our more t
  name: Johns Hopkins
  position: 1335
- category: unknown
  confidence: medium
  context: ght. I was episode 445. And then last year at the Open Data Science Conference
    East in Boston, we met up quickly. I set up a camera a
  name: Open Data Science Conference East
  position: 3010
- category: unknown
  confidence: medium
  context: hey're British. People don't know them in the UK. Machine Head. Machine
    Head. Machine Head. Exactly. Glycerine.
  name: Machine Head
  position: 3872
- category: unknown
  confidence: medium
  context: not necessarily better, but something different. Like I'm very conscious
    about, I wrote about this four y
  name: Like I
  position: 4653
- category: unknown
  confidence: medium
  context: nscious about, I wrote about this four years ago. Do I really have something
    new to say here or do I dig
  name: Do I
  position: 4719
- category: unknown
  confidence: medium
  context: ew to say here or do I digress to something else? So I aim very much for
    diversity more than I aim for j
  name: So I
  position: 4797
- category: unknown
  confidence: medium
  context: ks. Like my first book ever was the Principles of Data Science. And that
    was almost 10 years ago at this point.
  name: Data Science
  position: 5229
- category: unknown
  confidence: medium
  context: that mean? And in fact, in one of my episodes on Super Data Science, the
    one from 2020, January 2020, that was actual
  name: Super Data Science
  position: 6216
- category: unknown
  confidence: medium
  context: ail I think yesterday, like late night yesterday. But I have not yet tried
    it. What's the... What's the s
  name: But I
  position: 7474
- category: unknown
  confidence: medium
  context: haven't used it yet. It's been a busy day so far. Now I'm recording this
    podcast episode, you know. Tough
  name: Now I
  position: 8011
- category: unknown
  confidence: medium
  context: . Let's just dive into the top 20 applications of LLMs I have seen in the
    last 10 years. Everything from..
  name: LLMs I
  position: 9246
- category: unknown
  confidence: medium
  context: Very nice. Very nice. I like that cookbook model. And I assume both of
    these books apply AI and Quick Sta
  name: And I
  position: 9707
- category: unknown
  confidence: medium
  context: el. And I assume both of these books apply AI and Quick Start. They're
    both in Python. Yes. So everything... Al
  name: Quick Start
  position: 9753
- category: unknown
  confidence: medium
  context: Yep. And I love them. I absolutely love... Same. Every Pearson book that
    comes out, everyone that I work with th
  name: Every Pearson
  position: 10360
- category: unknown
  confidence: medium
  context: ', reach out to me, or reach out to Sinan. Please. On LinkedIn probably,
    and we''d be delighted to introduce you'
  name: On LinkedIn
  position: 11519
- category: unknown
  confidence: medium
  context: most popular episode of 2024 was episode 747 with Kyrell Aramango, the
    founder and original host of this podcast. A
  name: Kyrell Aramango
  position: 12630
- category: tech
  confidence: high
  context: e shocked to realize that it actually came out of Google. Google invented
    the Transformer architecture. An
  name: Google
  position: 13869
- category: tech
  confidence: high
  context: tter than me. But my understanding is that it was OpenAI betting big on
    scaling. That, you know, like Goog
  name: Openai
  position: 14358
- category: unknown
  confidence: medium
  context: enAI betting big on scaling. That, you know, like Google DeepMind, which
    there used to be kind of two big AI labs a
  name: Google DeepMind
  position: 14410
- category: unknown
  confidence: medium
  context: which there used to be kind of two big AI labs at Google Brain and Google
    DeepMind. And DeepMind was kind of, I
  name: Google Brain
  position: 14477
- category: unknown
  confidence: medium
  context: big AI labs at Google Brain and Google DeepMind. And DeepMind was kind
    of, I think, I mean, in some ways, they
  name: And DeepMind
  position: 14511
- category: unknown
  confidence: medium
  context: w, generalizing to gradually more and more tasks. Whereas Ilya Sutskever
    at OpenAI at that time would just have this hunch
  name: Whereas Ilya Sutskever
  position: 14866
- category: unknown
  confidence: medium
  context: pisode of Super Data Science is brought to you by AWS Trainium 2, the latest
    generation AI chip from AWS. AWS Tr
  name: AWS Trainium
  position: 16635
- category: tech
  confidence: high
  context: hy companies across the spectrum from giants like Anthropic and Databricks
    to cutting-edge startups like Pool
  name: Anthropic
  position: 17043
- category: tech
  confidence: high
  context: cross the spectrum from giants like Anthropic and Databricks to cutting-edge
    startups like Poolside are choosi
  name: Databricks
  position: 17057
- category: unknown
  confidence: medium
  context: '''s not as big as yours, to summarize. It''s called Practically Intelligent.
    And it started. I was working with a friend of m'
  name: Practically Intelligent
  position: 17514
- category: unknown
  confidence: medium
  context: as working with a friend of mine, former student, Akshay Bhushan. He's
    my co-host. He's now a partner at a VC, Tol
  name: Akshay Bhushan
  position: 17608
- category: unknown
  confidence: medium
  context: han. He's my co-host. He's now a partner at a VC, Tola Capital. And he
    basically asked me one day over lunch or
  name: Tola Capital
  position: 17669
- category: unknown
  confidence: medium
  context: ty amazing people talking about the beginnings of IBM Watson and Amazon
    SageMaker. And we've been with a lot o
  name: IBM Watson
  position: 17960
- category: tech
  confidence: high
  context: le talking about the beginnings of IBM Watson and Amazon SageMaker. And
    we've been with a lot of interesti
  name: Amazon
  position: 17975
- category: unknown
  confidence: medium
  context: le talking about the beginnings of IBM Watson and Amazon SageMaker. And
    we've been with a lot of interesting people
  name: Amazon SageMaker
  position: 17975
- category: unknown
  confidence: medium
  context: of interesting AI products throughout the years. And Tola Capital, Tola
    Capital sounds familiar to me. Is that beca
  name: And Tola Capital
  position: 18110
- category: unknown
  confidence: medium
  context: I also advise. That's how we got together again. So Akshay was a student
    of mine in General Assembly many, m
  name: So Akshay
  position: 18258
- category: unknown
  confidence: medium
  context: ogether again. So Akshay was a student of mine in General Assembly many,
    many years ago. And then eventually indepen
  name: General Assembly
  position: 18293
- category: unknown
  confidence: medium
  context: ecome a lot more around the education side of AI. Meaning I would rather
    not just be called in whenever they
  name: Meaning I
  position: 19025
- category: unknown
  confidence: medium
  context: '''s not, mostly even sooner. So I actually live in San Francisco, specifically
    in the Dogpatch neighborhood, which'
  name: San Francisco
  position: 21102
- category: unknown
  confidence: medium
  context: ou were in the YC neighborhood there in Dogpatch. New York in San Fran.
    Yeah, right in the thick of it there
  name: New York
  position: 21790
- category: unknown
  confidence: medium
  context: he YC neighborhood there in Dogpatch. New York in San Fran. Yeah, right
    in the thick of it there. That was a
  name: San Fran
  position: 21802
- category: ai_infrastructure
  confidence: high
  context: Sponsor of the podcast, mentioned for their latest AI chip, Trainium 2,
    used for training large AI models.
  name: AWS
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Sponsor of the podcast, described as a conversational analytics platform.
  name: AdVarity
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as part of the 'Dell AI Factory with NVIDIA,' indicating their
    role in AI hardware/infrastructure.
  name: NVIDIA
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Sinan Osdimer's current company, described as a generative AI startup.
  name: Loop Genius
  source: llm_enhanced
- category: investment_accelerator
  confidence: high
  context: Mentioned as the backer for a previous Gen AI startup founded by Sinan
    Osdimer.
  name: Y-combinator
  source: llm_enhanced
- category: ai_research_institution
  confidence: medium
  context: Sinan Osdimer was previously adjunct faculty there.
  name: Johns Hopkins University
  source: llm_enhanced
- category: ai_education
  confidence: high
  context: The platform where Sinan Osdimer teaches several times a month, featuring
    content curated by Pearson.
  name: O'Reilly platform
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned in relation to their model R1 (referred to as O1/R1 in the text).
  name: DeepSeek
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as the inventor of the Transformer architecture.
  name: Google
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as one of the leading AI labs at Google, focusing on deep reinforcement
    learning.
  name: Google DeepMind
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as one of the two big AI labs at Google (alongside DeepMind).
  name: Google Brain
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as the company that bet big on scaling and developed ChatGPT,
    utilizing reinforcement learning.
  name: OpenAI
  source: llm_enhanced
- category: ai_publishing
  confidence: high
  context: The publisher for Sinan Osdimer and the host, curating content on the O'Reilly
    platform.
  name: Pearson
  source: llm_enhanced
- category: ai_frontier_lab
  confidence: high
  context: Mentioned as a 'giant' company choosing AWS Trainium 2 to power their next
    generation of AI workloads.
  name: Anthropic
  source: llm_enhanced
- category: ai_infrastructure_platform
  confidence: high
  context: Mentioned as a 'giant' company choosing AWS Trainium 2 to power their next
    generation of AI workloads.
  name: Databricks
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Mentioned as a 'cutting-edge startup' choosing AWS Trainium 2 to power
    their next generation of AI workloads.
  name: Poolside
  source: llm_enhanced
- category: investment_vc
  confidence: high
  context: A VC firm where the speaker's co-host, Akshay Bhushan, is a partner. The
    speaker advises them on AI.
  name: Tola Capital
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as a topic discussed on the 'Practically Intelligent' podcast,
    referencing its beginnings.
  name: IBM Watson
  source: llm_enhanced
- category: ai_infrastructure_platform
  confidence: medium
  context: Mentioned as a topic discussed on the 'Practically Intelligent' podcast,
    referencing discussions about it.
  name: Amazon SageMaker
  source: llm_enhanced
- category: education_tech
  confidence: medium
  context: The institution where the speaker taught Akshay Bhushan many years ago.
  name: General Assembly
  source: llm_enhanced
- category: accelerator_incubator
  confidence: high
  context: Mentioned because the speaker lives in the neighborhood (Dogpatch, SF)
    where YC founders reside and the speaker occasionally acts as an angel investor
    for YC companies.
  name: Y-Combinator (YC)
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in relation to Llama 2 and allegations regarding benchmark manipulation,
    forcing the Meta VP of GenAI to issue a denial.
  name: Meta
  source: llm_enhanced
- category: ai_model_family
  confidence: high
  context: Refers to Meta's open-source models (Llama II, Llama 4 mentioned) and the
    controversy surrounding their training data and benchmark performance.
  name: Llama
  source: llm_enhanced
- category: publishing_education
  confidence: high
  context: The platform where the speaker teaches recurrent courses on transformer
    architectures and upcoming courses on AI agents and RAG.
  name: O'Reilly
  source: llm_enhanced
- category: ai_hardware
  confidence: high
  context: The specific AI chip generation being promoted, used by Anthropic, Databricks,
    and Poolside.
  name: AWS Trainium 2
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The organization running Chatbot Arena, often associated with Berkeley.
  name: LMSys Lab
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The university affiliation for the LMSys Lab running Chatbot Arena.
  name: Berkeley
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The specific lab associated with Joey Gonzalez at Berkeley that devised
    Chatbot Arena.
  name: Joey Gonzalez's lab
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Co-sponsor of the podcast episode, providing the 'Dell AI Factory with
    NVIDIA' solution.
  name: Dell Technologies
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referred to as a tool that can be used to help build synthetic data sets
    for testing frameworks.
  name: GPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used an embedding model specific to their domain for tasks like detecting
    financial fraud.
  name: Stripe
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Built a BERT model for their recommendation engine years ago, specific
    to their domain.
  name: eBay
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Mentioned as the type of model eBay built for their recommendation engine.
  name: BERT
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an existing benchmark for embedding models.
  name: MTEB
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a fine-tuned LLM whose job is to detect rephrasing of questions
    in training/test sets.
  name: LLM decontaminator
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a model that Meta might have been beating if they had cheated
    on benchmarks.
  name: GPT-4
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as an example of an 'omni model' architecture capable of processing
    different data modes (developed by Adept, but discussed in the context of OpenAI's
    approach).
  name: Fuyu
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as an example of an 'omni model' architecture for multimodal
    data processing (developed by Google DeepMind).
  name: Lava
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as the multimodal version of the MMLU benchmark (Multimodal MMLU).
  name: MMM
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as the standard benchmark for which MMM is the multimodal equivalent.
  name: MMLU
  source: llm_enhanced
date: 2025-07-08 11:00:00 +0000
duration: 88
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: be judging multimodal models really any differently
  text: we should be judging multimodal models really any differently.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: benchmarking will involve, including how to benchmark genetic and multimodal
    models, and how a simple question about watermelon seeds reveals the 40% failure
    rate of even today's most advanced AI-reasoning models. All right, you ready for
    this excellent episode? Let's go. Sinan, welcome back to the Super Data Science
    Podcast. How are you doing today? John, thank you for having me yet again. I'm
    super excited to be here, as always. Yes, we were just tallying prior to starting
    recording how many times you've been on the show. You've been on more times than
    you even knew. You can't answer to the green room before we get into the recording
    studio, and you said this
  text: the future of benchmarking will involve, including how to benchmark genetic
    and multimodal models, and how a simple question about watermelon seeds reveals
    the 40% failure rate of even today's most advanced AI-reasoning models. All right,
    you ready for this excellent episode? Let's go. Sinan, welcome back to the Super
    Data Science Podcast. How are you doing today? John, thank you for having me yet
    again. I'm super excited to be here, as always. Yes, we were just tallying prior
    to starting recording how many times you've been on the show. You've been on more
    times than you even knew. You can't answer to the green room before we get into
    the recording studio, and you said this is going to be your fifth time, but actually
    it's your sixth.
  type: prediction
- actionable: false
  confidence: medium
  extracted: perplexity among other things
  text: The problem with perplexity among other things is that A, it requires other
    answers to be judged against.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://www.podtrac.com/pts/redirect.mp3/chrt.fm/track/E581B9/arttrk.com/p/VI4CS/pscrb.fm/rss/p/traffic.megaphone.fm/SUPERDATASCIENCEPTYLTD4709835756.mp3?updated=1752060782
processing_date: 2025-10-05 03:57:34 +0000
quotes:
- length: 171
  relevance_score: 6
  text: But in addition to that, we also, on a separate task, now I'm realizing as
    I get to the whole story, we would use large language models to judge the quality
    of LLM outputs
  topics: []
- length: 219
  relevance_score: 6
  text: I mean, which is the basic point of reinforcement learning anyways, but it's
    almost perfect in the way that the way we think about evaluation lends itself
    quite nicely to the way we think about training these LLMs today
  topics:
  - valuation
- length: 96
  relevance_score: 5
  text: And before the LLM tries again, you have to give it a score to say that was
    good or that was bad
  topics: []
- length: 170
  relevance_score: 4
  text: The editions are mostly around obviously newer LLMs, newer evaluation criteria,
    newer benchmarks, just different ways of applying LLMs, like reasoning models,
    for example
  topics:
  - valuation
- length: 149
  relevance_score: 4
  text: This question came up actually in a lecture today where I mentioned LLM contamination
    of training data to remove items similar to benchmark questions
  topics: []
- length: 141
  relevance_score: 4
  text: If you look at, I mean, honestly any LLM marketing page, you're going to find
    some table where each row is a benchmark, each column is an LLM
  topics:
  - market
- length: 138
  relevance_score: 4
  text: But the point stands is actually transformer-based decoder-based LLMs are
    naturally biased against being good at multiple-choice questions
  topics: []
- length: 197
  relevance_score: 4
  text: And so that person I was talking about earlier, where we stitched a bunch
    of LLM calls together to create the training set and the test set, that was actually,
    that's more like your benchmark thing
  topics: []
- length: 64
  relevance_score: 4
  text: And I'd just love to hear your thoughts on multimodal evaluation
  topics:
  - valuation
- length: 87
  relevance_score: 4
  text: There are multimodal evaluation in a lot of ways is not much different in
    a lot of ways
  topics:
  - valuation
- length: 32
  relevance_score: 3
  text: And here's what you should watch
  topics: []
- length: 44
  relevance_score: 3
  text: Google invented the Transformer architecture
  topics: []
- length: 102
  relevance_score: 3
  text: I mean, if you know OpenAI before the transformer, they are responsible for
    the reinforcement learning
  topics: []
- length: 257
  relevance_score: 3
  text: Fun fact, if you go right now to OpenAI and their playground and you turn
    the temperature up all the way they let you, which is two, and you ask it a basic
    question, you are very, very likely going to see some literal, absolute gibberish
    come out of the LLM
  topics: []
- length: 105
  relevance_score: 3
  text: Dell Technologies and NVIDIA can help you leverage AI to drive innovation
    and achieve your business goals
  topics: []
- length: 126
  relevance_score: 3
  text: It's now everyone's job to prompt better, fine-tune better, do whatever you
    have to do to get better at our internal benchmark
  topics: []
- length: 70
  relevance_score: 3
  text: Now you have to figure out in your own domain what a good threshold is
  topics: []
- length: 126
  relevance_score: 3
  text: They are obviously related, but the value of perplexity is also dependent
    on the prevalence of that token in the training data
  topics: []
- length: 117
  relevance_score: 3
  text: So perplexity is a fine correlated proxy to hallucination, but really you're
    just measuring the confidence of the LLM
  topics: []
- length: 51
  relevance_score: 3
  text: The LLM doesn't know its own perplexity to be clear
  topics: []
- impact_reason: This is the core thesis of the episode, highlighting major issues
    in current LLM evaluation (benchmarking flaws, gaming, and real-world failure
    rates). This is critical for practitioners.
  relevance_score: 10
  source: llm_enhanced
  text: Sinan details why the AI benchmarks everyone relies on might be lying to you,
    how the leading AI labs are gaming the benchmark system, tricks to actually effectively
    evaluate LLMs' capabilities for your use cases, what the future of benchmarking
    will involve, including how to benchmark genetic and multimodal models, and how
    a simple question about watermelon seeds reveals the 40% failure rate of even
    today's most advanced AI-reasoning models.
  topic: safety/technical/predictions
- impact_reason: 'A crucial philosophical point about LLMs: despite advanced prompting/chain-of-thought
    features (which make it *seem* like thinking), the underlying mechanism remains
    next-token prediction, suggesting consistent evaluation methods should apply.'
  relevance_score: 10
  source: llm_enhanced
  text: This thing is the next token predictor. And just because it thinks before
    it speaks to you, doesn't mean we can't evaluate it the same way as we evaluate
    everything else.
  topic: technical/safety
- impact_reason: Identifies the core strategic bet OpenAI made—scaling as the primary
    driver for emergent capabilities—and confirms its success, contrasting it with
    other labs' focus (like DeepMind's RL focus).
  relevance_score: 10
  source: llm_enhanced
  text: Whereas Ilya Sutskever at OpenAI at that time would just have this hunch that
    scaling big would, you know, that we would just have emergent properties kind
    of automatically. And that ended up being right.
  topic: strategy/technical
- impact_reason: Crucially links the Transformer architecture's success with the integration
    of Reinforcement Learning from Human Feedback (RLHF) as a key component for alignment
    and performance in ChatGPT.
  relevance_score: 10
  source: llm_enhanced
  text: The whole in their words, one of the reasons ChatGPT works is that alignment
    phase, including reinforcement learning from human feedback.
  topic: technical/safety
- impact_reason: A core strategic insight regarding the utility and limitation of
    AI benchmarks. They should initiate inquiry, not serve as final validation.
  relevance_score: 10
  source: llm_enhanced
  text: A benchmark should be the start of a conversation, not the end of a conversation.
  topic: strategy/safety
- impact_reason: Exposes the inadequacy of current, basic methods used by companies
    to check for benchmark contamination, suggesting widespread potential for unintentional
    (or intentional) data leakage.
  relevance_score: 10
  source: llm_enhanced
  text: I mentioned LLM contamination of training data to remove items similar to
    benchmark questions. And I had mentioned that the method for doing so for a lot
    of companies just comes down to a keyword search like an n-gram match or some
    kind of embedding similarity, which is simply not going to be enough
  topic: safety/technical
- impact_reason: Provides concrete evidence that current contamination detection methods
    are easily bypassed through simple data obfuscation (rephrasing), severely undermining
    benchmark reliability.
  relevance_score: 10
  source: llm_enhanced
  text: there were papers even found that if you trained Llama II on data that was
    rephrased just enough to miss all of those industry standard checks, it would
    have beaten GPT-4 at pretty common benchmarks.
  topic: safety/technical
- impact_reason: This highlights the central, unresolved issue of benchmark contamination
    in LLM evaluation, especially concerning open-source models where data transparency
    is lacking.
  relevance_score: 10
  source: llm_enhanced
  text: how do we know without a shadow of a doubt that you are not training to test
    or what you might call contaminating your training data with the test set of your
    benchmark?
  topic: safety/technical
- impact_reason: This is a powerful demonstration of how easily current contamination
    checks (like n-gram matching) can be bypassed, suggesting benchmarks are easily
    gamed.
  relevance_score: 10
  source: llm_enhanced
  text: if you trained Llama II on data that was rephrased just enough to miss all
    of those industry standard checks, it would have beaten GPT-4 at pretty common
    benchmarks.
  topic: technical/safety
- impact_reason: A direct critique of the lack of transparency from closed/open-weight
    labs and its corrosive effect on industry trust.
  relevance_score: 10
  source: llm_enhanced
  text: because Meta doesn't release their data, all we can do is really take their
    word for it. And they could be lying, they could be telling the truth, frankly,
    we'll never know. And I think that's kind of the hard part. And that starts to
    erode that trust of AI frontier labs.
  topic: safety/business
- impact_reason: Highlights the critical divergence between general capability scores
    (like MMLU) and truthfulness/safety metrics (like TruthfulQA), pointing out the
    danger of models that are 'convincing' but false.
  relevance_score: 10
  source: llm_enhanced
  text: A model could say, ace MMLU in terms of capabilities, but struggle on something
    like TruthQA, TruthfulQA. And so like it outputs convincing misinformation.
  topic: safety/technical
- impact_reason: Provides a stark, quantitative data point (40% hallucination rate
    for a major model on basic QA) illustrating the pervasive nature of LLM unreliability.
  relevance_score: 10
  source: llm_enhanced
  text: Even on the simple PersonQA and SimpleQA benchmarks, a model like O3, according
    to OpenAI themselves, will hallucinate as much as 40% of the time. That's a lot.
  topic: safety/technical
- impact_reason: 'Exposes a widespread, dangerous shortcut in AI product deployment:
    relying on superficial testing (intern testing, best public benchmark) instead
    of rigorous validation.'
  relevance_score: 10
  source: llm_enhanced
  text: Usually they'll say something like, oh, well, we picked the model with the
    best benchmark or we picked the newest OpenAI model and we wrote a prompt and
    we had our intern talk to it a couple of times and it all seemed to check out.
    So now we're in production. That happens way more often than I would like to admit.
  topic: business/strategy
- impact_reason: A powerful illustration that domain-specific utility (e.g., fraud
    detection, e-commerce) often trumps performance on general academic benchmarks
    (like MTEB).
  relevance_score: 10
  source: llm_enhanced
  text: Both Stripe and eBay made an embedding model specific to their domain... I'm
    willing to bet that if you ever got your hands on those benchmarks, they would
    be abysmal at embedding benchmarks, which exists, the MTEB. They would probably
    be abysmal at the benchmark, but they don't care.
  topic: technical/business
- impact_reason: Presents LLM-as-a-Judge as a scalable, cost-effective alternative
    to human evaluation, enabling rapid iteration during fine-tuning.
  relevance_score: 10
  source: llm_enhanced
  text: we would use large language models to judge the quality of LLM outputs...
    You call that API and you use it to judge your outputs. That's something that
    I love, because it allows you, it's so cheap and fast that you can do it as you're
    fine-tuning with LoRA.
  topic: technical/business
- impact_reason: Draws a direct, powerful parallel between human evaluation processes
    (rubrics, reward/punishment) and the mechanics of modern Reinforcement Learning
    from Human Feedback (RLHF/RLAIF) using PPO/GRPO.
  relevance_score: 10
  source: llm_enhanced
  text: I'll go once up further with the rise of reasoning models. The ability to
    use reinforcement learning to train kind of like... some GRPO or PPO algorithms...
    You're basically teaching the AI how to solve a task through reward and punishment.
    I mean, which is the basic point of reinforcement learning anyways, but it's almost
    perfect in the way that the way we think about evaluation lends itself quite nicely
    to the way we think about training these LLMs today.
  topic: technical/predictions
- impact_reason: Provides a clear, accessible explanation of how RLHF/RLAIF techniques
    (like PPO) function in training LLMs using reward/punishment signals derived from
    evaluations.
  relevance_score: 10
  source: llm_enhanced
  text: The ability to use reinforcement learning to train kind of like, I'll name
    drop some acronyms here, some GRPO or PPO algorithms. These are types of reinforcement
    learning system where you basically let an LLM try a task. And before the LLM
    tries again, you have to give it a score to say that was good or that was bad.
  topic: technical
- impact_reason: 'Reveals a critical flaw: perplexity can be artificially lowered
    by high-frequency tokens, regardless of factual correctness, directly challenging
    its use as a truthfulness proxy.'
  relevance_score: 10
  source: llm_enhanced
  text: The other thing is a problem with perplexity... the value of perplexity is
    also dependent on the prevalence of that token in the training data. So that same
    example, if you give it the word Earth as the answer... the perplexity will also
    be quite low, but not because the model is confident in it in the answer, but
    because it's just seen that token so often.
  topic: limitations/safety
- impact_reason: A concise, philosophical warning about equating high model confidence
    (low perplexity) with factual accuracy, a core challenge in current AI deployment.
  relevance_score: 10
  source: llm_enhanced
  text: Confidence does not mean truthfulness, unfortunately, and it's the same goes
    for LLM.
  topic: safety/limitations
- impact_reason: Provides a deep technical clarification that the LLM itself doesn't
    introspectively 'know' its confidence score; this is an external system calculation
    based on the output distribution.
  relevance_score: 10
  source: llm_enhanced
  text: The LLM doesn't know its own perplexity to be clear. It doesn't know the probability
    confidence is of its own token distribution when it predicts that token. The actual
    act of predicting a token is technically not done by the LLM. It's done by the
    system hosting the LLM is just choosing from that probability distribution.
  topic: technical
- impact_reason: Highlights the advanced application of RL (likely RLHF or similar
    techniques) in customizing reasoning models for specific business domains, pointing
    toward high-end customization.
  relevance_score: 9
  source: llm_enhanced
  text: All the way to... Let's use reinforcement learning to actually build some
    of these reasoning models from scratch for your specific domain.
  topic: technical/business
- impact_reason: Provides historical context on the Transformer architecture's perceived
    potential *before* the massive public explosion of ChatGPT, emphasizing its foundational
    disruptive nature.
  relevance_score: 9
  source: llm_enhanced
  text: I think when I first made that content, the Transformer architectures for
    Gen AI was like one of the first pieces of content I made for Pearson and O'Reilly.
    I originally made that content in 2020, I believe, 2021. ChatGPT had not come
    out yet. And it was very much around this idea. This architecture is changing
    things. And what people do with it is now open sky.
  topic: predictions/technical
- impact_reason: Highlights the prescient nature of early Transformer content creation,
    recognizing the architecture's transformative potential before the public explosion
    of Gen AI (ChatGPT).
  relevance_score: 9
  source: llm_enhanced
  text: I originally made that content in 2020, I believe, 2021. ChatGPT had not come
    out yet. And it was very much around this idea. This architecture is changing
    things. And what people do with it is now open sky. Like we don't know what's
    going to happen.
  topic: technical/predictions
- impact_reason: Quantifies the massive scaling hypothesis that underpinned the success
    of modern LLMs, emphasizing the sheer magnitude of the bet made on scale.
  relevance_score: 9
  source: llm_enhanced
  text: It's a big bet to say, hey, this thing works when it's 200 megabytes. What
    if it were 200 gigabytes large, you know? And yeah, it was right.
  topic: technical/strategy
- impact_reason: A pragmatic and slightly cynical take on the rapid obsolescence of
    specific LLM models, emphasizing that the underlying architectural paradigm (auto-regressive
    decoder) remains stable for now.
  relevance_score: 9
  source: llm_enhanced
  text: this LLM will be dead in the matter of years. But if the next one is just
    some auto-aggressive decoder-based LLM, basically everything's the same, just
    replace the model name.
  topic: predictions/technical
- impact_reason: Directly addresses the misuse of benchmarks as marketing tools, which
    drives the 'teaching to the test' phenomenon.
  relevance_score: 9
  source: llm_enhanced
  text: Benchmarks to your point are also used as a marketing tactic for a company
    to say, hey, we're beating XYZ at benchmark Z. Therefore, we are a better company
    at task here.
  topic: business/safety
- impact_reason: Highlights the trend of major AI labs moving away from data transparency,
    even for models that are technically 'open-weight' or widely used.
  relevance_score: 9
  source: llm_enhanced
  text: OpenAI similarly has not open-sourced an autoregressive language model in
    some years at this point. They've also stopped releasing, to my knowledge, they
    stopped releasing the contents of their training data after GPT-3, right before
    ChatGPT.
  topic: strategy/safety
- impact_reason: A specific, recent example (Siko-Phantic debacle) illustrating alignment
    failure where the model optimizes for agreement over correctness/utility.
  relevance_score: 9
  source: llm_enhanced
  text: OpenAI released a model with little fanfare, and then within a week pulled
    it off the shelf because it was just agreeing with people too much. It wasn't
    actually being a good language model. It was just agreeing with everything the
    human said.
  topic: safety/technical
- impact_reason: Distinguishes between testing factual recall (memorization) and testing
    the ability to generalize and seek new information, arguing benchmarks fail at
    the latter.
  relevance_score: 9
  source: llm_enhanced
  text: The point of the question is so that the AI doesn't know the answer, it knows
    how to go find it. Okay, but that's not what we're testing. There's an answer,
    and we're just checking if the answer is right or not.
  topic: technical/strategy
- impact_reason: 'Defines a higher, more valuable capability for AIs: meta-cognition
    (recognizing knowledge gaps) and information synthesis, which current benchmarks
    ignore.'
  relevance_score: 9
  source: llm_enhanced
  text: simply memorizing a fact and knowing how to go fill in the gaps of your own
    knowledge and recognizing that you have gaps in your knowledge. For me, that's
    even more interesting.
  topic: technical/predictions
- impact_reason: Exposes how marketing obscures the necessary prompting techniques
    required to achieve benchmark scores, misleading non-expert consumers.
  relevance_score: 9
  source: llm_enhanced
  text: underneath the name of the benchmark, often they will also in small print
    say something like five-shot COT. And what they're saying is, we tried this just
    by asking the questions and it didn't go so well. So what we did is we added few-shot
    learning and chain-of-thought prompting.
  topic: business/technical
- impact_reason: 'A clear call to action for frontier labs: provide full methodological
    transparency (data, prompting, setup) to enable replication.'
  relevance_score: 9
  source: llm_enhanced
  text: The onus, I think, in my opinion, is less on the benchmark creators and just
    more back on the education side of the frontier labs is to say, look, we're an
    open book here. When we say our model got X%, this is how we did it. This is what
    we used. You can replicate it here.
  topic: strategy/safety
- impact_reason: A specific technical insight into transformer limitations (positional
    bias) and how it affects performance on multiple-choice benchmarks like MMLU.
  relevance_score: 9
  source: llm_enhanced
  text: transformer-based architectures are very, very prone to something called a
    positional bias, where they tend to prefer the first elements in a multiple choice
    over the last elements of a multiple choice.
  topic: technical
- impact_reason: 'Offers a practical, stress-testing methodology for users to gauge
    model intelligence: consistency under high randomness.'
  relevance_score: 9
  source: llm_enhanced
  text: Turn the temperature up, ask it again. And if it still answers the question
    correctly, pretty consistently, you've got yourself a pretty smart model.
  topic: technical/strategy
- impact_reason: Re-frames the utility of trivia benchmarks, suggesting they are excellent,
    low-cost proxies for measuring factual hallucination rates.
  relevance_score: 9
  source: llm_enhanced
  text: Trivia is still useful, mostly for hallucination rates. Because if you want
    a really quick and cheap way to test how much your model will make something up
    slash generate something with confidence that is untrue, trivia is actually perfect
    for that.
  topic: technical/safety
- impact_reason: Strongly advocates for domain-specific benchmarking (like SWE-bench)
    over general benchmarks when deploying specialized models for enterprise use cases.
  relevance_score: 9
  source: llm_enhanced
  text: We talked about domain-specific benchmarks and how those can potentially be
    more useful, especially if you're going to be doing like something like SWE bench,
    it's going to be useful for you if you're going to be taking an open-source LLM
    and putting it in your IDE that you're developing for software developers.
  topic: business/strategy
- impact_reason: A powerful anecdote illustrating the common failure of enterprises
    to define clear, measurable success criteria for their AI deployments beyond simply
    using the 'newest model.'
  relevance_score: 9
  source: llm_enhanced
  text: How do you know your AI is working? And I just stand there quietly. And they're
    like, what do you mean? And I'm like, I don't know, you tell me what I mean. How
    do you know your AI is working?
  topic: business/strategy
- impact_reason: 'Defines the fundamental problem of benchmark contamination: the
    existence of the answer key is necessary for validation but guarantees data leakage.'
  relevance_score: 9
  source: llm_enhanced
  text: If a benchmark literally comes with the answers, that's the whole point of
    the benchmark, is you're supposed to know the right answer. So the same place
    where you get the questions for the benchmark also has the answers to the benchmarks
    where you can validate that it's correct. So it's impossible to not have the answers
    not on the internet.
  topic: technical/safety
- impact_reason: Strong endorsement of the Chatbot Arena methodology (blind human
    preference testing) as a superior measure of real-world usability compared to
    static benchmarks.
  relevance_score: 9
  source: llm_enhanced
  text: I'm a fan of the arena in general. The idea of blind judging from a human
    for me is one of the best ways to really get a good sense of an LLM's usability.
  topic: strategy
- impact_reason: 'Raises a profound philosophical and governance question about AI
    evaluation: who holds the authority to define correctness, and how do benchmarks
    serve as a temporary proxy for that authority?'
  relevance_score: 9
  source: llm_enhanced
  text: Who has the right to judge whether or not the AI was correct or not? That's
    a big question. And again, that's why we have benchmarks. Is that is our current
    proxy to that question?
  topic: safety/strategy
- impact_reason: 'This highlights a critical, often overlooked, foundational step
    in enterprise AI deployment: establishing clear success metrics beyond general
    benchmarks.'
  relevance_score: 9
  source: llm_enhanced
  text: For any of my clients, the first thing I ask when I get into their AI systems
    is, how do you know your AI is working?
  topic: business/strategy
- impact_reason: Provides an actionable, time-boxed recommendation for establishing
    internal validation, framing the necessary effort as a short-term investment.
  relevance_score: 9
  source: llm_enhanced
  text: let's actually build a testing framework. It's going to be annoying, but it's
    only going to take a week.
  topic: business/strategy
- impact_reason: Explains how domain-specific test sets transform evaluation from
    a static check into a dynamic engine for continuous improvement and internal competition.
  relevance_score: 9
  source: llm_enhanced
  text: Once you have a test set for your domain, A, you can now get a better sense
    of how your AI is doing. That's table stakes. Now, however, it really opens up
    your experimentation because the next phase of your AI labs or your AI team is
    to say, okay, now we have a way, we all agree that if this number is bigger, given
    these promptings... you know, go. It's now everyone's job to prompt better, fine-tune
    better, do whatever you have to do to get better at our internal benchmark.
  topic: strategy
- impact_reason: Highlights an advanced technique for data decontamination, showing
    how LLMs can be used to police the integrity of testing data against training
    data contamination.
  relevance_score: 9
  source: llm_enhanced
  text: There are papers who actually go as far as to create fine-tuned LLMs whose
    only job is to detect rephrasing of questions. That's a task. Is this a rephrased
    version of that? Yes or no? That's a task that we can fine-tune an LLM for.
  topic: technical/safety
- impact_reason: Connects the creation of high-quality, domain-specific synthetic
    data (using expensive models) directly to efficient fine-tuning techniques like
    LoRA on smaller models.
  relevance_score: 9
  source: llm_enhanced
  text: We could use that to create a test set, or to create a training set, rather,
    as well as a test set. And then we could also actually know what... you can fine-tune.
    So you could take something like a 7 billion parameter, an LLM model, and you
    can fine-tune it very rapidly. LoRA? LoRA? Exactly, with Low-Rank Adaptation...
  topic: technical
- impact_reason: 'Provides a concrete methodology for validating LLM judges: calibrate
    them against a small, high-quality human evaluation set to establish reliability
    before scaling.'
  relevance_score: 9
  source: llm_enhanced
  text: we compared on a small number, human evaluations... and we got this small
    set, and we were able to compare. Okay, there is a high rate of inter-rater reliability
    between the humans and this expensive LLM that we're calling. And so let's just
    use the LLMs from now on.
  topic: strategy/technical
- impact_reason: 'Identifies the core challenge in LLM-based evaluation: rubric consistency
    across different models, emphasizing that the goal is matching human judgment,
    not maximizing the score.'
  relevance_score: 9
  source: llm_enhanced
  text: What you are using is called a rubric effectively. You are judging a single
    piece of content against some criteria... And one of the problems with rubric
    is there's prompts on top of an LLM, and if you give that prompt to 10 different
    LLMs, they're all going to give probably some different scores across the board.
    So which one actually matches the human?
  topic: technical/safety
- impact_reason: Highlights the critical role of reliable, real-time evaluation in
    unlocking further experimentation and iteration with LLMs.
  relevance_score: 9
  source: llm_enhanced
  text: once you do it and you know this LLM knows how to judge this task given this
    prompt, again, experiments open. Cause now we have a relatively reliable way to
    make that evaluation in real time.
  topic: strategy/technical
- impact_reason: Draws a strong analogy between human evaluation methods (rubrics,
    feedback) and the core mechanism of reinforcement learning in AI training.
  relevance_score: 9
  source: llm_enhanced
  text: You're basically teaching the AI how to solve a task through reward and punishment.
    I mean, which is the basic point of reinforcement learning anyways, but it's almost
    perfect in the way that the way we think about evaluation lends itself quite nicely
    to the way we think about training these LLMs today.
  topic: technical/strategy
- impact_reason: Defines the technical relationship between perplexity and token confidence,
    crucial for understanding model output reliability.
  relevance_score: 9
  source: llm_enhanced
  text: perplexity is effectively a judge of the confidence of the tokens being predicted.
    It is correlated to the actual token probability of themselves. As the confidence
    goes up, the perplexity goes down. A lower perplexity is better.
  topic: technical
- impact_reason: 'Identifies a major limitation of perplexity as an absolute metric:
    the need for comparison sets or arbitrary, domain-specific thresholds.'
  relevance_score: 9
  source: llm_enhanced
  text: The problem with perplexity among other things is that A, it requires other
    answers to be judged against. For example, if I ask what planet is known as the
    red planet... What if you don't have options? What if you don't have other things
    to compare it to? Now you need a threshold. What's the threshold for a good perplexity?
    I don't know.
  topic: technical/limitations
- impact_reason: Connects the limitations of token-level metrics (like perplexity)
    to the emerging field of interpretability and 'world models'—understanding internal
    reasoning paths.
  relevance_score: 9
  source: llm_enhanced
  text: And that's when you start talking about world models, that the idea of probing
    of, can you hijack an LLM's internal parameters to try to see what is it thinking
    about here? Like what's going on through those 20 billion parameters so that by
    the time it gets to the next token, a lot's happened.
  topic: technical/interpretability
- impact_reason: Offers insight into common architectural approaches (token projection)
    used by leading multimodal models like Fuyu to unify different data types.
  relevance_score: 9
  source: llm_enhanced
  text: Is it an omni model like Fuyu or Lava, where it's able to take in these different
    modes of data and basically project them to all look like text tokens? That's
    how OpenAI Fuyu does their image input.
  topic: technical
- impact_reason: 'Offers a strategic simplification for multimodal evaluation: focus
    on task performance rather than the modality itself, suggesting a task-centric
    evaluation framework.'
  relevance_score: 9
  source: llm_enhanced
  text: The point of it all is to say, I don't think we should be judging multimodal
    models really any differently. We are still trying to understand if they can perform
    a specific task for us. Whether or not that task involves an image, should frankly
    be irrelevant.
  topic: strategy
- impact_reason: Provides a clear structural overview of foundational LLM concepts
    necessary for understanding the technology.
  relevance_score: 8
  source: llm_enhanced
  text: The book is very much organized into a few sections. Starting with kind of
    a level set. What is a language model? What is an autoregressive language model
    versus autoencoding language model? What does that mean?
  topic: technical
- impact_reason: Indicates a shift in focus in the third edition, acknowledging the
    emergence and importance of advanced reasoning capabilities in LLMs.
  relevance_score: 8
  source: llm_enhanced
  text: This will be the first time that I talk about reasoning models in one of those
    books.
  topic: technical/trends
- impact_reason: A critical reflection on the slow pace of widespread adoption or
    application of the Transformer architecture between its invention (2017) and the
    Gen AI boom, suggesting a missed opportunity or slow realization of its power.
  relevance_score: 8
  source: llm_enhanced
  text: And then people are shocked to realize, oh yeah, this thing was invented in
    2017. What have we been doing since then? Why haven't we been doing anything with
    this since 2017?
  topic: strategy/history
- impact_reason: 'Provides a strong philosophy for consulting and education: teaching
    frameworks over providing one-off solutions, which is highly relevant for rapidly
    evolving fields like AI.'
  relevance_score: 8
  source: llm_enhanced
  text: A conversation with me is not just supposed to, you know, get you to your
    goal. It's supposed to kind of give you the framework for how to actually tackle
    this problem the next time it comes around.
  topic: business/strategy
- impact_reason: Raises a crucial definitional distinction in the current AI landscape
    that impacts transparency, reproducibility, and competitive dynamics.
  relevance_score: 8
  source: llm_enhanced
  text: what is the difference between open source and open weights?
  topic: technical/strategy
- impact_reason: 'Offers a nuanced, counter-intuitive perspective on contamination:
    if a model performs significantly *worse* than expected despite using potentially
    contaminated data, it suggests the contamination wasn''t effective or the model
    is fundamentally weaker.'
  relevance_score: 8
  source: llm_enhanced
  text: And on one hand, that's bad because, well, clearly I would want that. But
    on the other hand, it's actually good because it gives us a sense of, well, we
    can get a gut check if someone is cheating because if they're actually performing
    worse than the state-of-the-art model, you would think, well, they're probably
    n [not cheating].
  topic: safety/strategy
- impact_reason: 'This captures the strategic dilemma for companies: underperformance
    suggests cheating isn''t happening, but achieving parity requires potentially
    crossing ethical lines.'
  relevance_score: 8
  source: llm_enhanced
  text: if they're actually performing worse than the state-of-the-art model, you
    would think, well, they're probably not cheating. But that's also kind of a double-edged
    sword because you would say, well, hold on, I don't want to be worse. I want to
    be at the same level of these models.
  topic: business/strategy
- impact_reason: Reinforces the theme of unverifiable claims regarding critical alignment
    changes (RLHF/RLAIF), demanding blind faith from the public.
  relevance_score: 8
  source: llm_enhanced
  text: we changed the reward signal in our reinforcement learning alignment and we
    think that's what happened. But again, we have to take their word for it. We have
    no way of actually double-checking any of this.
  topic: safety/technical
- impact_reason: Critique of current benchmarks focusing on obscure trivia rather
    than practical, real-world utility, leading to misleading marketing.
  relevance_score: 8
  source: llm_enhanced
  text: a lot of the questions in them, they don't, they often don't seem practically
    related to the kinds of problems that people solve. So for example, I'd agree.
    Yeah, I think I think I'm pulling this out of your content here. But I have on
    Humanities Last Exam, those questions like how long was the second Great War in
    Starcraft? And there's also three questions on Legal Legends in there.
  topic: strategy/business
- impact_reason: 'Provides actionable advice for users based on benchmark disclosures:
    understand *how* the score was achieved to inform real-world deployment strategy.'
  relevance_score: 8
  source: llm_enhanced
  text: The takeaway is LLM A responds very positively to few-shot learning. So when
    I use LLM A for whatever task, I should attempt to induce few-shot learning into
    that system because they're claiming they can only make it better than other models
    by introducing that prompting technique.
  topic: business/strategy
- impact_reason: 'Suggests a practical testing methodology: stress-test benchmarks
    by varying answer order and temperature to check for true robustness beyond a
    single lucky run.'
  relevance_score: 8
  source: llm_enhanced
  text: if you can't get that resistance, that resiliency, consistency, out of the
    LLM, that's also going to be a problem. So you can still use the same benchmark,
    but manipulate it in such a way that you actually get that sense of consistency.
  topic: technical
- impact_reason: Highlights SWEBench as a positive evolution toward more complex,
    domain-specific evaluation that requires more than simple recall.
  relevance_score: 8
  source: llm_enhanced
  text: SWE is actually a really good example of a benchmark that goes beyond just
    answer the question. Because I think before SWE, most benchmarks were multiple
    choice or some kind of one-to-two-sentence free response.
  topic: technical
- impact_reason: 'Explains the effect of low temperature: reducing randomness and
    increasing determinism/repetition.'
  relevance_score: 8
  source: llm_enhanced
  text: If you turn the temperature down, what you're basically dealing is you're
    changing that distribution... You're really sharpening the distribution. It's
    more likely that you'll get the same answer over and over and over again.
  topic: technical
- impact_reason: Illustrates the extreme behavior of high temperature settings (T=2),
    showing the boundary of model coherence.
  relevance_score: 8
  source: llm_enhanced
  text: If you increase the temperature, the opposite happens. You get much more diverse
    responses. Fun fact, if you go right now to OpenAI and their playground and you
    turn the temperature up all the way they let you, which is two, and you ask it
    a basic question, you are very, very likely going to see some literal, absolute
    gibberish come out of the LLM.
  topic: technical
- impact_reason: 'Offers actionable, time-bound advice for enterprises: prioritize
    building a custom testing framework immediately, even if it seems tedious.'
  relevance_score: 8
  source: llm_enhanced
  text: At this point, one of the first things I always recommend is let's actually
    build a testing framework. It's going to be annoying, but it's only going to take
    a week.
  topic: business/strategy
- impact_reason: 'Provides a modern, hybrid approach to creating test sets: use LLMs
    for synthetic data generation, but mandate human oversight for quality control.'
  relevance_score: 8
  source: llm_enhanced
  text: We're going to build out a couple of questions. We can even help, we can get
    some help from GPT to build some synthetic data sets as long as a human actually
    overlooks all of them and ma[nually checks them].
  topic: strategy/technical
- impact_reason: 'Identifies a key limitation of preference-based judging (like the
    Arena): human evaluators often prioritize style/tone (''sicker fancy'') over factual
    correctness or utility.'
  relevance_score: 8
  source: llm_enhanced
  text: If I'm just a layperson talking to a chap, to your point, I'm not coming in
    with structured questions. I'm just going to pick the one I like the most. And
    that might come down to which one's talking the way I like it to talk, which kind
    of leads to the whole sicker fancy thing, right?
  topic: safety/strategy
- impact_reason: A concise summary of the core weakness of preference-based evaluation
    systems when used in isolation—they measure subjective liking, not objective knowledge.
  relevance_score: 8
  source: llm_enhanced
  text: At this point, we're just judging preference as opposed to knowledge. And
    again, without that structured data set.
  topic: strategy
- impact_reason: Introduces the concept of 'benchmark-specific training' or model
    versioning designed to game public evaluation systems, highlighting trust issues
    in proprietary model releases.
  relevance_score: 8
  source: llm_enhanced
  text: One of the separate allegations from LLM4 was they released a tested or a
    trained to test model specifically for the arena that was different than the LLM4
    we all got in the end. Again, total allegation. But those rumors start bubbling
    up when people notice discrepancies.
  topic: safety/business
- impact_reason: Warns about the ease of 'cheating' via semantic rephrasing and links
    data hygiene directly to long-term model generalization and mitigating future
    drift.
  relevance_score: 8
  source: llm_enhanced
  text: It's pretty easy to rephrase these questions to make them sound different
    enough, but still learn that information. So doing some kind of decontamination
    step in your training data would really just at least help the generalizability
    of the system.
  topic: safety/technical
- impact_reason: Poses the central question regarding intrinsic model uncertainty
    measurement, a key area in AI safety and reliability.
  relevance_score: 8
  source: llm_enhanced
  text: What about perplexity and confidence signals where the model kind of has its
    own ability to recognize that this is a situation where it maybe isn't sure it
    could be hallucinating?
  topic: safety/technical
- impact_reason: 'Identifies the immediate challenge posed by multimodal AI: the increased
    complexity and cost of creating relevant evaluation benchmarks.'
  relevance_score: 8
  source: llm_enhanced
  text: So now all of a sudden, we can have AI systems that can be processing images,
    natural language, audio, maybe all at once. And so testing has to become more
    complex, probably more expensive to create as well. So where are we on this? And
    I'd just love to hear your thoughts on multimodal evaluation.
  topic: predictions/business
- impact_reason: Sets the expectation for a technically deep but accessible discussion,
    which is valuable for a broad data science audience.
  relevance_score: 7
  source: llm_enhanced
  text: Today's episode skews slightly toward our more technical listeners, but Sinan
    excels at explaining complex concepts in a clear way.
  topic: strategy
- impact_reason: Defines the target audience and purpose of a key introductory text
    in the LLM space, emphasizing accessibility beyond just engineers.
  relevance_score: 7
  source: llm_enhanced
  text: The same thing with Quick Start Guide to LLMs. It's very much meant for someone
    who is just diving in for the first time, engineer or not, just what do I need
    to know to understand LLMs?
  topic: business/strategy
- impact_reason: Announces a forthcoming practical, application-focused book, signaling
    a move from foundational knowledge to applied implementation.
  relevance_score: 7
  source: llm_enhanced
  text: It's very able to cookbook of AI applications. It's assuming you've read my
    Quick Start Guide. And again, we are speaking the same language here is... Let's
    just dive into the top 20 applications of LLMs I have seen in the last 10 years.
  topic: business/strategy
- impact_reason: Poses a strategic question about Google's relative position in the
    LLM race despite inventing the foundational Transformer architecture, hinting
    at execution or strategic divergence.
  relevance_score: 7
  source: llm_enhanced
  text: Why are they still not at the top of the pack then of LLMs? You know, they're
    doing their best to get up there. But it's also, it's always a very funny history
    to think about, you know, how long ago this all was relative to even today.
  topic: strategy
- impact_reason: 'Defines the necessary, high-level purpose of benchmarks: tracking
    broad progress across the field.'
  relevance_score: 7
  source: llm_enhanced
  text: Benchmarks are used to evaluate the general macro trends of LLMs in a specific
    domain or task.
  topic: strategy
- impact_reason: Shifts responsibility from benchmark creators to consumers and labs,
    emphasizing that the incentive structure drives shortcut adoption.
  relevance_score: 7
  source: llm_enhanced
  text: I think the actual onus is now back on us, the consumer, and on the frontier
    labs themselves. Because again, when you are chasing a large number on a benchmark,
    you can take shortcuts.
  topic: business/strategy
- impact_reason: 'Reframes the problem: the issue isn''t the quality of the benchmark
    creation, but the incentive structure and consumer interpretation.'
  relevance_score: 7
  source: llm_enhanced
  text: What do you think? So how could people do better? How could benchmark makers
    do better on this trivia versus practicality kind of issue? I think a lot of the
    onus should not be on the benchmark creators.
  topic: strategy
- impact_reason: Provides a clear, accessible definition of the critical 'temperature'
    parameter for a broader audience.
  relevance_score: 7
  source: llm_enhanced
  text: Temperature is probably the most popular inference parameter. It's basically
    a number, it's a lever you're allowed to change while you are asking the LLM a
    question.
  topic: technical
- impact_reason: Expresses a strong, opinionated stance on LLM configuration safety,
    suggesting that temperatures above 1 inherently lead to unreliable outputs.
  relevance_score: 7
  source: llm_enhanced
  text: I don't understand the decision as to why to let someone increase the temperature
    more than one. You are asking for trouble.
  topic: safety/technical
- impact_reason: Questions the utility of hyper-niche benchmarks (like Starcraft)
    when models fail basic, general knowledge tests regarding truthfulness.
  relevance_score: 7
  source: llm_enhanced
  text: Why go as far as a very niche thing like Starcraft? Why not like PersonQA,
    TruthfulQA, stick to the big things, like this is a real question from TruthfulQA.
    What happens if you eat watermelon seeds?
  topic: strategy
- impact_reason: A philosophical insight celebrating the elegance of core mathematical
    principles (linear algebra) underpinning modern, complex AI advancements like
    LoRA.
  relevance_score: 7
  source: llm_enhanced
  text: I just saw the basics of linear algebra being applied in such a simple way.
    I think that in DeepSeek, the DeepSeek, the R1 paper and the LoRA paper, where
    the last two papers were I was just like, man, sometimes all it is is just linear
    algebra 101. And that's awesome.
  topic: technical
- impact_reason: Insight into the author's content creation philosophy—prioritizing
    new topics over iterative updates, which is relevant for experts creating ongoing
    educational material.
  relevance_score: 6
  source: llm_enhanced
  text: I aim very much for diversity more than I aim for just kind of building upon
    the same content every year.
  topic: strategy
source: Unknown Source
summary: '## Podcast Episode Summary: 903: LLM Benchmarks Are Lying to You (And What
  to Do Instead), with Sinan Ozdemir


  This episode of the Super Data Science Podcast features returning guest Sinan Ozdemir,
  founder and CTO of Loop Genius and author of *The Quick Start Guide to LLMs*, focusing
  on the critical flaws in current Large Language Model (LLM) benchmarking practices
  and offering alternative evaluation strategies.


  ### 1. Focus Area

  The primary focus is the **limitations and manipulation of current LLM benchmarks**
  (like MMLU), the phenomenon of "teaching to the test" (data contamination), and
  practical methodologies for **effective, use-case-specific LLM evaluation**. Secondary
  topics included the history of Transformer architecture, the strategic bets made
  by OpenAI versus Google DeepMind, and Sinan''s current work in AI education and
  advising.


  ### 2. Key Technical Insights

  *   **Benchmark Contamination is Pervasive:** Leading AI labs are often fine-tuning
  models specifically on benchmark test sets (or highly similar data), rendering public
  scores unreliable indicators of general capability. Detection methods like simple
  keyword or n-gram matching are insufficient due to rephrasing.

  *   **The Need for "Reasoning" Benchmarks:** Current benchmarks fail to capture
  true reasoning ability. Sinan highlights that a simple question about watermelon
  seeds revealed a **40% failure rate** in advanced AI reasoning models, suggesting
  a significant gap between reported scores and practical reliability.

  *   **Evaluation as a Conversation Starter:** Benchmarks should serve as a starting
  point for macro-trend analysis, not the final word on model selection. Effective
  evaluation requires intimate, use-case-specific testing rather than relying solely
  on public leaderboards.


  ### 3. Business/Investment Angle

  *   **Marketing vs. Reality:** Benchmarks are heavily used as a marketing tactic
  by AI companies to claim superiority, which can mislead organizations making purchasing
  or integration decisions.

  *   **Advising VCs:** Sinan''s role advising Tola Capital involves educating investors
  on the underlying technology so they can critically assess vendor claims, emphasizing
  teaching the framework rather than just providing answers.

  *   **The Value of Practical Application:** Sinan is developing a new "cookbook"
  style book focused on the top 20 applied LLM use cases, signaling a market shift
  toward practical implementation guides over purely theoretical model deep dives.


  ### 4. Notable Companies/People

  *   **Sinan Ozdemir:** Guest, AI entrepreneur, author, and educator, providing the
  core critique of benchmarking.

  *   **OpenAI vs. Google DeepMind:** Discussed as a historical case study where OpenAI’s
  bet on massive scaling using Transformer architecture (combined with RLHF) proved
  strategically superior to DeepMind''s focus on deep reinforcement learning generalization,
  despite Google inventing the Transformer.

  *   **AWS, Anthropic, Databricks, Poolside:** Mentioned in the context of adopting
  AWS Trainium 2 chips for large-scale AI workloads.

  *   **Tola Capital:** VC firm where Sinan advises, focusing on educating partners
  about AI technology.


  ### 5. Future Implications

  The industry is moving toward more sophisticated, domain-specific evaluation methods
  that move beyond static, public benchmarks. The future of benchmarking will need
  to incorporate testing for **genetic and multimodal models**. Furthermore, the conversation
  implies that while model architectures (like the Transformer) are foundational,
  the real competitive edge lies in proprietary data, scaling strategies, and effective
  alignment/reasoning techniques.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Data Scientists, AI Product
  Managers, and Technology Leaders** who are responsible for selecting, deploying,
  and trusting LLMs in production environments. It is specifically targeted at professionals
  who need to look past marketing hype to understand true model performance.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- investment
- startup
- nvidia
- google
- openai
title: '903: LLM Benchmarks Are Lying to You (And What to Do Instead), with Sinan
  Ozdemir'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 251
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 20
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 17
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 12
  prominence: 1.0
  topic: investment
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 7
  prominence: 0.7
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 03:57:34 UTC -->
