---
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew D'Amello,
    Senior Editor here at Emer
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew D'Amello, Senior
    Editor here at Emerge Technology R
  name: Matthew D
  position: 53
- category: unknown
  confidence: medium
  context: he AI and Business Podcast. I'm Matthew D'Amello, Senior Editor here at
    Emerge Technology Research. Today's guest
  name: Senior Editor
  position: 71
- category: unknown
  confidence: medium
  context: cast. I'm Matthew D'Amello, Senior Editor here at Emerge Technology Research.
    Today's guest is Michael Berger, Head of Ensure
  name: Emerge Technology Research
  position: 93
- category: unknown
  confidence: medium
  context: e at Emerge Technology Research. Today's guest is Michael Berger, Head
    of Ensure AI at Munique Ray. Michael return
  name: Michael Berger
  position: 138
- category: unknown
  confidence: medium
  context: esearch. Today's guest is Michael Berger, Head of Ensure AI at Munique
    Ray. Michael returns to the Emerge Pod
  name: Ensure AI
  position: 162
- category: unknown
  confidence: medium
  context: y's guest is Michael Berger, Head of Ensure AI at Munique Ray. Michael
    returns to the Emerge Podcast platform t
  name: Munique Ray
  position: 175
- category: unknown
  confidence: medium
  context: Ensure AI at Munique Ray. Michael returns to the Emerge Podcast platform
    to discuss the evolving landscape of AI
  name: Emerge Podcast
  position: 211
- category: unknown
  confidence: medium
  context: nd governance. Since his last appearance in 2022, Generative AI has shifted
    from niche discussions to widespread
  name: Generative AI
  position: 342
- category: unknown
  confidence: medium
  context: important moment, especially domestically in the United States. You're
    seeing a lot of congressional hearings. E
  name: United States
  position: 1526
- category: unknown
  confidence: medium
  context: pact that we've seen so far in the last 15 years. And I probably don't
    need to explain on today's show. B
  name: And I
  position: 1868
- category: unknown
  confidence: medium
  context: ocusing really on even if we can use the EU's new AI Act as a primary example,
    how are we seeing a lot of
  name: AI Act
  position: 1982
- category: unknown
  confidence: medium
  context: e according to the regulation of the legislation. What I believe is quite
    good when it comes to the EU AI
  name: What I
  position: 2438
- category: unknown
  confidence: medium
  context: What I believe is quite good when it comes to the EU AI Act is that it's
    really risk-based. So it tries to ba
  name: EU AI Act
  position: 2488
- category: unknown
  confidence: medium
  context: certain best practices and acceptable practices. So I don't think that
    one approach is better than the
  name: So I
  position: 5186
- category: unknown
  confidence: medium
  context: example, one case was with the Canadian airline. The Canadian airline utilized
    a ChatGPT-like model in passenge
  name: The Canadian
  position: 5972
- category: unknown
  confidence: medium
  context: e we had you for, or at least going back to 2022, Gen AI wasn't quite part
    of the conversation. So neither
  name: Gen AI
  position: 8546
- category: unknown
  confidence: medium
  context: e are models that can tell you the entire cast of The Godfather whether
    or not that's your business problem or no
  name: The Godfather
  position: 14576
- category: tech
  confidence: high
  context: today's episode, consider leaving us a review on Apple Podcasts and let
    us know what you learned, found
  name: Apple
  position: 20037
- category: unknown
  confidence: medium
  context: today's episode, consider leaving us a review on Apple Podcasts and let
    us know what you learned, found helpful,
  name: Apple Podcasts
  position: 20037
- category: unknown
  confidence: medium
  context: y known as Twitter, at Emerge, and that's spelled E M E R G E, as well
    as our LinkedIn page. On behalf of Danie
  name: E M E R G E
  position: 20226
- category: unknown
  confidence: medium
  context: R G E, as well as our LinkedIn page. On behalf of Daniel Fagella, our CEO
    and Head of Research, as well as the res
  name: Daniel Fagella
  position: 20282
- category: unknown
  confidence: medium
  context: Research, as well as the rest of the team here at Emerge AI Research, thanks
    so much for joining us today, and we'll c
  name: Emerge AI Research
  position: 20368
- category: ai_application
  confidence: high
  context: The organization where the guest, Michael Berger, is the Head, focusing
    on AI risk management and governance.
  name: Ensure AI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company employing the guest, which deals with AI risk management.
  name: Munique Ray
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used as a specific example of a large language model that produced incorrect
    information (hallucination) in a real-world case involving a Canadian airline.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in context of updates causing significant shifts in error rates
    across use cases, illustrating the instability of very large models.
  name: GPT-4
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Implied developer of ChatGPT and GPT-4, though the name itself is not explicitly
    stated, only the products.
  name: OpenAI
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The organization producing the podcast, focused on AI research and analysis
    for business leaders.
  name: Emerge AI Research
  source: llm_enhanced
- category: organization
  confidence: high
  context: The social media handle (X/Twitter) for the podcast/research organization.
  name: Emerge
  source: llm_enhanced
date: 2025-04-15 12:23:00 +0000
duration: 21
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: the insurance industry in these spaces. Yeah, I believe that your regulation
    legislation also brings more awareness about risk and also makes it clear what
    consequences occur if things go wrong or if things are not done according to the
    regulation of the legislation. What I believe
  text: the future of the insurance industry in these spaces. Yeah, I believe that
    your regulation legislation also brings more awareness about risk and also makes
    it clear what consequences occur if things go wrong or if things are not done
    according to the regulation of the legislation. What I believe is quite good when
    it comes to the EU AI Act is that it's really risk-based.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-._4.15.25_-_Michael_Berger_ep_2.mp3?dest-id=151434
processing_date: 2025-10-06 14:13:46 +0000
quotes:
- length: 184
  relevance_score: 6
  text: With real-world examples such as large language models producing incorrect
    outputs, he illustrates the importance of defining risk tolerance and implementing
    safeguards for AI adoption
  topics: []
- length: 176
  relevance_score: 6
  text: And I think that's a very welcome development because I believe at the end
    of the day that this can also unlock even more potential and growth for AI and
    generative AI adoption
  topics:
  - growth
- length: 215
  relevance_score: 4
  text: And I believe this gives rise to a new form of aggregation risk on which companies
    need to look out for when considering their AI and generative AI use cases, especially
    when it comes to the use of foundation models
  topics: []
- length: 223
  relevance_score: 4
  text: So I believe that every generative AI model use case also requires some form
    of a fine-tuning in case the AI model would be adopted to really be very effective
    and should be robust, and then I think fine-tuning is necessary
  topics: []
- impact_reason: 'Defines the core function of AI governance: managing inherent uncertainties
    (hallucinations, errors) specific to modern AI systems.'
  relevance_score: 10
  source: llm_enhanced
  text: He delves into the critical role of AI governance as a framework for managing
    uncertainties like hallucinations, probabilistic errors, and discrimination.
  topic: safety
- impact_reason: Provides a positive assessment of the EU AI Act's risk-based approach,
    which is a key global regulatory trend.
  relevance_score: 10
  source: llm_enhanced
  text: What I believe is quite good when it comes to the EU AI Act is that it's really
    risk-based. So it tries to basically regulate and prescribe risk mitigating steps
    depending on the defined risk levels and the defined use cases which fall into
    certain risk categories.
  topic: regulation
- impact_reason: A critical real-world example demonstrating legal liability for third-party
    model hallucinations, setting a precedent for downstream users.
  relevance_score: 10
  source: llm_enhanced
  text: The Canadian airline utilized a ChatGPT-like model in passenger interactions...
    the model hallucinated and made up this discount policy... the courts essentially
    found that the airline is liable for the hallucinations that the model produced,
    even though the model was not built by the airline itself.
  topic: safety/litigation
- impact_reason: 'A fundamental statement on the nature of LLMs: failure (probabilistic
    error) is inherent and cannot be entirely eliminated by technology alone.'
  relevance_score: 10
  source: llm_enhanced
  text: business leaders we just will need to accept that those models are probabilistic
    models and that they can fail at any point in time, so that there always exists
    a probability, and it's not a failure of hallucinations, and that this is not
    avoided by any form of technical means.
  topic: technical/safety
- impact_reason: 'Defines the goal of AI governance: operationalizing risk reduction
    to an acceptable, tolerable level, rather than aiming for perfect mitigation.'
  relevance_score: 10
  source: llm_enhanced
  text: This risk will need to be managed, and this I think gave rise to the whole
    conversation there in now, which comes to AI governance. Now, can we really manage
    this risk operationally? How can we bring this down? Even if we cannot fully mitigate
    the risk, how can we bring this risk down to a tolerable level?
  topic: safety/governance
- impact_reason: Explains why AI discrimination risk is fundamentally different and
    potentially more severe than historical human bias—it scales systematically.
  relevance_score: 10
  source: llm_enhanced
  text: previously humans were making decisions, and it might be that discrimination
    cases were more rare or at least not systematic. However, with an AI model being
    used across the board and impacting many people, now discrimination risk is a
    risk which can suddenly turn systematic.
  topic: safety/ethics
- impact_reason: Crucial insight on how AI shifts discrimination risk from potentially
    isolated human errors to systematic, widespread issues.
  relevance_score: 10
  source: llm_enhanced
  text: Now, I think here it's a significant change in risk because previously humans
    were making decisions, and it might be that discrimination cases were more rare
    or at least not systematic. However, with an AI model being used across the board
    and impacting many people, now discrimination risk is a risk which can suddenly
    turn systematic.
  topic: safety/ethics
- impact_reason: Introduces and defines 'aggregation risk' specifically tied to the
    widespread adoption of shared foundational models.
  relevance_score: 10
  source: llm_enhanced
  text: thinking about foundational models, so if we would have a foundational model
    on this being trained and used for a specific use case by many companies, then
    yeah, and if it's found that this model discriminates in this sensitive use case,
    then suddenly many companies might be impacted by this kind of risk. And I believe
    this gives rise to a new form of aggregation risk on which companies need to look
    out for when considering their AI and generative AI use cases, especially when
    it comes to the use of foundation models.
  topic: safety/business/trends
- impact_reason: Contrasts the risk profile of large, generalist foundation models
    versus smaller, specialized, fine-tuned models, signaling a strategic shift.
  relevance_score: 10
  source: llm_enhanced
  text: This idea that you could get out-of-the-box very large models that will go
    through the entire organization, be collecting data, and not be meant to have
    direct contact with customers—these are models that can tell you the entire cast
    of The Godfather whether or not that's your business problem or not. They're very
    smart, but those tend to carry more risk versus smaller models that are developed
    for very specific purposes, very fine-tuned, and they don't need to tell you the
    cast of The Godfather.
  topic: strategy/business
- impact_reason: Clear prediction favoring a 'many specialized models' architecture
    over a single generalist model for better risk management.
  relevance_score: 10
  source: llm_enhanced
  text: I think this also already shows that the future might more lie into having
    many models for specific tasks and fine-tuned on specific tasks rather than one
    kind of general master model which is good for any use case. This was something
    what I believe that the future is, and I think this is also more risk-adequate.
  topic: predictions/strategy
- impact_reason: Provides a concrete, alarming example of model instability and unpredictable
    error rate shifts post-retraining in large models.
  relevance_score: 10
  source: llm_enhanced
  text: as we have seen in the ChatGPT update from 2023, when it comes to GPT-4 update,
    prior to the update, the model was performing very well on some use cases, and
    the error rate was below 5%, but then suddenly on those same kind of use cases
    after the update, just by the nature of how the retraining was done, the error
    rate shot up to in excess of 90%.
  topic: technical/safety
- impact_reason: Reiterates the core concept of aggregation risk, emphasizing that
    even fine-tuning doesn't eliminate correlated failure modes across users of the
    same foundation model.
  relevance_score: 10
  source: llm_enhanced
  text: this reliance on foundational models introduces a new form of risk we need
    to be aware of, as do our insurance companies. As mentioned before, if many companies
    are utilizing the same foundation model for similar use cases, we see that hallucination
    rates, error rates of those kind of models, even if they are fine-tuned to the
    specifications of the individual company, those error rates and hallucination
    rates can be highly positively correlated. This is something to watch out for
    because this gives rise to aggregation risk.
  topic: safety/business
- impact_reason: 'A highly strategic, counter-intuitive piece of advice: sometimes
    choosing a sub-optimal but architecturally diverse model is better for risk mitigation
    (anti-aggregation).'
  relevance_score: 10
  source: llm_enhanced
  text: one of the recommendations we have when it comes to that is also to think
    about whether to get very foundation models or very model architectures. So not
    just to build the most optimal and choose the most optimal model for a specific
    use case, but it might even be optimal not to choose the optimal model architecture,
    but a weaker model architecture which is less related to other model architectures
    which a company is using for similar use cases. Thinking on a complete level,
    this can help also to mitigate this kind of aggregation risk.
  topic: strategy/safety
- impact_reason: This captures the fundamental shift in the AI landscape since 2022,
    marking the transition from theoretical discussion to practical enterprise deployment.
  relevance_score: 9
  source: llm_enhanced
  text: Generative AI has shifted from niche discussions to widespread adoption, bringing
    both opportunities and challenges.
  topic: strategy
- impact_reason: 'Explains the primary benefit of regulation: clarifying risk accountability
    and consequences for non-compliance.'
  relevance_score: 9
  source: llm_enhanced
  text: legislation also brings more awareness about risk and also makes it clear
    what consequences occur if things go wrong or if things are not done according
    to the regulation of the legislation.
  topic: safety/regulation
- impact_reason: Connects regulatory clarity directly to the necessity for internal,
    structured AI risk management frameworks (governance).
  relevance_score: 9
  source: llm_enhanced
  text: I believe this risk-conscious approach towards regulation is in my mind quite
    a good one. This also brings, of course, the awareness to the fact that there
    is risk and that a company adopting AI and generative AI models will need to think
    about this risk in terms of putting a proper structure in place, putting the process
    in place, thinking about how to think about the risk management...
  topic: business/strategy
- impact_reason: Contrasts the EU's proactive regulation with the US's reactive, litigation-driven
    approach to setting AI norms.
  relevance_score: 9
  source: llm_enhanced
  text: In the US, I think also a certain way it's handled by litigation, where also
    then, basically, case law is set and then basically out of this emerge certain
    best practices and acceptable practices.
  topic: strategy/regulation
- impact_reason: 'The core strategic imperative for AI adoption: accepting inherent
    risk to capture upside.'
  relevance_score: 9
  source: llm_enhanced
  text: we will need to accept this kind of risk and embrace the risk together with
    the potential upside that those kind of models create.
  topic: strategy
- impact_reason: 'Actionable advice: Companies must quantify and define their acceptable
    risk thresholds for AI deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: This requires to define tolerance levels, which I think many companies do
    need also to do.
  topic: business/strategy
- impact_reason: Highlights the critical intersection of high-stakes use cases (consumer
    impact) and systemic bias risk.
  relevance_score: 9
  source: llm_enhanced
  text: especially when we're looking to sensitive use cases where basically private
    consumers are impacted by decisions of an AI model, now comes also all this question
    about AI discrimination.
  topic: safety/ethics
- impact_reason: Directly links AI deployment to increased liability and tangible
    business costs stemming from errors and hallucinations.
  relevance_score: 9
  source: llm_enhanced
  text: with every AI initiative, AI model, of course, the risk increases that, yeah,
    then a model might make errors, might hallucinate, and because of that, some form
    of liability might be created or some form of an actual cost occur.
  topic: business/safety
- impact_reason: Explains the systemic nature of AI discrimination risk, extending
    beyond a single deployment.
  relevance_score: 9
  source: llm_enhanced
  text: if the AI model is found to be discriminatory, then it might impact many near
    consumer groups where those models have been used, not just for a single company
    but also potentially across companies.
  topic: safety/ethics
- impact_reason: Strong assertion that fine-tuning is a mandatory step for effectiveness
    and robustness in almost all generative AI use cases.
  relevance_score: 9
  source: llm_enhanced
  text: I believe that every generative AI model use case also requires some form
    of a fine-tuning in case the AI model would be adopted to really be very effective
    and should be robust, and then I think fine-tuning is necessary.
  topic: technical/strategy
- impact_reason: 'Explains the practical benefit of specialized models: easier testing,
    clearer scope definition, and quantifiable risk.'
  relevance_score: 9
  source: llm_enhanced
  text: the specific smaller models for use cases, they are fine-tuned to them. They
    can also be tested on those kind of use cases, so it's more clear where what the
    scope of the use cases is, rather than blurring the boundaries between use case,
    the big model, and it's more clear what the use cases are where those models are
    used. And then the testing becomes easier, and it's easier also to quantify what
    the risk is...
  topic: strategy/safety
- impact_reason: Summarizes the fundamental necessity of formal AI governance for
    responsible adoption.
  relevance_score: 9
  source: llm_enhanced
  text: AI governance is essential. Michael highlighted that AI governance serves
    as a critical framework for managing risks like hallucinations, probabilistic
    errors, and discrimination, enabling enterprises to adopt AI responsibly while
    addressing inherent uncertainties.
  topic: business/strategy
- impact_reason: Highlights the maturation of enterprise AI adoption, moving past
    initial excitement to realistic risk assessment.
  relevance_score: 8
  source: llm_enhanced
  text: enterprises are moving beyond the hype cycle, adopting a more grounded perspective
    on AI capabilities and risks.
  topic: business
- impact_reason: Argues that regulatory certainty, despite perceived burdens, ultimately
    accelerates adoption by reducing ambiguity.
  relevance_score: 8
  source: llm_enhanced
  text: it can also unlock even more potential and growth for AI and generative AI
    adoption. Because it's more clear now what needs to be done in terms of risk management,
    what are the consequences, and it basically removes certain uncertainty which
    was there prior to the regulation coming out.
  topic: business
- impact_reason: The litigation outcome forces clearer delineation of responsibility
    in the AI supply chain.
  relevance_score: 8
  source: llm_enhanced
  text: I think this making it very clear who takes on what kind of responsibility.
    I think this also just helps businesses to build out their risk management approach
    better.
  topic: business
- impact_reason: 'Quantifies the cumulative risk: every new deployment adds to the
    overall organizational risk exposure.'
  relevance_score: 8
  source: llm_enhanced
  text: I definitely increased the risk because more and more AI use cases are pursued.
    One more AI initiative, AI model put into production now, and with every AI initiative,
    AI model, of course, the risk increases that, yeah, then a model might make errors,
    might hallucinate, and because of that, some form of liability might be created
    or some form of an actual cost occur.
  topic: business
- impact_reason: Highlights the increasing risk profile of modern generative AI due
    to increased user interaction and learning.
  relevance_score: 8
  source: llm_enhanced
  text: arguably more risky forms of AI that we've seen in generative in the last
    two years. They're learning more and more from users.
  topic: safety/trends
- impact_reason: Highlights the role of financial tools (like AI insurance) in enabling
    innovation by hedging probabilistic risks.
  relevance_score: 8
  source: llm_enhanced
  text: Enterprises must strike a balance between leveraging AI's automation capabilities
    and understanding the risk tied to probabilistic systems. AI insurance can serve
    as a safety net, helping businesses mitigate downside risks while pursuing innovation.
  topic: business/strategy
- impact_reason: 'Provides actionable components for effective AI integration: defining
    tolerance and assessing stability.'
  relevance_score: 8
  source: llm_enhanced
  text: Emerging standards and risk management and governance, such as defining tolerance
    thresholds and assessing model stability, are key for enterprises looking to integrate
    AI into their operations effectively and sustainably.
  topic: business/strategy
source: Unknown Source
summary: '## Comprehensive Summary: Global AI Regulations and Their Impact on Industry
  Leaders - with Michael Berger of Munich Re


  This podcast episode features Michael Berger, Head of Ensure AI at Munich Re, discussing
  the maturation of enterprise AI adoption, driven significantly by the rise of Generative
  AI, and the critical role of global regulation and robust AI governance in managing
  associated risks.


  ### 1. Focus Area

  The discussion centers on the **evolving landscape of AI risk management and governance**
  in the wake of widespread Generative AI adoption. Key topics include:

  *   The impact of emerging global AI legislation (specifically the EU AI Act) on
  business strategy and insurance.

  *   The shift from AI hype to grounded risk assessment, focusing on inherent model
  failures like hallucinations and probabilistic errors.

  *   The necessity of formal AI governance frameworks to define risk tolerance and
  implement operational safeguards.

  *   The changing nature of AI risk, particularly the systematic risk of discrimination
  when using foundational models.


  ### 2. Key Technical Insights

  *   **Probabilistic Nature of Failure:** Business leaders must accept that AI models
  are inherently probabilistic and can fail (e.g., hallucinate) at any time; this
  risk cannot be entirely eliminated by technical means alone.

  *   **Systematic Discrimination Risk:** The use of foundational models across many
  companies for sensitive use cases introduces a new, systematic risk of discrimination
  that can impact numerous consumer groups simultaneously.

  *   **Shift to Specialized Models:** The future favors numerous, fine-tuned, task-specific
  AI models over large, general "master models," as specialized models are easier
  to test, scope, and quantify risk for.


  ### 3. Business/Investment Angle

  *   **Regulatory Certainty Unlocks Potential:** Legislation like the EU AI Act,
  despite industry criticism, provides necessary legal infrastructure and certainty
  regarding consequences, which can ultimately unlock further, more confident AI adoption.

  *   **Litigation as a US Regulatory Driver:** In jurisdictions like the US, regulatory
  clarity is emerging through litigation (e.g., the Canadian airline hallucination
  case), forcing businesses to clearly define responsibility for model outputs.

  *   **Aggregation Risk Awareness:** Companies must be aware of aggregation risk—where
  many entities rely on the same foundational model, leading to highly correlated
  error rates across the industry. A potential mitigation strategy involves deliberately
  choosing *less* optimal, but less correlated, model architectures.


  ### 4. Notable Companies/People

  *   **Michael Berger (Munich Re):** Expert providing the insurance and risk management
  perspective on operationalizing AI governance.

  *   **EU AI Act:** Highlighted as a primary example of proactive, risk-based regulatory
  infrastructure setting global standards.

  *   **Canadian Airline Case:** Used as a real-world example where courts held an
  airline liable for a hallucinated policy generated by a ChatGPT-like model, emphasizing
  the liability shift to AI adopters.


  ### 5. Future Implications

  The industry is moving toward a more mature, risk-aware phase of AI integration.
  The focus will be less on deploying the largest, most general models and more on
  **operationalizing AI governance**—defining clear risk tolerance thresholds and
  implementing granular mitigation strategies tailored to specific, fine-tuned use
  cases. AI insurance is expected to play a vital role as a safety net for residual,
  unmitigated risks.


  ### 6. Target Audience

  This episode is highly valuable for **Business Leaders, Chief Risk Officers (CROs),
  Legal Counsel, and AI Strategy Executives** operating internationally, particularly
  those in regulated industries or those concerned with scaling AI adoption responsibly
  amidst evolving global legal frameworks.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- apple
- openai
title: Global AI Regulations and Their Impact on Industry Leaders - with Micheal Berger
  of Munich Re
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 102
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 20
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 14:13:46 UTC -->
