---
companies:
- category: unknown
  confidence: medium
  context: Hello and welcome to Infinite Curiosity. This is your host, Pratik Joshi.
    In each episode
  name: Infinite Curiosity
  position: 21
- category: unknown
  confidence: medium
  context: welcome to Infinite Curiosity. This is your host, Pratik Joshi. In each
    episode of this podcast, we talk to an a
  name: Pratik Joshi
  position: 60
- category: unknown
  confidence: medium
  context: d dive deep into a specific topic. Today, we have Stefano Armon on the
    show. He is the co-founder of Inception La
  name: Stefano Armon
  position: 200
- category: unknown
  confidence: medium
  context: tefano Armon on the show. He is the co-founder of Inception Labs and an
    associate professor at Stanford. Inception
  name: Inception Labs
  position: 251
- category: unknown
  confidence: medium
  context: e? Yeah, so my story, I've been doing research in Generative AI for a long
    time. Back in 2019, the world of Gener
  name: Generative AI
  position: 6447
- category: unknown
  confidence: medium
  context: nerative AI for image generation was dominated by Generative Adversarial
    Networks, GANs, right? Everybody was building GANs, and th
  name: Generative Adversarial Networks
  position: 6557
- category: unknown
  confidence: medium
  context: s superior, and very soon after we started having Stable Diffusion, Midjourney,
    and everybody basically switched to
  name: Stable Diffusion
  position: 7014
- category: tech
  confidence: high
  context: ry soon after we started having Stable Diffusion, Midjourney, and everybody
    basically switched to that, which
  name: Midjourney
  position: 7032
- category: unknown
  confidence: medium
  context: y the community as being a research breakthrough. And I was very excited
    about the results that we were s
  name: And I
  position: 8045
- category: unknown
  confidence: medium
  context: ssive models, which is comparable to what. Right. At Inception, you built
    your own model, Mercury. And so many t
  name: At Inception
  position: 12354
- category: tech
  confidence: high
  context: lding AI applications and I'm familiar with using OpenAI, Gemini, and Anthropic's
    offerings, what's differ
  name: Openai
  position: 17793
- category: tech
  confidence: high
  context: s and I'm familiar with using OpenAI, Gemini, and Anthropic's offerings,
    what's different for me if I have to
  name: Anthropic
  position: 17813
- category: tech
  confidence: high
  context: an advantage that it's going to be impossible to replicate with an autoregressive
    model that is fundamentall
  name: Replicate
  position: 20233
- category: tech
  confidence: high
  context: ive for you so far? We are building everything on Nvidia GPUs. And so at
    the moment, our stack is kind of
  name: Nvidia
  position: 20485
- category: unknown
  confidence: medium
  context: ive for you so far? We are building everything on Nvidia GPUs. And so at
    the moment, our stack is kind of pre-o
  name: Nvidia GPUs
  position: 20485
- category: tech
  confidence: high
  context: ere to use one of the AI inference chips like the Groq, the SambaNova,
    or the Cerebras. You can get thos
  name: Groq
  position: 20988
- category: tech
  confidence: high
  context: one of the AI inference chips like the Groq, the SambaNova, or the Cerebras.
    You can get those kind of speed
  name: Sambanova
  position: 20998
- category: tech
  confidence: high
  context: erence chips like the Groq, the SambaNova, or the Cerebras. You can get
    those kind of speeds, but you can ge
  name: Cerebras
  position: 21016
- category: tech
  confidence: high
  context: models. We know there's been an announcement from Google, Gemini Diffusion,
    which has been an announcement
  name: Google
  position: 22262
- category: unknown
  confidence: medium
  context: We know there's been an announcement from Google, Gemini Diffusion, which
    has been an announcement from Baidu then a
  name: Gemini Diffusion
  position: 22270
- category: unknown
  confidence: medium
  context: lity. And this, I mean, there is something called Copilot Arena, it's like
    an academic benchmark that people use
  name: Copilot Arena
  position: 24073
- category: unknown
  confidence: medium
  context: y more controllable, they're easier to constrain. So I'm pretty excited
    about new things that we're not
  name: So I
  position: 26546
- category: unknown
  confidence: medium
  context: e rapid-fire round, and it's a two-part question. Part A is what's coming
    next at Inception in the next 12
  name: Part A
  position: 28174
- category: unknown
  confidence: medium
  context: ming next at Inception in the next 12 months, and Part B is outside of
    your work at Inception, what AI adv
  name: Part B
  position: 28243
- category: ai_startup
  confidence: high
  context: The company co-founded by Stefano Armon to commercialize diffusion language
    models (DLMs).
  name: Inception Labs
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Where Stefano Armon is an associate professor and where the research on
    diffusion language models originated.
  name: Stanford
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a traditional autoregressive LLM.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a traditional autoregressive LLM.
  name: Claude
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as an example of a traditional autoregressive LLM.
  name: Gemini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a successful image generative model based on diffusion, which
    inspired the application of diffusion to language.
  name: Stable Diffusion
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a successful image generative model based on diffusion.
  name: Midjourney
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The top machine learning conference where the paper on diffusion language
    models won the best paper award.
  name: ICLR
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The name given to Inception's diffusion language models.
  name: Mercury models
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an existing serving engine framework typically used for efficiently
    serving autoregressive LLMs.
  name: vLLM
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an existing serving engine framework typically used for efficiently
    serving autoregressive LLMs.
  name: TGI (Text Generation Inference)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an existing serving engine framework typically used for efficiently
    serving autoregressive LLMs.
  name: TensorRT
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as a scale benchmark for early diffusion language model testing.
  name: GPT-2
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as the dominant approach for image generation before diffusion
    models became superior.
  name: Generative Adversarial Networks (GANs)
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a major provider of autoregressive LLMs (like GPT models)
    whose API developers are familiar with, and whose API structure the speaker's
    company is mimicking for compatibility.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a major provider of autoregressive LLMs (Claude) whose offerings
    developers are familiar with. Claude is specifically noted as being better at
    coding.
  name: Anthropic
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in relation to 'Gemini Diffusion', indicating their work on diffusion
    language models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as having built an in-house diffusion language model, though
    it was described as a prototype without API access.
  name: Baidu
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The company whose GPUs the speaker's stack is currently built and optimized
    for. Jensen's keynote (referring to Jensen Huang, CEO of Nvidia) was mentioned
    in relation to hardware evolution.
  name: Nvidia
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an example of an AI inference chip that provides high speeds,
    which the speaker claims their software optimization on commodity GPUs can match.
  name: Groq
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an example of an AI inference chip provider.
  name: SambaNova
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an example of an AI inference chip provider.
  name: Cerebras
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as one of the leading source code IDEs with AI features, which
    launched with the speaker's Mercury diffusion language models as the default for
    next-edit functionality.
  name: ContinueDev
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The company hosting the podcast/interview, which is developing the diffusion
    language models (Mercury) and focusing on reasoning models.
  name: Inception
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Described as an academic benchmark used to compare the quality of answers
    from autoregressive language models, scored by real developers.
  name: Copilot Arena
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The name given to the speaker's diffusion language models, which are reportedly
    number one on Copilot Arena for quality and speed.
  name: Mercury
  source: llm_enhanced
date: 2025-10-09 14:00:00 +0000
duration: 39
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: be trying to know the major hardware providers are already thinking about
    how should the hardware call evolve and change to better support these kinds of
    workloads
  text: we should be trying to know the major hardware providers are already thinking
    about how should the hardware call evolve and change to better support these kinds
    of workloads.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/729e93d829654744b2c8a1e9fe4adef6/
processing_date: 2025-10-09 20:10:22 +0000
quotes:
- length: 211
  relevance_score: 5
  text: And so what's exciting about the diffusion language models is that we can
    basically get the speed that you would get if you were to use one of the AI inference
    chips like the Groq, the SambaNova, or the Cerebras
  topics: []
- length: 123
  relevance_score: 4
  text: Back in 2019, the world of Generative AI for image generation was dominated
    by Generative Adversarial Networks, GANs, right
  topics: []
- length: 175
  relevance_score: 4
  text: And when it comes to diffusion for language, what does a step mean, and also
    how many steps do you typically need for a high-quality output that the average
    user is happy with
  topics: []
- length: 143
  relevance_score: 4
  text: There is basically a system that is designed to serve an LLM efficiently and
    will queue and handle the GPU resources as efficiently as possible
  topics: []
- length: 182
  relevance_score: 4
  text: When it comes to evaluations, evals, is it fair to use the exact same metrics
    to compare all the other LLMs with diffusion LLMs, or do we need some new evals
    to make it at least fair
  topics:
  - valuation
- length: 203
  relevance_score: 4
  text: And if I'm a developer who's building AI applications and I'm familiar with
    using OpenAI, Gemini, and Anthropic's offerings, what's different for me if I
    have to integrate a diffusion LLM into my product
  topics: []
- length: 180
  relevance_score: 4
  text: And so because they're built from the perspective of trying to come up with
    an inference algorithm that is optimal for what GPUs are good at, which is matrix-matrix
    multiplications
  topics: []
- length: 57
  relevance_score: 4
  text: Now, I'm going to move to the market and competition here
  topics:
  - competition
  - market
- length: 67
  relevance_score: 4
  text: So who do you view as the most direct competition to diffusion LLMs
  topics:
  - competition
- length: 258
  relevance_score: 4
  text: The moment you change the inference algorithm to something that is fundamentally
    more parallel, that changes the way you build GPUs, the number of FLOPs that you
    need, and then kind of a mature line of memory bandwidth and the amounts of memory
    that you need
  topics: []
- length: 248
  relevance_score: 3
  text: If you think about generating text left to right, one word at a time, it's
    by definition something sequential, where you have to go through that through
    essentially a for loop, and there is no really easy way to parallelize this kind
    of computation
  topics: []
- length: 41
  relevance_score: 3
  text: We are building everything on Nvidia GPUs
  topics: []
- impact_reason: Provides an accessible explanation of the diffusion process, contrasting
    it directly with sequential generation, drawing parallels to image/video models.
  relevance_score: 10
  source: llm_enhanced
  text: The object that you're giving to the user is not created left to right, one
    token at a time, but it's created throughout a finite process where you start
    with a rough guess of what the answer should be, and then you refine it by removing
    noise, by fixing mistakes, until you get to a beautiful image or a beautiful video...
  topic: technical
- impact_reason: The core thesis of the company and the research breakthrough—applying
    diffusion to discrete text/code generation.
  relevance_score: 10
  source: llm_enhanced
  text: What we're doing at Inception Labs is we figured out a way to apply that technology,
    diffusion technology, to text and code generation, which was previously and typically
    approached using an autoregressive approach.
  topic: technical
- impact_reason: 'Highlights the key advantage: parallel generation versus sequential
    generation, which has massive implications for speed and efficiency.'
  relevance_score: 10
  source: llm_enhanced
  text: We generate the whole thing in parallel. We start with a rough guess of what
    the code should be, and then we fix it using, again, a deep neural network. We
    fix mistakes, we make it better, until we finally generate a solution that we
    can give to our users.
  topic: technical
- impact_reason: 'The quantifiable business value proposition of Diffusion LLMs: breaking
    the traditional quality/cost/speed triangle.'
  relevance_score: 10
  source: llm_enhanced
  text: The reason we've been building diffusion language models is that they are
    allowing us to kind of shift the paradigm frontier of the kind of trade-offs you
    get between those three things. And we can achieve models that have, let's say,
    same quality as an autoregressive model, but might be 10 times faster, or it could
    be 10 times cheaper to serve...
  topic: business
- impact_reason: Explains *why* diffusion models are inherently more efficient on
    modern hardware (GPUs).
  relevance_score: 10
  source: llm_enhanced
  text: A diffusion language model is built from the ground up to be extremely parallelizable.
    A diffusion language model is able to modify many things at the same time, and
    so it's taking full advantage of GPUs, a parallel processing hardware...
  topic: technical
- impact_reason: 'The specific research breakthrough that made Diffusion LLMs viable:
    solving the continuous-to-discrete problem, validated by ICLR best paper award.'
  relevance_score: 10
  source: llm_enhanced
  text: We figured out a way to essentially extend the math that would typically require
    continuous objects to the discrete space of text and code, published the paper,
    kind of showing it for the first time, the diffusion language models were competitive
    without the autoregressive models at very small scale, like a GPT-2 scale, in
    terms of quality, and then they were significantly faster.
  topic: technical
- impact_reason: 'Crucial distinction between the training objectives: next-token
    prediction vs. error correction/denoising.'
  relevance_score: 10
  source: llm_enhanced
  text: The key difference is really in the way the neural network is trained. So
    instead, in a traditional autoregressive model, the neural network is trying to
    predict the next token. That's the only thing that you care about... A diffusion
    language model, on the other hand, it's trained through denoising. And so the
    neural network is trained to fix mistakes.
  topic: technical
- impact_reason: 'Crucial insight for AI infrastructure builders: novel model architectures
    require novel serving stacks, as existing optimized frameworks (like vLLM) are
    tailored for sequential autoregressive inference.'
  relevance_score: 10
  source: llm_enhanced
  text: If you had to pick the hardest engineering challenge to make it production-ready...
    a big engineering challenge was developing our own serving engine. So as you know,
    whenever you want to serve an autoregressive model efficiently in production traffic,
    you would use a system like vLLM or TGI, or TensorRT... in order to serve the
    diffusion language models efficiently, we had to develop our own version of something
    like one of these frameworks.
  topic: business
- impact_reason: 'A crucial lesson learned transitioning from academic research to
    commercial AI: real-world data quality and filtering are paramount, often outweighing
    architectural novelty.'
  relevance_score: 10
  source: llm_enhanced
  text: It surprised me how important data was. I mean, I knew that data was important.
    I've heard about it. But in the academic setting, we were always able to get around
    it and just use standard datasets... But then in the real world, the kind of data
    that we use for training the model, the quality of the data, the filtering that
    we do is really, really important.
  topic: business/strategy
- impact_reason: Identifies the single biggest current limitation of diffusion LLMs—lack
    of reasoning capabilities—which is the primary differentiator favoring autoregressive
    models for complex tasks.
  relevance_score: 10
  source: llm_enhanced
  text: existing diffusion language models are not capable of reasoning. That's one
    thing that has enabled the autoregressive models to drastically improve the quality
    of the answers.
  topic: technical/limitations
- impact_reason: Positions diffusion LLMs as a fundamental paradigm shift in inference
    economics, moving the industry focus from training scale to inference efficiency.
  relevance_score: 10
  source: llm_enhanced
  text: What we are seeing is that we are paradigm-shifting what's possible with autoregressive
    models. And so for the same speed, we can reduce the cost, or for the same cost,
    we can increase the speed. And so we are excited about this technology because
    eventually it's all going to be an inference game.
  topic: business/predictions
- impact_reason: Offers a strong theoretical justification for why diffusion architectures
    are naturally suited for code tasks (non-sequential, iterative editing) compared
    to sequential text generation.
  relevance_score: 10
  source: llm_enhanced
  text: If you think about code, I think it's a lot less left-to-right. If you think
    about a big codebase, there's not necessarily an order between the files, and
    even the way you write code involves a lot more back and forth and edits and changes.
    And so that was actually one of the reasons we started out with code. We felt
    like that's going to be a space where diffusion could be really, really good.
  topic: strategy/technical
- impact_reason: A concrete, near-term prediction about a major shift in the dominant
    model architecture for a critical AI application area (coding).
  relevance_score: 10
  source: llm_enhanced
  text: I wouldn't be surprised, and I think it's very likely that diffusion models
    will become the default for a lot of the coding use cases in the next 12 to 18
    months.
  topic: predictions
- impact_reason: This is a deep insight into the hardware implications of algorithmic
    shifts (like moving to parallel inference methods), affecting the entire hardware-software
    stack.
  relevance_score: 10
  source: llm_enhanced
  text: The moment you change the inference algorithm to something that is fundamentally
    more parallel, that changes the way you build GPUs, the number of FLOPs that you
    need, and then kind of a mature line of memory bandwidth and the amounts of memory
    that you need. So the game changes, and so it's going to be exciting to think
    about these effects on the whole stack from hardware all the way to the applications.
  topic: technical/strategy
- impact_reason: Clearly defines the standard paradigm (autoregressive) that Diffusion
    LLMs aim to replace, setting the context for the innovation.
  relevance_score: 9
  source: llm_enhanced
  text: Most existing LLMs like ChatGPT, like Claude, like Gemini are so-called autoregressive.
    And what this means is that they generate text... they generate answers one word
    at a time, left to right.
  topic: technical
- impact_reason: Pinpoints the fundamental architectural bottleneck of current LLMs
    that prevents massive efficiency gains.
  relevance_score: 9
  source: llm_enhanced
  text: The key limitation of an autoregressive model is very sequential. If you think
    about generating text left to right, one word at a time, it's by definition something
    sequential, where you have to go through that through essentially a for loop,
    and there is no really easy way to parallelize this kind of computation.
  topic: technical
- impact_reason: Detailed explanation of the inference process for diffusion models,
    emphasizing the iterative refinement from noise/garbage.
  relevance_score: 9
  source: llm_enhanced
  text: At inference time, the way you generate an answer is you start with some garbage,
    completely wrong, to start with something that could be completely random or with
    a very bad guess of what the answer should be, and then you ask the network to
    fix some mistakes in here, and then you take the output of the network, and you
    feed it back in, and you say, fix more mistakes, and you keep going until you
    get to the good quality answer.
  topic: technical
- impact_reason: 'Quantifies the efficiency gain: one diffusion step can potentially
    fix multiple tokens, whereas one autoregressive step produces only one token.'
  relevance_score: 9
  source: llm_enhanced
  text: A step basically typically refers to one forward pass through the neural network,
    which will produce an improved version, essentially, of the input. It will fix
    some of the mistakes. And so that's the key kind of iteration. The key difference
    compared to an autoregressive model, you basically need one step for every token
    you want to produce in the end.
  topic: technical
- impact_reason: Directly links the parallel nature of diffusion steps to cost savings
    and throughput improvements.
  relevance_score: 9
  source: llm_enhanced
  text: In a diffusion model, each forward pass through the neural network will be
    able to modify multiple tokens. And so for the same cost, you get potentially
    more, because if you can fix more than one token, then you get a benefit.
  topic: business
- impact_reason: Directly contrasts the speed advantage of diffusion LLMs (Mercury)
    over traditional autoregressive models, a key selling point.
  relevance_score: 9
  source: llm_enhanced
  text: our diffusion language models, Mercury models, as we call them, they're significantly
    faster than autoregressive models, which is comparable to what.
  topic: technical/predictions
- impact_reason: Highlights a major, non-trivial engineering hurdle in deploying novel
    model architectures (diffusion LLMs) that requires custom serving infrastructure,
    unlike established autoregressive models which benefit from mature frameworks.
  relevance_score: 9
  source: llm_enhanced
  text: a big engineering challenge was developing our own serving engine. So as you
    know, whenever you want to serve an autoregressive model efficiently in production
    traffic, you would use a system like vLLM or TGI, or TensorRT. There is basically
    a system that is designed to serve an LLM efficiently... And in order to serve
    the diffusion language models efficiently, we had to develop our own version of
    something like one of these frameworks.
  topic: technical/business
- impact_reason: 'Points out the irony and the immediate future direction: diffusion
    models excel at image/video generation, yet current text-based diffusion LLMs
    lack inherent multimodality.'
  relevance_score: 9
  source: llm_enhanced
  text: Another thing that doesn't quite exist today, which is a little bit ironic,
    but existing diffusion language models are not multimodal, even though the diffusion
    models are the best approach for image and video generation.
  topic: technical/future trends
- impact_reason: 'Crucial business/developer experience insight: lowering integration
    friction by adopting the OpenAI API standard allows rapid adoption of novel architectures.'
  relevance_score: 9
  source: llm_enhanced
  text: we've done all the work to make it very easy for developers to use the diffusion
    language models. So we're currently serving them in an OpenAI-compatible API.
    So you can literally just swap out your API key and the URL, and everything should
    work out of the box.
  topic: business/strategy
- impact_reason: 'Explains *why* diffusion models have a long-term hardware advantage:
    their structure better exploits the parallel nature of modern GPUs compared to
    the sequential nature of autoregressive decoding.'
  relevance_score: 9
  source: llm_enhanced
  text: The core of the technology is more efficient on GPUs. We think we're going
    to have an advantage that it's going to be impossible to replicate with an autoregressive
    model that is fundamentally sequential and cannot be parallelized to that extent.
  topic: technical/strategy
- impact_reason: 'A powerful statement on software vs. hardware dependency: superior
    algorithms (diffusion) can unlock performance previously only achievable with
    specialized, expensive AI inference chips.'
  relevance_score: 9
  source: llm_enhanced
  text: You can get those kind of speeds, but you can get them on just commodity GPUs
    because we have a better algorithm. So all improvements are at the software layer
    and not at the hardware.
  topic: strategy/technical
- impact_reason: Provides concrete evidence of market adoption and performance leadership
    in a specific, high-value domain (code editing), validated by an external benchmark
    (Copilot Arena).
  relevance_score: 9
  source: llm_enhanced
  text: Mercury models are number one for quality, they're number one for speed. We
    launched with ContinueDev... The default for next-edit is Mercury diffusion language
    models.
  topic: business/predictions
- impact_reason: Highlights a key advantage of newer model types (likely diffusion)
    regarding safety, alignment, or predictable output generation.
  relevance_score: 9
  source: llm_enhanced
  text: We know that the future models are generally more controllable, they're easier
    to constrain.
  topic: technical/safety
- impact_reason: A direct roadmap announcement signaling a move beyond pure generation
    toward models capable of complex reasoning, which is the next frontier in LLMs.
  relevance_score: 9
  source: llm_enhanced
  text: At Inception, the next thing is going to be a reasoning model. So that's coming
    pretty soon.
  topic: business/technical
- impact_reason: A classic, fundamental trade-off in AI deployment, setting up the
    argument for why a new architecture is needed.
  relevance_score: 8
  source: llm_enhanced
  text: It's always about those three things [quality, cost, and speed]. And you see
    that there's always trade-offs, and you typically cannot get all three at the
    same time.
  topic: business
- impact_reason: Historical context showing the successful precedent of diffusion
    models disrupting the dominant paradigm (GANs) in image generation.
  relevance_score: 8
  source: llm_enhanced
  text: Back in 2019, the world of Generative AI for image generation was dominated
    by Generative Adversarial Networks, GANs... we started working on a different
    approach, score-based diffusion models, and we were able to show that, yeah, there
    is a better approach to generate images...
  topic: strategy
- impact_reason: Highlights the long-term research challenge of adapting continuous
    diffusion math to the discrete nature of language.
  relevance_score: 8
  source: llm_enhanced
  text: For the last four or five years, I've been trying in my lab at Stanford to
    figure out a way to get this diffusion-based approach to work, not just on image
    and video generation, but also on text and code that can generate discrete objects.
  topic: technical
- impact_reason: A strong prediction about the ease of integrating text and visual
    modalities using diffusion principles, suggesting a unified multimodal future.
  relevance_score: 8
  source: llm_enhanced
  text: We think we're going to get some great results, just because we know diffusion
    works so well on those modalities, and now we know how to get text to work. It
    should be relatively easy to be able to build a multimodal kind of single model
    and capture all the modalities and learn across the different modalities.
  topic: predictions/technical
- impact_reason: Provides validation that diffusion LLMs can achieve parity in quality
    with optimized autoregressive models on standard benchmarks, shifting the focus
    to speed/cost advantages.
  relevance_score: 8
  source: llm_enhanced
  text: we've been able to benchmark the diffusion language models on the same set
    of tasks that are used for autoregressive models, and that's why we can confidently
    say that the quality of the diffusion language models are already matching the
    one of some of the best speed-optimized autoregressive models, including from
    frontier labs.
  topic: technical/strategy
- impact_reason: 'Defines the competitive strategy: offering a ''better trade-off''
    triangle—faster speed at same quality, or higher quality at same speed/cost.'
  relevance_score: 8
  source: llm_enhanced
  text: We are winning in terms of the speed-optimized setting for autoregressive
    models... we can match the quality, we can be much faster, or we can put in a
    bigger model that preserves the original latency budget but could give you higher
    quality answers.
  topic: business/competition
- impact_reason: Provides a real-world competitive assessment, emphasizing that production
    readiness (serving traffic, API access) is the true differentiator, not just research
    announcements.
  relevance_score: 8
  source: llm_enhanced
  text: Both the Gemini diffusion and the one from Baidu, they were more like prototypes.
    It's a demo that you can play with maybe, but they have not been providing API
    access to the models. They're not serving production traffic. So we're still far
    ahead from that perspective.
  topic: business/competition
- impact_reason: Suggests that the shift to diffusion models will necessitate and
    enable entirely new interaction paradigms beyond current prompt/response methods.
  relevance_score: 8
  source: llm_enhanced
  text: We can imagine a future where people interface with these models in a different
    way because of the fundamentally different approach that they use to generate
    text or code.
  topic: strategy/predictions
- impact_reason: Connects the development of reasoning models directly to measurable
    improvements on standard benchmarks (like MMLU) across critical tasks like software
    engineering.
  relevance_score: 8
  source: llm_enhanced
  text: Based on the general results, we're able to significantly increase the quality
    of the answers and the intelligence score as measured by MMLU and other benchmarks
    that people care about to measure how good these models are in a variety of important
    real-world tasks, software engineering, real tasks.
  topic: technical
- impact_reason: Predicts that improved reasoning capabilities will unlock more sophisticated
    AI agents and change how users interact with the technology.
  relevance_score: 8
  source: llm_enhanced
  text: It's going to enable a different kind of user experience. It's going to enable
    agents that are even more complicated.
  topic: predictions/strategy
- impact_reason: Summarizes the core optimization challenge in AI inference serving,
    framing it as a trade-off triangle (throughput, latency, cost).
  relevance_score: 8
  source: llm_enhanced
  text: It's all about throughput versus latency and then how can you use the GPUs
    to kind of get the best throughput versus cost trade-off when you sort of write
    it out.
  topic: technical/business
- impact_reason: Indicates that the shift in AI algorithms is already influencing
    the roadmap and design philosophy of major hardware manufacturers.
  relevance_score: 8
  source: llm_enhanced
  text: The major hardware providers are already thinking about how should the hardware
    call evolve and change to better support these kinds of workloads.
  topic: technical/strategy
- impact_reason: Emphasizes that for end-user applications, especially agents, latency
    (speed) is a critical, measurable metric that often trumps marginal quality differences.
  relevance_score: 7
  source: llm_enhanced
  text: speed is the other metric that matters... I think at the end of the day, users
    care about end-to-end, and they can see how quickly they get the answer, how quickly
    will their agent complete the task.
  topic: business/strategy
- impact_reason: 'A classic network effect argument applied to AI infrastructure:
    wider adoption drives optimization and cost reduction in serving layers.'
  relevance_score: 7
  source: llm_enhanced
  text: I'd expect that if more people start building on this technology, there's
    going to be even more improvement in terms of the efficiency, the speed, and the
    reduction in cost.
  topic: business/strategy
- impact_reason: Highlights the importance of human-preference-based evaluation (like
    Copilot Arena) over purely automated metrics for assessing generative model quality.
  relevance_score: 7
  source: llm_enhanced
  text: There is something called Copilot Arena, it's like an academic benchmark that
    people use to compare the quality of the answers from autoregressive language
    models, and the answers are scored by developers, by real people...
  topic: technical
- impact_reason: Provides a concrete example of successful product integration and
    market traction for their specific diffusion models in a high-value domain (IDEs).
  relevance_score: 7
  source: llm_enhanced
  text: We launched with ContinueDev, one of the leading source code IDEs with a lot
    of AI features. The default for next-edit is Mercury diffusion language models.
  topic: business
- impact_reason: Emphasizes the crucial role of developer feedback and voluntary adoption
    in discovering novel use cases for new AI paradigms.
  relevance_score: 7
  source: llm_enhanced
  text: The moment we have these prototypes, we get them in the hands of developers
    and the technology by choice, I think that it's going to be a lot of exciting
    opportunities in terms of different ways of interfacing with the models.
  topic: strategy
- impact_reason: A brief but significant announcement confirming multimodal capabilities
    as a key upcoming feature alongside reasoning models.
  relevance_score: 7
  source: llm_enhanced
  text: And then multimodal, that [is coming next].
  topic: technical
- impact_reason: Confirms early market validation for using diffusion models in code
    generation.
  relevance_score: 6
  source: llm_enhanced
  text: We're seeing a lot of traction in that space [code generation], and we're
    very excited about it.
  topic: business
- impact_reason: 'Illustrates a common business strategy during technological transitions:
    minimizing friction for existing users while planning for future paradigm shifts.'
  relevance_score: 6
  source: llm_enhanced
  text: We're still trying to keep things backward compatible as much as possible
    so that it's easy for people to use and they don't need to make too many changes...
  topic: business/strategy
source: Unknown Source
summary: '## Podcast Summary: Diffusion LLMs - The Fastest LLMs Ever Built | Stefano
  Ermon


  This episode of *Infinite Curiosity* features Stefano Ermon, co-founder of Inception
  Labs and Stanford Associate Professor, discussing the revolutionary concept of **Diffusion
  Language Models (Diffusion LLMs)** as a fundamental shift away from traditional
  autoregressive LLMs (like GPT, Claude).


  ### 1. Focus Area

  The primary focus is on applying **diffusion modeling**, successfully used in image
  and video generation, to **text and code generation**. The discussion centers on
  the architectural differences, performance advantages (speed and cost efficiency),
  and the commercialization efforts by Inception Labs with their model, **Mercury**.


  ### 2. Key Technical Insights

  *   **Autoregressive vs. Diffusion Generation:** Autoregressive models generate
  text sequentially, one token at a time (left-to-right), which is inherently slow
  due to its sequential nature. Diffusion LLMs generate the entire output in parallel
  by starting with a rough, noisy guess and iteratively refining it through multiple
  denoising steps, analogous to sculpting or outlining a document before editing.

  *   **Training Methodology:** Diffusion LLMs are trained via **denoising**. Noise
  is intentionally introduced into clean data, and the neural network (often transformer-based)
  is trained to fix these mistakes. At inference, the model refines a random initial
  state until a high-quality output is achieved.

  *   **Efficiency through Parallelism:** Because diffusion models modify multiple
  tokens simultaneously in each forward pass (step), they leverage the parallel processing
  capabilities of GPUs far more effectively than sequential autoregressive models,
  leading to significant speed and cost advantages.


  ### 3. Business/Investment Angle

  *   **Performance Frontier Shift:** Diffusion LLMs are shifting the trade-off frontier
  between quality, speed, and cost, offering models that can match autoregressive
  quality while being significantly faster (potentially 10x) or cheaper to serve.

  *   **Engineering Hurdles:** A major commercialization challenge was developing
  a custom serving engine optimized for diffusion inference, as existing frameworks
  (like vLLM) are built for autoregressive architectures.

  *   **Data Importance:** Moving from academic research to production highlighted
  the critical, often underestimated, importance of high-quality, filtered training
  data for real-world performance.


  ### 4. Notable Companies/People

  *   **Stefano Ermon:** Co-founder of Inception Labs, pioneer in applying diffusion
  models to language, and the central expert in the discussion.

  *   **Inception Labs:** Developing and commercializing Diffusion LLMs under the
  name **Mercury**.

  *   **Google and Baidu:** Mentioned as having announced internal diffusion language
  model prototypes, though Inception claims to be ahead in production deployment.

  *   **ContinueDev:** A leading source code IDE that has integrated Mercury as the
  default for its "next-edit" feature.


  ### 5. Future Implications

  *   **Dominance in Specific Niches:** Diffusion LLMs are predicted to excel in applications
  where speed and iterative refinement are crucial, particularly **code generation
  and editing workflows** (autocomplete, auto-edit), where the non-linear nature of
  code aligns well with diffusion refinement.

  *   **Multimodality Potential:** Ermon is highly optimistic about building truly
  multimodal models using diffusion, leveraging the proven success of diffusion in
  image/video generation with the new text capabilities.

  *   **Hardware Agnostic Efficiency:** The algorithmic improvements in Diffusion
  LLMs allow them to achieve speeds comparable to specialized inference chips (like
  Groq) using only commodity Nvidia GPUs, suggesting software innovation can bypass
  hardware specialization.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Research Scientists, CTOs,
  and AI Product Managers** interested in next-generation LLM architectures, inference
  optimization, and competitive advantages in the rapidly evolving generative AI landscape.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- investment
- openai
- anthropic
- nvidia
title: Diffusion LLMs - The Fastest LLMs Ever Built | Stefano Ermon, cofounder of
  Inception Labs
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 57
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 22
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 11
  prominence: 1.0
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 3
  prominence: 0.3
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 3
  prominence: 0.3
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-09 20:10:22 UTC -->
