---
companies:
- category: unknown
  confidence: medium
  context: This is the Everyday AI Show, the everyday podcast where we simplify AI
    and br
  name: Everyday AI Show
  position: 12
- category: tech
  confidence: high
  context: o boost your career, business, and everyday life. Apple's latest AI research
    paper has gone viral. So vir
  name: Apple
  position: 197
- category: unknown
  confidence: medium
  context: hat usually has absolutely nothing to do with AI. So Apple's "The Illusion
    of Thinking" paper shows evidence
  name: So Apple
  position: 382
- category: unknown
  confidence: medium
  context: has absolutely nothing to do with AI. So Apple's "The Illusion of Thinking"
    paper shows evidence that large reas
  name: The Illusion
  position: 394
- category: unknown
  confidence: medium
  context: or this one. I am. If you're new here, welcome to Everyday AI. My name
    is Jordan Wilson. I'm the host, and we d
  name: Everyday AI
  position: 1167
- category: unknown
  confidence: medium
  context: u're new here, welcome to Everyday AI. My name is Jordan Wilson. I'm the
    host, and we do this every single day. T
  name: Jordan Wilson
  position: 1191
- category: unknown
  confidence: medium
  context: ', but we''ll see, because at least for today, it''s Hot Take Tuesday,
    and I got takes. Last year''s audience, it''s good'
  name: Hot Take Tuesday
  position: 2296
- category: unknown
  confidence: medium
  context: his paper, "The Illusion of Thinking." All right. As I said, it's been
    grabbing a lot of headlines recen
  name: As I
  position: 2611
- category: unknown
  confidence: medium
  context: ay, this Jordan guy, he's obviously very pro-AI." Am I sure? Yeah, you
    could say that. If I'm being hone
  name: Am I
  position: 2802
- category: unknown
  confidence: medium
  context: ery pro-AI." Am I sure? Yeah, you could say that. If I'm being honest,
    I'm pro-AI because I feel there's
  name: If I
  position: 2839
- category: unknown
  confidence: medium
  context: lk a little bit very quickly about my background. So I mentioned it a couple
    times in our 540-plus episo
  name: So I
  position: 3158
- category: unknown
  confidence: medium
  context: as an investigative reporter. I did okay. I was a Pulitzer Fellow. I won
    ACP Story of the Year. So when I look at t
  name: Pulitzer Fellow
  position: 3300
- category: unknown
  confidence: medium
  context: orter. I did okay. I was a Pulitzer Fellow. I won ACP Story of the Year.
    So when I look at these things, I do
  name: ACP Story
  position: 3323
- category: tech
  confidence: high
  context: hat it doesn't say, whether I have a preconceived notion on if I agree
    with it or not. But essentially, wh
  name: Notion
  position: 3751
- category: unknown
  confidence: medium
  context: ght? This whole thinking thing, it's an illusion. And I'm actually very
    excited to break this one down, b
  name: And I
  position: 3997
- category: unknown
  confidence: medium
  context: s was three days before their big conference, the Worldwide Developer Conference.
    And in this paper, they publicly claim that adva
  name: Worldwide Developer Conference
  position: 4712
- category: unknown
  confidence: medium
  context: ial intelligence. Maybe that's why they call that Apple Intelligence because
    they couldn't actually figure out artific
  name: Apple Intelligence
  position: 6315
- category: unknown
  confidence: medium
  context: And I have taken my time mainly because I do our Hot Takes on Tuesday.
    And this paper came out, I believe it
  name: Hot Takes
  position: 7135
- category: unknown
  confidence: medium
  context: ', Apple was crushing the world. This was like ''92 Dream Team kind of
    dominance for Olympic basketball. It wasn'
  name: Dream Team
  position: 7981
- category: tech
  confidence: high
  context: market cap in 2021, and the next closest company, Microsoft, had only a
    $1.6 billion—now, I'm not the best at
  name: Microsoft
  position: 8127
- category: unknown
  confidence: medium
  context: ven close. They were blowing out the competition. Like I said, this is
    '92 Dream Team, this is '97 Bulls,
  name: Like I
  position: 8515
- category: unknown
  confidence: medium
  context: a half trillion dollars behind Microsoft, right? Which I would say, depending
    on how you look at it, you c
  name: Which I
  position: 8912
- category: tech
  confidence: high
  context: you could say it's Microsoft, you could say it's Google. I would probably
    say Microsoft is Apple's closes
  name: Google
  position: 9013
- category: unknown
  confidence: medium
  context: all, a small language model that lived on device. Its Ajax model, they
    didn't even say it by name in the mai
  name: Its Ajax
  position: 11014
- category: unknown
  confidence: medium
  context: ent publications like Payments, Axios, Bloomberg, PC Magazine, Computerworld.
    Let's read some of these headline
  name: PC Magazine
  position: 11757
- category: unknown
  confidence: medium
  context: 'ld. Let''s read some of these headlines, shall we? The Verge: "This is
    a crisis. New Apple report claims we''ll'
  name: The Verge
  position: 11831
- category: unknown
  confidence: medium
  context: 'eadlines, shall we? The Verge: "This is a crisis. New Apple report claims
    we''ll get no Siri upgrades at WWDC'
  name: New Apple
  position: 11861
- category: unknown
  confidence: medium
  context: for false advertising over Apple Intelligence." "Why Apple still hasn't
    cracked AI." "Two more class-action
  name: Why Apple
  position: 12116
- category: unknown
  confidence: medium
  context: ht? I have to be honest. I'm recording this on an Apple Mac mini. The camera
    I'm using for the livestream her
  name: Apple Mac
  position: 12515
- category: unknown
  confidence: medium
  context: s not. Right? Like I'm looking for it. I'm like, "Hey Siri, find me the
    AI on this iPhone." Ten minutes late
  name: Hey Siri
  position: 12885
- category: tech
  confidence: high
  context: ng AI at WWDC this year. So that was yesterday on Monday, Apple had their
    WWDC event where they essentiall
  name: Monday
  position: 14447
- category: unknown
  confidence: medium
  context: artificial intelligence, it's Apple Intelligence. Our AI is better than
    AI," right? And here we are a year
  name: Our AI
  position: 14820
- category: unknown
  confidence: medium
  context: look like the paper on the right. This paper is "Personhood Credentials."
    All right. This was a pretty meaningful researc
  name: Personhood Credentials
  position: 16739
- category: tech
  confidence: high
  context: s from multiple big companies. You have them from OpenAI, Harvard, Microsoft,
    University of Oxford. You kn
  name: Openai
  position: 17067
- category: unknown
  confidence: medium
  context: veryone, David here, one of the product leads for Google Gemini. Check
    out V03, our state-of-the-art AI video gen
  name: Google Gemini
  position: 17641
- category: unknown
  confidence: medium
  context: ideos with native audio generation. Try it with a Google AI Pro plan or
    get the highest access with the Ultra pla
  name: Google AI Pro
  position: 17836
- category: unknown
  confidence: medium
  context: tual models. So, they tested thinking versions of DeepSeek Coder 1 and
    Claude 3 Opus Sonnet. They did a lot of mor
  name: DeepSeek Coder
  position: 22364
- category: unknown
  confidence: medium
  context: hinking versions of DeepSeek Coder 1 and Claude 3 Opus Sonnet. They did
    a lot of more technical testing. They t
  name: Opus Sonnet
  position: 22394
- category: unknown
  confidence: medium
  context: of thought, whereas DeepSeek Coder 1 and Claude 3 Opus Sonnet Thinking
    do in the API. So, they did the right thing there
  name: Opus Sonnet Thinking
  position: 22586
- category: unknown
  confidence: medium
  context: I'm like, "Oh gosh, the media is going to get..." Because I saw it literally
    once it came out, right? Because
  name: Because I
  position: 23877
- category: unknown
  confidence: medium
  context: I don't touch the stove because it's hot, right? But I've learned those
    different things that lead me to
  name: But I
  position: 30406
- category: unknown
  confidence: medium
  context: ve to go to the bottom for this one." They said, "For Claude 3.5 Sonnet
    Thinking and non-thinking models, we u
  name: For Claude
  position: 35496
- category: unknown
  confidence: medium
  context: bottom for this one." They said, "For Claude 3.5 Sonnet Thinking and non-thinking
    models, we use a maximum generat
  name: Sonnet Thinking
  position: 35511
- category: big_tech
  confidence: high
  context: Released a research paper ('The Illusion of Thinking') criticizing large
    reasoning models; facing scrutiny and lawsuits over its own AI product rollout
    ('Apple Intelligence').
  name: Apple
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as Apple's closest competitor in market capitalization, whose
    significant growth is attributed to AI investments.
  name: Microsoft
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as a potential competitor to Apple in market capitalization context.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced indirectly via its product, ChatGPT, which the host's iPhone
    Siri suggested using.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the external model Siri suggested using when unable to fulfill
    a query.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Google's state-of-the-art AI video generation model, accessible via the
    Gemini app.
  name: Gemini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific reasoning large language model tested by Apple researchers in
    their study.
  name: DeepSeek Coder 1
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific reasoning large language model tested by Apple researchers in
    their study.
  name: Claude 3 Opus Sonnet
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an institution whose researchers contributed to the meaningful,
    multi-institutional research paper ('Personhood Credentials').
  name: Harvard
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an institution whose researchers contributed to the meaningful,
    multi-institutional research paper ('Personhood Credentials').
  name: University of Oxford
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an application the speaker tested on an iPhone, implying it
    is a large language model/AI service from Google.
  name: Google Gemini
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific large language model (LLM) developed by Anthropic, which was
    central to the critique of the Apple study regarding token limits and performance.
  name: Claude 3.5 Sonnet
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as the best model used by Apple in their study in terms of 'thinking'
    capability, suggesting it is an LLM provider.
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned briefly by the speaker when stating they built a working Tower
    of Hanoi solution using Claude 3.5 Sonnet, not Claude 4.
  name: Claude 4
  source: llm_enhanced
date: 2025-06-10 14:00:00 +0000
duration: 54
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/www.buzzsprout.com/2175779/episodes/17312438-ep-543-apple-s-weaponized-research-inside-its-illusion-of-thinking-paper.mp3
processing_date: 2025-10-05 11:04:40 +0000
quotes:
- length: 172
  relevance_score: 7
  text: 'And I think this will probably go down as the biggest failure in business
    history: Apple''s absolute failure to put together anything resembling some artificial
    intelligence'
  topics: []
- length: 119
  relevance_score: 6
  text: 5 trillion market cap, if Apple stayed on a similar or that same growth trajectory
    as Microsoft did, do the math, y'all
  topics:
  - market
  - growth
- length: 142
  relevance_score: 6
  text: Multiple trillion-dollar market cap mistake from Apple, which would very likely,
    I think, qualify that to be the biggest business blunder ever
  topics:
  - market
- length: 61
  relevance_score: 5
  text: '" "Apple''s AI headaches could lead to lukewarm revenue growth'
  topics:
  - revenue
  - growth
- length: 84
  relevance_score: 5
  text: Here was, on the hardest, on an eight-disc, on a 10-disc, here's what the
    output was
  topics: []
- length: 123
  relevance_score: 4
  text: Maybe that's why they call that Apple Intelligence because they couldn't actually
    figure out artificial intelligence, right
  topics: []
- length: 126
  relevance_score: 4
  text: Apple is blowing out the rest of the world in terms of, "We are the biggest,
    we are the best company," and it's not even close
  topics: []
- length: 81
  relevance_score: 4
  text: They're like, "Oh, it's not even artificial intelligence, it's Apple Intelligence
  topics: []
- length: 185
  relevance_score: 4
  text: And then they gave a system prompt and then a prompt to different reasoning
    large language models, and then they had their then output their text output their
    answer in text form, right
  topics: []
- length: 128
  relevance_score: 4
  text: But going by how large language models work and the requirements in their
    own paper, they would have to output every single move
  topics: []
- length: 56
  relevance_score: 3
  text: It's marketing from Apple, and you shouldn't fall for it
  topics:
  - market
- length: 22
  relevance_score: 3
  text: Here's what's happened
  topics: []
- length: 126
  relevance_score: 3
  text: Why has Apple put out multiple papers that literally go against the power
    and the capabilities of large language models and AI
  topics: []
- length: 105
  relevance_score: 3
  text: But essentially, pre-generative AI, and this was in 2021, pre-generative AI,
    Apple was crushing the world
  topics: []
- length: 85
  relevance_score: 3
  text: 1 trillion market cap in 2021, and the next closest company, Microsoft, had
    only a $1
  topics:
  - market
- length: 43
  relevance_score: 3
  text: 1 trillion market cap versus Microsoft's $1
  topics:
  - market
- length: 94
  relevance_score: 3
  text: 'Fast forward to today: yeah, Apple is the third biggest company in the US
    by market cap, right'
  topics:
  - market
- length: 131
  relevance_score: 3
  text: And if you took the same growth rates, if you take the growth rates that Microsoft
    had from 2021 until today, going from about a $1
  topics:
  - growth
- length: 52
  relevance_score: 3
  text: That means Apple is at about a $5 trillion valuation
  topics:
  - valuation
- length: 136
  relevance_score: 3
  text: So essentially, if they would have made the similar moves that Microsoft did,
    presumably, they would be a $5 trillion market cap company
  topics:
  - market
- length: 104
  relevance_score: 3
  text: And it said, "Apple is reportedly spending millions of dollars a day training
    its AI in Apple internally
  topics: []
- length: 85
  relevance_score: 3
  text: Imagine spending millions of dollars a day just on training AI models if you're
    Apple
  topics: []
- length: 126
  relevance_score: 3
  text: They should have grown at the same rate that Microsoft grew over the last
    four years because of generative AI, but they didn't
  topics: []
- length: 255
  relevance_score: 3
  text: So essentially what's happening here is all these other companies are running
    away, getting insane revenue from their AI offerings, and Apple's like, "What
    if we just throw some deception and doubt and confusion in the ring here right
    before our big event
  topics:
  - revenue
- length: 128
  relevance_score: 3
  text: Otherwise, good research that changes the conversation on artificial intelligence
    would usually look like the paper on the right
  topics: []
- length: 83
  relevance_score: 3
  text: 'On the left, that''s what marketing looks like: only Apple researchers, nothing
    else'
  topics:
  - market
- length: 89
  relevance_score: 3
  text: You have to move discs between pegs, never placing a larger disc on top of
    a smaller disc
  topics: []
- length: 180
  relevance_score: 3
  text: All the discs start on the left tower, and you have to ultimately move them
    all the way over to the right tower with the largest disc on the bottom and the
    smallest disc on the top
  topics: []
- length: 130
  relevance_score: 3
  text: And there are many different ways that you can solve them, but also you have
    to think of the context window of these models, right
  topics: []
- length: 122
  relevance_score: 3
  text: You've got to be either have a computer science math, like crazy logical brain,
    or you have to just study this game, right
  topics: []
- impact_reason: 'This is the central thesis of the podcast: the research is not objective
    science but a marketing tactic, warning the audience about deceptive research
    framing.'
  relevance_score: 10
  source: llm_enhanced
  text: If you dig deeper, you'll find flawed logic, cherry-picked testing, and an
    all-or-nothing grading rule that would flunk Einstein. In other words, if you
    take enough time to deconstruct this study, you'll find it's not much of a study
    at all. It's marketing from Apple, and you shouldn't fall for it.
  topic: safety/ethics
- impact_reason: 'Identifies a critical flaw in the study''s setup: artificially restricting
    the models from using their most powerful tool (code generation/use).'
  relevance_score: 10
  source: llm_enhanced
  text: Essentially, they said that AI reasoning models couldn't use code. What? Okay,
    which is the single most effective way to solve the problems that the researchers
    were giving them.
  topic: technical
- impact_reason: Points to a specific, technical constraint (token limits) that was
    allegedly hidden to make the models appear less capable.
  relevance_score: 10
  source: llm_enhanced
  text: Apple failed to disclose that their hardest tests were technically physically
    impossible for the AI to pass due to its handcuffed token limits.
  topic: technical
- impact_reason: A powerful, quantifiable prediction/analysis showing the massive
    financial opportunity cost ($2 Trillion) of failing to adopt generative AI quickly.
  relevance_score: 10
  source: llm_enhanced
  text: If Apple stayed on a similar or that same growth trajectory as Microsoft did,
    do the math, y'all. That means Apple is at about a $5 trillion valuation. Instead,
    they're staggering at $3 trillion. So essentially, if they would have made the
    similar moves that Microsoft did, presumably, they would be a $5 trillion market
    cap company. So they have left—you can make the argument that they've left $2
    trillion in market cap on the table by not figuring out AI.
  topic: business
- impact_reason: Quantifies the financial opportunity cost of Apple's AI delay by
    comparing it to Microsoft's success, framing it as a massive, historic business
    error.
  relevance_score: 10
  source: llm_enhanced
  text: They should have grown at the same rate that Microsoft grew over the last
    four years because of generative AI, but they didn't. They didn't, but they should
    have. Multiple trillion-dollar market cap mistake from Apple, which would very
    likely, I think, qualify that to be the biggest business blunder ever.
  topic: business/predictions
- impact_reason: Suggests Apple's controversial research paper was a deliberate strategic
    distraction (a 'red herring') to mask their lack of large reasoning models compared
    to competitors.
  relevance_score: 10
  source: llm_enhanced
  text: Apple needed a red herring to claim that AI reasoning is an illusion, right?
    Because all these other companies, even though Apple has their own edge AI, these
    are small language models that live on device. They don't have a large reasoning
    model.
  topic: strategy/technical
- impact_reason: 'Points out a fundamental flaw in Apple''s choice of benchmark puzzles:
    using classic, well-documented problems that are guaranteed to be in the training
    data, thus invalidating the ''clean'' test premise.'
  relevance_score: 10
  source: llm_enhanced
  text: They thought, 'Oh, well, we can use a game like Tower of Hanoi that's non-deterministic
    because it's a game.' Wrong. All the solutions, the algorithm, everything about
    this Tower of Hanoi is on the internet. It's in the training data. So, their original
    reasoning for creating their games to test these reasoning models with is absolutely
    bonkers.
  topic: technical
- impact_reason: 'Identifies a critical methodological flaw: forcing the model to
    output every step externally, ignoring internal CoT tokens, which artificially
    inflates output requirements.'
  relevance_score: 10
  source: llm_enhanced
  text: All right, that's important to talk about because one of the requirements
    that the models had to do in the output—so Apple said that they weren't counting
    thinking tokens, that's not usually how it works—so that chain-of-thought processing,
    which is a lot of what's happening under the hood, but they did require the model
    to spit out every single move.
  topic: technical/methodology
- impact_reason: 'The central technical argument: the study''s constraints (64K output
    limit) made solving the higher-disc problems mathematically impossible based on
    the required output format.'
  relevance_score: 10
  source: llm_enhanced
  text: 'Conservatively, a 13-disc problem of this Tower of Hanoi would require 65,000
    output tokens. I''m going to repeat that: the study was not possible, right? They
    did it all the way up to 15, 20 discs.'
  topic: technical/limitations
- impact_reason: A provocative counter-argument suggesting that 'reasoning' in biological
    or artificial systems might just be an extended, slower form of pattern recognition,
    blurring the line between the two stages.
  relevance_score: 10
  source: llm_enhanced
  text: But all stage two reasoning thinking is, it's just stage one, but slower,
    right? So I don't know, like even the concept of arguing against reasoning models
    seems a little bit illogical when it's just really made up of stage one thinking
    anyways.
  topic: technical/philosophy
- impact_reason: 'Identifies the second major methodological flaw: restricting tool
    use and coding capability, which are essential for complex, multi-step problem-solving
    in modern LLMs.'
  relevance_score: 10
  source: llm_enhanced
  text: 'Critique two: it is designed to guarantee failure on the harder levels of
    this testing by doing no tool use. Apple didn''t give the models tool use, and
    they couldn''t write code, which is the obvious and the only way that a reasoning
    model would actually solve the puzzle.'
  topic: technical/methodology
- impact_reason: Reinterprets the AI's 'failure' (giving up or seeking a shortcut)
    as evidence of higher-level strategic intelligence rather than a lack of reasoning
    ability.
  relevance_score: 10
  source: llm_enhanced
  text: 'Critique three: mistaking intelligence for a flaw. So they said giving up
    is actually a smarter strategy. So in this case, the AI correctly identified an
    impossible brute force task and sought a shortcut. That''s the reality, and the
    algorithm failure is a red herring. It proves the AI is a complex mind, not a
    simple machine.'
  topic: technical/theory
- impact_reason: 'A core takeaway for AI deployment: performance in complex tasks
    is highly dependent on providing the necessary external capabilities (tools, code
    execution, larger context windows).'
  relevance_score: 10
  source: llm_enhanced
  text: You can see when you give the model the tools that it needs, it does the job.
  topic: business/deployment
- impact_reason: A strong philosophical statement challenging the interpretation of
    unexpected AI behavior, arguing for viewing it as complex cognition rather than
    simple error.
  relevance_score: 10
  source: llm_enhanced
  text: the algorithm failure is a red herring. It proves the AI is a complex mind,
    not a simple machine.
  topic: safety/strategy
- impact_reason: A strong, concise condemnation of the paper's methodology and intent,
    framing it as corporate strategy rather than neutral research.
  relevance_score: 9
  source: llm_enhanced
  text: It's strategic deception, it's cherry-picked science, and it's weaponized
    research at best.
  topic: strategy
- impact_reason: 'Highlights a key logical fallacy in the paper: interpreting resource
    management/failure avoidance as a fundamental model failure.'
  relevance_score: 9
  source: llm_enhanced
  text: They also misrepresented the AI's intelligent decision to give up on impossible
    brute force tasks as a reasoning collapse, which I would say is not the case.
  topic: technical
- impact_reason: Connects the research release directly to Apple's business strategy
    and competitive positioning ahead of a major product announcement.
  relevance_score: 9
  source: llm_enhanced
  text: 'Also, the paper''s timing reveals its true purpose: a strategic media strike
    to distract from Apple''s own AI weakness right before WWDC and their lack of
    AI.'
  topic: business
- impact_reason: A highly provocative statement about the scale of Apple's perceived
    failure in the generative AI race.
  relevance_score: 9
  source: llm_enhanced
  text: 'Apple has failed. And I think this will probably go down as the biggest failure
    in business history: Apple''s absolute failure to put together anything resembling
    some artificial intelligence.'
  topic: business
- impact_reason: Strongly asserts that the motivation behind the paper was malicious
    or deceptive corporate maneuvering.
  relevance_score: 9
  source: llm_enhanced
  text: This wasn't a good-faith scientific study. It really wasn't. It was a calculated
    act of corporate deception disguised as research.
  topic: safety/ethics
- impact_reason: Quantifies the market cap reversal, directly linking the loss of
    AI momentum to significant financial underperformance relative to competitors.
  relevance_score: 9
  source: llm_enhanced
  text: 'Fast forward to today: yeah, Apple is the third biggest company in the US
    by market cap, right? And now they are a half trillion dollars behind Microsoft.'
  topic: business
- impact_reason: Highlights severe legal and reputational consequences stemming from
    overpromising and underdelivering on AI features.
  relevance_score: 9
  source: llm_enhanced
  text: Apple's rollout of AI was absolutely so bad that they are now facing multiple
    class-action lawsuits because they couldn't deliver this simplest version of AI
    that they promoted.
  topic: safety/ethics
- impact_reason: Sets the immediate, high-stakes context regarding Apple's struggles
    with AI product delivery and subsequent legal/reputational damage.
  relevance_score: 9
  source: llm_enhanced
  text: Drama at Apple as AI failures cause heads to roll. Apple sued for false advertising
    over Apple Intelligence. Why Apple still hasn't cracked AI. Two more class-action
    lawsuits target misleading Apple Intelligence claims.
  topic: business/safety
- impact_reason: 'Provides a clear litmus test for distinguishing legitimate, industry-shifting
    research from internal marketing efforts: the diversity of institutional collaboration.'
  relevance_score: 9
  source: llm_enhanced
  text: 'On the left, that''s what marketing looks like: only Apple researchers, nothing
    else. Real research that changes the conversation on artificial intelligence would
    usually look like the paper on the right... [which had] researchers from multiple
    big companies. OpenAI, Harvard, Microsoft, University of Oxford.'
  topic: strategy/technical
- impact_reason: Critiques the methodology of Apple's benchmark as being unrealistically
    strict and unforgiving, failing to account for the probabilistic nature of LLMs.
  relevance_score: 9
  source: llm_enhanced
  text: They built what they said was a sterile testing environment using four classic
    logic puzzles... if a model got any of these four puzzles in a single move in
    any of these four puzzles wrong, test over, failure. So, not good. That is hyper-strict,
    unforgiving. That's not how large language models, especially reasoning models,
    would generally work.
  topic: technical
- impact_reason: Highlights the media's tendency to sensationalize research findings
    (especially negative AI news) based on press releases, suggesting a systemic issue
    in tech journalism.
  relevance_score: 9
  source: llm_enhanced
  text: Oh gosh, the media is going to completely fall for this, right? Being a former
    journalist, nothing against—I go to all these conferences, I meet brilliant tech
    journalists, and then there are some that are overwhelmed, and you get all these
    press releases, and you're like, "Okay, this is a salacious headline. Okay, looks
    factual. It's a research paper. Sure, let's go with it," right? It's going to
    get clicks, right?
  topic: strategy/safety
- impact_reason: A strong indictment of the study, labeling it as marketing masquerading
    as rigorous research, which is a significant concern for the scientific integrity
    of AI publications.
  relevance_score: 9
  source: llm_enhanced
  text: Apple, hey, Apple researchers, next time do what smart researchers do—yeah,
    I'm getting mad because I read a lot of research papers, and this one I knew was
    marketing, and that made me upset, right?
  topic: safety/ethics
- impact_reason: Provides a clear, accessible framework (Stage 1 vs. Stage 2 thinking)
    for differentiating between fast pattern matching and slower, deliberate reasoning.
  relevance_score: 9
  source: llm_enhanced
  text: Stage one is quick, intuitive, automatic responses based on pattern recognition
    learned from data. That's stage one. And then stage two represents a more deliberate,
    analytical, or conscious approach that involves reasoning and planning.
  topic: technical/theory
- impact_reason: Accuses the researchers of confirmation bias or data dredging, suggesting
    the methodology was designed around achieving a pre-determined negative result.
  relevance_score: 9
  source: llm_enhanced
  text: They find—you cherry-pick, right? This is almost like they got results and
    it then seems like they just reverse-engineered the entire study.
  topic: safety/ethics
- impact_reason: Directly refutes the concept of an 'AI wall' in this context, attributing
    the observed limitation entirely to the study's artificial constraints.
  relevance_score: 9
  source: llm_enhanced
  text: 'Critique four: well, this wall that they''re talking about, it''s imaginary
    because Apple''s wall was just an artifact of their own restrictive rules by cutting
    down the token output and restricting tool use.'
  topic: predictions/limitations
- impact_reason: Demonstrates the practical counter-evidence by successfully implementing
    the complex task using the same model family when appropriate tools/context are
    provided.
  relevance_score: 9
  source: llm_enhanced
  text: So yeah, let's look live. What could go wrong here doing this live? All right.
    So I built a working Tower of Hanoi in Claude 3.5 Sonnet with Thinking.
  topic: technical/proof
- impact_reason: This reframes an apparent failure (giving up) as a sign of higher-level
    strategic intelligence, suggesting AI is moving beyond simple execution to strategic
    decision-making.
  relevance_score: 9
  source: llm_enhanced
  text: mistaking intelligence for a flaw. So they said giving up is actually a smarter
    strategy.
  topic: predictions/strategy
- impact_reason: 'Highlights a key capability: recognizing computational infeasibility
    and pivoting to an alternative, efficient strategy.'
  relevance_score: 9
  source: llm_enhanced
  text: the AI correctly identified an impossible brute force task and sought a shortcut.
  topic: technical
- impact_reason: Specifies the exact model version (3.5 Sonnet) and the enabling feature
    ('Thinking'), which is crucial for technical reproducibility and understanding
    performance drivers.
  relevance_score: 9
  source: llm_enhanced
  text: So I didn't use Claude 4, I used Claude 3.5 Sonnet with Thinking.
  topic: technical
- impact_reason: Summarizes the core, controversial claim of the paper being discussed—that
    current LLMs have hard reasoning limits.
  relevance_score: 8
  source: llm_enhanced
  text: Apple's "The Illusion of Thinking" paper shows evidence that large reasoning
    models slam into a wall the moment that tasks get too demanding.
  topic: limitations
- impact_reason: Provides the host's core strategic belief regarding the inevitability
    and dominance of LLMs in the current technological landscape.
  relevance_score: 8
  source: llm_enhanced
  text: I'm pro-AI because I feel there's no real choice. Right? I believe in large
    language models, the power of large language models, just the way the entire world
    is investing in them. There's really no other solution.
  topic: strategy
- impact_reason: Clearly articulates the philosophical core of the paper Apple is
    promoting—the debate over whether LLMs truly 'think.'
  relevance_score: 8
  source: llm_enhanced
  text: Essentially, what this study said is, "Hey, these large language models that
    reason, or what they call large reasoning models, they don't really think." Right?
    This whole thinking thing, it's an illusion.
  topic: limitations
- impact_reason: Reveals Apple's high internal expectations for their proprietary
    model (Ajax) before their public rollout seemed to underdeliver.
  relevance_score: 8
  source: llm_enhanced
  text: At the time, they said that their internal model, which was codenamed Ajax
    and it did come out under a similar name, they said it is the most advanced language
    model and it is more powerful than ChatGPT.
  topic: technical
- impact_reason: Cites external media reports confirming internal turmoil and the
    direct impact of AI struggles on core product updates (Siri).
  relevance_score: 8
  source: llm_enhanced
  text: 'Apple has produced not a—not one that works, at least. Right? So some headlines
    here from some different publications like Payments, Axios, Bloomberg, PC Magazine,
    Computerworld. Let''s read some of these headlines, shall we? The Verge: "This
    is a crisis. New Apple report claims we''ll get no Siri upgrades at WWDC due to
    AI turmoil."'
  topic: business
- impact_reason: A personal anecdote demonstrating the real-world impact of Apple's
    AI delay on consumer purchasing decisions and trust.
  relevance_score: 8
  source: llm_enhanced
  text: 'I''m recording this on an Apple Mac mini. The camera I''m using for the livestream
    here, it''s the new iPhone. And one of the reasons I bought this new iPhone is
    because they''re like, "We''re going to have all this new AI on the iPhone." And
    here it is almost a year later: there''s not a single thing on this iPhone that''s
    "quote unquote" AI.'
  topic: business
- impact_reason: A strong, hyperbolic statement quantifying the perceived severity
    of Apple's strategic failure in the AI race.
  relevance_score: 8
  source: llm_enhanced
  text: Apple has fumbled the bag harder than any company has ever fumbled the bag,
    I would say, from a business perspective.
  topic: business
- impact_reason: Explains the core premise of Apple's research—the claim of universal
    data contamination—and critiques it as an aggressive dismissal of peer work.
  relevance_score: 8
  source: llm_enhanced
  text: Apple argued that standard AI tests for math and coding are unreliable due
    to data contamination. Data contamination is like kind of saying, 'Hey, all these
    other studies that all these other researchers do from multiple companies, multiple
    universities, yeah, they got it wrong because their data is bad.'
  topic: technical/safety
- impact_reason: Summarizes Apple's key reported findings, particularly the concept
    of 'effort collapse' where advanced models allegedly stop trying on hard tasks.
  relevance_score: 8
  source: llm_enhanced
  text: 'The results from what Apple reported: they said on easy puzzles, standard
    models did better, or non-thinking models. On medium puzzles, these reasoning
    or thinking models exceeded. And then on hard puzzles, they said all models completely
    failed. They didn''t even try, right? And this is what they called the ''effort
    collapse.'''
  topic: technical
- impact_reason: Highlights the immediate media sensationalism triggered by the research
    and the speaker's concern that journalists would accept the findings uncritically.
  relevance_score: 8
  source: llm_enhanced
  text: I saw it literally once it came out, right? Because it was trending on Twitter
    right away, because you saw these headlines, like, 'Oh, reasoning models collapsing,'
    'the AI wall,' right? Like all these AI doomsday articles. And I'm reading this,
    and I'm like, 'Oh gosh, the media is going to completely fall for this,' right?
  topic: safety/strategy
- impact_reason: Summarizes the core, seemingly damning, conclusions of the criticized
    study (effort collapse and algorithm failure), setting up the subsequent rebuttal.
  relevance_score: 8
  source: llm_enhanced
  text: 'So they just said, "Oh, reasoning models give up." And then also the algorithm
    failure. Their supposed killer blow was in a separate test: they gave the models
    the step-by-step instructions or the algorithm, and it didn''t help, and they
    all still failed at some point.'
  topic: technical/predictions
- impact_reason: 'Provides a crucial contextual comparison: if the task is near-impossible
    for top humans, AI failure at that level is not evidence of a fundamental reasoning
    flaw.'
  relevance_score: 8
  source: llm_enhanced
  text: So essentially, it's not surprising necessarily that AI couldn't, right? Because
    if you get the smartest humans in the world and give them a 15-disc problem, are
    they going to be able to do it? I don't even know if it's possible, right?
  topic: strategy/limitations
- impact_reason: 'Articulates the philosophical argument the study was trying to prove:
    that LLM ''reasoning'' is merely sophisticated next-token prediction, not genuine
    cognition.'
  relevance_score: 8
  source: llm_enhanced
  text: 'So their conclusion at least: well, reasoning models are an illusion, and
    the thinking that we see is a trick, right? They''re not actually thinking. They''re
    just—they''re just doing next-token prediction. It''s stage one thinking, not
    stage two, right?'
  topic: technical/philosophy
- impact_reason: Points out the brittle scoring mechanism, where a single minor error
    invalidates an otherwise successful, complex sequence of reasoning.
  relevance_score: 8
  source: llm_enhanced
  text: 'And also the absurd scoring: one mistake in your output rules pretty much
    ensures failure.'
  topic: technical/methodology
- impact_reason: Critiques perceived limitations in competitor models (implied to
    be Apple's) as being self-imposed constraints (policy/configuration) rather than
    inherent model capability limits.
  relevance_score: 8
  source: llm_enhanced
  text: this wall that they're talking about, it's imaginary because Apple's wall
    was just an artifact of their own restrictive rules by cutting down the token
    output and restricting tool use.
  topic: business/strategy
- impact_reason: A direct claim of successful execution of a classic, non-trivial
    algorithmic problem using a specific, current model version.
  relevance_score: 8
  source: llm_enhanced
  text: I built a working Tower of Hanoi in Claude 3.5 Sonnet.
  topic: technical
- impact_reason: Highlights the massive public and mainstream attention Apple's research
    received, even outside the core tech community.
  relevance_score: 7
  source: llm_enhanced
  text: Apple's latest AI research paper has gone viral. So viral, actually, it showed
    up in my wife's nightly business newsletter. She reads that usually has absolutely
    nothing to do with AI.
  topic: strategy
- impact_reason: Establishes the host's credibility and methodology for deep analysis,
    blending traditional investigative skills with modern AI tools.
  relevance_score: 7
  source: llm_enhanced
  text: I started my career as an investigative reporter. I did okay. I was a Pulitzer
    Fellow. I won ACP Story of the Year. So when I look at these things, I don't just
    read the study. Yes, I read the study manually twice. I fed it to three separate
    large language models. I combined a bunch to have conversations with the paper.
  topic: strategy
- impact_reason: Reiterates the core claim and promises a detailed debunking, setting
    the stage for technical critique.
  relevance_score: 7
  source: llm_enhanced
  text: They're saying that these large reasoning models don't actually think. Their
    entire experiment was fundamentally ripped. All right, and I'm going to show you
    why and show you how.
  topic: technical
- impact_reason: Provides concrete historical financial data illustrating Apple's
    previous dominance before the AI shift, setting up the contrast.
  relevance_score: 7
  source: llm_enhanced
  text: Apple was crushing the world. This was like '92 Dream Team kind of dominance
    for Olympic basketball. It wasn't close. Apple had a $2.1 trillion market cap
    in 2021, and the next closest company, Microsoft, had only a $1.6 billion—now,
    I'm not the best at math, but that's not close, right? Having a half a billion
    dollar or sorry, half trillion dollar—sorry—that was $2.1 trillion market cap
    versus Microsoft's $1.6 trillion market cap.
  topic: business
- impact_reason: A definitive summary statement on Apple's strategic misstep in the
    AI race.
  relevance_score: 7
  source: llm_enhanced
  text: So yeah, Apple has fumbled the bag harder than any company has ever fumbled
    the bag, I would say, from a business perspective.
  topic: business
- impact_reason: Highlights Apple's characteristic marketing spin (rebranding AI as
    'Apple Intelligence') and the subsequent failure to deliver on that branding.
  relevance_score: 7
  source: llm_enhanced
  text: They rebranded it because it's Apple. They're like, 'Oh, it's not even artificial
    intelligence, it's Apple Intelligence. Our AI is better than AI,' right?
  topic: business/strategy
- impact_reason: Identifies the specific models chosen for testing and notes the technical
    justification for selecting models that expose their full chain of thought (CoT)
    for reasoning evaluation.
  relevance_score: 7
  source: llm_enhanced
  text: They tested thinking versions of DeepSeek Coder 1 and Claude 3 Opus Sonnet.
    They did a lot of more technical testing. They technically tested some OpenAI
    models, but OpenAI doesn't show the complete chain of thought, whereas DeepSeek
    Coder 1 and Claude 3 Opus Sonnet Thinking do in the API. So, they did the right
    thing there, right, by making those the baseline models...
  topic: technical
- impact_reason: Details Apple's secondary argument intended to prove that even explicit
    algorithmic guidance cannot overcome the failure of reasoning models on complex
    tasks.
  relevance_score: 7
  source: llm_enhanced
  text: 'Their supposed killer blow was in a separate test: they gave the models the
    step-by-step instructions or the algorithm, and it didn''t help, and they all
    still failed at some point.'
  topic: technical
- impact_reason: Emphasizes the verifiable, functional nature of the AI's output,
    moving beyond theoretical discussion to practical demonstration.
  relevance_score: 7
  source: llm_enhanced
  text: this is a working, verifiable Tower of Hanoi that I just built.
  topic: technical
- impact_reason: 'Defines the goal of Apple''s proprietary testing methodology: establishing
    a ''sterile'' benchmark for reasoning.'
  relevance_score: 6
  source: llm_enhanced
  text: Their proposed solution was to create a clean and controllable environment
    to test what they said was a true, unvarnished reasoning limit of modern AI.
  topic: technical
- impact_reason: Uses the Tower of Hanoi as a concrete example to illustrate the exponential
    complexity scaling in logic puzzles, which is central to testing true reasoning
    capabilities.
  relevance_score: 6
  source: llm_enhanced
  text: I can solve it with t [three discs]... But as you add more discs, there's
    more complexity. So, as an example, they gave games like this, but there are three
    other ones. Let's just talk about the Tower of Hanoi.
  topic: technical
- impact_reason: Provides necessary context for the technical demonstration, grounding
    the abstract problem in a relatable analogy (pyramid).
  relevance_score: 5
  source: llm_enhanced
  text: the object is you have to move these three discs from Tower 1 on the left
    all the way to Tower 3 on the right, and you can never have a wider—so there's
    it's kind of like a pyramid for a podcast audience.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: EP 543: Apple’s Weaponized Research: Inside its illusion
  of thinking paper


  This episode of The Everyday AI Show, hosted by Jordan Wilson, is a deep-dive critique
  of Apple''s recently published research paper, "The Illusion of Thinking." The host
  argues that the paper is not a good-faith scientific study but rather a calculated
  act of **strategic deception and weaponized research** designed to mask Apple''s
  significant shortcomings in the generative AI space just before its Worldwide Developer
  Conference (WWDC).


  ### 1. Focus Area

  The primary focus is a **critical deconstruction of a specific AI research paper**
  ("The Illusion of Thinking") published by Apple. The discussion centers on:

  *   **Large Reasoning Models (LRMs):** Analyzing claims that LRMs "slam into a wall"
  when tasks become too demanding.

  *   **Research Methodology Critique:** Exposing flaws in Apple''s experimental design,
  benchmarking, and scoring criteria.

  *   **Corporate Strategy:** Linking the paper''s release to Apple''s competitive
  positioning against rivals (like Microsoft and Google) in the AI market.


  ### 2. Key Technical Insights

  *   **Flawed Benchmarking Design:** Apple''s "sterile testing environment" used
  classic logic puzzles (like the Tower of Hanoi) which are already extensively documented
  online, undermining their claim that the data was "clean" and uncontaminated.

  *   **All-or-Nothing Scoring:** The study employed an unforgiving, binary grading
  system where a single incorrect move in a complex, multi-step puzzle resulted in
  an immediate failure (a score of zero), which the host argues would "flunk Einstein"
  and does not reflect real-world model performance or iterative reasoning.

  *   **Misinterpretation of "Effort Collapse":** Apple framed the models'' decision
  to stop generating output on seemingly impossible tasks (due to complexity or token
  limits) as a "reasoning collapse" or "giving up," rather than a rational response
  to resource constraints or task infeasibility.


  ### 3. Business/Investment Angle

  *   **Trillion-Dollar Market Cap Gap:** The host calculates that Apple may have
  left **$2 trillion in market capitalization on the table** by failing to keep pace
  with the generative AI growth trajectory seen by competitors like Microsoft since
  2021.

  *   **Strategic Distraction:** The paper served as a "red herring" or "smokescreen"
  released just before WWDC to distract media and shareholders from Apple''s lack
  of meaningful AI product releases, following a year where they heavily promoted
  "Apple Intelligence."

  *   **Legal Repercussions:** Apple is facing multiple class-action lawsuits related
  to false advertising over their initial "Apple Intelligence" promises, highlighting
  the severe business risk of overpromising in the AI space.


  ### 4. Notable Companies/People

  *   **Apple:** The central subject, criticized for its perceived failure to develop
  competitive large-scale AI models despite massive internal investment.

  *   **Microsoft & Google:** Mentioned as the primary competitors who have successfully
  capitalized on the generative AI boom, leading to significant market cap gains.

  *   **DeepSeek Coder 1 and Claude 3 Opus Sonnet:** The specific reasoning models
  Apple tested in their paper.

  *   **Jordan Wilson (Host):** A former investigative reporter who applies journalistic
  rigor to deconstruct the paper both manually and using LLMs.


  ### 5. Future Implications

  The conversation suggests that the industry is moving toward a point where **corporate-sponsored
  "research" will increasingly be scrutinized as marketing collateral.** The host
  implies that Apple''s strategy—releasing misleading research to manage expectations
  before a major event—sets a dangerous precedent. The future requires consumers and
  professionals to look beyond sensational headlines and demand transparency regarding
  methodology, author independence, and the true purpose of published findings.


  ### 6. Target Audience

  This episode is highly valuable for **AI Professionals, Tech Strategists, Investors,
  and Tech Journalists** who need to cut through corporate messaging and understand
  the underlying technical and strategic realities of major tech players in the AI
  race.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- apple
- microsoft
- google
- openai
title: 'EP 543: Apple’s Weaponized Research: Inside its illusion of thinking paper'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 164
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 17
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 4
  prominence: 0.4
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 11:04:40 UTC -->
