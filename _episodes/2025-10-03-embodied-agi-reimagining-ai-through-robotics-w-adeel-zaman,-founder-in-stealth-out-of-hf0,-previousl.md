---
companies:
- category: tech
  confidence: high
  context: t still OK as the relationship we have today with Google or with large
    companies, and like Meta and so for
  name: Google
  position: 757
- category: tech
  confidence: high
  context: day with Google or with large companies, and like Meta and so forth? Or
    is it not OK? And welcome to Art
  name: Meta
  position: 798
- category: unknown
  confidence: medium
  context: eta and so forth? Or is it not OK? And welcome to Artificial Insights,
    where we talk to leaders and thinkers in AI abou
  name: Artificial Insights
  position: 849
- category: unknown
  confidence: medium
  context: rs in AI about how to do AI right. I'm your host, Daniel Menary. And this
    is season four. Here, we sit down every
  name: Daniel Menary
  position: 955
- category: unknown
  confidence: medium
  context: they do and why they do it. Today, I'm joined by Adeel Zaman. Adeel entered
    the University of Artilou at just
  name: Adeel Zaman
  position: 1170
- category: unknown
  confidence: medium
  context: ndustry and we grew that to $43 million annually. And I actually solved
    the cold startup problem of that
  name: And I
  position: 1900
- category: unknown
  confidence: medium
  context: roblem of that marketplace through deep learning. So I was always kind
    of truly intrigued by using deep
  name: So I
  position: 1990
- category: unknown
  confidence: medium
  context: em to just tackle them for one specific articles. But I think it makes
    sense. I think if you look at huma
  name: But I
  position: 5533
- category: unknown
  confidence: medium
  context: oing inside our brains and that's what really the Foundation Model pushes
    unlocked. So I'm trying to bring that to t
  name: Foundation Model
  position: 6042
- category: unknown
  confidence: medium
  context: people think of AGI, they think of something like Iroh Bot and it's this
    embodied thing that has ability to
  name: Iroh Bot
  position: 6874
- category: unknown
  confidence: medium
  context: ce. And even itself, because, you know, I live in San Francisco, I've been
    way, like, take kind of Waymo all the
  name: San Francisco
  position: 13423
- category: unknown
  confidence: medium
  context: nience is, I'll take Google example. I'm fully on Google Maps and, you
    know, Gmail and so forth. So they have a
  name: Google Maps
  position: 18571
- category: unknown
  confidence: medium
  context: asks, not only for yourself. So for example, with Grizzly AI, it will,
    once you teach it, you can actually put
  name: Grizzly AI
  position: 20476
- category: unknown
  confidence: medium
  context: botics world are in unison in this concept, where Tesla Optimus, let's
    say, tries to make Tesla Optimus as good a
  name: Tesla Optimus
  position: 24058
- category: tech
  confidence: high
  context: as it can and then they'll sell to you. Where an OpenAI will, you know,
    make a GPT-5 as good as it can fr
  name: Openai
  position: 24169
- category: unknown
  confidence: medium
  context: n a very AI first manner, a big believer of that. What I mean by that is
    like, I'm kind of thinking for fi
  name: What I
  position: 29404
- category: unknown
  confidence: medium
  context: igm. Cool. Yeah, that reminds me even of AlphaGo. Because AlphaGo was a
    small, strong predictor net with a very lar
  name: Because AlphaGo
  position: 35296
- category: big_tech
  confidence: high
  context: Mentioned as a large company with whom the current relationship regarding
    data access and AI power might be questioned.
  name: Google
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned alongside Google as a large company whose relationship with users
    regarding AI data access is being questioned.
  name: Meta
  source: llm_enhanced
- category: media/podcast
  confidence: high
  context: The name of the podcast/show where the interview is taking place, focused
    on leaders and thinkers in AI.
  name: Artificial Insights
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Adeel Zaman's previous company, which brought e-commerce to the construction
    industry and used deep learning to solve the cold startup problem.
  name: Dozer
  source: llm_enhanced
- category: ai_startup_ecosystem
  confidence: high
  context: The residency Adeel Zaman is currently building his new company out of,
    implying a focus on early-stage AI startups.
  name: HF0 residency
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The creator of the RT2 paper, which showed how multimodal foundation models
    could be improved by adding robot action data.
  name: DeepMind
  source: llm_enhanced
- category: ai_model_provider
  confidence: medium
  context: Mentioned as an example of a large multimodal foundation model (VLM) that
    could potentially be leveraged for embodied AI.
  name: Claude
  source: llm_enhanced
- category: ai_model_provider
  confidence: high
  context: Mentioned as an example of a large multimodal foundation model (VLM) and
    as the starting point for the foundation model approach in knowledge work.
  name: GPT
  source: llm_enhanced
- category: ai_concept/hypothetical
  confidence: medium
  context: Mentioned as a conceptual example of embodied AGI that people think of.
  name: Iroh Bot
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of autonomous driving, contrasting its current
    approach with the speaker's proposed real-time voice interaction paradigm for
    robots.
  name: Waymo
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned alongside Waymo as an autonomous driving system that has not
    adopted the proposed real-time voice interaction paradigm.
  name: FSD (Full Self-Driving)
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned in the context of cloud interaction and potentially a large model
    or platform being referenced by the speaker.
  name: GPC
  source: llm_enhanced
- category: big_tech_ai_model
  confidence: high
  context: A specific, large, and impressive LLM model that the speaker admires but
    finds too costly to run locally, and which is the subject of memory/personalization
    research.
  name: Gemini 2.5 Pro
  source: llm_enhanced
- category: ai_research_application
  confidence: high
  context: Referenced as a historical example of a system architecture involving a
    small predictor net prompting a larger model.
  name: AlphaGo
  source: llm_enhanced
date: 2025-10-03 11:40:53 +0000
duration: 37
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/ec1044ba195e4138be8469dbddf78908/
processing_date: 2025-10-06 03:09:31 +0000
quotes:
- length: 171
  relevance_score: 5
  text: And third, probably the biggest bucket is, because you actually need to, because
    you're trying to differentiate yourself from your market, whatever market you're
    in, right
  topics:
  - market
- length: 88
  relevance_score: 4
  text: And I actually solved the cold startup problem of that marketplace through
    deep learning
  topics:
  - market
- length: 160
  relevance_score: 4
  text: And it was really interesting where it showed that you could take a multimodal,
    multimodal foundation model like a VLM, let's say we had access to Claude or GPT
  topics: []
- length: 207
  relevance_score: 4
  text: Like there's nothing, but there are definitely companies in every sub-market
    that are like, okay, we've learned this new task and we do this better and that's
    why we get the next contractor, the next revenue
  topics:
  - revenue
  - market
- length: 127
  relevance_score: 3
  text: And I think arguably it's probably one of the biggest often unlocked we can
    do to prove our standard living as a society, right
  topics: []
- length: 87
  relevance_score: 3
  text: And so that's, I would argue that's probably the biggest thing that's holding
    this back
  topics: []
- length: 234
  relevance_score: 3
  text: Now there are some technical challenges, I will say, like the biggest one
    being cost, like how do you, because, like I think the bigger models perform better,
    I think, you know, the better less than something we've all kind of learned
  topics: []
- length: 130
  relevance_score: 3
  text: Where an OpenAI will, you know, make a GPT-5 as good as it can from pre-training
    and post-training and then they'll give it to you
  topics: []
- length: 217
  relevance_score: 3
  text: I almost have to take to ask this question because I feel like you've talked
    about so many of them already, but do you have a deeply held belief about artificial
    intelligence that has changed over this journey for you
  topics: []
- impact_reason: Raises a critical, timely question about data sovereignty, privacy,
    and the power dynamic between users and large model providers, given the increasing
    capability of AI.
  relevance_score: 10
  source: llm_enhanced
  text: If the AI is this strong, it can do all of these things and it has access
    to your data across all these paradigms. A, is it how important is it then? Is
    it still OK as the relationship we have today with Google or with large companies,
    and like Meta and so forth? Or is it not OK?
  topic: safety/ethics/business
- impact_reason: 'Identifies the two major unsolved challenges for embodied AI foundation
    models: data acquisition/structuring and developing continuous, human-like learning
    paradigms beyond static pre-training.'
  relevance_score: 10
  source: llm_enhanced
  text: One is how do you get the data in the right way? And the other one is how
    do you create a great paradigm of learning? Today, the models, they just get trained
    by the big labs and then they get released, but there isn't this paradigm of continuous
    learning that humans definitely have.
  topic: technical/strategy
- impact_reason: 'Describes a novel paradigm for embodied AI interaction: real-time,
    explicit Chain-of-Thought (CoT) reasoning communicated directly to the human operator
    via voice/screen.'
  relevance_score: 10
  source: llm_enhanced
  text: We let the foundation model reason real time. So when the X given is excavating
    on your, there's a touchscreen inside it. You can actually just, it just talks
    to you. It's like, I'm deciding to go left because I think I need to pick this
    rock up first and move it here before I can go do this next step.
  topic: technical/product
- impact_reason: 'Highlights the technical path forward: using natural language feedback
    from humans to generate Reinforcement Learning rewards, overcoming the limitations
    of hard-coded reward functions.'
  relevance_score: 10
  source: llm_enhanced
  text: I think there's two things. One is we can do RL to keep improving its learning.
    And that's something I'm really excited about with kind of RL from language guided
    rewards.
  topic: technical
- impact_reason: Describes a unified technical architecture where language generation
    (reasoning) and action execution are interleaved in a single token stream, a cutting-edge
    approach for embodied AI.
  relevance_score: 10
  source: llm_enhanced
  text: it is talking out loud about why it's making the decisions it's making. And
    it's, from a technical perspective, it's one stream of tokens. It's like, it's
    just spinning out the language tokens and the action tokens at the same time.
  topic: technical
- impact_reason: Pinpoints model size and the resulting cost of per-user retraining
    as the primary bottleneck for personalized AI deployment.
  relevance_score: 10
  source: llm_enhanced
  text: The challenge, the reason we haven't gone here is A, the model's too big.
    So it's just too costly to retrain it for every user. And so that's, I would argue
    that's probably the biggest thing that's holding this back.
  topic: business
- impact_reason: Contrasts the shared-weight, centralized update model (like FSD)
    with the proposed individual ownership model, emphasizing the value of proprietary,
    personalized training data/effort.
  relevance_score: 10
  source: llm_enhanced
  text: If they invest more time and teaching better, it's not like FSD, the weights
    are shared, right? Everybody teaches how to drive. Yeah. And you know, the weights
    are shared and then you re get the next FSD version where you get a better driver.
  topic: business
- impact_reason: 'Presents a technical solution to the cost problem: decoupling the
    massive base model weights from the small, user-specific reward/fine-tuning layers.'
  relevance_score: 10
  source: llm_enhanced
  text: you can kind of flop up the base and only the user only owns their kind of
    reward model and reinforcement learning that gets redone.
  topic: technical
- impact_reason: Critiques the current 'deliver-and-lock-in' paradigm of AI products,
    arguing it fundamentally fails to replicate the human employment relationship
    where continuous improvement is expected.
  relevance_score: 10
  source: llm_enhanced
  text: if we don't solve it, we're kind of stuck in this realm of, like right now,
    in the robotics world and the non-robotics world are in unison in this concept,
    where Tesla Optimus tries to make Tesla Optimus as good as it can and then they'll
    sell to you. Where an OpenAI will, you know, make a GPT-5 as good as it can from
    pre-training and post-training and then they'll give it to you. And once they
    give it to you, you can't really improve it to your liking.
  topic: strategy
- impact_reason: Strong technical argument favoring weight modification (true learning/fine-tuning)
    over external memory retrieval (like RAG) for deep, persistent knowledge acquisition,
    mirroring human cognition.
  relevance_score: 10
  source: llm_enhanced
  text: my current inclination is that you need to bake this learning into the weights,
    or at least that's how humans work. You know, like, we don't, the reason with
    this great, like if you learn something like a week ago and you kind of know it
    now, it's not because you like have some database...
  topic: technical
- impact_reason: 'Presents a concrete, cost-effective architectural solution for personalization:
    using a small, specialized model to handle memory/personalization, augmenting
    a large, static foundation model.'
  relevance_score: 10
  source: llm_enhanced
  text: to solve the cost problem, one thing that we're doing, and I think should
    be done, is a small model for memory and personalization and keeping the large
    model as is.
  topic: technical/business
- impact_reason: 'Poses the central philosophical/strategic question of the segment:
    Is pure knowledge-based scaling sufficient for AGI/white-collar replacement, or
    is physical grounding necessary?'
  relevance_score: 10
  source: llm_enhanced
  text: Are we, are we gonna be able to get to a white collar worker through the current
    AI models improving without this AGI having the ability to operate in the physical
    world?
  topic: predictions/safety
- impact_reason: Proposes embodied learning as a necessary component to overcome the
    diminishing returns seen in purely textual/data-driven scaling for complex tasks.
  relevance_score: 10
  source: llm_enhanced
  text: I think we may start getting diminishing returns and I think maybe kind of
    building it with this embodied understanding in the beginning will actually help
    it perform better.
  topic: technical/strategy
- impact_reason: Emphasizes the importance of multi-modality and environmental interaction
    (embodiment) for deeper understanding, not just better performance on specific
    tasks.
  relevance_score: 10
  source: llm_enhanced
  text: I am really curious about kind of that across modality, right, and across
    environment. Like the ability to just operate an environment helps you understand
    something that I think will help these models actually perform better across it.
  topic: technical/architecture
- impact_reason: This is the speaker's core thesis—a strong, potentially controversial
    claim that physical interaction is necessary before true Artificial General Intelligence
    can be achieved.
  relevance_score: 10
  source: llm_enhanced
  text: embodied agi is a prerequisite to agi.
  topic: predictions/strategy
- impact_reason: Highlights the massive gap between AI progress in the digital/knowledge
    space versus the physical world (manufacturing, construction), identifying a huge
    untapped market/opportunity.
  relevance_score: 9
  source: llm_enhanced
  text: We'd be surprised if we walked into manufacturing facilities, into construction
    job sites, in any kind of physical space, and seen how much of it is done by machine
    learning. And it's a very, very small percentage.
  topic: predictions/strategy
- impact_reason: 'This is a core insight into the power of foundation models: cross-domain
    data mixing leads to emergent capabilities and better performance, moving beyond
    siloed training.'
  relevance_score: 9
  source: llm_enhanced
  text: If you combine different pieces of data that you don't think actually have
    any commonality to them, it would actually make it better.
  topic: technical/strategy
- impact_reason: Frames the application of AI to physical tasks (embodied AI) not
    just as a business opportunity, but as a societal necessity to avoid a dystopian
    outcome where productivity gains are limited to white-collar work.
  relevance_score: 9
  source: llm_enhanced
  text: How can we take the leading AI models and apply them to the physical world?
    I feel like there's a lot of awesome work being done in the knowledge work space.
    And that's great. But we've been a dystopian future if we haven't made progress
    in using AI to accelerate tasks of the physical world, right?
  topic: predictions/strategy
- impact_reason: Strong endorsement of the unified foundation model paradigm extending
    into robotics, suggesting a single, generalist model for the physical world is
    achievable.
  relevance_score: 9
  source: llm_enhanced
  text: I really believe in the foundation model approach where there's going to be
    a one, one AI model that's going to be able to do all the, like a lot of different
    physical tasks or can learn a new task.
  topic: technical/predictions
- impact_reason: A strong assertion that interactive, reasoning-based control (voice/CoT)
    will replace traditional, pre-programmed or purely reactive robotics control.
  relevance_score: 9
  source: llm_enhanced
  text: I really believe that this may be the future paradigm robotics as opposed
    to the one that I've seen, that's what kind of inspired me.
  topic: predictions
- impact_reason: 'A crucial business/product insight: embodied AI, like human employees,
    requires continuous collaboration, feedback, and adaptation on the job site, not
    just perfect deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: I just don't believe in this concept that some robots can get trained to a
    great accuracy and get sold to us and then it just, like, works out of the box.
    That's actually not how employees work.
  topic: business/strategy
- impact_reason: Critiques the current reliance on dense, pre-defined reward functions
    in RL and proposes that language-guided feedback mimics the sparser, human-like
    learning process better.
  relevance_score: 9
  source: llm_enhanced
  text: One of the bare cases for it [RL] is it only works for coding and math because
    it's very filed with rewards. And the approach, the way I think we can kind of
    jump over this is what if, because if we look at back to human learning, you know,
    humans don't have their file rewards, even for math...
  topic: technical
- impact_reason: 'A concise definition of the proposed RL paradigm: human-in-the-loop,
    language-based reward generation for continuous improvement.'
  relevance_score: 9
  source: llm_enhanced
  text: It's just RL with the rewards being from the, in this case, the owner, right,
    giving you feedback back or whoever the robot's working with.
  topic: technical
- impact_reason: Explains the mechanism for translating human natural language feedback
    (from the owner) directly into a quantifiable reward signal for model training/fine-tuning.
  relevance_score: 9
  source: llm_enhanced
  text: in the back end, it's basically taking that language, converting it into a
    reward model and then training.
  topic: technical
- impact_reason: 'Raises critical safety and security concerns related to personalized
    fine-tuning: the risk of owners intentionally or unintentionally teaching harmful
    behaviors (alignment risk).'
  relevance_score: 9
  source: llm_enhanced
  text: And the second one is it's, well, you know, the creates and security concerns
    because some owner could really teach their model to do something bad or not,
    or even capability concerns.
  topic: safety
- impact_reason: 'Highlights the radical nature of the proposal: advocating for individualized,
    non-shared AI instances over the current trend of centralized, shared model improvement.'
  relevance_score: 9
  source: llm_enhanced
  text: I think what you're saying is unique in what I've heard where you're actually
    saying people should have individual instances of this AI that is customized,
    that is taught, rather than just shared among everybody.
  topic: strategy
- impact_reason: Argues that individual ownership solves both privacy concerns (by
    localizing data) and incentivizes user investment in improving the model.
  relevance_score: 9
  source: llm_enhanced
  text: But one thing I do know is it definitely does benefit if you can deliver good
    individually owned AI, right? If you, and secondly, I think it does create a great
    paradigm of incentive to teach.
  topic: business
- impact_reason: Emphasizes that for physical robots operating in sensitive personal
    environments (home, property), the owner *must* control the final alignment via
    RL.
  relevance_score: 9
  source: llm_enhanced
  text: you have four grizzly robots, you know, one is X, give it or one is a skids
    here, once you're humanoid, you truly want to really be the one doing reinforcement
    learning. You know, you want to be defining the rewards for, because it has X,
    like it's near your kids, you know, it's near, it's near your kind of property,
    like you need to, you probably want to drive that model more so.
  topic: safety
- impact_reason: 'Addresses the technical challenge of model versioning and migration:
    how to transfer personalized RL training from an older base model (e.g., Q1-3)
    to a newer, superior base model (e.g., Q1-5).'
  relevance_score: 9
  source: llm_enhanced
  text: When Q1-5 comes out, based off your training, Q1-5 will probably do better.
    So we have to kind of create this paradigm of being able to update.
  topic: technical
- impact_reason: 'Provides the core philosophical justification for personalized AI:
    replicating the dynamic, collaborative improvement cycle inherent in human employment.'
  relevance_score: 9
  source: llm_enhanced
  text: Like when you hire somebody, you can work with them to improve their intelligence,
    improve their, the way they work. And so obviously there's technical challenges,
    but I do hope we can solve this.
  topic: strategy
- impact_reason: 'Highlights a key limitation of current centralized LLM deployment:
    lack of customer-specific fine-tuning or personalization after deployment, contrasting
    it with human employment.'
  relevance_score: 9
  source: llm_enhanced
  text: Where an OpenAI will, you know, make a GPT-5 as good as it can from pre-training
    and post-training and then they'll give it to you. And once they give it to you,
    you can't really improve it to your liking.
  topic: limitations/business
- impact_reason: A powerful anecdote illustrating extreme leverage achieved by an
    AI-first founder, questioning traditional scaling models (0 vs 120 employees).
  relevance_score: 9
  source: llm_enhanced
  text: I'm kind of doing as one person. And you can kind of do that, but I, you know,
    I'm talking to you out, do I have, those are, we grew to 120 employees, right?
    And I'm thinking about this, well, you know, do I have zero employees or do I
    have 120 employees?
  topic: business/strategy
- impact_reason: Clearly contrasts the standard RAG/context window approach with the
    proposed 'small model augmentation' approach, framing it as a form of online learning.
  relevance_score: 9
  source: llm_enhanced
  text: What you're saying is, like, the common way to do memory is essentially distilling
    a list of facts. And then I go and look at that list of facts and I put it back
    in the LLM. You might have an approach where you have a small model that you're
    actually building as the way that learns, just any other model learns from the
    memories that augments the big model.
  topic: technical
- impact_reason: Identifies the gap between human real-time learning and current LLM
    update cycles, reinforcing the need for efficient online learning mechanisms.
  relevance_score: 9
  source: llm_enhanced
  text: humans have online learning. Like as we go, we learn like in real time as
    we kind of operate in the world. So what we try to try that, obviously what we
    try to try that is to retrain the Gemini 2.5 Pro with every day of interactions.
    Because that's not possible right now...
  topic: technical/limitations
- impact_reason: Argues that true understanding (like a human worker) requires embodied
    experience or grounding beyond just language processing, suggesting a limitation
    in pure LLM scaling.
  relevance_score: 9
  source: llm_enhanced
  text: But now, if we look at today's average media by color worker human, right?
    The only intelligence that we know that's been able to perform at that level actually
    learning a different way, kind of learn some base case understanding the world
    because a lot of the knowledge that we intake, we as humans or in-during pre-training
    for these models, it's referencing a lot of that about the world, right? That,
    yes, you can kind of read from language, but you can't really fully understand.
  topic: safety/limitations
- impact_reason: 'Highlights the central debate in AI research: whether purely language/data-driven
    models can achieve AGI without physical interaction (embodiment).'
  relevance_score: 9
  source: llm_enhanced
  text: the current AI models improving without this AGI having the ability to operate
    in the physical world?
  topic: technical/predictions
- impact_reason: Contrasts human learning (which involves embodied, world-model understanding)
    with current LLM training (which relies heavily on linguistic data).
  relevance_score: 9
  source: llm_enhanced
  text: the only intelligence that we know that's been able to perform at that level
    actually learning a different way, kind of learn some base case understanding
    the world because a lot of the knowledge that we intake, we as humans or in-during
    pre-training for these models, it's referencing a lot of that about the world,
    right?
  topic: technical/safety
- impact_reason: A nuanced warning against infinite scaling, suggesting that current
    methods will hit diminishing returns before achieving true general intelligence.
  relevance_score: 9
  source: llm_enhanced
  text: we may not be, like I'm not saying we're gonna hit a hard wall, but we're
    gonna keep getting diminishing returns, right?
  topic: strategy/predictions
- impact_reason: Distinguishes between question-answering capabilities (where current
    models excel) and complex, long-horizon task execution, which remains a major
    hurdle.
  relevance_score: 9
  source: llm_enhanced
  text: the best AI agent at doing, and specifically, I think more on like more agents,
    right? Because yes, you can get question answered really well. We already arguably
    are a close agi at a question answer approach, but the next part of like long
    horizon task, like, hey, go out and do this for me, right?
  topic: technical/limitations
- impact_reason: A specific, powerful example illustrating the benefit of multimodal/cross-domain
    training, even when the added domain (poetry) seems unrelated to the target task
    (coding).
  relevance_score: 8
  source: llm_enhanced
  text: The data ablation study that showed that because they added poetry encoding
    into the same model, it actually performed better at poetry encoding.
  topic: technical
- impact_reason: Connects embodied AI directly to improving global living standards
    through efficiency gains in essential physical sectors like infrastructure and
    manufacturing.
  relevance_score: 8
  source: llm_enhanced
  text: I think arguably it's probably one of the biggest often unlocked we can do
    to prove our standard living as a society, right? To be able to do these tasks
    at a lower cost and more efficiently.
  topic: predictions/strategy
- impact_reason: 'Pinpoints the core differentiator in building embodied AI: the learning
    mechanism and task adaptation, rather than just the base model architecture.'
  relevance_score: 8
  source: llm_enhanced
  text: The main difference is how it is kind of programmed to learn from its interactions.
    And how it's programmed to kind of learn different tasks.
  topic: technical/strategy
- impact_reason: Explains the challenge of decoupling high-level reasoning (the 'what')
    from low-level motor control (the 'how') in robotics, a key area of research.
  relevance_score: 8
  source: llm_enhanced
  text: If you pick up a mug, right? Like you're making the decision to pick up a
    mug and you're kind of subconsciously grasping it where you're not actually spitting
    out actuator movements. Your brain is not consciously saying, what is the, you
    know, what's the coordinates that your finger needs to move to to grasp this mug?
  topic: technical
- impact_reason: 'Defines the product strategy: leveraging voice as the primary, continuous
    interface for both debugging/training (when failing) and operational control (when
    succeeding).'
  relevance_score: 8
  source: llm_enhanced
  text: Our product is fully voice AI first, right? And you can talk to it all the
    time. You can talk to it when it's not working. You can be like, hey, next time
    you should try this and this and also when it is working.
  topic: business/product
- impact_reason: Reiterates the importance of explicit, audible reasoning (CoT) as
    a core feature for trust and collaboration in embodied AI systems.
  relevance_score: 8
  source: llm_enhanced
  text: The other aspect I'm really excited about is this con, is this kind of reasoning,
    like having a chain of thought reasoning alongside your robot working?
  topic: technical/product
- impact_reason: A specific vision for human-robot interaction in autonomous vehicles
    (and robots generally) emphasizing real-time, natural language override and course
    correction.
  relevance_score: 8
  source: llm_enhanced
  text: I could see a feature where you're like, hey, actually, Waymo, can you do
    a quick short left turn here? Actually, change my mind, I need to go here, just
    through voice.
  topic: predictions
- impact_reason: 'Identifies the core gap between current AI and human intelligence:
    the massive, personalized investment in teaching and reinforcement over time.'
  relevance_score: 8
  source: llm_enhanced
  text: what makes, you know, humans have this like kind of incredible advantage right
    now over the over our best digital intelligence models, which is that there are
    so many thousands of hours spent on teaching every human.
  topic: strategy
- impact_reason: Presents a near-future scenario of deeply integrated, multi-domain
    personal AI agents and forces a re-evaluation of the convenience vs. privacy trade-off
    in this new context.
  relevance_score: 8
  source: llm_enhanced
  text: The question I think in a lot of people's mind is, does is that privacy can
    be a trade off? Okay, but the AI. So let's just say in five years from now, we
    live in a world where, you know, we have a home AI which is connected to the same
    as our phone API, right? And let's assume that I can control all of our robots.
  topic: predictions
- impact_reason: 'Introduces a novel business model: monetizing personalized AI training
    efforts via a marketplace while retaining ownership/control of the instance.'
  relevance_score: 8
  source: llm_enhanced
  text: Once you teach it, you can actually put it up on a marketplace or others can
    use it for their projects. And you can kind of make money off the hard work of
    you teaching yours, but it's yours.
  topic: business
- impact_reason: Suggests using the intelligence of the *new* model itself to intelligently
    transfer or prune the learned behaviors from the *old* personalized model, reducing
    update costs.
  relevance_score: 8
  source: llm_enhanced
  text: as the models get better, they get smarter. We can actually use the same models
    to determine what parts of the reinforcement learning that was done on the previous
    model should be utilized and should not be utilized.
  topic: technical
- impact_reason: 'Provides a crucial market segmentation insight: 90% of users will
    likely use off-the-shelf models, leaving a critical 10% who need deep customization.'
  relevance_score: 8
  source: llm_enhanced
  text: I would say, actually, I would say in both the physical world and knowledge
    work, it would be that probably 90% approximately, right, of customers do not
    have the incentive or motivation or dynamics to do this.
  topic: business
- impact_reason: Argues that the competitive advantage derived from unique, specialized
    capabilities (which requires custom AI) exists equally in both physical and knowledge
    sectors.
  relevance_score: 8
  source: llm_enhanced
  text: But there are definitely companies in every sub-market that are like, okay,
    we've learned this new task and we do this better and that's why we get the next
    contractor, the next revenue. So I actually think the dynamic exists across physical
    world and knowledge task...
  topic: strategy
- impact_reason: Critiques the current industry standard for personalization (RAG/context
    stuffing) as insufficient and highlights the high cost of full model fine-tuning.
  relevance_score: 8
  source: llm_enhanced
  text: The challenge is that's a little bit of a cost problem that we'll have to
    resolve. Today, all the leading, there's a whole group startups in companies that
    are trying to solve memory and personalization using a big model like Gemini 2.5
    Pro. And all the approaches around either RAG, right? Where you store it, you
    just store that text and you do RAG on it or obviously putting the context, but
    that's okay. I think those approaches have been tried or like a graph-based search.
  topic: technical/business
- impact_reason: 'Accurately summarizes the dominant, current research trajectory:
    scaling data and compute for knowledge work tasks.'
  relevance_score: 8
  source: llm_enhanced
  text: This is, I would say, the more a higher probability belief among the researchers
    at the leading, like the leading researchers today, which is, there's a lot of
    merit to that, which is that, hey, the current approach is working really well.
    We just keep gathering more and more data on knowledge work tasks and we do larger
    training runs, right? And we do more kind of smarter RL for post training and
    we'll be able to get there...
  topic: technical/trends
- impact_reason: A cautionary prediction that the current scaling laws for knowledge
    work agents will eventually hit diminishing returns if they lack physical grounding
    or deeper learning mechanisms.
  relevance_score: 8
  source: llm_enhanced
  text: we're gonna keep getting diminishing returns, right? Like today, the best
    AI agent at doing, and specifically, I think more on like more agents, right?
    Beca
  topic: predictions
- impact_reason: Summarizes the prevailing, data-scaling hypothesis for achieving
    AGI, often associated with large language models (LLMs).
  relevance_score: 8
  source: llm_enhanced
  text: the current approach is working really well. We just keep gathering more and
    more data on knowledge work tasks and we do larger training runs, right? And we
    do more kind of smarter RL for post training and we'll be able to get there, right?
  topic: technical/strategy
- impact_reason: 'A clear prediction about the near-term impact of current AI scaling
    efforts: automating/assisting the median white-collar job.'
  relevance_score: 8
  source: llm_enhanced
  text: we'll be able to have and the kind of an average median white collar worker
    that's gonna be available to everyone.
  topic: predictions/business
- impact_reason: Highlights the contrarian nature of the speaker's belief regarding
    embodiment as a prerequisite for AGI, signaling a key divergence from mainstream
    scaling hypotheses.
  relevance_score: 8
  source: llm_enhanced
  text: I, you know, most people do not agree with this, but I think I really [believe
    that].
  topic: strategy
- impact_reason: Provides a human analogy (transfer learning in the brain) to explain
    the mechanism behind the success of large foundation models.
  relevance_score: 7
  source: llm_enhanced
  text: I think there's kind of transfer learning we're doing inside our brains and
    that's what really the Foundation Model pushes unlocked.
  topic: technical
- impact_reason: Defines the speaker's practical view of embodied AGI—intelligence
    demonstrated through physical interaction and task completion in the real world.
  relevance_score: 7
  source: llm_enhanced
  text: I think when people think of AGI, they think of something like Iroh Bot and
    it's this embodied thing that has ability to work in the world.
  topic: predictions
- impact_reason: A philosophical take on embodiment, suggesting that intelligence
    evolves best by mastering simpler physical forms first (like starting with an
    excavator before a humanoid).
  relevance_score: 7
  source: llm_enhanced
  text: I kind of think about this. I imagine it was put into human first with all
    these like different limbs, right? Would it have been able to try and do these
    tasks? And I don't know if it would have. And I think it was really able to kind
    of evolve in being able to learn a easier embodiment and then kind of grow from
    there.
  topic: strategy/predictions
- impact_reason: Reiterates the tension between the proven scaling laws (bigger models
    perform better) and the economic feasibility of deploying individualized large
    models.
  relevance_score: 7
  source: llm_enhanced
  text: The biggest one being cost, like how do you, because, like I think the bigger
    models perform better, I think, you know, the better less than something we've
    all kind of learned. And I think it's gonna keep going.
  topic: technical
- impact_reason: Highlights the open research questions surrounding the efficiency
    and methodology of continuous, personalized model updating.
  relevance_score: 7
  source: llm_enhanced
  text: What is it worth it to take every user's interactions and retrain a new large
    model, right? And how do you divide up when you do and when you don't?
  topic: technical
- impact_reason: Raises a strategic question about whether resource constraints affect
    the need for deep personalization differently across knowledge work vs. physical
    work sectors.
  relevance_score: 7
  source: llm_enhanced
  text: I do think there might be a difference between knowledge work and physical
    work in this space, where, you know, maybe an average office company that's small
    business doesn't have the resources to try and train this new AI to do their specific
    ways.
  topic: business/strategy
- impact_reason: Emphasizes the potential for radical efficiency gains when building
    companies using first-principles thinking applied to modern AI tools.
  relevance_score: 7
  source: llm_enhanced
  text: I'm kind of thinking for first principles is how many, like how efficient
    could you really be in today? Well, using AI. So I think, like, deeply, deeply
    efficient.
  topic: business
- impact_reason: Suggests that true excellence and innovation (even in knowledge work)
    requires collaboration and unique approaches that standard, generalized models
    may not replicate.
  relevance_score: 7
  source: llm_enhanced
  text: I don't know if it's a capability issue, right? There's always going to be,
    if you're trying to truly excel at what you're doing, you're going to work with
    your team member to do that report in a different way...
  topic: strategy
- impact_reason: Identifies a major macroeconomic driver (labor shortage) for AI/robotics
    adoption, especially in sectors like construction/re-industrialization.
  relevance_score: 7
  source: llm_enhanced
  text: we have a lot of labor shortage. So right now, one of the challenges is that
    just to grow, a lot of companies I talked to a lot of contractors, they don't
    grow because they worry that it's just going to be so hard to increase my labor
    force, right?
  topic: business/predictions
- impact_reason: Connects national infrastructure needs directly to the necessity
    of deploying advanced robotics/AI to overcome labor constraints.
  relevance_score: 7
  source: llm_enhanced
  text: I think we do need to re-industrialize in America. And it's, but to that,
    there is a lot of demand. There's a lot of demand for energy, for infrastructure,
    that needs to be built. But the labor just isn't there right now.
  topic: strategy
source: Unknown Source
summary: '## Podcast Episode Summary: Embodied AGI: Reimagining AI Through Robotics
  w/ Adeel Zaman


  This 37-minute episode of *Artificial Insights* features Adeel Zaman, founder of
  a stealth company emerging from the HF0 residency, discussing his vision for **Embodied
  AGI**—applying foundation model advancements to the physical world, particularly
  in robotics and construction.


  ---


  ### 1. Focus Area

  The primary focus is the **application of large foundation models (like LLMs/VLMs)
  to physical tasks via robotics**, aiming to bridge the gap between advanced knowledge
  work AI and the largely untouched physical labor sector (manufacturing, construction).
  The discussion centers on creating a unified, general intelligence for the physical
  world through embodied systems.


  ### 2. Key Technical Insights

  *   **Foundation Model Synergy in Embodiment:** The success of models like DeepMind''s
  RT-2, which leverages multimodal foundation models by adding robot action data,
  suggests that general world knowledge aids physical task performance. This mirrors
  the finding that mixing seemingly disparate data (like poetry and code) improves
  overall model performance.

  *   **Real-Time Reasoning and Action Tokenization:** Zaman is implementing a paradigm
  where the foundation model reasons *out loud* (chain-of-thought) in real-time alongside
  executing physical actions. Technically, this involves spinning out **language tokens
  and action tokens simultaneously** from a single stream.

  *   **Subconscious vs. Conscious Control:** A key technical goal is enabling the
  foundation model to handle the "subconscious" aspects of motor control (e.g., grasping
  mechanics) while the conscious reasoning layer handles high-level decision-making,
  similar to human cognition.


  ### 3. Business/Investment Angle

  *   **Massive Untapped Market:** The physical world (construction, manufacturing)
  currently utilizes very little machine learning, representing a massive opportunity
  to improve societal standards of living through efficiency gains.

  *   **Initial Market Entry:** Zaman is starting with high-value, foundational equipment
  in construction, specifically **excavators and skid steers**, before moving to more
  complex embodiments like humanoids.

  *   **Incentivized Learning Marketplace:** A core business model involves allowing
  owners to deeply customize and teach their individual AI instances. These customized
  models could then be optionally placed on a marketplace, allowing the original owner
  to monetize the unique training derived from their specific operational data.


  ### 4. Notable Companies/People

  *   **Adeel Zaman:** Former CTO/Co-Founder of **DOZR** (an e-commerce marketplace
  for construction, scaled to $43M ARR using deep learning to solve the cold start
  problem). Currently building an Embodied AI company out of **HF0**.

  *   **DeepMind:** Mentioned for the **RT-2 paper**, which demonstrated the effectiveness
  of grounding multimodal foundation models with robot action data.

  *   **Tesla Optimus / OpenAI:** Used as examples of the current paradigm where large
  labs train models to peak performance and then release them without deep, continuous
  user-specific improvement.


  ### 5. Future Implications

  The conversation strongly suggests a future where AI agents are **individually owned
  and continuously customized**, moving away from the current model of shared, static
  software updates (like FSD). This shift is crucial for achieving true utility in
  physical and white-collar AI workers, mirroring how humans learn on the job. The
  debate over **data privacy and ownership** will intensify as these powerful, data-hungry
  AIs become integrated into personal and professional lives.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Robotics Developers, Venture
  Capitalists focused on Deep Tech/Industrial Automation, and AI Strategists** interested
  in the transition from LLMs to embodied intelligence and the architectural challenges
  of personalized AI deployment.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- google
- meta
- openai
title: 'Embodied AGI: Reimagining AI Through Robotics w/ Adeel Zaman, Founder in Stealth
  out of HF0, previously CTO & Co-Founder of DOZR'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 81
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 8
  prominence: 0.8
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 4
  prominence: 0.4
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 4
  prominence: 0.4
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 03:09:31 UTC -->
