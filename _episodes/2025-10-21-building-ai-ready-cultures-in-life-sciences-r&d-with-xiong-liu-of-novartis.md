---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2159
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2194
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2560
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2622
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5303
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7029
- category: ai_media_and_research
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying GenAI and LLMs in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_finance
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    implying their use/involvement in enterprise AI.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_defense
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, implying their use/involvement in enterprise AI.
  name: Raytheon
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: An AI-first tool for organizing ideas and making connections, built by
    Steven Johnson, which functions as a personal expert based on uploaded documents.
  name: Notebook LM
  source: llm_enhanced
- category: ai_infrastructure_research
  confidence: high
  context: Mentioned as the proposer of the transformer architecture, which underpins
    modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: The specific URL provided for the Notebook LM tool, confirming its association
    with Google.
  name: Notebook LM (google.com)
  source: llm_enhanced
- category: consulting_ai_analysis
  confidence: high
  context: Mentioned for conducting a recent survey on generative AI investment and
    scaling in the life sciences sector.
  name: Deloitte
  source: llm_enhanced
- category: ai_company
  confidence: medium
  context: Referenced indirectly through the mention of 'GPT-like models' and the
    explosion of LLMs, which is synonymous with OpenAI's impact.
  name: OpenAI (Implied via GPT)
  source: llm_enhanced
- category: ai_research_model
  confidence: high
  context: A specific language model architecture mentioned as being used prior to
    GPT-like models for information extraction in clinical trials.
  name: BERT
  source: llm_enhanced
- category: ai_company_or_model
  confidence: high
  context: Referenced directly as 'GPT-like models' which leverage the decoder part
    of the transformer architecture for data generation.
  name: GPT (Implied)
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 18:18:14 +0000
quotes:
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 100
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs, foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 103
  relevance_score: 4
  text: Fine-tuning meaning, yes, we take the foundation models, but we can still
    take the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 220
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work, and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 121
  relevance_score: 4
  text: So, you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning
  topics: []
- length: 85
  relevance_score: 4
  text: But now the GPT, they also leverage the decoding part of the transformer architecture
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 167
  relevance_score: 4
  text: Although the goal of the foundation model is, it's tried to, you know, leverage
    all the available data, minimizing, you know, the specifics, you know, of specific
    data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 72
  relevance_score: 3
  text: So, we can use the data you have to train, to fine-tune the model, right
  topics: []
- length: 106
  relevance_score: 3
  text: So, that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: Clearly maps the historical evolution of AI/ML, emphasizing the current
    'paradigm shift' towards foundation models, which is crucial context for understanding
    modern AI strategy.
  relevance_score: 10
  source: llm_enhanced
  text: We have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks. And more recently, we have a paradigm shift into those language models,
    foundation models.
  topic: technical
- impact_reason: Provides a precise technical and conceptual distinction between traditional
    ML (task-specific, labeled data) and the foundation model approach (leveraging
    massive, broad pre-training).
  relevance_score: 10
  source: llm_enhanced
  text: The difference is that previously when we do machine learning, we just collect
    our own data, we label the data, and then apply algorithms to train models...
    But now people have been developing these new paradigms, which is along with LLMs,
    foundation models. So the idea is that yes, you still care about the specific
    data you are working on, but at the same time, we want to collect all the available
    data, relevant data, or irrelevant data, but as long as they fall into the same
    domain, and feed those data into those transformer architectures to build a much
    large-scale pre-trained models.
  topic: technical
- impact_reason: 'Offers a concrete, high-value example of foundation model utility
    in life sciences: leveraging public molecular data (like cell atlases) to gain
    insights into specific indications even with limited proprietary data.'
  relevance_score: 10
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So, for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data. So, for example, cell atlases, etc.
  topic: technical
- impact_reason: Provides a clear technical distinction between BERT (encoder-only,
    good for representation/embeddings) and generative models, explaining the shift
    in capability from feature extraction to generation.
  relevance_score: 10
  source: llm_enhanced
  text: So, the difference is that the previously BERT architecture, they use the
    decoder part of the transformer. So, you can think it's as a very good representation
    model. So, many, it can capture the hidden context among the, in the text between
    words, etc., that where a human sees. So, the outcome is some representations
    or embeddings of those text tokens, etc.
  topic: technical
- impact_reason: 'Explains *why* GPT-style models are generative: leveraging the decoder
    stack allows them to create novel sequences, which is transferable to biological
    sequences (molecules, genes).'
  relevance_score: 10
  source: llm_enhanced
  text: But now the GPT, they also leverage the decoding part of the transformer architecture.
    So, meaning they are now able to generate new types of data. So, for example,
    a new types of sequence that represent your language.
  topic: technical
- impact_reason: 'Crucial insight into controlled generation: foundation models can
    incorporate complex, multi-objective constraints (like toxicity and solubility)
    simultaneously during molecule design.'
  relevance_score: 10
  source: llm_enhanced
  text: So, now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So, for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/applications
- impact_reason: 'Identifies a major bottleneck: the gap between theoretical AI capability
    (architecture/concept) and the physical limitations of experimental biology in
    generating necessary high-fidelity data (e.g., single-cell sequencing).'
  relevance_score: 10
  source: llm_enhanced
  text: I mean, that is potentially possible. But unfortunately, to get those, you
    know, single-cell RNA-seq data, all those kind of more, you know, experiment-driven
    data, probably we're not exactly there yet, because, you know, we still rely on
    those traditional biotechnology methods to collect those data through experiments
    on cell lines, on human tissues, etc. So, there's, you know, where AI, the architecture,
    the concept there, but the, sometimes we are limited by the, you know, experimental
    side.
  topic: limitations/technical
- impact_reason: Directly addresses the hallucination problem in high-stakes domains.
    Unlike text/code, biological outputs require slow, expensive experimental validation,
    amplifying the risk of hallucination.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.,
    for example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: 'Proposes the critical solution for high-stakes AI deployment: developing
    domain-specific, knowledge-checking benchmarks and objective metrics to quantify
    model reliability beyond superficial checks.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think the way is that to define those, you know, knowledge-checking
    benchmarks and also have, you know, objective metrics to measure against those
    AI models, because right now the technology cannot give you 100% answers, how
    those nations are always there, right?
  topic: safety/strategy
- impact_reason: Highlights the significant gap between investment interest and successful,
    scaled deployment of GenAI in the life sciences sector, a critical business challenge.
  relevance_score: 9
  source: llm_enhanced
  text: Flash forward to 2024, a recent Deloitte survey notes that while 73% of life
    sciences leaders are investing in generative AI initiatives, fewer than 20% have
    successfully scaled these technologies beyond pilot projects.
  topic: business
- impact_reason: 'Explains the core value proposition of foundation models: knowledge
    transfer and utility even with limited domain-specific data (few-shot/zero-shot
    learning potential).'
  relevance_score: 9
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical
- impact_reason: Provides a clear, accessible definition of fine-tuning in the context
    of foundation models—adjusting weights rather than full retraining—which is critical
    for practitioners.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning, yes, we take the foundation models, but we can still
    take the data you have, right? So, we can use the data you have to train, to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems...
  topic: technical
- impact_reason: Articulates the powerful analogy of treating biological data (genetics)
    as language, justifying the application of LLMs/foundation models directly to
    molecular design and drug development simulation.
  relevance_score: 9
  source: llm_enhanced
  text: Genetics are just code, they're just a language. So we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of meant much of across the drug development process,
    not least of which are the clinical trials challenges...
  topic: predictions
- impact_reason: 'Offers a specific, cutting-edge application: using GenAI to synthesize
    biological data (cell types, gene expressions) for *in silico* testing, accelerating
    early discovery.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical
- impact_reason: 'Lists the critical, non-technical hurdles for AI adoption in R&D:
    benchmarking, cross-functional alignment, and measuring ROI.'
  relevance_score: 9
  source: llm_enhanced
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business
- impact_reason: Directly links foundation models to accelerating the discovery of
    novel biological targets and the design of new molecules, a core goal in pharma/biotech.
  relevance_score: 9
  source: llm_enhanced
  text: So, that way can allow us to better focus on discovering, you know, novel
    targets, novel gene pathways, etc. And also then when it goes to the new molecule
    design, then also a lot of foundation models have been built.
  topic: business/applications
- impact_reason: Emphasizes the indispensable role of domain experts (SMEs) in guiding
    data selection, model development, and crucially, benchmarking, even when using
    generalist foundation models.
  relevance_score: 9
  source: llm_enhanced
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves. Although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data. But, you know, we still need
    the domain knowledge people, you know, people have those kind of expertise to
    guide the, you know, the selection of the data and also more importantly, then
    these models are, you build, now how do we benchmark with them?
  topic: strategy/business
- impact_reason: 'A strategic warning: AI adoption in life sciences will be a hybrid
    model, not a complete replacement of traditional methods. Subject matter experts
    must understand this blended paradigm.'
  relevance_score: 9
  source: llm_enhanced
  text: It behooves them to kind of have some awareness of the new and old paradigm
    that we were talking about before, how both are still present, but how we thought
    that AI would impact life sciences organization or that they'd just be this one
    school way of doing things is not the case. And that door remains open
  topic: strategy/predictions
- impact_reason: 'Describes a strategic shift in data management for AI adoption:
    centralizing data first, then applying advanced algorithms, contrasting with older,
    siloed approaches.'
  relevance_score: 8
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization...
  topic: strategy
- impact_reason: Identifies the Transformer architecture as the fundamental, unifying
    technical breakthrough underpinning the current wave of foundation models across
    modalities (text, image, molecular data).
  relevance_score: 8
  source: llm_enhanced
  text: So, all those models, you know, essentially they have the same, I mean, by
    nature, they have the same setup. So, that is based on the breakthrough of the
    transformer architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: Reiterates the foundational importance of the Transformer architecture
    as the basis for current GenAI success, even in specialized domains like biology.
  relevance_score: 8
  source: llm_enhanced
  text: So, that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago.
  topic: technical
- impact_reason: 'A concise summary of the organizational requirement for successful
    AI implementation in specialized fields: mandatory cross-disciplinary teamwork
    (AI + Biology/Chemistry).'
  relevance_score: 8
  source: llm_enhanced
  text: So, it's basically a cross-discipline collaboration, whether in house or externally,
    yeah.
  topic: strategy
- impact_reason: 'Strategic observation: The maturity of NLP/LLMs provides a head
    start for language-like biological data (genetics, sequences) compared to other
    data modalities in life sciences.'
  relevance_score: 8
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: Highlights that organizational culture and alignment across diverse
    stakeholders (scientists, modelers, leadership) are key hurdles for effective
    GenAI adoption.
  relevance_score: 8
  source: llm_enhanced
  text: So, there's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: Poses a key strategic question regarding the build vs. buy decision
    for foundational models, especially in specialized domains like healthcare.
  relevance_score: 7
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
- impact_reason: 'A classic but critical reminder: the success of foundation models
    is gated by the quality, not just the quantity, of the domain-specific data plugged
    in.'
  relevance_score: 7
  source: llm_enhanced
  text: So, the first thing is about the data challenge, right? So, so the foundation
    models, although they take all kinds of available data, but still they require
    high-quality data.
  topic: business/technical
source: Unknown Source
summary: '## Comprehensive Summary: Building AI-Ready Cultures in Life Sciences R&D


  This podcast episode, featuring **Zhong Liu, Director of Data Science in AI at Novartis**,
  focuses on the paradigm shift in Life Sciences R&D driven by **Generative AI (GenAI)
  and Foundation Models (FMs)**, and the cultural and technical challenges in scaling
  these technologies beyond pilot projects.


  ### 1. Focus Area

  The discussion centers on the transition from traditional, task-specific Machine
  Learning (ML) to leveraging large-scale Foundation Models in R&D workflows, including
  **molecule design, digital twin creation, and clinical protocol optimization**.
  Key themes include the architectural differences between older models (like BERT)
  and generative models (like GPT), the necessity of domain-specific fine-tuning,
  and the organizational hurdles to achieving meaningful AI adoption and ROI.


  ### 2. Key Technical Insights

  *   **Paradigm Shift to Foundation Models:** The core technical evolution is moving
  from training models solely on proprietary, labeled data to utilizing massive, pre-trained
  FMs (built on transformer architectures) that capture broad domain knowledge. This
  allows for better performance even with limited downstream, specific data via **fine-tuning**
  (adjusting weights rather than full retraining).

  *   **Generative Capabilities via Decoders:** The advancement from representation
  models (like BERT, using the decoder part of the transformer) to generative models
  (like GPT, leveraging the decoder part) enables the creation of novel outputs, such
  as new molecular sequences (represented by SMILES strings or graphs) constrained
  by desired properties (e.g., toxicity, solubility).

  *   **Domain-Specific Benchmarking is Crucial:** Due to the risk of **hallucination**
  in complex biological outputs, standard validation is insufficient. The industry
  must develop **domain-driven evaluation metrics and knowledge-checking benchmarks**
  to objectively score and select optimal models for specific tasks (e.g., target
  identification, pathway analysis).


  ### 3. Business/Investment Angle

  *   **Scaling Gap:** Despite high investment (73% of life sciences leaders investing
  in GenAI), scaling remains a major hurdle (fewer than 20% successful beyond pilots),
  indicating a gap between experimentation and meaningful integration into core workflows.

  *   **Data Quality vs. Quantity:** While FMs thrive on massive data, high-quality,
  domain-specific experimental data (like single-cell RNA-seq) still lags behind the
  theoretical capabilities of the AI architectures, creating a bottleneck limited
  by traditional biotechnology methods.

  *   **ROI Measurement Challenge:** Establishing clear, objective metrics to measure
  the Return on Investment (ROI) for AI-driven research is essential for leadership
  buy-in and sustained investment, especially given the difficulty in instantly validating
  complex biological outputs.


  ### 4. Notable Companies/People

  *   **Zhong Liu (Novartis):** The expert guest, providing insights from his role
  as Director of Data Science in AI, focusing on practical implementation and cultural
  alignment within a major pharmaceutical company.

  *   **Google (Transformer Architecture):** Mentioned as the originator of the foundational
  transformer architecture that underpins modern LLMs and FMs.

  *   **Deloitte:** Referenced for a recent survey highlighting the industry-wide
  challenge of scaling GenAI adoption.


  ### 5. Future Implications

  The industry is moving toward **hybrid development models**—leveraging public FMs
  while simultaneously building proprietary, domain-specific versions tailored to
  internal data and needs. Success hinges on **cross-discipline collaboration** between
  AI modelers, basic scientists, and leadership. The future involves deeper automation
  in drug discovery and clinical trials, heavily reliant on the ability of FMs to
  process and generate complex biological "language."


  ### 6. Target Audience

  This episode is highly valuable for **Life Sciences R&D leaders, Chief Digital/Information
  Officers, Data Science Directors, and AI Strategy Executives** within pharma and
  biotech. It is targeted at professionals needing strategic insights into AI adoption,
  organizational alignment, and the technical evolution of ML in a highly regulated,
  data-intensive domain.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 119
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 11
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 18:18:14 UTC -->
