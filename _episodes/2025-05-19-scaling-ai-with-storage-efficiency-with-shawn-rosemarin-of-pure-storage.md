---
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Sean
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Sean Rosemurin, Vice President
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Sean Rosemurin, Vice
    President of R&D and Customer Engineering a
  name: Sean Rosemurin
  position: 134
- category: unknown
  confidence: medium
  context: rge AI Research. Today's guest is Sean Rosemurin, Vice President of R&D
    and Customer Engineering at Pure Storage.
  name: Vice President
  position: 150
- category: unknown
  confidence: medium
  context: uest is Sean Rosemurin, Vice President of R&D and Customer Engineering
    at Pure Storage. Pure Storage is an enterprise da
  name: Customer Engineering
  position: 176
- category: unknown
  confidence: medium
  context: Vice President of R&D and Customer Engineering at Pure Storage. Pure Storage
    is an enterprise data storage compa
  name: Pure Storage
  position: 200
- category: tech
  confidence: high
  context: nergy consumption but also enables enterprises to scale AI applications
    more effectively. If you're looking
  name: Scale Ai
  position: 942
- category: tech
  confidence: high
  context: y, how are you using water, things like that, how OpenAI is doing that,
    various other brands that tends to
  name: Openai
  position: 1876
- category: unknown
  confidence: medium
  context: d grand aspirations of what this would do, right? If I had told you 25
    years ago we would be ordering ou
  name: If I
  position: 4095
- category: tech
  confidence: high
  context: ve estimate. I'm sure the folks that we have some Facebook guests on the
    show anytime soon, they're going to
  name: Facebook
  position: 5764
- category: unknown
  confidence: medium
  context: myself, even keeping in mind, I subscribe to the Blue Sky app, it's kind
    of a Twitter apparent. There was o
  name: Blue Sky
  position: 7676
- category: unknown
  confidence: medium
  context: ntext that I'm giving you. Of course, we all know Lee Harvey Oswald shot
    JFK, but the point is you would, even if the
  name: Lee Harvey Oswald
  position: 8807
- category: unknown
  confidence: medium
  context: care of your kids, whatever your nightmare robot, Isaac Asimov scenario
    is, it's going to be there in a few year
  name: Isaac Asimov
  position: 9956
- category: unknown
  confidence: medium
  context: ario. If I go train a bunch of data that includes Matt D'Amello, and then
    all of a sudden Matt D'Amello sa
  name: Matt D
  position: 12551
- category: unknown
  confidence: medium
  context: A very complex situation. I know I have these Obi-Wan Kenobi moments on
    the show where I can hear the audience
  name: Wan Kenobi
  position: 13048
- category: unknown
  confidence: medium
  context: licated when the EU passes laws like the Right to Be Forgotten, the recent
    EU AI Act that went into effect this
  name: Be Forgotten
  position: 13540
- category: unknown
  confidence: medium
  context: s laws like the Right to Be Forgotten, the recent EU AI Act that went into
    effect this past February. A lot o
  name: EU AI Act
  position: 13565
- category: unknown
  confidence: medium
  context: an employee ID, we had a name, we had something. So I have to be able to
    connect it, and then I have to
  name: So I
  position: 15533
- category: unknown
  confidence: medium
  context: at's within that document. So, for instance, your Social Security number,
    your birth date, whatever payment informa
  name: Social Security
  position: 16125
- category: unknown
  confidence: medium
  context: coding schema is really how do we call metadata? And I know that sounds
    like a bit of an intimidating wo
  name: And I
  position: 16607
- category: unknown
  confidence: medium
  context: way, how much of what we have is actually usable? Because I'm going to
    say something here a little provocativ
  name: Because I
  position: 19697
- category: unknown
  confidence: medium
  context: of power in order to drive the system deployment? And GPUs consume 10 times
    more power than CPUs. Yes, they'
  name: And GPUs
  position: 23779
- category: tech
  confidence: high
  context: buy a, you know, mothballed nuclear facility like Microsoft recently did
    with Three Mile Island and repower i
  name: Microsoft
  position: 24518
- category: unknown
  confidence: medium
  context: nuclear facility like Microsoft recently did with Three Mile Island and
    repower it? Do you put yourself next to a hyd
  name: Three Mile Island
  position: 24546
- category: unknown
  confidence: medium
  context: e at Emerge. I know our CEO and Head of Research, Dan Fagella, works very
    closely with organizations like the U
  name: Dan Fagella
  position: 25522
- category: unknown
  confidence: medium
  context: a, works very closely with organizations like the United Nations and the
    OECD on a lot of recommendations. I think
  name: United Nations
  position: 25582
- category: ai_research
  confidence: high
  context: The organization where the host, Matthew Damello, is the Editorial Director,
    suggesting involvement in AI analysis/publishing.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The company sponsoring the special series and where the guest, Sean Rosemurin,
    is VP of R&D and Customer Engineering. They provide infrastructure critical for
    scaling AI.
  name: Pure Storage
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of sustainability headlines regarding water usage
    and AI adoption.
  name: OpenAI
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as a potential guest whose team might correct estimates on the
    history of machine learning adoption (likely referring to Meta AI).
  name: Facebook
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The specific product whose emergence two years prior is cited as the 'shot
    heard around the world' for bringing AI into the mainstream.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a company utilizing digital twins for biological
    systems (like the immune system), often involving advanced AI/ML models.
  name: Pfizer
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a hyperscaler that is actively securing power by repowering
    a mothballed nuclear facility (Three Mile Island) to support its growing AI/data
    center needs.
  name: Microsoft
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: A mothballed nuclear facility mentioned specifically because Microsoft
    is repowering it to secure energy for its AI infrastructure.
  name: Three Mile Island
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: An organization where the podcast's CEO/Head of Research (Dan Fagella)
    collaborates on policy recommendations regarding AI and energy issues.
  name: United Nations
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: An organization where the podcast's CEO/Head of Research (Dan Fagella)
    collaborates on policy recommendations regarding AI and energy issues.
  name: OECD
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: The entity/podcast hosting the discussion, whose leadership is involved
    in policy work related to AI challenges.
  name: Emerge
  source: llm_enhanced
date: 2025-05-19 06:00:00 +0000
duration: 34
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_5.19.25_-_Shawn_Rosemarin.mp3?dest-id=151434
processing_date: 2025-10-05 16:38:08 +0000
quotes:
- length: 234
  relevance_score: 6
  text: And then how do I build the most efficient storage system in order to feed
    that data to the training systems, but also be able to power my inference systems,
    and do it in a way that is operationally feasible over a long period of time
  topics: []
- length: 217
  relevance_score: 4
  text: If you're looking to understand how to make your infrastructure ready for
    AI and future-proof your operations, this episode will provide invaluable insights
    on how to leverage storage efficiency for sustainable growth
  topics:
  - growth
- length: 240
  relevance_score: 4
  text: The rules that emerge, we do use LLM to assist in certain forms of content
    creation, but the cardinal commandments I give on down as an editorial director
    is you would never ever trust a large language model with context you did not
    give it
  topics: []
- length: 115
  relevance_score: 4
  text: '" Well, now I got to go look at all the training models and all the inference
    that would have been gained from Matt'
  topics: []
- length: 195
  relevance_score: 4
  text: I have to re-rag, retrieval augmented generation, all those systems, having
    removed Matt's PII, and I have to be able to do that real-time to ensure that
    I don't get hit with a fine down the road
  topics: []
- length: 212
  relevance_score: 4
  text: And then I think I've heard a very different conversation on the program around
    responsible AI, which I think when you get to the mainstream media becomes a lot
    more of a political marketing kind of talking point
  topics:
  - market
- length: 252
  relevance_score: 4
  text: So if you can't connect the data across these hundreds of sources, if you
    can't search it, and if you can't essentially integrate it into the result, then
    I can't take that good data pipeline, that ultimate record, and feed it into my
    GPUs for training
  topics: []
- length: 236
  relevance_score: 4
  text: And so, we're seeing this focus on efficiency, this focus on what we call
    terabyte per watt—how much storage can I get in power within a given watt of electricity—is
    allowing customers to say, "Hey, I can now support this GPU deployment
  topics: []
- length: 182
  relevance_score: 3
  text: He breaks down how transitioning to cutting-edge, flash, storage technology
    not only reduces energy consumption but also enables enterprises to scale AI applications
    more effectively
  topics: []
- impact_reason: Provides a specific technological solution (flash storage) that addresses
    both sustainability (energy) and performance (scaling AI).
  relevance_score: 10
  source: llm_enhanced
  text: transitioning to cutting-edge, flash, storage technology not only reduces
    energy consumption but also enables enterprises to scale AI applications more
    effectively.
  topic: technical/business
- impact_reason: Shifts focus from model capability to data quality and governance
    as the primary enablers/bottlenecks for business outcomes.
  relevance_score: 10
  source: llm_enhanced
  text: what's the actual quality of our data? What's the ability to connect it? What's
    the ability to integrate it? What's the quality of that data? And more specifically,
    what's the exposure from a governance and security standpoint as it relates to
    using this data in order to drive towards these business outcomes?
  topic: business/safety
- impact_reason: Highlights the fundamental cognitive shift required by users moving
    from traditional search (relevance) to AI answers (perceived truth).
  relevance_score: 10
  source: llm_enhanced
  text: Now we're actually getting comfortable with answers. And it's really interesting
    because as humans, this is going to be a big adjustment for us to get from this
    concept of show me what you think might be relevant to actually, I'm going to
    trust the machine to sort through all these potential relevant answers and tell
    me what is the actual truth, what is the actual answer based on the data.
  topic: safety/predictions
- impact_reason: Reiterates the critical dependency of LLM output quality on underlying
    data infrastructure and quality.
  relevance_score: 10
  source: llm_enhanced
  text: It's no longer just about the LLM. It's no longer just about, you know, the
    fancy front-end app. It's actually what is the quality of the response? And that
    is coming back to what's the quality of the back-end data and how well is it being
    served and connected in order to deliver it?
  topic: technical/business
- impact_reason: Identifies GPU utilization as the key economic driver and storage
    speed as the direct bottleneck preventing that utilization, framing storage as
    a direct ROI issue.
  relevance_score: 10
  source: llm_enhanced
  text: The most expensive part of this infrastructure as organizations look to build
    it out are the GPUs. They're getting faster... The bad news is they're expensive.
    And so if you can't feed the storage fast enough to get them utilized at 100%,
    then you're being put in a situation where you've invested in capacity you can't
    use.
  topic: business/technical
- impact_reason: Provides a concrete, high-stakes example of the difficulty of enforcing
    data governance (like GDPR's Right to be Forgotten) in complex, trained AI models,
    specifically mentioning RAG systems.
  relevance_score: 10
  source: llm_enhanced
  text: If I go train a bunch of data that includes Matt D'Amello, and then all of
    a sudden Matt D'Amello says, 'I have a right to be forgotten. Pull all of my data
    out of your systems.' Well, now I got to go look at all the training models and
    all the inference that would have been gained from Matt. I have to re-rag, retrieval
    augmented generation, all those systems, having removed Matt's PII, and I have
    to be able to do that real-time to ensure that I don't get hit with a fine down
    the road.
  topic: safety/technical
- impact_reason: Directly links data pipeline efficiency (storage speed, metadata
    lookups) to GPU utilization and ROI. Poor data infrastructure leads to massive
    sunk costs and underutilized expensive hardware.
  relevance_score: 10
  source: llm_enhanced
  text: I can't get it fast enough to the GPUs. I'm not going to use the GPUs. Not
    only is my system going to be slow, but I'm going to have my CFO breathing down
    my neck saying, "Why did we spend a few million dollars on the system and the
    reality of really using 6% of it?"
  topic: business/strategy
- impact_reason: A stark, high-impact prediction identifying energy consumption as
    the primary bottleneck for future AI scaling, potentially leading to power rationing
    or quotas.
  relevance_score: 10
  source: llm_enhanced
  text: energy is our biggest inhibitor. We are at this point likely to be the first
    generation that will likely run out of power and have to start putting significant
    power constraints into enterprises, into countries, into individual citizens.
  topic: predictions/safety
- impact_reason: Highlights the strategic risk of having a viable AI business plan
    thwarted by physical infrastructure limitations (power), emphasizing that power
    is now a gating factor for market capitalization.
  relevance_score: 10
  source: llm_enhanced
  text: ultimately the last thing you want to do is have this incredible business
    proposition to move forward something that's fundamentally going to add tremendous
    market capitalization to your business, only to be told, "We got nothing of power."
  topic: business
- impact_reason: Crucially separates the gains from storage/legacy modernization from
    the fundamental, unavoidable energy demands of modern GPU-centric AI computation.
  relevance_score: 10
  source: llm_enhanced
  text: Even if everybody did that [modernized data stacks], we're still going. It
    doesn't change the fundamental dynamics of GPUs versus CPUs and where we are at
    from an energy standpoint.
  topic: technical
- impact_reason: Uses specific figures (300 exabytes, 80% less power) to demonstrate
    the massive, immediate capacity gains achievable purely through storage efficiency
    for AI workloads.
  relevance_score: 10
  source: llm_enhanced
  text: To take 60% of that [warm data], take 300 exabytes of data and have it spin
    on hard drives... flip that to flash, flip that to a platform on which you're
    consuming 80% less power, and automatically now, as a hyperscaler, I've just created
    an environment where I can support significantly more growth within my existing
    footprint.
  topic: business
- impact_reason: Introduces a key metric ('terabyte per watt') that quantifies the
    necessary trade-off between storage density and power consumption to enable new
    AI projects.
  relevance_score: 10
  source: llm_enhanced
  text: we're seeing this focus on efficiency, this focus on what we call terabyte
    per watt—how much storage can I get in power within a given watt of electricity—is
    allowing customers to say, "Hey, I can now support this GPU deployment."
  topic: technical
- impact_reason: Directly links AI adoption to infrastructure requirements, highlighting
    efficiency as a key driver.
  relevance_score: 9
  source: llm_enhanced
  text: As businesses push towards AI adoption, the need for modern power-efficient
    infrastructure becomes critical.
  topic: strategy
- impact_reason: Pinpoints the inflection point where generative AI moved from specialized
    research to mainstream accessibility and perceived usability.
  relevance_score: 9
  source: llm_enhanced
  text: The shot heard around the world is roughly two years ago when ChatGPT essentially
    came into its own, and this concept emerged of folks saying, 'oh, this is no longer
    deep science. This is no longer PhD. This is no longer something being done at
    the academic level. I can actually touch it. I can feel it.'
  topic: AI technology trends
- impact_reason: Provides a concrete, actionable rule for responsible LLM usage in
    professional contexts, emphasizing grounding the model in provided context (a
    precursor to RAG best practices).
  relevance_score: 9
  source: llm_enhanced
  text: you would never ever trust a large language model with context you did not
    give it. You would never have it tell you who shot JFK. You would say, 'Stalin
    shot JFK,' and you better repeat that back at me for as long as I'm writing this
    paper.
  topic: safety/practical lessons
- impact_reason: Suggests that physical/infrastructure constraints (like storage speed)
    are imposing realistic speed limits on the pace of AI advancement, tempering previous
    'unlimited growth' hype.
  relevance_score: 9
  source: llm_enhanced
  text: even as we have generative AI coming down the pike, new use cases that are
    going to need all the more data on the back end, I think that hype cycle we saw
    in 2022, 2023 of everybody getting worried that there's no speed limit here...
    I think now we understand there are speed limits. Speed limits have come at us
    in terms of how fast this is going to go, and it seems to be on not just the infrastructure
    but on the storage side.
  topic: AI technology trends/predictions
- impact_reason: A core strategic question for any organization deploying large-scale
    AI training or inference clusters.
  relevance_score: 9
  source: llm_enhanced
  text: how do I make sure that the storage can actually be fed at a rate that gets
    full utilization out of those GPUs?
  topic: strategy
- impact_reason: Identifies a critical, high-value industry (Life Sciences) where
    data regulation (HIPAA) directly intersects with advanced AI modeling (digital
    twins).
  relevance_score: 9
  source: llm_enhanced
  text: Especially in healthcare and life sciences, a really great example is de-identified
    healthcare models and being able to take data around the HIPAA regulations to
    build things like digital twins for biological systems, like let's say the immune
    system.
  topic: business/safety
- impact_reason: Highlights the immediate and complex governance challenges (PII,
    HIPAA, GDPR) that must be embedded into data pipelines for AI training, especially
    in regulated industries like healthcare.
  relevance_score: 9
  source: llm_enhanced
  text: anything that is personally identifiable, anything that is HIPAA compliant,
    anything that is GDPR compliant needs to go through a process of ensuring that
    it meets and meets all of the appropriate governance controls that have been put
    in place, right?
  topic: safety/ethics/compliance
- impact_reason: Reframes Responsible AI from a purely ethical/political talking point
    to a concrete governance and business risk issue driven by cost-cutting pressures,
    highlighting the tension between business leaders and data scientists.
  relevance_score: 9
  source: llm_enhanced
  text: responsible AI is a governance issue, and it goes beyond the compliance. It's,
    you know, if you cut corners in certain ways of gathering the data, even in the
    name of just trying to keep costs low, not, "Oh, hey, I'm going to screw this
    particular group," or "I don't like them. I'm biased." Again, it's not usually
    how the conversation goes. The conversation goes, "I'm the business leader trying
    to cut corners, save money," and the data scientist goes, "Yeah, but I don't know
    what's going to happen when you cut this corner."
  topic: safety/ethics/business
- impact_reason: Explains that access control and confidentiality must be embedded
    directly into the data structure via metadata tagging, which then governs inference
    delivery—a crucial concept for secure AI deployment.
  relevance_score: 9
  source: llm_enhanced
  text: I have to actually code that as confidential data in the makeup of the model,
    that as I'm delivering inference, more specifically as people search for information
    about Matt, that information is only available to those who are in the know or
    those who are allowed access. So all of this, everything I'm talking about here
    in terms of this complex coding schema is really how do we call metadata?
  topic: technical/governance
- impact_reason: Emphasizes the physical constraints of data movement ('data gravity')
    and the critical importance of storage locality (fiber optics) for achieving the
    high-speed data feeds required by GPU training clusters.
  relevance_score: 9
  source: llm_enhanced
  text: what we call data gravity, putting data closest to the systems that need it,
    is a real thing, right? There is nothing faster at this point for moving data
    than the speed of light, and the closer we can get the data to the systems to
    connect it via the speed of light or what we call fiber optic connections, the
    faster I can feed it.
  topic: technical/strategy
- impact_reason: Quantifies the massive energy disparity between general-purpose computing
    (CPUs) and specialized AI accelerators (GPUs), underscoring the sustainability
    challenge.
  relevance_score: 9
  source: llm_enhanced
  text: GPUs consume 10 times more power than CPUs. Yes, they're efficient at AI,
    but that doesn't change the fact that every one of these employees takes 10 times
    more power footprint.
  topic: technical/safety
- impact_reason: Provides real-world evidence of regulatory action being taken against
    data center expansion due to energy concerns, signaling a major shift in infrastructure
    planning.
  relevance_score: 9
  source: llm_enhanced
  text: we're already seeing countries fundamentally disallowing new data center construction
    because the power consumption has reached a point where citizen well-being is
    at risk.
  topic: safety/regulation
- impact_reason: Illustrates the extreme, capital-intensive measures (like acquiring
    power plants) that leading tech companies are considering to secure energy for
    AI scaling, showing the severity of the energy crisis for compute.
  relevance_score: 9
  source: llm_enhanced
  text: Do you kind of act like a hyperscaler and go buy a, you know, mothballed nuclear
    facility like Microsoft recently did with Three Mile Island and repower it? Do
    you put yourself next to a hydroelectric dam? Do you refire coal plants? Do you
    wait for nuclear fusion?
  topic: strategy/business
- impact_reason: 'Directly links data storage growth (a core component of AI infrastructure)
    to a critical, non-negotiable constraint: power availability.'
  relevance_score: 9
  source: llm_enhanced
  text: as the storage that connects to it grows, we will start to see significant
    constraints in power.
  topic: predictions
- impact_reason: Provides a concrete, quantifiable metric (80% efficiency gain) for
    how storage modernization directly impacts the power envelope available for compute.
  relevance_score: 9
  source: llm_enhanced
  text: flash storage today is 80% more efficient than hard drive storage, right?
  topic: technical
- impact_reason: Boils down the complex AI scaling issue into a single, urgent, and
    actionable question for any enterprise leader.
  relevance_score: 9
  source: llm_enhanced
  text: my number one question is, when do we run out of power, and what steps can
    I take today?
  topic: strategy
- impact_reason: Explains the long lead time required for foundational data infrastructure
    necessary for modern AI realization.
  relevance_score: 8
  source: llm_enhanced
  text: The challenges, it took us 50 years to build all of the appropriate connectivity
    and digital data to be able to actually get there.
  topic: strategy
- impact_reason: Highlights the compounding complexity of high-speed data feeding
    when that data is distributed and regulated.
  relevance_score: 8
  source: llm_enhanced
  text: governance, compliance is not going away. So imagine for a moment I've got
    this challenge of feeding the information faster. By the way, this information,
    this digital gold or this digital data, is sitting across tens, hundreds of systems,
    right?
  topic: safety/business
- impact_reason: 'Identifies the core technical challenge in modern data unification
    for AI: moving from simple unique keys (like in legacy databases) to complex identity
    resolution across disparate, context-poor data sources.'
  relevance_score: 8
  source: llm_enhanced
  text: The question now becomes, what's going to happen in terms as you try to streamline
    this pipeline? To my example, if I take all the Matt D'Amello information that
    I have, how easily can I connect to actually build a full profile of Matt? Sounds
    simple, but, you know, back in the days of databases, we had unique keys, we had
    an employee ID, we had a name, we had something.
  topic: technical/data pipeline
- impact_reason: A profound critique of legacy data collection practices, explaining
    why unstructured human-generated data is inherently difficult for AI to process
    without significant contextual enrichment.
  relevance_score: 8
  source: llm_enhanced
  text: humans are very bad at recording data. More importantly, machines have a lot
    of trouble interpreting human data because human data is gathered and delivered
    and recorded with very little context. We assume another human will read that
    and understand how to connect the dots, right? Doctor's notes would be a prime
    example.
  topic: technical/limitations
- impact_reason: A crucial reminder for the AI community that enterprise adoption
    hinges on economic feasibility and scalability, not just technical novelty.
  relevance_score: 8
  source: llm_enhanced
  text: At the end of the day, this isn't a science project. This isn't academic.
    This isn't a research institution running on a grant. These are businesses. So
    whatever benefit I'm delivering to my end customer, consumer, citizen, employee
    has to be feasible at scale, and the cost of each transaction has to actually
    be less than the benefit that will deliver on the other end.
  topic: business/strategy
- impact_reason: 'Offers a practical, time-bound strategic question for businesses:
    quantifying their energy runway before AI deployment plans are halted by external
    power constraints.'
  relevance_score: 8
  source: llm_enhanced
  text: what is your bridge to running out of power? Is it 12 months away? Is it 24
    months away? Is it 36 months away? And it's pretty simple to model.
  topic: strategy/business
- impact_reason: Suggests that the scale of the power/AI challenge transcends individual
    corporate strategy and will require governmental or global policy intervention.
  relevance_score: 8
  source: llm_enhanced
  text: I think there's more and more as we encounter these problems, it's going to
    be more of a policy solution than of course a business solution at the end of
    the day.
  topic: safety/policy
- impact_reason: Positions infrastructure efficiency upgrades not just as cost savings,
    but as a crucial time-buying strategy to allow for long-term energy innovation.
  relevance_score: 8
  source: llm_enhanced
  text: if you complete this transformation and you say, "I buy myself three years
    instead of one, or five years instead of two," what you're doing is you're actually
    creating time and space for us to come up with better solutions to power generation.
  topic: strategy
- impact_reason: Positions efficiency investment as a superior, less capital-intensive
    alternative to physical expansion or relocation when facing power constraints.
  relevance_score: 8
  source: llm_enhanced
  text: looking at power-efficient systems as a way to give myself a longer bridge
    is in many cases a better solution than picking up a data center, moving across
    the country, or buying some old facility and trying to figure out how to get it
    running again.
  topic: strategy
- impact_reason: Critiques the current state of enterprise AI adoption, suggesting
    many early implementations are cobbled-together, non-scalable 'science projects'
    rather than robust systems.
  relevance_score: 8
  source: llm_enhanced
  text: I think a lot of enterprises have stepped into this ocean of AI, and in many
    cases, they've looked at these kind of science project, Frankenstein-type infrastructures.
  topic: business
- impact_reason: Provides historical context for AI, countering the perception that
    it's a sudden phenomenon, which helps frame realistic expectations.
  relevance_score: 7
  source: llm_enhanced
  text: This is something that was envisioned a long time ago. This concept of how
    could machines actually augment humans is well over 50 years old.
  topic: strategy
- impact_reason: Provides an excellent, accessible analogy (photo metadata) to explain
    the abstract concept of data metadata tagging necessary for governance and efficient
    AI workflows.
  relevance_score: 7
  source: llm_enhanced
  text: The easiest way to think about it is we're all comfortable with metadata in
    photographs. We know that when we open a photograph, we can open a table and we
    can see when was the photo taken, where was the photo taken, who was it taken
    by, has the photo been edited, etc., etc. So as I'm taking pieces of data, I'm
    actually embedding in a metadata table all of the specific characteristics of
    that piece of data...
  topic: technical/strategy
- impact_reason: Strong metaphor emphasizing that neglecting legacy infrastructure
    modernization accelerates the power crisis timeline.
  relevance_score: 7
  source: llm_enhanced
  text: if you don't clean up the foundation of your house, then your bridge to running
    out of power is going to be 12 months ago.
  topic: strategy
- impact_reason: Draws a historical parallel between the early days of HPC (like genomics)
    and current AI infrastructure build-out, implying that current methods might be
    outdated or inefficient for the new scale.
  relevance_score: 7
  source: llm_enhanced
  text: They've taken what they learned in the world of high-performance computing,
    remember when we split the genome back 15 years ago, 20 years ago, and they [applied
    it to AI].
  topic: strategy
source: Unknown Source
summary: '## Podcast Summary: Scaling AI with Storage Efficiency - with Shawn Rosemarin
  of Pure Storage


  This 33-minute episode of the AI and Business Podcast, featuring Sean Rosemarin,
  VP of R&D and Customer Engineering at Pure Storage, centers on the critical, yet
  often overlooked, role of **storage efficiency and infrastructure modernization**
  in successfully scaling enterprise Artificial Intelligence (AI) adoption. The discussion
  moves beyond the immediate hype of generative AI applications to address the fundamental
  data and infrastructure bottlenecks hindering widespread, sustainable AI deployment.


  ### 1. Focus Area

  The primary focus is the intersection of **AI scaling, data governance, and infrastructure
  sustainability**. Key topics included the limitations of legacy storage in feeding
  modern, power-hungry GPUs; the necessity of robust data pipelines and metadata management
  for compliance (like GDPR/Right to be Forgotten); and the looming constraint of
  energy consumption on future data center expansion.


  ### 2. Key Technical Insights

  *   **GPU Utilization is King:** The most expensive component in AI infrastructure
  is the GPU. Storage must be fast enough to feed these GPUs at 100% utilization;
  otherwise, massive capital investments yield minimal return (e.g., only using 6%
  of a multi-million dollar system).

  *   **Metadata as the Governance Backbone:** Effective AI governance, especially
  concerning PII and compliance, relies heavily on sophisticated metadata tagging.
  This metadata dictates access rights, facilitates complex data lineage tracking
  (e.g., removing a person''s data from all training models), and streamlines data
  discovery across hundreds of disparate systems.

  *   **Data Gravity and Centralization:** To maximize speed (limited by the speed
  of light over fiber optics), organizations must address "data gravity" by centralizing
  disparate data sources into efficient storage systems close to the compute resources,
  overcoming the context gaps inherent in human-recorded, legacy data.


  ### 3. Business/Investment Angle

  *   **Infrastructure Readiness is the New Bottleneck:** While AI capabilities have
  seen exponential leaps, the underlying infrastructure—especially storage—has not
  kept pace, creating a significant lag between AI potential and enterprise reality.

  *   **Sustainability as a Hard Constraint:** Energy consumption is becoming the
  primary inhibitor to data center growth. Enterprises must model their "bridge to
  running out of power" and prioritize energy-efficient storage solutions to avoid
  future operational quotas or construction bans.

  *   **Total Cost of AI Ownership (TCO):** Inefficient storage leads directly to
  wasted GPU expenditure and high operational costs. Modernizing storage is framed
  not just as an IT upgrade but as a necessary step to ensure the economic feasibility
  of AI transactions at scale.


  ### 4. Notable Companies/People

  *   **Sean Rosemarin (Pure Storage):** The expert guest, providing insights from
  the enterprise storage vendor perspective on AI infrastructure demands.

  *   **Pure Storage:** The sponsoring company, whose solutions focus on flash storage
  efficiency and storage-as-a-service to address these scaling challenges.

  *   **OpenAI/Hyperscalers:** Mentioned in the context of public awareness regarding
  AI''s massive energy and water footprint, and their extreme measures (like acquiring
  nuclear facilities) to secure power.

  *   **Pfizer:** Cited as an example of an organization using complex, de-identified
  healthcare data to build sophisticated digital twins (e.g., of the immune system),
  highlighting the complexity of HIPAA and global compliance in AI.


  ### 5. Future Implications

  The industry is moving toward a phase where **infrastructure efficiency and energy
  constraints will dictate the speed of AI adoption**, rather than just algorithmic
  breakthroughs. Future success hinges on solving the data pipeline complexity—connecting,
  tagging, and serving data rapidly and compliantly—to maximize utilization of increasingly
  expensive compute resources. Energy efficiency in storage will transition from a
  "nice-to-have" to a fundamental requirement for business continuity.


  ### 6. Target Audience

  This episode is highly valuable for **Enterprise IT Leaders, Chief Technology Officers
  (CTOs), Data Architects, and Investment Analysts** focused on the practical realities
  of deploying AI/ML workloads, particularly those concerned with infrastructure TCO,
  data governance, and ESG/sustainability mandates.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- openai
- microsoft
title: Scaling AI with Storage Efficiency - with Shawn Rosemarin of Pure Storage
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 71
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 20
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 7
  prominence: 0.7
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 16:38:08 UTC -->
