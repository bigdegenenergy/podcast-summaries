---
companies:
- category: unknown
  confidence: medium
  context: of my favorite conversations, for sure. Hey, I'm Guillain Valdon. Growing
    up, really, I was pursuing theories of e
  name: Guillain Valdon
  position: 122
- category: unknown
  confidence: medium
  context: erstand the universe. I was a big fan of Feynman, Stephen Hawking growing
    up, and I was seven. I was talking about
  name: Stephen Hawking
  position: 261
- category: unknown
  confidence: medium
  context: of thought that was a generalization of Wheeler's It From Bit, which was
    the It From Qubit program, seeking to
  name: It From Bit
  position: 473
- category: unknown
  confidence: medium
  context: alization of Wheeler's It From Bit, which was the It From Qubit program,
    seeking to unify theoretical physics thr
  name: It From Qubit
  position: 500
- category: tech
  confidence: high
  context: g neurotransmitters. This podcast is supported by Google. Hi folks, Paige
    Bailey here from the Google Deep
  name: Google
  position: 1372
- category: unknown
  confidence: medium
  context: s. This podcast is supported by Google. Hi folks, Paige Bailey here from
    the Google DeepMind DevRel team. For ou
  name: Paige Bailey
  position: 1390
- category: unknown
  confidence: medium
  context: d by Google. Hi folks, Paige Bailey here from the Google DeepMind DevRel
    team. For our developers out there, we know there
  name: Google DeepMind DevRel
  position: 1417
- category: unknown
  confidence: medium
  context: antum computing and machine learning at Alphabet. And I also happen to
    be the founder of a philosophical
  name: And I
  position: 2190
- category: unknown
  confidence: medium
  context: ed effective accelerationism under the pseudonym, Beth J. Zos, online.
    Happy to be here at MLST. Hello eve
  name: Beth J
  position: 2308
- category: unknown
  confidence: medium
  context: py to be here at MLST. Hello everyone. Welcome to Machine Learning Street
    Talk. I'm your host for today, Maxel Ramsted. Very exc
  name: Machine Learning Street Talk
  position: 2382
- category: unknown
  confidence: medium
  context: ne Learning Street Talk. I'm your host for today, Maxel Ramsted. Very excited
    to be here. I'm studying in for Tim
  name: Maxel Ramsted
  position: 2437
- category: unknown
  confidence: medium
  context: ted. Very excited to be here. I'm studying in for Tim Scarf, who unfortunately
    is down with a little bit of C
  name: Tim Scarf
  position: 2497
- category: unknown
  confidence: medium
  context: unfortunately is down with a little bit of COVID. But I'm sure we'll have
    a really interesting conversati
  name: But I
  position: 2562
- category: unknown
  confidence: medium
  context: ergrad at McGill and then I went to Waterloo, the Perimeter Institute for
    Theoretical Physics. Met some of the greatest
  name: Perimeter Institute
  position: 4650
- category: unknown
  confidence: medium
  context: n I went to Waterloo, the Perimeter Institute for Theoretical Physics.
    Met some of the greatest minds on Earth. They're
  name: Theoretical Physics
  position: 4674
- category: unknown
  confidence: medium
  context: iversity of Waterloo, which was the Institute for Quantum Computing. Right?
    And there is very interesting because we
  name: Quantum Computing
  position: 9977
- category: unknown
  confidence: medium
  context: to go work at Google and build a product known as TensorFlow Quantum, which
    was product focused on creating software t
  name: TensorFlow Quantum
  position: 11094
- category: unknown
  confidence: medium
  context: igital emulations of physics-inspired algorithms. So I would say that's
    physics-inspired. I would say a
  name: So I
  position: 13471
- category: tech
  confidence: high
  context: he room is at room temperature, because that, the gradient of temperatures
    is, is, is very difficult to, to
  name: Gradient
  position: 17568
- category: unknown
  confidence: medium
  context: e are still trying to hang on for control, right? With ML interpretability
    research and all this AI safety
  name: With ML
  position: 20852
- category: unknown
  confidence: medium
  context: things for a technical audience, I guess, they're Markov Chain Monte Carlo
    accelerators, right? And we support discrete vari
  name: Markov Chain Monte Carlo
  position: 22988
- category: unknown
  confidence: medium
  context: compute, right, thinking at test time, you know, Monte Carlo tree search
    and, you know, all sorts of RL rollou
  name: Monte Carlo
  position: 24964
- category: unknown
  confidence: medium
  context: ked on the hardware we had at the time, which was Google TPUs. And, you
    know, the hope is that, you know, there
  name: Google TPUs
  position: 26586
- category: unknown
  confidence: medium
  context: it. And, and, you know, I think that should win a Nobel Prize or something
    someday. But that that's basically h
  name: Nobel Prize
  position: 31707
- category: tech
  confidence: high
  context: the point there is that a Hamiltonian gives you a notion of energy, right,
    in the classical regime. And so
  name: Notion
  position: 31909
- category: unknown
  confidence: medium
  context: amics more generally diffusion in that landscape. And Langevin dynamics
    is happens to also be an MCMC algorithm,
  name: And Langevin
  position: 32875
- category: unknown
  confidence: medium
  context: led a team. I led quantum machine learning at an Alphabet X. And, and then
    I quit all that. I quit quantum co
  name: Alphabet X
  position: 35652
- category: unknown
  confidence: medium
  context: f you, if you're, I think there is some work from Max Tegmark on correspondences
    between sort of the informatio
  name: Max Tegmark
  position: 37410
- category: unknown
  confidence: medium
  context: rok model, you know, we'd run out of power in the United States. You can't
    deploy it to everyone can have everyon
  name: United States
  position: 45458
- category: tech
  confidence: high
  context: and very high quantum complexity. All all systems cohere. We have, you
    know, multiple billion the many com
  name: Cohere
  position: 48015
- category: tech
  confidence: high
  context: e idea. So, yeah, I mean, really, e/ac is sort of meta-culture. It's kind
    of I call it like a cultural h
  name: Meta
  position: 52913
- category: unknown
  confidence: medium
  context: duction and consumption, aka the Kardashev scale. The Kardashev scale is
    a log scale tracking how much free energ
  name: The Kardashev
  position: 53159
- category: unknown
  confidence: medium
  context: this a lot. Uh, my, my good friend and colleague, Axel Constant, actually,
    uh, and I have a longstanding debate a
  name: Axel Constant
  position: 57491
- category: big_tech
  confidence: high
  context: The podcast is supported by Google. Mentioned in relation to DeepMind DevRel
    team and the release of Gemini 2.5 Flash.
  name: Google
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Paige Bailey is mentioned as being from the Google DeepMind DevRel team.
  name: Google DeepMind
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A specific AI model released by Google, discussed for its speed and reasoning
    power.
  name: Gemini 2.5 Flash
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Guillain Valdon's current company, pioneering thermodynamic computing for
    probabilistic inference.
  name: Extropic
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Guillain Valdon formerly worked on quantum computing and machine learning
    at Alphabet.
  name: Alphabet
  source: llm_enhanced
- category: media/community
  confidence: high
  context: The podcast platform/show hosting the conversation.
  name: MLST (Machine Learning Street Talk)
  source: llm_enhanced
- category: research_figure
  confidence: high
  context: Mentioned as a childhood hero of Guillain Valdon, relevant to theoretical
    physics background.
  name: Feynman
  source: llm_enhanced
- category: research_figure
  confidence: high
  context: Mentioned as a childhood hero of Guillain Valdon, relevant to theoretical
    physics background.
  name: Stephen Hawking
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Where Guillain Valdon studied/worked during his theoretical physics career.
  name: Perimeter Institute for Theoretical Physics
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Where Guillain Valdon worked and became interested in quantum systems and
    AI.
  name: Institute for Quantum Computing (University of Waterloo)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A product Guillain Valdon helped build at Google, focused on learning quantum
    mechanical representations.
  name: TensorFlow Quantum
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned as one of the first startups involved in quantum computing/programming
    (likely referring to Rigetti Computing).
  name: Reggetti
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Google's Tensor Processing Units, the hardware platform that dictated early
    Transformer model design choices.
  name: TPUs
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The dominant neural network architecture discussed, implying the ecosystem
    of companies building and using them.
  name: Transformer
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The speaker quit their role leading quantum machine learning at this division
    of Alphabet.
  name: Alphabet X
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced as an example of a large, agentic AI model whose scaling would
    lead to power shortages.
  name: ChatGPT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced alongside ChatGPT as an example of a large, agentic AI model
    whose scaling would strain power resources.
  name: Grok
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a hypothetical future technology where the skull becomes an
    extension of cognition via a thermodynamic computer.
  name: Neuralink
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Referenced indirectly via the mention of 'ChatGPT,' which is developed
    by OpenAI.
  name: OpenAI (Implied)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: low
  context: The speaker discusses a current wafer-scale system running on high power
    (20kW+), contrasting it with their low-power goal, implying existing hardware
    manufacturers/AI infrastructure providers.
  name: Wafer-scale system developers (Implied)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The speaker describes the work of their own organization (likely Extropic)
    which is building hardware based on probabilistic approximate inference.
  name: Thermodynamic Computer Developers (Implied)
  source: llm_enhanced
- category: big_tech
  confidence: low
  context: Mentioned generally when discussing that their work is not just for the
    current wave of AI, implying current leaders like Google, Meta, or OpenAI.
  name: Current AI Wave Leaders (Implied)
  source: llm_enhanced
- category: ai_startup
  confidence: medium
  context: Mentioned generally in the context of needing to 'fuck around and find
    out' (FaFO algorithm) to learn what is optimal in a complex market.
  name: Startups
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned in the context of over-regulation serving existing large AI power
    holders.
  name: Incumbents
  source: llm_enhanced
date: 2025-07-21 20:07:52 +0000
duration: 84
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: also literally sort of let go, loosen our grip on electrons and hardware,
    and that's what we do, right? Like essentially we go from having a very tight
    grip and yanking these signals around to kind of loosening our grip, letting there
    be fuzz and kind of gently guiding the signals, right? So in some sense you're
    kind of switching teams, right? So you know, in the more classical kind of way
    of thinking of this, what we're trying to do is to keep the noise at bay, right?
    To make our systems as non-stochastic as possible
  text: we should also literally sort of let go, loosen our grip on electrons and
    hardware, and that's what we do, right? Like essentially we go from having a very
    tight grip and yanking these signals around to kind of loosening our grip, letting
    there be fuzz and kind of gently guiding the signals, right? So in some sense
    you're kind of switching teams, right? So you know, in the more classical kind
    of way of thinking of this, what we're trying to do is to keep the noise at bay,
    right? To make our systems as non-stochastic as possible.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://anchor.fm/s/1e4a0eac/podcast/play/105799998/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-6-21%2F404299996-44100-2-abef86652c6ad.mp3
processing_date: 2025-10-05 00:42:55 +0000
quotes:
- length: 107
  relevance_score: 5
  text: And in startups, you kind of learn this, the, you know, you have to actually
    go on the market and try stuff
  topics:
  - market
- length: 137
  relevance_score: 4
  text: There's kind of the new additions to machine learning that don't learn the
    fundamentals, just go straight to transformers, whatever's hot
  topics: []
- length: 59
  relevance_score: 3
  text: And that was actually my entry into artificial intelligence
  topics: []
- length: 129
  relevance_score: 3
  text: So algorithms, the biggest workloads that are eating the world and consuming
    a ton of energy are actually probabilistic workloads
  topics: []
- length: 79
  relevance_score: 3
  text: They were what worked on the hardware we had at the time, which was Google
    TPUs
  topics: []
- length: 222
  relevance_score: 3
  text: You know, initially when I left theoretical physics and went all in on quantum
    machine learning, everybody told me like, really, you, you're going to be a quantum
    machine learning lead at Google and everybody's doubting me
  topics: []
- length: 115
  relevance_score: 3
  text: But if you're a startup and you have to to scale or die, eat the world or
    die, you have to kill every risk possible
  topics: []
- length: 173
  relevance_score: 3
  text: And so and so people thinking we're just going to scale transformers and get
    to the moon and scale scale intelligence to the whole planet, they're just flat
    out wrong, right
  topics: []
- impact_reason: The core mission statement for the speaker's company, defining 'thermodynamic
    computing' as a novel hardware approach for probabilistic inference.
  relevance_score: 10
  source: llm_enhanced
  text: I'm the founder of Extropic, a company pioneering thermodynamic computing,
    a new form of computing for probabilistic inference using exotic stochastic physics
    of electrons.
  topic: technical/strategy
- impact_reason: Draws a direct parallel between the opacity of advanced physical
    models (like tensor networks) and modern deep learning systems, acknowledging
    the trade-off between complexity and interpretability.
  relevance_score: 10
  source: llm_enhanced
  text: The model is no longer interpretable, right? If you have a... if you have
    something like a deep learning system... they're just as opaque as the system
    that I was trying to predict...
  topic: AI technology trends/safety
- impact_reason: Articulates the necessity of hardware matching the model architecture
    (physics-informed models require physics-based accelerators), leading to the speaker's
    current work.
  relevance_score: 10
  source: llm_enhanced
  text: The dual to that is if I want to run and learn and train these representations
    that are physics-inspired or physics-based, then I need a physics-based accelerator,
    right?
  topic: technical/strategy
- impact_reason: Crucially differentiates quantum computing (proven complexity advantage)
    from stochastic computing (practical, but not proven, intractability of emulation),
    guiding expectations for different hardware types.
  relevance_score: 10
  source: llm_enhanced
  text: in the case of quantum mechanical computer, there's proven separations of
    complexity there, and there's experiments showing that you can't emulate them
    at scale. In our case, with stochastic computers, there's no complexity class
    separation. It's just kind of just orders of magnitude speed up or energy efficiency
    gain, which practically makes them intractable to emulate, you know, at scale.
  topic: technical
- impact_reason: 'Reveals a major practical limitation and energy sink in fault-tolerant
    quantum computing: the massive overhead required for error correction (algorithmic
    refrigeration).'
  relevance_score: 10
  source: llm_enhanced
  text: to have a quantum mechanical computer, you try to keep the computer at perfect
    zero temperature, right? zero entropy. So you're constantly pumping out entropy
    that's, that is seeping into the system through noise, right? And that's quantum
    error correction and, and fault tolerance. And it turns out that most of your
    computation, most of your, um, energy is going to be sunk into that pumping.
  topic: technical
- impact_reason: A strong critique of the push for absolute interpretability in AI
    safety, arguing it mirrors a futile attempt to maintain control against the power
    of emergent systems like deep learning.
  relevance_score: 10
  source: llm_enhanced
  text: We have two sort of, you know, just like we did with deep learning, we let
    go of software 1.0 and period of programming and let gradient descent be a better
    programmer than you, and that was very humbling for some. Some are still trying
    to hang on for control, right? With ML interpretability research and all this
    AI safety research and they want to still want to feel like they're in control
    or that they understand the complex system instead of kind of letting it do its
    thing and letting it figure out what's best, right?
  topic: safety/strategy
- impact_reason: Connects fundamental thermodynamics (Maxwell's Demon) directly to
    the energy cost of classical/quantum determinism, justifying the shift toward
    harnessing noise.
  relevance_score: 10
  source: llm_enhanced
  text: Maxwell's demon shows you that knowledge comes at a cost, right? Like reducing,
    reducing entropy in a system, keeping something in a deterministic state always
    costs you energy. And so at every clock cycle, we're trying to, you know, prevent
    the classical or quantum computer from decaying to a naturally probabilistic and
    relaxed, thermo closer to thermal state, then we have to pay the price. We have
    to pay energy to maintain determinism and reduce entropy.
  topic: technical/strategy
- impact_reason: 'Highlights the fundamental inefficiency/paradox in modern AI stacks:
    building deterministic hardware only to run inherently stochastic software (like
    MCMC/sampling) on top of it.'
  relevance_score: 10
  source: llm_enhanced
  text: as you just described it, we spent an inordinate amounts of, you know, energy
    and effort pumping the noise out of the system. But then, you know, with sampling-based
    methods, like we then reintroduce it through the software.
  topic: business/strategy
- impact_reason: Identifies the primary energy bottleneck in modern computing (AI/ML)
    as stemming from probabilistic algorithms, setting the stage for hardware specialization.
  relevance_score: 10
  source: llm_enhanced
  text: The biggest workloads that are eating the world and consuming a ton of energy
    are actually probabilistic workloads. They are probabilistic graphical models.
  topic: business/predictions
- impact_reason: Describes the potential for hardware innovation to fundamentally
    alter the algorithmic landscape, leading to rapid, disruptive model evolution
    (a 'Cambrian explosion' in AI algorithms).
  relevance_score: 10
  source: llm_enhanced
  text: if you change that hardware substrate to a different substrate that has, you
    know, different preferences in terms of the structure of the algorithm that you're
    running, then you're changing the fitness landscape. And if you have a sudden
    shift in the fitness landscape, you have a sort of Cambrian explosion, you have
    this sort of high-temperature phase of search, right?
  topic: predictions
- impact_reason: 'The core technical pitch: their hardware natively implements Langevin
    dynamics/diffusion, which perfectly aligns with MCMC algorithms used in Bayesian
    inference, making it a ''literal analogy'' analog computer.'
  relevance_score: 10
  source: llm_enhanced
  text: you could view our chips as a time-dependent programmable energy function,
    right? And you have something akin to Langevin dynamics more generally diffusion
    in that landscape. And Langevin dynamics is happens to also be an MCMC algorithm,
    right? So there's a, right. There's a perfect match there between the algorithm
    that you can use for Bayesian inference for anything really and the native physics
    of the chip, right?
  topic: technical
- impact_reason: Provides a concrete, highly impressive energy efficiency metric for
    their fundamental unit (P-bit), demonstrating massive gains over traditional digital
    computation.
  relevance_score: 10
  source: llm_enhanced
  text: our P bits can generate bits of controllable bits of entropy with only a few
    hundreds of attojoules.
  topic: technical
- impact_reason: Provides a theoretical model for a multi-layered, multi-paradigm
    AI architecture where the required computational complexity (quantum, probabilistic,
    classical) changes as information is distilled.
  relevance_score: 10
  source: llm_enhanced
  text: Then you can imagine the first few layers are quantum and then some layers
    are probabilistic and later they're deterministic just like as you distill distill
    the information. You don't need that sort of you know initially initially you
    need some quantum complexity, later you need some entropy, and then later you
    just need to do some classical repart, you know, coordinate transformations, right?
    And they all work together.
  topic: AI theory/technical
- impact_reason: Posits that efficient probabilistic hardware (like thermodynamic
    compute) is the missing link required to finally realize the potential of EBMs,
    which are theoretically powerful but computationally expensive.
  relevance_score: 10
  source: llm_enhanced
  text: Would you say that this is really the, you know, the key to unlocking their
    potential [Energy-Based Models (EBMs)] because up until now they've been basically
    limited by their their inefficient sampling?
  topic: AI theory/technical
- impact_reason: 'This is the core motivation: current scaling methods are unsustainable
    due to power constraints, necessitating a fundamental hardware shift.'
  relevance_score: 10
  source: llm_enhanced
  text: we're stuck in a like a hardware stack that works in a certain way, we optimize
    our software for that, then we build bigger hardware and yeah, so this is how
    we break free from yeah, but I think I think we're I think we're going to break
    free one way or another because frankly even if you just do a back-of-the-envelope
    calculation, well right now we're going to run out of power trying to scale.
  topic: strategy/safety (Energy)
- impact_reason: A stark, near-term prediction about the energy bottleneck preventing
    the mass, high-intensity deployment of current large AI models.
  relevance_score: 10
  source: llm_enhanced
  text: if everybody were to use like an agentic, you know, big model, ChatGPT or
    Grok model, you know, we'd run out of power in the United States. You can't deploy
    it to everyone can have everyone using it like with high intensity right now.
    We're going to run out of power.
  topic: safety (Energy)/predictions
- impact_reason: Raises the existential threat of waste heat generation from massive
    computation, arguing that energy supply alone cannot solve the scaling problem;
    hardware efficiency is mandatory.
  relevance_score: 10
  source: llm_enhanced
  text: Even if we produce the power, let's say there was some moonshot and let's
    say we even figured out nuclear fusion, right? Let's say we just did that and
    we scaled that very quickly. We run out of or, you know, we're going to generate
    more, we're going to double or triple the amount of heat being radiated by the
    Earth, right? So we're going to literally cook ourselves to death, right? We're
    literally cooked.
  topic: safety (Existential)/predictions
- impact_reason: 'Quantifies the efficiency gap closure: moving from 100 million times
    less efficient than the brain to within 10x, showing the path toward biologically
    plausible energy consumption for large models.'
  relevance_score: 10
  source: llm_enhanced
  text: And then if you have a hundred or, you know, you have a few hundred billion
    parameter model running on 20 watts, then we're, you know, we're in the brain,
    we're in the same ballpark as the brain, not quite. It might be like within 10x,
    but that's still much better than where we are, which is like 100 million x, right?
  topic: technical/predictions
- impact_reason: A powerful critique of centralized AI access models, framing them
    as assimilation rather than true democratization, highlighting control risks.
  relevance_score: 10
  source: llm_enhanced
  text: democratizing access. It's like, hey, I give you access to the one God model
    that amortizes learning across the fleet. Now you're all mind-merged with my Borg
    mind. Congratulations, you've been assimilated. That's not democracy, right?
  topic: safety/ethics
- impact_reason: Pinpoints prompt control as a direct mechanism for subversive control
    over individuals using AI extensions of cognition.
  relevance_score: 10
  source: llm_enhanced
  text: If you control its constitutional prompts, you're essentially controlling
    people by proxy and their thoughts and their will and their actions, right?
  topic: safety/ethics
- impact_reason: Links the core goal of e/ac directly to thermodynamic principles
    (free energy maximization) and the Kardashev scale, providing a measurable metric
    for civilizational success.
  relevance_score: 10
  source: llm_enhanced
  text: we're trying to maximize growth of civilization as measured by our free energy
    production and consumption, aka the Kardashev scale.
  topic: strategy
- impact_reason: A profound statement framing information/bits as subject to thermodynamic
    selection pressure, generalizing Darwinian evolution to the informational substrate
    of reality.
  relevance_score: 10
  source: llm_enhanced
  text: Every bit of information specifying configurations of matter is fighting for
    its existence in the future. And this selection pressure on those bits of information
    or whether or not it, they can further host organisms, an ability to understand
    their environment, predict the future, capture free energy, use it strategically
    and grow.
  topic: technical/strategy
- impact_reason: 'Raises the critical counterpoint to unbounded growth: the analogy
    to cancer, suggesting that unconstrained free energy maximization can be self-destructive
    to the host system.'
  relevance_score: 10
  source: llm_enhanced
  text: My worry is that, you know, um, things that grow without boundary, without
    bounds, I mean, in the biological case, you have like cancer, for example.
  topic: safety/ethics
- impact_reason: Addresses the cancer analogy by arguing that true optimization requires
    strategic conservation and long-term planning (securing *more* energy), not immediate
    dissipation (blowing up the planet). This defines intelligent growth vs. destructive
    dissipation.
  relevance_score: 10
  source: llm_enhanced
  text: it's, it's, it's, it's basically infinite time horizon, right? And so actually
    blowing up the planet does, uh, you know, ex- you know, burn up a bunch of free
    energy. But in the long term, you actually, it's, it's, you know, the same reason
    life exists. Uh, it's, it's much better to sort of conserve and strategically
    use free energy to secure more free energy and, and keep growing rather than have
    some order rather than just like burn all and go and have chaos, right?
  topic: safety/strategy
- impact_reason: This candidly addresses the interpretability crisis in complex systems
    (like advanced AI), suggesting an empirical, trial-and-error approach ('FaFO algorithm')
    is sometimes necessary when theoretical understanding lags.
  relevance_score: 10
  source: llm_enhanced
  text: And, and it's, we don't have as much control and interpretability, we don't
    understand the world as well, right? We kind of have to, you know, pardon my French,
    but fuck around and find out, the FaFO algorithm, right?
  topic: safety/technical
- impact_reason: 'This is a concise, high-level explanation of the core mechanism
    of Active Inference: minimizing divergence between internal models and sensory
    data (predictive coding).'
  relevance_score: 10
  source: llm_enhanced
  text: in active inference, uh, you know, you have perception and action, right?
    You can update your, you know, based on your input, um, sensory information, you
    can update your model of the world. So you're minimizing divergence of your model
    of your, your own internal model to the statistics of the world.
  topic: technical
- impact_reason: This defines the action component of Active Inference—that action
    is taken to make the external world conform to the internal predictive model,
    a key concept for embodied AI and goal-seeking agents.
  relevance_score: 10
  source: llm_enhanced
  text: But then the dual of that is taking actions in the world to minimize divergence,
    uh, between the world and your predictive model of it.
  topic: technical
- impact_reason: 'This is a powerful warning about confirmation bias and goal-setting
    in complex systems: focusing attention on negative possibilities can inadvertently
    steer the system toward them.'
  relevance_score: 10
  source: llm_enhanced
  text: And if we, if we look at very negative outcomes and we're obsessed with them,
    we will drive whatever, whatever system we're thinking about towards those negative
    outcomes.
  topic: safety/strategy
- impact_reason: 'The optimistic corollary: positive focus in goal-setting and modeling
    steers complex systems toward beneficial outcomes, linking directly to the concept
    of ''e/ac'' (effective accelerationism/optimism).'
  relevance_score: 10
  source: llm_enhanced
  text: Right? And so, you know, to me, it's like, if we're optimistic about the future,
    we tend to steer things towards that optimistic outcome.
  topic: strategy
- impact_reason: Introduces the concept of the brain as a 'thermodynamic computer,'
    linking biological computation directly to physical/chemical processes, which
    is foundational to the speaker's company, Extropic.
  relevance_score: 9
  source: llm_enhanced
  text: That, I would argue, is a thermodynamic computer, right? Because there are
    master equations describing the chemical reaction networks in your brain of neurotransmitters
    hopping around.
  topic: technical/strategy
- impact_reason: Highlights a key feature in modern LLM deployment (Gemini 2.5 Flash)
    that allows developers to fine-tune the trade-off between computational depth
    (reasoning) and operational efficiency (latency/cost).
  relevance_score: 9
  source: llm_enhanced
  text: Crucially, we've added controls like setting thinking budgets, so you can
    decide how much reasoning to apply, optimizing for latency and cost.
  topic: technical/business
- impact_reason: A critical turning point in the speaker's career, identifying the
    limitation of traditional, overly simplistic physical models.
  relevance_score: 9
  source: llm_enhanced
  text: What I realized going through theoretical physics was that the reductionist
    approach to physics was failing us.
  topic: strategy/philosophy
- impact_reason: Highlights the fundamental challenge of complexity in physics—the
    failure of micro-level equations to predict macro-level emergent behavior, justifying
    the need for complex systems modeling.
  relevance_score: 9
  source: llm_enhanced
  text: The universe is very complex, you know, even if you... even beyond just quantum
    gravity, just looking at condensed matter systems, you can have equations describing
    the microscopics, but you can't predict the emergent properties.
  topic: technical/philosophy
- impact_reason: 'A moment of strategic pivot: shifting from being the discoverer
    of fundamental laws to being the builder of tools (AI/computers) that can process
    complexity beyond human capacity.'
  relevance_score: 9
  source: llm_enhanced
  text: '...maybe I can''t be the hero of the story. I won''t be the one to figure
    out, you know, the grand unifying theory of physics, but maybe I could build a
    computer or computer software that can understand the universe or chunks of the
    universe for us.'
  topic: strategy
- impact_reason: Provides a strong technical justification for needing non-classical
    computation (like quantum computing or physics-based methods) for modeling quantum
    phenomena.
  relevance_score: 9
  source: llm_enhanced
  text: I came out at trying to understand quantum mechanical systems. So you had
    systems that have quantum complexity, and there you can show that actually classical
    representations will struggle to capture quantum correlations.
  topic: technical
- impact_reason: 'Explains the rationale behind Quantum Neural Networks (QNNs): using
    a controllable quantum system to model an uncontrollable quantum system.'
  relevance_score: 9
  source: llm_enhanced
  text: And so now we could fight fire with fire, right? We can have parametric quantum
    complexity that's tunable to understand quantum complexity of our world.
  topic: technical
- impact_reason: Historical insight into the early development of QNNs, clarifying
    that the term is a mathematical descriptor, not a biological analogy.
  relevance_score: 9
  source: llm_enhanced
  text: I wrote up some of the first algorithms for quantum neural networks. So that's
    what we called them. They have nothing to do with actual neurons. They're just
    parameterized quantum programs...
  topic: technical
- impact_reason: Provides a formal, technical definition distinguishing quantum physics-based
    computing by controlling unitary evolution, crucial for understanding the quantum
    computing paradigm.
  relevance_score: 9
  source: llm_enhanced
  text: a physics-based computer, at least for quantum mechanical computer is one
    where you have a sort of parameterized Schrödinger evolution that you control,
    and you can show that it has some unitarity.
  topic: technical
- impact_reason: Highlights the speaker's transition from quantum (unitary) to stochastic/thermodynamic
    physics-based computing, showing a spectrum of non-classical hardware approaches.
  relevance_score: 9
  source: llm_enhanced
  text: I would say a physics-based computer, at least for quantum mechanical computer
    is one where you have a sort of parameterized Schrödinger evolution that you control...
    in our case, and we'll get to that, you know, we're building physics-based stochastic
    computers or stochastic thermodynamic computers.
  topic: technical
- impact_reason: 'Offers a high-level, unifying definition for physics-based computing:
    exploiting inherent physical properties (like probability densities) rather than
    simulating them digitally.'
  relevance_score: 9
  source: llm_enhanced
  text: a physics-based computer is essentially a computer whose components exploit,
    you know, the actual physical properties of the components in order to, you know,
    perform computations more efficiently. So rather than like, you know, use digital
    computation to simulate what, for example, is like a quantum phenomenon or a thermodynamic
    phenomenon, you actually use, you know, the probability densities that these systems
    embody, for example, at equilibrium, and, you know, exploit those properties in
    computation.
  topic: technical
- impact_reason: A profound philosophical parallel drawn between accepting non-interpretable
    physics (like quantum mechanics) and accepting non-interpretable AI (like deep
    learning).
  relevance_score: 9
  source: llm_enhanced
  text: I think we've got to learn to let go, you know, just like I, I learned to,
    in a way, uh, let go of control, mentally by giving up the reductionist approach,
    which is like the human interpretable approach to physics.
  topic: strategy/safety
- impact_reason: 'The core economic argument for stochastic/thermodynamic computing:
    operating near equilibrium drastically reduces the energy cost compared to fighting
    entropy for determinism.'
  relevance_score: 9
  source: llm_enhanced
  text: Whereas if a thermodynamic computer is not always at equilibrium, but we're
    dancing much closer to equilibrium, and it's much cheaper energetically to sit
    in those states and maintain those states, right?
  topic: business/technical
- impact_reason: Describes a practical, hybrid hardware architecture that mirrors
    the structure of probabilistic algorithms like Metropolis-Hastings, showing a
    path to integration.
  relevance_score: 9
  source: llm_enhanced
  text: our most recent chip is a mixed-signal chip. So we use digital classical components
    and sort of stochastic electronics, and the two have to interact just like, you
    know, in a Metropolis-Hastings algorithm, you have some components that of the
    algorithm that have some entropy, some proposals, and then you have some non-random
    parts, right? Like computing acceptance or rejection.
  topic: technical
- impact_reason: A powerful reframing of large language models (Transformers) as inherently
    probabilistic machines, justifying the need for probabilistic hardware.
  relevance_score: 9
  source: llm_enhanced
  text: You could think of even a transformer, you know, you have a softmax layer.
    The transformer is a big probabilistic computer already.
  topic: technical/predictions
- impact_reason: 'Highlights the fundamental inefficiency in current AI hardware/software
    co-design: hardware is optimized for determinism (noise removal), while leading
    algorithms (sampling-based) rely on reintroducing noise/stochasticity.'
  relevance_score: 9
  source: llm_enhanced
  text: there's a kind of paradox in our current architectures where as you just described
    it, we spent an inordinate amounts of, you know, energy and effort pumping the
    noise out of the system. But then, you know, with sampling-based methods, like
    we then reintroduce it through the software.
  topic: technical
- impact_reason: 'Directly diagnoses the inefficiency bottleneck in modern deep learning
    infrastructure: mismatch between probabilistic algorithms and deterministic hardware.'
  relevance_score: 9
  source: llm_enhanced
  text: running probabilistic software on this stack that was made for determinism,
    that's highly inefficient.
  topic: technical
- impact_reason: A powerful strategic insight suggesting that current dominant models
    (Transformers) are artifacts of available hardware constraints, not necessarily
    the ultimate algorithmic solution.
  relevance_score: 9
  source: llm_enhanced
  text: transformers are not sacred, right? They were what worked on the hardware
    we had at the time, which was Google TPUs.
  topic: strategy
- impact_reason: 'Explains the ''Thermal Danger Zone'' or ''Moore''s Wall'' concept:
    as classical transistors shrink, they hit fundamental thermodynamic limits where
    thermal noise becomes dominant, forcing a shift away from pure determinism.'
  relevance_score: 9
  source: llm_enhanced
  text: you're kind of, you're going from the quantum zone of physics to the thermodynamic,
    you're kind of edging on it. And then if you're in the classical zone, but you
    try to get smaller, then you're getting in a thermal zone again, right?
  topic: technical
- impact_reason: Provides an intuitive, physical explanation of the 'Probabilistic
    Bit' (P-bit) as a tunable system representing a fractional bit, crucial for implementing
    probabilistic computation efficiently.
  relevance_score: 9
  source: llm_enhanced
  text: The probabilistic bit you could think of as a, you know, a double-well system
    and you can, you can tune the tilt and so on. So, so if you have, if you consider
    bouncy balls in this landscape, you can control how much time the bouncy balls
    spend in one well or another. And you can consider one well zero, the other well
    one. So, essentially you have a signal that's essentially dancing between zero
    and one, right? And you can control how much time it spends in zero and one. And,
    and so that's like a fractional bit, right?
  topic: technical
- impact_reason: 'Poses the key strategic question for novel hardware: replacement
    vs. specialization. The answer points toward a heterogeneous, multi-scale computing
    stack.'
  relevance_score: 9
  source: llm_enhanced
  text: do you think this is a whole wholesale replacement for the current stack?
    Or do you see like, because I could see a world where like you interface, you
    use thermodynamic compute when it's relevant, but you mesh this with digital and
    quantum at the appropriate junctures so that you get what you really get is like
    a multi-scale stack where each hardware bit is specialized for the kinds of computations
    that run natively on that kind of hardware.
  topic: strategy
- impact_reason: Strong claim about the potential energy efficiency gains of thermodynamic
    computing over digital computing, framing it as a paradigm shift in energy consumption.
  relevance_score: 9
  source: llm_enhanced
  text: Essentially, it's as efficient as we could imagine building a thermodynamic
    computer. And again, it was just to get people to imagine, hey, actually, or realize
    rather, hey, actually, there's forms of computing that are far more energy-efficient
    by like an unfathomable amount of what is magnitude, far more efficient than than
    digital computers, right?
  topic: technical/predictions
- impact_reason: 'Crucial business decision: choosing room-temperature silicon implementation
    over cryogenically cooled superconducting chips for product viability and deployment
    ease.'
  relevance_score: 9
  source: llm_enhanced
  text: But for us the product is again the the silicon the silicon chips which, you
    know, we could have decided to run them in cryo CMOS but we ended up deciding
    to go for room temperature.
  topic: business/strategy
- impact_reason: Provides a historical and theoretical lineage, asserting that current
    deep learning (backprop) is fundamentally a simplified, deterministic approximation
    of EBMs, positioning their hardware as supporting the 'ancestors' of modern AI.
  relevance_score: 9
  source: llm_enhanced
  text: Modern neural networks are just, you know, mean approximations of EBMs, right?
    Like neural networks came from EBMs. The 2024 Nobel Prize was, you know, in backprop
    came from looking at mean-field of EBMs.
  topic: AI theory
- impact_reason: 'Identifies the ''hardware lock-in'' problem: algorithms are constrained
    by current digital hardware (GPUs), and new hardware paradigms are needed to unlock
    new algorithmic possibilities.'
  relevance_score: 9
  source: llm_enhanced
  text: So there's been a bias in algorithms towards what runs well on today's today's
    hardware and hopefully with the money and computers it changes that and so we're
    trying to get people interested in the space and to start imagining what they
    would do with.
  topic: strategy/AI theory
- impact_reason: Directly challenges the prevailing 'scale transformers indefinitely'
    narrative based on thermodynamic and power constraints.
  relevance_score: 9
  source: llm_enhanced
  text: And so and so people thinking we're just going to scale transformers and get
    to the moon and scale scale intelligence to the whole planet, they're just flat
    out wrong, right?
  topic: strategy/predictions
- impact_reason: 'Provides a concrete comparison: scaling a wafer-scale system using
    their technology results in 20 watts consumption, compared to potentially 20-100kW
    for current digital wafer-scale systems, demonstrating massive efficiency gains.'
  relevance_score: 9
  source: llm_enhanced
  text: you would have about 1.5 billion P bits, which you could think of as as neurons
    in about 20 20 billion parameters per wafer and then you could do multi-layer
    programs of those and that would run on not 20 kilowatts, which is what a current
    wafer-scale system would run on or more, maybe a hundred. It's 20 watts, right?
  topic: technical/business
- impact_reason: A clear, long-term prediction linking embodied intelligence directly
    to the ubiquity of 'thermodynamic computers' (likely referring to their specialized
    hardware).
  relevance_score: 9
  source: llm_enhanced
  text: looking 10 to 20 years out, like how do you envision the future and what role
    does acceleration play in your vision for the future? Yeah, I mean, 10 to 20 years.
    I think we'll have embodied intelligence. I think thermodynamic computers will
    be pretty ubiquitous. They're going to be what runs embodied intelligence.
  topic: predictions
- impact_reason: A core philosophical/strategic goal emphasizing individual ownership
    and control over cognitive extensions (AI), contrasting with centralized models.
  relevance_score: 9
  source: llm_enhanced
  text: my goal is for everyone to own and control the extension of their cognition
    because that is important to maintain intelligent intelligence.
  topic: safety/strategy
- impact_reason: 'Direct business/policy advice: decentralized AI is crucial, and
    over-regulation risks entrenching incumbents and centralizing power.'
  relevance_score: 9
  source: llm_enhanced
  text: That's why I've been pushing for decentralized AI and I've been pushing for
    sort of avoiding sort of over-regulation of AI that would cause red tape inflation
    and would mostly serve the incumbents and would cause a centralization of AI power.
  topic: business/strategy
- impact_reason: Introduces the concept of a 'soft merge' via shared perception/action
    states (Markov blanket) as a near-term reality, linking AI integration to active
    inference theory.
  relevance_score: 9
  source: llm_enhanced
  text: I think something more plausible in a 10-year timescale is a sort of soft
    merge, right? I mean, you're the active inference expert here, but you know, if
    you have the same Markov blanket, right, you have the same perception and action
    states, right?
  topic: technical/predictions
- impact_reason: Cites specific scientific backing (stochastic thermodynamics) for
    the idea that high free energy consumption correlates with higher probability/success
    in system trajectories.
  relevance_score: 9
  source: llm_enhanced
  text: the theorems of stochastic thermodynamics tell us that actually trajectories
    of the system that have basically consume more free energy are exponentially more
    likely, right?
  topic: technical
- impact_reason: 'A stark, dramatic summary of the e/ac philosophy: non-acceleration
    leads to obsolescence/death.'
  relevance_score: 9
  source: llm_enhanced
  text: the alternative is basically growth or death is what you're suggesting, right?
    Yeah, you know, we have the saying accelerate or die.
  topic: strategy
- impact_reason: Frames opposition to acceleration (deceleration) not just as disagreement,
    but as a deliberate, psychological form of warfare against growth.
  relevance_score: 9
  source: llm_enhanced
  text: I've used spreading, uh, deceleration as a form of, uh, warfare that is, that
    is psychological. In fact, that's, that's how I view it.
  topic: strategy/safety
- impact_reason: Advocates for a paradigm shift in societal thinking, moving from
    reductionism to complex systems thinking rooted in the Free Energy Principle across
    all domains.
  relevance_score: 9
  source: llm_enhanced
  text: it's also kind of a call to kind of popularizing complexism and complex systems
    thinking and the free energy principle at large. And we have to think through
    this lens about all systems in society from policymaking to to technology, to
    innovation, to to to to basically everything economics.
  topic: strategy
- impact_reason: This highlights a fundamental philosophical shift in approaching
    complex systems, moving away from purely reductionist models toward complex systems
    thinking, which is highly relevant for understanding advanced AI/ML systems.
  relevance_score: 9
  source: llm_enhanced
  text: it's a different way of thinking, one that is somewhat scary, um, you know,
    again, it's like, it's again going from reductionism and rationalism to sort of,
    um, complexism and post-rationalism.
  topic: strategy
- impact_reason: 'This provides a strong business/product development analogy: complex,
    real-world systems (like markets or advanced AI deployment) require open-loop
    experimentation because prior models lack sufficient predictive power.'
  relevance_score: 9
  source: llm_enhanced
  text: in startups, you kind of learn this, the, you know, you have to actually go
    on the market and try stuff. Sometimes your prior, your model-based prior can't
    do that well because the system is too complex to have a, a model that has, um,
    good predictive power, you actually have to be in a, in a sort of open loop.
  topic: business/strategy
- impact_reason: This points to the Free Energy Principle (a core concept in theoretical
    neuroscience and active inference) as a potentially revolutionary lens for scientific
    study, applicable to AI design.
  relevance_score: 9
  source: llm_enhanced
  text: most feels a study could be revolutionized by sort of thinking through the
    lens of complex self-adaptive systems and the free energy principle.
  topic: technical
- impact_reason: This controversial example illustrates the danger of exploring 'bad'
    subspaces of possibility (even defensively) in complex systems, leading to the
    creation of scenarios previously considered impossible or unnatural.
  relevance_score: 9
  source: llm_enhanced
  text: An example of this, right? An example, slightly controversial is, um, bioweapons
    research. And for example, COVID, right? Um, I would, I would, you know, I would
    say COVID was an accident, uh, from bioweapons research, it was probably defensive.
    And, um, we were trying to explore what would be a really bad scenario. What if
    we had, you know, this mutation and this mutation would combine and, and it would
    be a really bad virus. And then they start experimenting and designing in that
    neighborhood of, of virus subspaces that would have never occurred naturally.
  topic: safety/predictions
- impact_reason: A direct conclusion linking focused exploration of negative possibilities
    (via research or AI training) to realizing those negative outcomes.
  relevance_score: 9
  source: llm_enhanced
  text: Right? Just from evolution. Um, but because we were exploring that subspace
    of bad things, because we're obsessed with it, uh, we made it happen.
  topic: safety
- impact_reason: This connects the philosophical concept of 'false belief' (necessary
    for action) back to the technical framework of prediction error minimization,
    suggesting that initial, potentially inaccurate models drive initial movement.
  relevance_score: 9
  source: llm_enhanced
  text: oh, yeah, you know, from the negative inference point of view, every action
    begins with the false belief, right? So first, you, you believe that you're moving
    and then you reduce the prediction error in the direction of action, right?
  topic: technical/strategy
- impact_reason: A provocative philosophical statement equating the human brain to
    a 'kick-ass AI supercomputer,' setting the stage for comparing biological intelligence
    with artificial intelligence.
  relevance_score: 8
  source: llm_enhanced
  text: We have proof of existence of a really kick-ass AI supercomputer that we're
    both using right now to talk to each other. It's our brains.
  topic: predictions/strategy
- impact_reason: A direct, timely product insight from Google DeepMind highlighting
    the industry trend of balancing speed, cost, and reasoning capabilities in large
    language models.
  relevance_score: 8
  source: llm_enhanced
  text: Gemini 2.5 Flash aims right at that challenge. It's got the speed you expect
    from Flash, but with upgraded reasoning power.
  topic: AI technology trends/business
- impact_reason: 'Clearly defines the classical reductionist paradigm in physics and
    its goal: achieving control through simple, interpretable models.'
  relevance_score: 8
  source: llm_enhanced
  text: The reductionist approach is kind of the traditional way we've done physics...
    reduce all the physics to a very simple model with few parameters... And with
    this predictive power, I can steer the world.
  topic: strategy
- impact_reason: Connects foundational physics concepts (Wheeler's idea) to quantum
    information theory, showing the intellectual lineage leading to modern computational
    physics.
  relevance_score: 8
  source: llm_enhanced
  text: I guess I got swept up in the school of thought that was a generalization
    of Wheeler's It From Bit, which was the It From Qubit program, seeking to unify
    theoretical physics through quantum information theory.
  topic: technical
- impact_reason: A concise summary of the motivation behind advanced AI/computation—surpassing
    inherent biological cognitive limits.
  relevance_score: 8
  source: llm_enhanced
  text: basically building a digital brain to overcome the limitations of our fleshy
    brains.
  topic: predictions/strategy
- impact_reason: Offers a highly technical, non-anthropocentric definition of intelligence
    rooted in information compression and parameterized modeling.
  relevance_score: 8
  source: llm_enhanced
  text: To me, intelligence was just... I'm trying to compress, learn, compress the
    representations of systems through parameterized distributions...
  topic: technical/philosophy
- impact_reason: Provides a specific definition for quantum computing as a form of
    physics-based computing, contrasting it with classical computing.
  relevance_score: 8
  source: llm_enhanced
  text: What do you mean precisely by physics-based computing? ... I would say that's
    physics-based computing if it leverages quantum mechanical evolutions as a resource.
  topic: technical
- impact_reason: Strategic insight into the fragmentation of the non-classical computing
    field, suggesting a need for community building and cross-pollination of tools.
  relevance_score: 8
  source: llm_enhanced
  text: there's all these arcipelagos [of physics-based computing] that don't necessarily
    talk to each other. And I think there could be, you know, more work in trying
    to, you know, find the community. Because there's a lot of tools that could cross-pollinate
    between these different substrates.
  topic: strategy
- impact_reason: 'Details the technical mechanism of their stochastic computer: mapping
    MCMC problems directly onto controlled, natural stochastic dynamics of electrons.'
  relevance_score: 8
  source: llm_enhanced
  text: We're embedding into the stochastic dynamics that are parametric and that
    we have control over those parameters. We're embedding our continuous or discrete
    variable MCMC into the dynamics of the electrons on the device, right?
  topic: technical
- impact_reason: A strong statement reframing the Transformer architecture, suggesting
    that despite being run on deterministic hardware, its core function (softmax)
    is inherently probabilistic.
  relevance_score: 8
  source: llm_enhanced
  text: even a transformer, you know, you have a softmax layer. The transformer is
    a big probabilistic computer already.
  topic: technical
- impact_reason: 'Validates the company''s core thesis: the mainstream AI trend (diffusion,
    MCTS) is naturally moving towards the probabilistic methods their hardware is
    designed for.'
  relevance_score: 8
  source: llm_enhanced
  text: even the current sort of incumbent algorithms are converging towards sampling
    and more probabilistic algorithms, which, you know, wasn't obvious when we started
    the company in 2022, but that was the prediction, right?
  topic: predictions
- impact_reason: Argues that current hardware scaling efforts are merely 'ad hoc'
    fixes for an unavoidable physical limit (thermal noise), implying that a paradigm
    shift is necessary.
  relevance_score: 8
  source: llm_enhanced
  text: the reason we can't scale down the deterministic computers is is is because
    of this thermal danger zone and they're innovating in all sorts of ways. They're
    doing all sorts of weird fins and all sorts of weird designs that are sort of
    hardware-level error correction. But those are, those are always going to be ad
    hoc in some sense, right?
  topic: technical
- impact_reason: Highlights the massive engineering challenge and breakthrough involved
    in transitioning from superconducting prototypes to scalable, programmable stochastic
    physics on silicon.
  relevance_score: 8
  source: llm_enhanced
  text: Moving to silicon. Yeah, and getting programmable stochastic physics in silicon
    was a whole kind of order of magnitude more difficulty.
  topic: technical
- impact_reason: 'Emphasizes the critical difference between academic research and
    startup execution: successfully de-risking manufacturing is essential for scaling
    and commercial viability.'
  relevance_score: 8
  source: llm_enhanced
  text: because there's nothing stopping us from scaling at this point, because we,
    you know, we've de-risked the manufacturing as well, which is usually not something
    academics think about.
  topic: business
- impact_reason: A concrete, aggressive scaling goal that signals the transition from
    research prototype to a potentially impactful, large-scale system.
  relevance_score: 8
  source: llm_enhanced
  text: we are looking at scaling to millions of degrees of freedom next year.
  topic: business
- impact_reason: Highlights the critical difference between academic research and
    startup execution, emphasizing the necessity of de-risking manufacturing for commercial
    viability and scaling.
  relevance_score: 8
  source: llm_enhanced
  text: there's nothing stopping us from scaling at this point, because we, you know,
    we've de-risked the manufacturing as well, which is usually not something academics
    think about. But if you're a startup and you have to to scale or die, eat the
    world or die, you have to kill every risk possible.
  topic: business/strategy
- impact_reason: Connects concepts from physics (renormalization group, sampling in
    quantum mechanics) to machine learning hierarchies, suggesting a deep, underlying
    theoretical link between physical laws and information compression in AI.
  relevance_score: 8
  source: llm_enhanced
  text: if you're trying to learn from quantum mechanics and you don't sample, you
    get to just go mechanics. You don't sample, you get to you, you tell me mechanics.
    You can imagine if I had a God neural network that just compressed all the information
    in a certain region of space, which is actually the thought experiment that got
    me into into quantum machine learning.
  topic: technical/AI theory
- impact_reason: Acknowledges the massive overhead of cryogenic cooling for superconducting
    systems, justifying their pivot away from that path toward room-temperature solutions.
  relevance_score: 8
  source: llm_enhanced
  text: If you had a ginormous dilution fridge and you had millions and millions of
    of of of of P bits and superconductors, you'd have the most energy-efficient.
    Probably not, probably not either.
  topic: technical/strategy
- impact_reason: Philosophical argument contrasting biological intelligence (highly
    energy-efficient) with current AI scaling, reinforcing the need for thermodynamic
    efficiency in artificial intelligence.
  relevance_score: 8
  source: llm_enhanced
  text: I run on a glass of water and a banana, maybe a coffee in the morning. Like
    this certainly this is not like a naturalistic way to think about how intelligence
    scales, right? Because you're a thermodynamic computer, right? That that's right.
    That that's the thesis.
  topic: AI theory/philosophy
- impact_reason: Contrasts the challenge of building quantum computers (fighting decoherence)
    with their approach, which leverages natural physical processes (thermodynamics/probabilistic
    inference) that are inherently robust in nature.
  relevance_score: 8
  source: llm_enhanced
  text: In quantum computing, there's no like quantum computer in nature that has
    very, very high quantum coherence and we have exquisite control and very high
    quantum complexity. All all systems cohere. We have, you know, multiple billion
    the many computers out there in the wild and they work pretty well.
  topic: technical/strategy
- impact_reason: Defines their approach as 'probabilistic inference in physics' rather
    than neuromorphic, broadening the applicability beyond just AI to simulation and
    optimization.
  relevance_score: 8
  source: llm_enhanced
  text: We're not obsessed with sort of biomimicry. We're not upset. We don't call
    ourselves neuromorphic computing and we're just doing again, probabilistic approximate
    probabilistic inference as a service now, but but in hardware, in physics, and
    and to us that's kind of the parent workload of most of AI and actually not just
    for AI, it's for broader computing right?
  topic: strategy/technical
- impact_reason: This clarifies the company's approach, focusing on fundamental physics
    for probabilistic inference hardware rather than strictly mimicking biological
    brains (neuromorphic), suggesting a broader, physics-based computing paradigm.
  relevance_score: 8
  source: llm_enhanced
  text: We're just trying to to tap into the same physics. We're not obsessed with
    sort of biomimicry. We're not upset. We don't call ourselves neuromorphic computing
    and we're just doing again, probabilistic approximate probabilistic inference
    as a service now, but but in hardware, in physics...
  topic: technical
- impact_reason: Predicts a future of highly dense, personalized, and continuously
    learning AI integrated into everyday devices.
  relevance_score: 8
  source: llm_enhanced
  text: We're going to have greater intelligence density in all our devices. We're
    going to have personalized always-online learning intelligence.
  topic: predictions
- impact_reason: Defines the soft merge as a functional merging of agency through
    shared sensory/motor loops, independent of invasive hardware.
  relevance_score: 8
  source: llm_enhanced
  text: We're sharing perception and we're sharing actions through my body. We are
    the same agent, right? So that is a form of merge and you don't need a neural
    interface for that, right?
  topic: technical
- impact_reason: 'Describes the self-reinforcing nature of the e/ac meme: by optimizing
    for growth, it is inherently designed to be the most persistent and viral idea.'
  relevance_score: 8
  source: llm_enhanced
  text: I will design the highest fitness selfish meme that is e/ac, which is figure
    out what is optimal for growth and do it. And so by construction, it should be
    the most viral thing. And it will persist.
  topic: strategy
- impact_reason: Poses a fundamental philosophical question about the reducibility
    of human morality to thermodynamic principles (Free Energy Principle).
  relevance_score: 8
  source: llm_enhanced
  text: Axel Constant, actually, uh, and I have a longstanding debate as to whether,
    um, ethics, morality, and this kind of thing can be reduced, uh, to free energy
    minimization at the end of the day.
  topic: safety/ethics
- impact_reason: Acknowledges the inherent lack of interpretability and control in
    complex systems, framing necessary experimentation ('FaFO') as 'exploration and
    discovery' in the absence of perfect models.
  relevance_score: 8
  source: llm_enhanced
  text: it's, we don't have as much control and interpretability, we don't understand
    the world as well, right? We kind of have to, you know, pardon my French, but
    fuck around and find out, the FaFO algorithm, right? Like, but really it's kind
    of like exploration and, and discovery, right?
  topic: technical/strategy
- impact_reason: 'A highly practical business lesson derived from the psychological
    principle: optimism is a necessary precondition for success in high-uncertainty
    ventures like startups.'
  relevance_score: 8
  source: llm_enhanced
  text: As a startup founder, if you're not optimistic about your startup, statistically,
    you are, you are screwed, right?
  topic: business
- impact_reason: This frames the 'e/ac' movement as a strategic, meme-based effort
    to counteract pervasive technological and societal pessimism by injecting radical
    optimism into the discourse.
  relevance_score: 8
  source: llm_enhanced
  text: one of the things I find most inspiring about e/ac is that you're trying to
    be like, you're trying to present a radically optimistic meme for the future.
  topic: strategy
- impact_reason: Reveals the speaker's dual identity, linking deep technological research
    with a specific, often controversial, philosophical stance on technological progress
    (accelerationism).
  relevance_score: 7
  source: llm_enhanced
  text: I also happen to be the founder of a philosophical movement called effective
    accelerationism under the pseudonym, Beth J. Zos, online.
  topic: strategy/philosophy
- impact_reason: Articulates a grand, long-term strategic vision rooted in fundamental
    physics, framing technological development as a means for civilizational expansion.
  relevance_score: 7
  source: llm_enhanced
  text: I was pursuing theories of everything. I want to understand the universe as
    one does, and eventually leverage that knowledge to expand civilization to the
    stars.
  topic: strategy
- impact_reason: Highlights a concrete, early industry effort (TensorFlow Quantum)
    at the intersection of ML and quantum computing.
  relevance_score: 7
  source: llm_enhanced
  text: '...building a product known as TensorFlow Quantum, which was product focused
    on creating software that allows us to learn quantum mechanical representations
    of quantum mechanical systems in our world.'
  topic: AI technology trends/business
- impact_reason: Acknowledges the spectrum between purely digital emulation and native
    physical computation, suggesting that the distinction isn't always binary.
  relevance_score: 7
  source: llm_enhanced
  text: But it is, it's true that it's kind of a, it's kind of a continuum, right?
    Because you can have physics-inspired computers that are doing digital emulations
    of physics-inspired algorithms.
  topic: technical
- impact_reason: Identifies a critical communication gap between hardware engineers
    focused on probabilistic computation and ML practitioners focused on current hot
    models (like Transformers), advocating for cross-disciplinary dialogue.
  relevance_score: 7
  source: llm_enhanced
  text: there's a gap there. But hopefully, you know, I think coming on this podcast
    and hopefully getting more of the machine learning community to talk to each other.
    I think probabilistic, and I'll hopefully we'll have a resurgence.
  topic: strategy
- impact_reason: Uses the human brain as the ultimate example of an energy-efficient,
    thermodynamic computer, setting a high bar for future AI hardware.
  relevance_score: 7
  source: llm_enhanced
  text: at the end of the day, we have proof of existence of a really kick-ass AI
    supercomputer that we're both using right now to talk to each other. So it's our
    brains and that I would argue is a thermodynamic computer, right?
  topic: strategy
- impact_reason: 'Describes the initial advantage of using superconducting circuits:
    direct mapping between desired physics (Hamiltonian/Energy) and circuit design,
    enabling the creation of a macroscopic thermodynamic computer.'
  relevance_score: 7
  source: llm_enhanced
  text: we had a very direct way to go from, hey, I want this energy, here's how I
    design my circuit. Right? And so that's where we started. Furthermore, it was
    the way to build the most macroscopic thermodynamic computer you can build.
  topic: technical
- impact_reason: Illustrates the high-stakes, high-risk entrepreneurial decision to
    abandon established, high-profile fields (QC/QML) to pursue a fundamentally new
    computing paradigm.
  relevance_score: 7
  source: llm_enhanced
  text: I quit all that. I quit quantum computing, quantum machine learning and it's
    like, I'm going to take even more risk and build a whole new paradigm of computing
    from scratch from the concept up.
  topic: business
- impact_reason: A concrete, aggressive scaling goal for their hardware (P bits),
    indicating rapid progress in hardware capacity.
  relevance_score: 7
  source: llm_enhanced
  text: So, you know, we are looking at scaling to millions of degrees of freedom
    next year.
  topic: technical/business
- impact_reason: 'A pragmatic warning about hardware specialization: using the wrong
    tool (probabilistic hardware for deterministic tasks) leads to inefficiency.'
  relevance_score: 7
  source: llm_enhanced
  text: In principle, you could use a probabilistic computer for deterministic operations,
    but it's not going to be the best at that.
  topic: strategy/technical
- impact_reason: A clear, long-term prediction regarding the maturity of AI, specifically
    pointing toward embodied agents.
  relevance_score: 7
  source: llm_enhanced
  text: I think 10 to 20 years out, I think we'll have embodied intelligence.
  topic: predictions
- impact_reason: Highlights that the underlying hardware/physics approach is applicable
    beyond current AI trends to general scientific computing, simulation, and optimization.
  relevance_score: 7
  source: llm_enhanced
  text: it's for broader computing right? There's simulation, optimization, and, you
    know, statistical inference science at large can use this. So it's not just it's
    not just for AI.
  topic: strategy
- impact_reason: A concise, technical-sounding definition of Effective Accelerationism
    (e/ac) as a guiding cultural setting.
  relevance_score: 7
  source: llm_enhanced
  text: e/ac is sort of meta-culture. It's kind of I call it like a cultural hyperparameter
    prescription, if you will.
  topic: strategy
- impact_reason: 'Applies the accelerationist principle directly to business strategy:
    stagnation leads to inevitable disruption.'
  relevance_score: 7
  source: llm_enhanced
  text: If a company is not obsessed with growth, eventually just gets disrupted by
    one that outgrows them and then has more resources to to outcompete them, right?
  topic: business
- impact_reason: 'States the current ambition: to apply complex systems/FEP thinking
    (which has influenced software) to revolutionize hardware development.'
  relevance_score: 7
  source: llm_enhanced
  text: Clearly, it's eaten the software world and we're trying to make this sort
    of school of thought eat the hardware world.
  topic: strategy
source: Unknown Source
summary: '## Podcast Summary: Pushing Compute to the Limits of Physics


  This 83-minute episode features a highly technical discussion between host Maxel
  Ramsted and guest Guillain Valdon (also known online as Beth J. Zos), founder of
  **Extropic**, a company pioneering **thermodynamic computing**. The conversation
  traces Valdon’s intellectual journey from theoretical physics and quantum gravity
  to the development of novel, physics-based hardware accelerators for probabilistic
  inference.


  ---


  ### 1. Focus Area

  The primary focus is on **next-generation computing paradigms**, specifically contrasting
  **Quantum Computing** and **Thermodynamic Computing** as alternatives to classical
  digital computation, particularly for complex probabilistic tasks in AI/ML (like
  Markov Chain Monte Carlo - MCMC). The discussion heavily integrates concepts from
  theoretical physics, quantum information theory, and complex systems.


  ### 2. Key Technical Insights

  *   **The Failure of Reductionism in Physics:** Valdon argues that the traditional
  reductionist approach in physics (seeking simple, few-parameter models) is failing
  for complex systems. The future lies in a **complexism approach**, viewing the universe
  as a massive, self-simulating quantum computer, necessitating computation that mirrors
  this complexity (e.g., tensor networks, deep learning).

  *   **Quantum Computing’s Energy Bottleneck:** While powerful, quantum computing
  requires extreme energy expenditure to maintain near-zero entropy (quantum coherence)
  against environmental noise, necessitating massive overhead for error correction.

  *   **Thermodynamic Computing as a "Hotter" Alternative:** Extropic’s approach harnesses
  the **exotic stochastic physics of electrons** to build computers that operate closer
  to thermal equilibrium. This allows them to naturally execute probabilistic algorithms
  like MCMC much more energy-efficiently than digital emulation, essentially "letting
  go of the tight grip" on determinism.


  ### 3. Business/Investment Angle

  *   **Inefficiency of Current AI Stacks:** There is a fundamental mismatch: modern
  probabilistic AI software (like Transformers with softmax layers or MCTS) is run
  on hardware optimized for determinism, leading to massive energy inefficiency when
  reintroducing stochasticity via software.

  *   **Market for Specialized Accelerators:** Thermodynamic computing targets the
  intractable emulation of MCMC algorithms, offering orders of magnitude speed-up
  and energy efficiency gains for probabilistic inference tasks, creating a niche
  market for physics-based accelerators.

  *   **The "Letting Go" Mindset:** Similar to how deep learning required researchers
  to let gradient descent take control over explicit programming (Software 1.0), hardware
  development must embrace noise and stochasticity rather than fighting it to unlock
  new computational regimes.


  ### 4. Notable Companies/People

  *   **Guillain Valdon (Beth J. Zos):** Founder of Extropic; former researcher at
  Alphabet working on quantum computing and TensorFlow Quantum; proponent of effective
  accelerationism.

  *   **Extropic:** Company pioneering thermodynamic computing, focusing on stochastic
  hardware for probabilistic inference.

  *   **Google DeepMind (Sponsor):** Mentioned Gemini 2.5 Flash as an example of balancing
  model intelligence, speed, and cost in current AI development.

  *   **Theoretical Physics Icons:** Feynman and Stephen Hawking were cited as early
  influences on Valdon’s pursuit of a "theory of everything."


  ### 5. Future Implications

  The conversation suggests a future where computation moves away from absolute determinism.
  Instead of fighting entropy (as in classical and quantum computing), future accelerators
  will **harness natural physical processes** (stochastic dynamics) to solve problems
  that are inherently probabilistic. This shift implies a diversification of hardware
  substrates beyond silicon CMOS and superconducting qubits, moving toward specialized,
  physics-native accelerators for specific computational classes.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML researchers, hardware architects, quantum
  computing specialists, and technology investors** interested in the fundamental
  limits of computation, novel accelerator design, and the intersection of physics
  and advanced AI.'
tags:
- artificial-intelligence
- ai-infrastructure
- startup
- generative-ai
- google
- meta
title: Pushing compute to the limits of physics
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 116
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 12
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 9
  prominence: 0.9
  topic: startup
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 2
  prominence: 0.2
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 00:42:55 UTC -->
