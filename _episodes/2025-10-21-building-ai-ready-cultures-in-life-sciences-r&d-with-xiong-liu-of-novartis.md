---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2159
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2194
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2559
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2621
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5301
- category: unknown
  confidence: medium
  context: lking about generic AI versus the domain-specific Gen AI. When we delve
    into those domain-specific use cas
  name: Gen AI
  position: 7024
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 17977
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24100
- category: ai_research
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company where the guest, Jung Liu, is the Director of Data Science
    in AI, focusing on applying generative AI in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured as an executive
    thought leader on the podcast.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured as an executive
    thought leader on the podcast.
  name: Raytheon
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AI-first tool for organizing ideas and making connections, built by
    Steven Johnson.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as the originator of the transformer architecture, which underpins
    modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The URL provided for the Notebook LM tool, indicating its association with
    Google.
  name: NotebookLM.google.com
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 14:07:46 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 99
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 219
  relevance_score: 4
  text: Maybe we can give some more detailed examples about how GenAI and the foundation
    models work and then we can delve into, you know, the development, right, how
    we build up and, you know, put those models into work, right
  topics: []
- length: 229
  relevance_score: 4
  text: I just give some examples about using foundation models, GenAI to generate
    new cell types and gene expressions so that we can do all kinds of in silico,
    you know, treatment methods to see, to observe the predicted phenotypes, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 70
  relevance_score: 3
  text: So we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: 'This is a core technical and strategic distinction between traditional
    ML and the foundation model approach: leveraging massive, broad datasets for pre-training
    versus relying solely on small, task-specific labeled datasets.'
  relevance_score: 10
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs foundation models. So
    the idea is that yes, you still care about the specific data you are working on,
    but at the same time, we want to collect all the available data, relevant data,
    or irrelevant data, as long as they fall into the same domain, and fill those
    data into those transformer architectures to build a much large-scale pre-trained
    models.
  topic: technical/strategy
- impact_reason: Provides a concrete, high-impact example of foundation models in
    life sciences (drug discovery), showing how pre-trained models on public biological
    data accelerate research in data-limited specific indications.
  relevance_score: 10
  source: llm_enhanced
  text: When we delve into those domain-specific use cases, we probably can better
    understand the benefit of foundation models. So for example, when we study the
    disease pathways and identify new targets for specific indications... But now
    with foundation models, it has a lot of pre-trained models based on publicly available
    molecular data. So for example, cell atlases, etc.
  topic: technical/predictions
- impact_reason: Provides a critical, quantifiable metric illustrating the 'AI implementation
    gap' or 'pilot purgatory' challenge prevalent across industries, especially life
    sciences.
  relevance_score: 10
  source: llm_enhanced
  text: 73% of life sciences leaders are investing in generative AI initiatives, fewer
    than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business/strategy
- impact_reason: 'Highlights a critical capability for scientific generative AI: constrained
    generation based on desired physical/biological properties (e.g., toxicity, solubility).'
  relevance_score: 10
  source: llm_enhanced
  text: And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/predictions
- impact_reason: Emphasizes the indispensable role of domain experts (SMEs) in guiding
    data selection and, crucially, in the benchmarking and validation process for
    domain-specific models.
  relevance_score: 10
  source: llm_enhanced
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves. Although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data. But, you know, we still need
    the domain knowledge people, you know, people have those kind of expertise to
    guide the, you know, the selection of the data and also more importantly, then
    these models are, you build, now how do we benchmark with them?
  topic: strategy/safety
- impact_reason: 'Crucial insight into the limitations of AI advancement in biotech:
    AI capabilities often outpace the experimental capacity to generate the necessary
    high-fidelity training data.'
  relevance_score: 10
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
    So, there's, you know, where AI, the architecture, the concept there, but the,
    sometimes we are limited by the, you know, experimental side.
  topic: limitations/strategy
- impact_reason: Highlights the extreme danger of hallucination in high-stakes domains
    like drug discovery, where validation requires slow, expensive physical experiments
    rather than instant text/code checks.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the biomedical biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: 'Proposes a necessary paradigm shift in evaluation: moving beyond
    general metrics to hybrid ''domain + AI-driven'' evaluation metrics to reliably
    score specialized models.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven those evaluation is a
    key here, because there could be so many versions of found data models. Now, if
    we, you know, have those kind of domain plus AI-driven those evaluation metrics,
    so we can score across different models, right?
  topic: safety/strategy
- impact_reason: Clearly outlines the historical evolution of AI/ML, highlighting
    the current 'paradigm shift' towards foundation models, which is crucial context
    for understanding modern AI strategy.
  relevance_score: 9
  source: llm_enhanced
  text: we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks. And more recently, we have a paradigm shift into those language models,
    foundation models.
  topic: technical/strategy
- impact_reason: 'Explains the primary value proposition of foundation models: enabling
    strong performance on data-scarce, domain-specific tasks through transfer learning.'
  relevance_score: 9
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data. So that's why they are called foundation models. They could be useful for
    many downstream tasks.
  topic: technical/business
- impact_reason: Offers a clear, accessible definition of fine-tuning in the context
    of foundation models, differentiating it from full retraining, which is vital
    for practitioners.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right? So we can use the data you have to train to fine-tune
    the model, right? It's called fine-tune. It doesn't mean we retrain the whole,
    you know, the foundation model, large language model, etc. We just adjust the
    weights based on the data you have so that it can better be personalized for your
    problems.
  topic: technical
- impact_reason: Connects foundation models directly to the concept of 'digital twins'
    in healthcare, suggesting a powerful application for personalized medicine and
    trial simulation.
  relevance_score: 9
  source: llm_enhanced
  text: The folks, the professionals running the clinical trials can run that foundational
    model based on what they're seeing with this specific patient and that kind of
    acts like the digital twin.
  topic: predictions/business
- impact_reason: A powerful analogy framing biological data (genetics) as a language
    amenable to LLM processing, suggesting massive efficiency gains in early-stage
    drug development.
  relevance_score: 9
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of much of across the drug development process.
  topic: predictions/technical
- impact_reason: Lists the key practical challenges organizations face when moving
    from experimentation to meaningful application of AI in R&D (benchmarking, alignment,
    ROI measurement).
  relevance_score: 9
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business/strategy
- impact_reason: 'Showcases a cutting-edge, specific application of GenAI in R&D:
    *in silico* generation of biological data (cell types, gene expressions) to test
    treatments virtually.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, GenAI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: technical/predictions
- impact_reason: Provides a clear technical distinction between BERT-style models
    (encoder-only, focusing on representation/embeddings) and generative models.
  relevance_score: 9
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they use the decoder
    part of the transformer. So you can think it's as a very good representation model.
    So many, it can capture the hidden context among the, in the text between words,
    etc. that where a human sees. So the outcome is some representations or embeddings
    of those text tokens, etc.
  topic: technical
- impact_reason: Clearly explains the shift from representation learning (BERT) to
    generative capabilities (GPT-like models) enabled by leveraging the decoder stack.
  relevance_score: 9
  source: llm_enhanced
  text: but now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: 'Provides a clear strategic roadmap for AI adoption in specialized
    industries: a hybrid approach combining leveraging public models with building
    proprietary, expert-guided versions, necessitating deep cross-discipline collaboration.'
  relevance_score: 9
  source: llm_enhanced
  text: So, I think it's a combination, it's a hybrid method, right? So we could leverage
    what's already been published. And also, if for the not working, you know, specifically,
    then we have to build our own versions, you know, then again, we have to have
    this AI, computer science, domain experts in house, we need to, you know, work
    with those, of course, disciplinary functional teams, disease areas, biologists,
    chemistries, etc. So, yeah, so it's basically a cross-discipline collaboration,
    whether in house or externally, yeah.
  topic: strategy/business
- impact_reason: Identifies organizational alignment and communication between technical
    builders (AI modelers) and domain experts (scientists/leadership) as a critical
    execution hurdle.
  relevance_score: 9
  source: llm_enhanced
  text: So, it's quite important that we have to do those benchmarking and select
    models accordingly. And then it goes to the execution, right? So, there's a lot
    of cultural organizational issue. I see the key that really we should align, you
    know, between different parties, like scientists, basic scientists, or AI modelers,
    AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: Describes a strategic shift in data management for AI adoption—centralizing
    data first, then applying advanced algorithms, moving beyond simple document processing
    to broader strategic automation.
  relevance_score: 8
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later, discriminating
    about it later, but from that data repository we can drive more advanced algorithms
    to help with strategic tasks across the entire organization that need that kind
    of whole-scale approach.
  topic: strategy/business
- impact_reason: Reiterates that the underlying success of modern GenAI across diverse
    modalities (text, molecules, images) stems from the foundational Transformer architecture.
  relevance_score: 8
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: 'Highlights a crucial strategic decision point for enterprises: whether
    to rely on general-purpose models or invest in developing/fine-tuning domain-specific
    models.'
  relevance_score: 8
  source: llm_enhanced
  text: We're talking about generic AI versus the domain-specific Gen AI.
  topic: strategy
- impact_reason: Confirms the foundational role of the Transformer architecture across
    diverse domain-specific foundation models, unifying the underlying technology.
  relevance_score: 8
  source: llm_enhanced
  text: essentially they have the same, I mean, by nature, they have the same setup.
    So that is based on the breakthrough of the transformer architecture that Google
    proposed a long time ago.
  topic: technical
- impact_reason: Illustrates the potential for applying NLP-style pre-training techniques
    (Transformers) to structured scientific data like molecules (SMILES strings or
    graphs).
  relevance_score: 8
  source: llm_enhanced
  text: So all those information publicly available data are from different databases.
    They could all fit into those kind of transformer architecture to let you learn
    the hidden embedding signatures of those molecules, atoms, etc.
  topic: technical/applications
- impact_reason: Identifies a structural advantage for language-centric fields like
    genetics in adopting early generative AI, given the maturity of NLP models.
  relevance_score: 8
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: 'Offers strategic advice to domain experts: they must understand
    the hybrid nature of AI integration—it won''t replace existing methods entirely
    but augment them.'
  relevance_score: 8
  source: llm_enhanced
  text: It behooves them to kind of have some awareness of the new and old paradigm
    that we were talking about before, how both are still present, but how we thought
    that AI would impact life sciences organization or that they'd just be this one
    school way of doing things is not the case.
  topic: strategy
- impact_reason: 'Reiterates the persistent bottleneck in AI: the need for high-quality,
    domain-specific data, even when using large foundation models.'
  relevance_score: 7
  source: llm_enhanced
  text: The first thing is about the data challenge, right? So, so the foundation
    models, although they take all kinds of available data, but still they require
    high-quality data.
  topic: business/technical
source: Unknown Source
summary: '## Comprehensive Summary: Building AI-Ready Cultures in Life Sciences R&D


  This 30-minute podcast episode, featuring **Zhong Liu, Director of Data Science
  in AI at Novartis**, focuses on the paradigm shift in Life Sciences R&D driven by
  **Generative AI (GenAI)** and **Foundation Models (FMs)**, and the cultural and
  technical challenges associated with scaling these technologies beyond pilot projects.


  ### 1. Focus Area

  The discussion centers on the transition from traditional, task-specific Machine
  Learning (ML) to leveraging large-scale Foundation Models in drug discovery, protocol
  design, and clinical workflows. Key themes include the architectural shift in AI
  (from supervised/deep learning to transformer-based FMs), the necessity of domain-specific
  fine-tuning, and the organizational alignment required to achieve meaningful ROI
  from GenAI adoption in a highly regulated industry.


  ### 2. Key Technical Insights

  *   **Shift to Foundation Models:** The core technical evolution is moving from
  training models solely on proprietary, labeled data to utilizing massive, pre-trained
  FMs (like LLMs) that capture broad domain knowledge (e.g., public molecular data,
  cell atlases). This allows for better performance even with small, specific datasets
  via **fine-tuning**.

  *   **BERT vs. GPT Architectures:** The evolution from BERT-like models (excellent
  for text representation/embeddings) to GPT-like models (leveraging the decoder for
  data *generation*) is crucial. This generative capability is now being applied to
  molecular design (generating new SMILES strings or graphs) constrained by desired
  properties (e.g., toxicity, solubility).

  *   **Digital Twins and Genetics as Language:** Genetics and biological systems
  are increasingly viewed as a language, allowing FMs to model population-level biological
  functions. This enables personalized testing, acting as a form of **digital twin**
  for specific patient data or compound interactions *in silico*.


  ### 3. Business/Investment Angle

  *   **Scaling Gap:** Despite high investment interest (73% of leaders investing
  in GenAI), few life sciences organizations (under 20%) have successfully scaled
  beyond pilots, indicating significant implementation hurdles.

  *   **Data Quality vs. Availability:** While FMs thrive on vast data, the primary
  bottleneck remains the availability of high-quality, *experiment-driven* domain-specific
  data (e.g., single-cell RNA-seq) needed for deep fine-tuning.

  *   **ROI and Evaluation:** Measuring the ROI of GenAI is challenging because outputs
  (like novel molecules or pathways) cannot be instantly validated like text or code.
  Developing **domain-driven evaluation metrics and knowledge-checking benchmarks**
  is essential for scoring and selecting optimal models.


  ### 4. Notable Companies/People

  *   **Zhong Liu (Novartis):** The expert guest, providing insights from a major
  pharmaceutical company on practical AI adoption, cultural alignment, and the technical
  evolution of models in R&D.

  *   **Google (Transformer Architecture):** Mentioned as the originator of the foundational
  transformer architecture that underpins modern LLMs and domain-specific FMs.

  *   **Deloitte:** Cited for a recent survey highlighting the gap between GenAI investment
  and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is moving toward hybrid development models, leveraging public FM breakthroughs
  while building proprietary, fine-tuned versions for specific indications. Success
  hinges on **cross-disciplinary collaboration** between AI professionals, basic scientists,
  and leadership. The future requires robust, domain-specific evaluation frameworks
  to manage the inherent risk of model hallucination in high-stakes biomedical applications.


  ### 6. Target Audience

  This episode is highly valuable for **AI/Tech Professionals** working in Life Sciences,
  **R&D Leadership**, **Data Science Directors**, and **Business Strategists** focused
  on digital transformation and AI investment decisions within the pharmaceutical
  and biotech sectors.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 120
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 13
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 14:07:46 UTC -->
