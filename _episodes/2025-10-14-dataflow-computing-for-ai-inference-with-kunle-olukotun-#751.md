---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'you could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Hey folks, Stephen Johnson here, co-founder of Notebook LM. As an author,
    I'
  name: Stephen Johnson
  position: 11
- category: unknown
  confidence: medium
  context: Hey folks, Stephen Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 47
- category: tech
  confidence: high
  context: 'and helping you brainstorm. Try it at notebooklm.google.com.


    We have a data flow problem, right? So let'''
  name: Google
  position: 413
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the TWIML AI podcast. I am
    your host, Sam Charrington. Today I
  name: TWIML AI
  position: 817
- category: unknown
  confidence: medium
  context: episode of the TWIML AI podcast. I am your host, Sam Charrington. Today
    I'm joined by Kunle Olukotun. Kunle is a p
  name: Sam Charrington
  position: 851
- category: unknown
  confidence: medium
  context: WIML AI podcast. I am your host, Sam Charrington. Today I'm joined by Kunle
    Olukotun. Kunle is a professor
  name: Today I
  position: 868
- category: unknown
  confidence: medium
  context: m your host, Sam Charrington. Today I'm joined by Kunle Olukotun. Kunle
    is a professor of electrical engineering a
  name: Kunle Olukotun
  position: 888
- category: unknown
  confidence: medium
  context: of electrical engineering and computer science at Stanford University and
    co-founder and chief technologist at SambaNov
  name: Stanford University
  position: 975
- category: tech
  confidence: high
  context: iversity and co-founder and chief technologist at SambaNova Systems. Before
    we get going, be sure to take a m
  name: Sambanova
  position: 1036
- category: unknown
  confidence: medium
  context: iversity and co-founder and chief technologist at SambaNova Systems. Before
    we get going, be sure to take a moment to
  name: SambaNova Systems
  position: 1036
- category: unknown
  confidence: medium
  context: ght? So there's a famous computer architect named Jim Smith. He designed
    Cray computers back in the day. And
  name: Jim Smith
  position: 4321
- category: unknown
  confidence: medium
  context: 'in different directions and you''ve got... Yeah.


    So I think the key difference is whether you have inst'
  name: So I
  position: 5535
- category: unknown
  confidence: medium
  context: 'ynchronization and access to memory. Both. Both.


    And I thought those were mutually exclusive positions.'
  name: And I
  position: 6678
- category: unknown
  confidence: medium
  context: '? So what you want is you want asynchrony, right? So GPUs are starting
    to get into this side, but we''ve tak'
  name: So GPUs
  position: 15777
- category: unknown
  confidence: medium
  context: ', but we''ve taken it to the extreme in data flow. As I said, it''s everything
    all at once, all the time,'
  name: As I
  position: 15869
- category: tech
  confidence: high
  context: bandwidth, but what goes into it when, you know, Meta comes out with a
    new model or a Quora or, you kno
  name: Meta
  position: 17963
- category: unknown
  confidence: medium
  context: a tensor, you say, am I going to tile the tensor? Am I going to parallelize
    a tensor? Am I going to shar
  name: Am I
  position: 19421
- category: tech
  confidence: high
  context: 'ey know how to do it.


    And what we''ve gotten very adept at doing this sort of thing, right? And fundament'
  name: Adept
  position: 20288
- category: unknown
  confidence: medium
  context: on our intermediate representation we call STEPs, Streaming Tensor Programs.
    And, you know, I think we're going to put it on
  name: Streaming Tensor Programs
  position: 31932
- category: tech
  confidence: high
  context: anguage, and create an ML library? So, of course, NVIDIA has CUDA, and
    then... Right? So what if you wante
  name: Nvidia
  position: 36153
- category: unknown
  confidence: medium
  context: '? Right? And the problem is, of course, if you... The LLM is not going
    to be very good at doing this becaus'
  name: The LLM
  position: 36362
- category: ai_application
  confidence: high
  context: AI-first tool for organizing ideas and making connections by uploading
    documents, mentioned by its co-founder Stephen Johnson.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in relation to Notebook LM (notebooklm.google.com) and historically
    via TensorFlow.
  name: Google
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Where Kunle Olukotun is a professor of electrical engineering and computer
    science.
  name: Stanford University
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Kunle Olukotun is the co-founder and chief technologist. They build reconfigurable
    data flow architectures (RDUs) optimized for AI computation, especially LLM inference.
  name: SambaNova Systems
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The conference where the host and guest last spoke in 2018, a major AI/ML
    research conference.
  name: NeurIPS
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the current favorite domain-specific language framework for
    designing ML algorithms, generating data flow graphs.
  name: PyTorch
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: An early domain-specific language predating PyTorch, used for similar ML
    algorithm design.
  name: Coma
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a framework that was in contention with PyTorch but has since
    'followed by the wayside' for general ML algorithm design.
  name: TensorFlow
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned via computer architect Jim Smith, who designed them, used as
    an analogy for building specialized hardware (vector computer for vector problems).
  name: Cray computers
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as an example of a reconfigurable chip (FPGA), used for comparison
    against SambaNova's coarser-grained approach.
  name: Xilinx
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used as a specific example of a large language model (7-billion parameter
    model) whose architecture (decoder structure) is mapped onto SambaNova's RDUs.
  name: Llama 2 7B
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a known trick used with GPUs to reduce memory bandwidth requirements,
    which SambaNova's architecture can achieve across an entire decoder.
  name: FlashAttention
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The company the speaker is associated with, developing RDU architecture
    for AI workloads.
  name: SambaNova
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a company that releases new models whose optimization needs
    to be mapped to the architecture.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company that releases new models whose optimization needs
    to be mapped to the architecture.
  name: Quora
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company whose model (with multi-headed latent attention)
    was quickly implemented on their system.
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company working with the speaker's company on low-latency,
    real-time voice use cases.
  name: ElevenLabs
  source: llm_enhanced
- category: ai_frameworks
  confidence: high
  context: Mentioned as an orchestration framework for agentic systems that the speaker's
    company is starting to support.
  name: CrewAI
  source: llm_enhanced
- category: ai_frameworks
  confidence: high
  context: Mentioned as an orchestration framework for agentic systems that the speaker's
    company is starting to support.
  name: AutoGen
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a fairly capable reasoning-based LLM (67-billion parameter)
    used in agentic systems for planning.
  name: DeepSeek R1
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in the context of CUDA, their established library/platform for
    programming specialized hardware/accelerators.
  name: NVIDIA
  source: llm_enhanced
date: 2025-10-14 19:39:00 +0000
duration: 58
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: write to one other thing about inference while we're on that subject,
    which is one of the things that we have, because we've got this high-capacity
    memory, is we can hold multiple models at the same time, up to 5 trillion parameters
    in total
  text: we should write to one other thing about inference while we're on that subject,
    which is one of the things that we have, because we've got this high-capacity
    memory, is we can hold multiple models at the same time, up to 5 trillion parameters
    in total.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: sequential instruction execution is, you know, it has a benefit
  text: the problem with sequential instruction execution is, you know, it has a benefit
    is that humans can understand what's going on, but the downside is it keeps things
    sequential, right? Sequential, right? So what you want is you want asynchrony,
    right? So GPUs are starting to get into this side, but we've taken it to the extreme
    in data flow.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN9142835882.mp3?updated=1760471537
processing_date: 2025-10-16 05:48:57 +0000
quotes:
- length: 115
  relevance_score: 4
  text: Now, so it turns out the architecture was very well suited for LLMs, fundamentally,
    especially for inference, right
  topics: []
- length: 181
  relevance_score: 4
  text: And so when you think about doing inference on a large language model, it's
    fundamentally constrained by the bandwidth to fetch the parameters and to fetch
    the KV cache from the HBM
  topics: []
- length: 273
  relevance_score: 4
  text: You talked about like the memory bandwidth limitation being a significant
    issue, and the way we usually do inference like with GPUs, we've got all these
    tricks like FlashAttention and speculative decoding and other things that are
    trying to help us overcome that limitation
  topics: []
- length: 140
  relevance_score: 4
  text: Along the lines of customization, you know, inference is clearly important,
    and in the big picture, we'll see a lot more of it than training
  topics: []
- length: 94
  relevance_score: 4
  text: So when we switched from inference, which is memory-bound, to training, which
    is compute-bound
  topics: []
- length: 103
  relevance_score: 4
  text: The LLM is not going to be very good at doing this because it doesn't have
    any training examples, right
  topics: []
- length: 71
  relevance_score: 3
  text: So now you have to reuse that hardware for different parts of the graph
  topics: []
- length: 40
  relevance_score: 3
  text: And so you have to be on the reconfigure
  topics: []
- length: 250
  relevance_score: 3
  text: Shortly after there, we saw a shift, a shift at SambaNova from like this theoretical
    computer that is scalable that you go figure out what you want to do with it to
    like, we're going to get really good at this LLM thing and make it available to
    folks
  topics: []
- length: 157
  relevance_score: 3
  text: As you know, in any computer system, you have to make sure that the critical
    resource is used 100% of the time because that's going to limit your performance
  topics: []
- length: 258
  relevance_score: 3
  text: Yeah, I remember thinking that, you know, in the early days of SambaNova,
    like, having to have enterprise engineers that have built their own custom models
    like manage that mapping process sounded like a difficult problem just from a
    go-to-market perspective
  topics:
  - market
- length: 81
  relevance_score: 3
  text: But even if you have to create a custom kernel, you can do it at the Python
    level
  topics: []
- length: 37
  relevance_score: 3
  text: And the problem is, of course, if you
  topics: []
- impact_reason: 'Explains the necessity of reconfigurability in DSAs: the model *is*
    the configuration, and models change frequently.'
  relevance_score: 10
  source: llm_enhanced
  text: So why does it need to be reconfigurable? Well, because you don't have one
    graph, right? Because every graph is your model. The graph is your model. It's
    how you change what the hardware does—you reconfigure it, right?
  topic: technical/architecture
- impact_reason: Clearly delineates the fundamental difference between traditional
    Von Neumann/instruction-set architectures and data flow/reconfigurable architectures.
  relevance_score: 10
  source: llm_enhanced
  text: So I think the key difference is whether you have instructions, right? You
    know, so is it instructions versus kernels, or instructions versus configuration,
    right? Do you fetch an instruction every cycle and decide what to do, or do you
    configure your hardware and leave it that way for the whole program?
  topic: technical/architecture
- impact_reason: Highlights a major performance and complexity bottleneck in traditional
    parallel computing (shared memory synchronization) that data flow architectures
    inherently avoid.
  relevance_score: 10
  source: llm_enhanced
  text: The other difference is kind of a technical issue, but also is very important,
    which is how do you synchronize the computation? Right? Do you have parallel computation,
    and the way that parallel computation typically works is the way that you communicate
    this through a single address space called shared memory, right? And whenever
    you do this, you need some sort of synchronization to make sure that the data
    that one thread put in the memory is ready for the next thread to go pick up,
    right? The locks, barriers, all this stuff. Reconfigurable data flow has none
    of that.
  topic: technical/synchronization
- impact_reason: Pinpoints the memory bandwidth bottleneck (especially HBM access
    for parameters and KV cache) as the primary constraint for LLM inference.
  relevance_score: 10
  source: llm_enhanced
  text: Inference. It's fundamentally constrained by the memory bandwidth, right?
    So I'm actually going to back up and say, okay, when I talked to you about 2018,
    this idea was pretty theoretical. Now we have five generations of chip... And
    so when you think about doing inference on a large language model, it's fundamentally
    constrained by the bandwidth to fetch the parameters and to fetch the KV cache
    from the HBM. That's the limit.
  topic: technical/inference
- impact_reason: 'The ultimate goal achieved: keeping computation local to avoid the
    HBM bottleneck by tiling the model structure across the available compute fabric.'
  relevance_score: 10
  source: llm_enhanced
  text: And no data ever goes between the different components of the decoder ever
    crosses the HBM accelerator boundary.
  topic: technical/optimization
- impact_reason: Connects their hardware approach to a known compiler optimization
    (kernel fusion), explaining how they achieve benefits similar to techniques like
    FlashAttention across the entire decoder.
  relevance_score: 10
  source: llm_enhanced
  text: This idea in the compiler land is known as fusion, right? So you've essentially
    created a single kernel, single fused kernel that encompasses the whole decoder,
    right?
  topic: technical
- impact_reason: 'Quantifies the benefit of their deep fusion strategy: achieving
    massive memory bandwidth reduction across the entire transformer block, not just
    the attention mechanism.'
  relevance_score: 10
  source: llm_enhanced
  text: We can go even further and fuse the whole decoder, right? So you get FlashAttention
    benefits across the whole decoder, right? Which is a huge reduction in memory
    bandwidth requirements.
  topic: technical
- impact_reason: Provides a quantitative metric (2-3x higher utilization) directly
    attributable to their asynchronous, data-flow execution model versus sequential
    instruction access.
  relevance_score: 10
  source: llm_enhanced
  text: And what we do is we can achieve a fraction of busy time that we can achieve
    on the RDUs two to three times higher than GPUs can achieve because we have this
    data flow idea of asynchronous execution of all the components, right?
  topic: technical
- impact_reason: Clearly defines the source of wasted bandwidth on GPUs (intermediate
    data movement between kernels) and how their architecture avoids it via on-chip
    data movement.
  relevance_score: 10
  source: llm_enhanced
  text: And so the idea is you don't move any data that you shouldn't have to across
    to using the HBM bandwidth. So you only use the HBM bandwidth for the absolutely
    necessary data, right? So a GPU puts intermediate data between the kernels that
    flows back and forth the GPU—that's wasted bandwidth, right?
  topic: technical
- impact_reason: Provides a benchmark utilization figure (up to 90%) that their architecture
    achieves on the critical resource (HBM bandwidth), contrasting it sharply with
    GPU performance.
  relevance_score: 10
  source: llm_enhanced
  text: And then you also make sure that that critical resource is used at the highest
    utilization that you can master, which is, you know, up to 90%, which GPUs cannot
    touch for multiple reasons.
  topic: technical
- impact_reason: Provides a specific, aggressive performance claim (10x faster than
    GPUs) for matrix-vector operations, which are crucial for auto-regressive inference.
  relevance_score: 10
  source: llm_enhanced
  text: We are optimized for both tokens per second because we have this kind of very
    low-latency capability for when you're doing a matrix-vector style computation,
    right? So we do that very quickly, and we're, you know, sort of ten X or faster
    than GPUs at that point.
  topic: technical
- impact_reason: 'Highlights a massive system-level advantage: the ability to host
    multi-trillion parameter capacity and switch between models in 1ms, enabling efficient
    multi-tenancy and custom model serving.'
  relevance_score: 10
  source: llm_enhanced
  text: We can hold multiple models at the same time, up to 5 trillion parameters
    in total. And because you've got... much higher bandwidth to the accelerator than
    you would have if you were trying to communicate over PCIe to the host. Now you
    can switch models in about a millisecond, right?
  topic: business/technical
- impact_reason: 'Demonstrates a key architectural advantage: ultra-low latency model
    switching, enabling high utilization in environments serving many fine-tuned,
    custom models.'
  relevance_score: 10
  source: llm_enhanced
  text: Now you can switch models in about a millisecond, right? And so you can think
    about, if you've got multiple custom models potentially, because you fine-tune
    the models, you can then switch between these models with very low latency.
  topic: technical/business
- impact_reason: Pinpoints agentic systems as a major emerging workload and explains
    how model switching provides a significant resource efficiency advantage over
    traditional siloed GPU setups.
  relevance_score: 10
  source: llm_enhanced
  text: Then the other use cases are sort of these agentic systems that require a
    large number of models to coexist at the same time, right? And then using this
    model-switching capability, we can support that with far fewer resources than
    you would be required if you had to put each of these models on a separate GPU-based
    system.
  topic: predictions/use cases
- impact_reason: 'Provides a forward-looking view on model evolution: increasing dynamism
    (MoE, variable context, graph nets) requires a shift in underlying hardware architecture.'
  relevance_score: 10
  source: llm_enhanced
  text: the models are fundamentally becoming more dynamic, right? So you're getting
    these mixture-of-experts style of models. You're getting environments where you've
    got multiple users with different context lengths... You are getting models like
    graph-based neural nets that have a kind of dynamic data access pattern.
  topic: technical/trends
- impact_reason: 'Defines the next generation of compute architecture research: Dynamic
    Reconfigurable Data Flow, drawing parallels to the historical shift from VLIW
    to dynamically scheduled CPUs.'
  relevance_score: 10
  source: llm_enhanced
  text: So that's where we're going from an architecture point of view. So, can you
    take the fundamental benefits of reconfigurable data flow architectures and put
    the "D" in front of it? Can you make them dynamic, right, with low overhead...
  topic: technical/strategy
- impact_reason: 'A candid admission from an experienced hardware vendor: compiler
    and software infrastructure remains the ''long pole'' and hardest part of building
    specialized AI systems.'
  relevance_score: 10
  source: llm_enhanced
  text: Even from SambaNova's point of view, the most difficult part of the whole
    endeavor has been delivering the compiler infrastructure for our systems.
  topic: business/lessons learned
- impact_reason: 'This highlights a critical limitation of current generative AI (LLMs)
    in the context of novel hardware development: they cannot bootstrap code/libraries
    for architectures they haven''t seen in training data.'
  relevance_score: 10
  source: llm_enhanced
  text: The LLM is not going to be very good at doing this because it doesn't have
    any training examples, right? This is a brand new architecture.
  topic: limitations
- impact_reason: 'This reveals the proposed solution methodology: using agentic systems
    and self-improvement loops, suggesting a novel, autonomous approach to solving
    the bootstrapping problem that LLMs cannot handle alone.'
  relevance_score: 10
  source: llm_enhanced
  text: And so we created an agentic system for doing this and an adaptive self-improving
    loop around that in order to implement the solution.
  topic: technical
- impact_reason: This is the core philosophical driver behind domain-specific architectures
    (DSAs) for AI, moving away from general-purpose instruction-driven computing.
  relevance_score: 9
  source: llm_enhanced
  text: So let's build a data flow computer, right? So let's build a computer that
    matches the data flows as represented by the algorithm in the hardware.
  topic: technical/architecture
- impact_reason: 'Defines the critical constraints and goals for modern AI hardware
    development: efficiency, speed, and scale for large models.'
  relevance_score: 9
  source: llm_enhanced
  text: Our focus is on how you do very efficient, fast inference on big models as
    energy efficiently as possible.
  topic: business/strategy
- impact_reason: 'Identifies key future workloads: massive models and multi-agent
    systems, emphasizing the need for low-latency switching.'
  relevance_score: 9
  source: llm_enhanced
  text: The focus has been on very large models, models with trillions of parameters
    that can fit in a single rack, on models that are good for agentic solutions where
    you've got hundreds of agents that potentially need to work together and switch
    between these agents with very low latency...
  topic: predictions/technical
- impact_reason: A powerful analogy summarizing the principle of building hardware
    tailored to the computational paradigm (vector processing for vector problems,
    data flow for ML graphs).
  relevance_score: 9
  source: llm_enhanced
  text: And his favorite quote that I often use is, you know, if you have a vector
    problem, and most of the problems are vector problems, then you should build a
    vector computer, right? So, we have a data flow problem, right? So let's build
    a data flow computer, right?
  topic: strategy/architecture
- impact_reason: Describes the token-based synchronization mechanism used in data
    flow computing, which is inherently deadlock-free and efficient for asynchronous
    data dependencies.
  relevance_score: 9
  source: llm_enhanced
  text: And the way that you do the synchronization is by using data, using data flow
    tags, right? You've got hardware mechanisms that essentially use tokens, which
    determine when data is ready, right?
  topic: technical/synchronization
- impact_reason: Addresses the challenge of mapping extremely large models (which
    is common today) onto finite hardware resources, necessitating dynamic reconfiguration.
  relevance_score: 9
  source: llm_enhanced
  text: Eventually you're going to get a graph that is bigger than the system that
    you're trying to map it on. So now you have to reuse that hardware for different
    parts of the graph.
  topic: technical/scaling
- impact_reason: 'Validates the data flow approach specifically for the current dominant
    AI workload: large-scale inference.'
  relevance_score: 9
  source: llm_enhanced
  text: Now, so it turns out the architecture was very well suited for LLMs, fundamentally,
    especially for inference, right? And so inference was something that we saw was
    really going to be a bottleneck.
  topic: business/strategy
- impact_reason: 'This is the core optimization goal: maximizing data locality and
    minimizing off-chip memory traffic.'
  relevance_score: 9
  source: llm_enhanced
  text: So what you want to do then is make sure that you don't send any extraneous
    data that should not be necessary over that highly constrained, that critical
    resource, that HBM bandwidth, right?
  topic: technical/optimization
- impact_reason: Provides a concrete example of mapping a complex ML structure (a
    decoder block) onto multiple hardware units (RDUs) to maximize parallelism and
    locality.
  relevance_score: 9
  source: llm_enhanced
  text: And so reconfigurable data flow does this by, you know, you think go back
    to your streaming idea, right? So if you take an LLM model, right? Let's take
    a simple one like Llama 2 7B, right? 7-billion parameter model. It's got these
    decoders... What you can do is that you can take that whole decoder and map it
    onto a set of 16 RDE chips in space.
  topic: technical/mapping
- impact_reason: Connects the hardware mapping strategy directly to established compiler
    optimization techniques (kernel fusion), validating the approach within existing
    ML software paradigms.
  relevance_score: 9
  source: llm_enhanced
  text: So now, of course, this idea in the compiler land is known as fusion, right?
    So you've essentially created a single kernel, single fused kernel that encompasses
    the whole decoder, right?
  topic: technical/compiler
- impact_reason: Introduces the core hardware unit (RDU) of their novel architecture,
    which is central to their performance claims.
  relevance_score: 9
  source: llm_enhanced
  text: We call the reconfigurable data flow architecture and the chip, we call a
    reconfigurable data flow unit, RDU.
  topic: technical
- impact_reason: Reiterates the consensus bottleneck in LLM inference, setting the
    stage for why their architecture is designed the way it is.
  relevance_score: 9
  source: llm_enhanced
  text: The performance limiter for inference is, as we've just discussed, HBM bandwidth
    for the parameters and the KV cache, right?
  topic: technical
- impact_reason: Offers a philosophical critique of traditional instruction-based
    computing (like GPUs) versus data-flow computing, prioritizing performance (asynchrony)
    over human readability.
  relevance_score: 9
  source: llm_enhanced
  text: The problem with sequential instruction execution is, you know, it has a benefit
    is that humans can understand what's going on, but the downside is it keeps things
    sequential, right? Sequential, right? So what you want is you want asynchrony,
    right?
  topic: strategy/technical
- impact_reason: Explains that their architecture inherently solves the prefetching
    problem by decoupling execution units from memory access rates, a significant
    simplification over traditional approaches.
  relevance_score: 9
  source: llm_enhanced
  text: I want to... It comes naturally. It comes naturally from once you kind of
    design things in this data flow manner, you don't have to prefetch, right? Because
    you naturally have prefetching going on because that unit accessing the HBM is
    working independently at the rate at which the HBM can go, right? You don't have
    to prefetch.
  topic: technical
- impact_reason: 'Addresses the go-to-market concern: rapid adaptation to new models
    (even variants like DeepSeek) is possible without requiring low-level CUDA programming,
    lowering the barrier for adoption.'
  relevance_score: 9
  source: llm_enhanced
  text: And as long as they're basically transformer-based models, then we can do
    this fairly quickly. But even if you have to create a custom kernel, you can do
    it at the Python level. You don't ever have to write kernel—I should say CUDA.
  topic: business/technical
- impact_reason: Quantifies the superior latency retention under high throughput (large
    batch sizes), directly addressing the typical latency-throughput trade-off that
    plagues GPUs.
  relevance_score: 9
  source: llm_enhanced
  text: And what you see on an SN40L-based system is, yes, the latency increases [with
    batch size], but it doesn't increase nearly as much. We're still kind of 5X better
    in terms of latency, even at these very, very high throughputs.
  topic: technical
- impact_reason: Breaks down inference latency into queuing and serialization components,
    arguing that their architecture eliminates the serialization latency inherent
    in serial batch processing on GPUs.
  relevance_score: 9
  source: llm_enhanced
  text: The queuing latency you're going to pay, but the question is, so there are
    two components to the latency. There's one is the queuing latency, and then in
    the GPU, you know, as a fair way to think about it, there's like a serialization
    latency because the batches process serially. And then in your case, the batches
    process in parallel.
  topic: technical
- impact_reason: 'Provides clear business justification for their fast model switching
    capability: maximizing hardware utilization in multi-tenant or multi-custom-model
    serving environments.'
  relevance_score: 9
  source: llm_enhanced
  text: If you dedicated a GPU to a custom model, then the utilization of that GPU
    might be low. But if you can switch between these models with very low latency,
    then you can keep the overall utilization of your system high, even though you're
    supporting a bunch of custom models.
  topic: business
- impact_reason: Clarifies the fundamental computational bottleneck difference between
    inference (memory-bound) and training (compute-bound), which dictates architectural
    optimization strategies.
  relevance_score: 9
  source: llm_enhanced
  text: the benefit of training or even pre-fill is it's basically compute-bound,
    right? So when we switched from inference, which is memory-bound, to training,
    which is compute-bound.
  topic: technical
- impact_reason: Highlights the critical importance of ecosystem support (orchestration
    frameworks like CrewAI/AutoGen) for deploying complex agentic AI, moving beyond
    just hardware/model support.
  relevance_score: 9
  source: llm_enhanced
  text: it's not just supporting the models, it's now supporting the frameworks that
    orchestrate the models. Yeah. So frameworks like CrewAI and AutoGen, and so we're
    starting to work with these partners to support the ability to develop these agentic
    workflows on our system.
  topic: strategy/ecosystem
- impact_reason: 'Articulates the core architectural challenge for future systems:
    enabling low-latency, partial reconfiguration to handle dynamic model structures
    efficiently.'
  relevance_score: 9
  source: llm_enhanced
  text: How about if you fundamentally think about something that's much more dynamic,
    right? That can change with much lower latency, or can change just a portion of
    it, right?
  topic: technical
- impact_reason: 'Introduces a novel, high-impact optimization for agentic systems:
    caching the high-level plan generated by the reasoning model, not just the context.'
  relevance_score: 9
  source: llm_enhanced
  text: What we want to do is sort of how well could we use caching to cache that
    plan, right? So that it could be reused by subsequent uses. And so it turned out
    to be a pretty good idea, and we got significant improvements, right?
  topic: technical/strategy
- impact_reason: 'Crucial distinction in caching strategy for complex workflows: caching
    semantic plans over raw context improves reuse across similar high-level tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: you can't just cache the result. You've got to cache something that's got
    more semantic content to it than that. And it ends up being different from like
    context caching, which is very broad.
  topic: technical
- impact_reason: 'Presents a concrete example of using AI (an agentic, self-improving
    loop) to solve a traditionally hard engineering problem: automated compiler/library
    generation for new hardware.'
  relevance_score: 9
  source: llm_enhanced
  text: So we created an agentic system for doing this and an adaptive self-improving
    loop around that in order to implement the solution [creating an ML library for
    a new architecture].
  topic: technical/strategy
- impact_reason: 'This clearly defines the goal: creating a new ML library that integrates
    seamlessly with established frameworks like PyTorch, a crucial step for hardware
    adoption.'
  relevance_score: 9
  source: llm_enhanced
  text: So what if you wanted to create a new library for, for example, that could
    be targeted by PyTorch for your new architecture?
  topic: technical
- impact_reason: Highlights a practical, immediate application of AI (RAG/personal
    knowledge management) that abstracts away complexity for the end-user (authors/information
    workers).
  relevance_score: 8
  source: llm_enhanced
  text: Upload your documents and Notebook LM instantly becomes your personal expert,
    uncovering insights and helping you brainstorm.
  topic: business/application
- impact_reason: Connects the high-level ML framework (PyTorch) directly to the underlying
    computational model (data flow graph), which is essential for hardware mapping.
  relevance_score: 8
  source: llm_enhanced
  text: What you get out of PyTorch, when you write your algorithm in PyTorch, what
    you get is what computer scientists would call a data flow graph, right?
  topic: technical/model representation
- impact_reason: Distinguishes advanced data flow architectures from FPGAs based on
    the granularity and speed of configuration, suggesting a much higher level of
    abstraction.
  relevance_score: 8
  source: llm_enhanced
  text: You don't think FPGA because it's not, you know, you don't program it with
    a hardware description. That's what I'm thinking of, like the timescale of utility
    of configuration is what I'm trying to get at.
  topic: technical/architecture
- impact_reason: Provides a concrete metric for the speed of configuration, making
    the concept of dynamic hardware mapping practical for ML workloads.
  relevance_score: 8
  source: llm_enhanced
  text: Yeah, yeah. So it's much, much quicker, right? So you're on the order of microseconds,
    right, to reconfigure.
  topic: technical/performance
- impact_reason: Suggests that by keeping computation local (fused), the reliance
    on high-speed, expensive, external interconnects for intra-model communication
    is reduced.
  relevance_score: 8
  source: llm_enhanced
  text: Meaning, meaning you don't need, like, InfiniBand or some super low-latency
    interconnects.
  topic: technical/interconnect
- impact_reason: 'Sets up the comparison: their architecture doesn''t eliminate optimization
    tricks but executes them with vastly superior efficiency.'
  relevance_score: 8
  source: llm_enhanced
  text: The question that I was going to ask was, do you not have to do these kinds
    of things [like FlashAttention], or you can do them on steroids, right?
  topic: technical
- impact_reason: 'Details the compiler''s role in mapping models: tiling, parallelizing,
    and sharding tensors across the RDU fabric, mirroring manual optimization efforts.'
  relevance_score: 8
  source: llm_enhanced
  text: The way that you do things is that you need to parallelize within the chip,
    right, by splitting up the tensors. You might want to reduce the size of things
    by tiling, right? Just by taking a tile of computation, thinking back to what
    you had to do manually in something like FlashAttention, right? And then you also
    want to think about how you shard the computation across multiple chips, right?
  topic: technical
- impact_reason: A strategic prediction confirming the industry shift towards prioritizing
    inference efficiency and deployment over raw training scale for most applications.
  relevance_score: 8
  source: llm_enhanced
  text: inference is clearly important, and in the big picture, we'll see a lot more
    of it than training.
  topic: predictions
- impact_reason: Identifies real-world, high-value use cases (like real-time voice)
    that specifically demand the low-latency capabilities of the system.
  relevance_score: 8
  source: llm_enhanced
  text: we're seeing a lot of use cases that are very latency-sensitive, right? So
    real-time voice is one that seems to be something that we've been working with
    ElevenLabs and a couple of other companies on, and they really care about low
    latency.
  topic: business/use cases
- impact_reason: Provides a clear, structural breakdown of how modern agentic systems
    operate (Reasoning LLM creates plan -> Agents execute plan using specialized tools/models).
  relevance_score: 8
  source: llm_enhanced
  text: you've got kind of two components, right, which is you're given some high-level
    tasks that you want to perform, and you give that task to a fairly capable reasoning-based
    LLM... and it comes up with a plan, right? And then that plan gets executed by
    a bunch of agents that may access the web, but may invoke a bunch of different
    specialized LLMs.
  topic: use cases/technical
- impact_reason: 'Underlines the fundamental driver for specialization in hardware:
    energy constraints, leading to a proliferation of domain-specific accelerators.'
  relevance_score: 8
  source: llm_enhanced
  text: accelerators are proliferating for both ML and for other kinds of problems
    because, as you know, everything is energy-constrained, right? And so the only
    way to get around this has become more specialized.
  topic: strategy/hardware trends
- impact_reason: Addresses the limitations of current LLMs (lack of search capability)
    when applied to complex, combinatorial problems like compiler optimization.
  relevance_score: 8
  source: llm_enhanced
  text: The question then is, could you use AI to make this problem easier? And, you
    know, LLMs are very capable, but they're not as capable as you might think, right?
    In particular, they can't really do any search of the space, right?
  topic: safety/limitations
- impact_reason: 'This sets up the core technical problem: bridging a new hardware
    architecture (via its low-level language) to a high-level ML framework (like PyTorch).'
  relevance_score: 8
  source: llm_enhanced
  text: re, as represented by some low-level representation that we call an architecture-specific
    programming language, and create an ML library?
  topic: technical
- impact_reason: Provides a necessary bridge for the audience to understand the scale
    of the hardware unit being discussed (RDU vs. GPU).
  relevance_score: 7
  source: llm_enhanced
  text: So you can think of an RDU as analogous to a GPU in some sense.
  topic: technical/architecture
- impact_reason: Summarizes the tensor-centric approach of their compiler for optimizing
    model mapping onto the specialized hardware.
  relevance_score: 7
  source: llm_enhanced
  text: It's all about the tensors. You take a tensor, you say, am I going to tile
    the tensor? Am I going to parallelize a tensor? Am I going to shard the tensor?
    And then that's the way you get them mapped.
  topic: technical
- impact_reason: Signals the release of novel, foundational research (STEPs) that
    underpins their approach to handling dynamic computations.
  relevance_score: 7
  source: llm_enhanced
  text: We've just submitted a paper, and it's on our intermediate representation
    we call STEPs, Streaming Tensor Programs.
  topic: technical
- impact_reason: 'This summarizes the engineering challenge: mapping specialized hardware
    functions into a usable ML library abstraction.'
  relevance_score: 7
  source: llm_enhanced
  text: So you want to create your new ML library, and you've got your new domain-specific
    architecture with specialized functions, and you want to create your new ML library.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: Dataflow Computing for AI Inference with Kunle Olukotun
  - #751


  This episode features Sam Charrington interviewing **Kunle Olukotun** (Professor
  at Stanford and Co-founder/Chief Technologist at SambaNova Systems) about the evolution
  of computer architecture, specifically focusing on **reconfigurable data flow architectures**
  and their application to accelerating large-scale AI inference.


  ### 1. Focus Area

  The primary focus is on **Dataflow Computing for AI Inference**. The discussion
  centers on how architectures that directly map the computational graph of ML models
  (derived from frameworks like PyTorch) can overcome the inherent bottlenecks of
  traditional instruction-based architectures (like GPUs) when running massive models
  (trillions of parameters) and agentic workloads.


  ### 2. Key Technical Insights

  *   **Reconfigurable Data Flow Architecture:** This paradigm shifts from fetching
  sequential instructions to **configuring hardware** (coarser-grained tensor units,
  or RDUs) to directly match the model''s data flow graph. This eliminates the need
  for complex synchronization mechanisms like locks and shared memory, relying instead
  on **data flow tags/tokens** for low-overhead communication.

  *   **HBM Bandwidth Optimization via Fusion:** The architecture is specifically
  designed to minimize reliance on external memory bandwidth (HBM), which is the primary
  bottleneck in LLM inference. By **fusing** large sections of the model (e.g., an
  entire decoder block, going beyond techniques like FlashAttention) onto the chip
  fabric, intermediate data remains local, maximizing the utilization of the critical
  HBM interface (up to 90% utilization, significantly higher than GPUs).

  *   **Asynchronous Execution and Latency Management:** The data flow approach naturally
  supports extreme asynchrony, allowing computation and memory access to be heavily
  overlapped. This results in superior performance across the latency-throughput trade-off:
  significantly lower latency even at high batch sizes compared to GPUs, where serialization
  latency dominates.


  ### 3. Business/Investment Angle

  *   **Inference as the Bottleneck:** The conversation highlights that efficient,
  fast, and energy-efficient inference for trillion-parameter models is the critical
  commercial challenge in the current AI landscape.

  *   **Multi-Model Serving Efficiency:** SambaNova''s architecture, leveraging high-capacity
  on-chip memory (1.5TB DDR), allows systems to hold multiple large models simultaneously
  and switch between them in milliseconds (around 1ms). This is crucial for multi-tenancy
  and serving specialized, fine-tuned models efficiently.

  *   **Domain-Specific Compiler Advantage:** While the architecture is specialized,
  the compiler abstracts complexity. Since most modern models are transformer-based,
  adapting to new models (like DeepSeek) takes engineers only about a week, using
  Python-based descriptions rather than low-level CUDA programming.


  ### 4. Notable Companies/People

  *   **Kunle Olukotun:** Pioneer in multicore architecture and parallel programming,
  now leading the charge in dataflow hardware at SambaNova.

  *   **SambaNova Systems:** The company implementing these concepts, currently shipping
  the **SN40L** chip (100B transistors, 5nm), which features three memory tiers including
  high-capacity DDR.

  *   **Jim Smith (Cray Architect):** Quoted to establish the principle: "If you have
  a vector problem, build a vector computer." Applied here: If you have a data flow
  problem, build a data flow computer.


  ### 5. Future Implications

  The conversation suggests a future where hardware architecture is increasingly **tightly
  coupled to the computational graph** of the dominant ML workloads (transformers).
  The industry is moving away from general-purpose instruction-set architectures (ISAs)
  for AI acceleration toward highly specialized, reconfigurable fabrics that prioritize
  data locality and asynchronous execution to solve the memory wall problem inherent
  in scaling LLMs.


  ### 6. Target Audience

  **AI/ML Infrastructure Engineers, Computer Architects, Hardware Designers, and Technology
  Strategists** focused on optimizing large-scale LLM deployment and next-generation
  accelerator design.'
tags:
- artificial-intelligence
- ai-infrastructure
- startup
- generative-ai
- google
- meta
- nvidia
title: 'Dataflow Computing for AI Inference with Kunle Olukotun - #751'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 61
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 59
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 2
  prominence: 0.2
  topic: startup
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 1
  prominence: 0.1
  topic: generative ai
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-16 05:48:57 UTC -->
