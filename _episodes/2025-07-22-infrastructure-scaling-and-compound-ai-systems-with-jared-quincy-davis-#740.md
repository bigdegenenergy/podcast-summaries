---
companies:
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the Twimmel AI podcast. I
    am your host, Sam Charrington. Today,
  name: Twimmel AI
  position: 658
- category: unknown
  confidence: medium
  context: pisode of the Twimmel AI podcast. I am your host, Sam Charrington. Today,
    I'm joined by Jared Quincy Davis. Jared i
  name: Sam Charrington
  position: 694
- category: unknown
  confidence: medium
  context: your host, Sam Charrington. Today, I'm joined by Jared Quincy Davis. Jared
    is founder and CEO of Foundry. Before we g
  name: Jared Quincy Davis
  position: 732
- category: unknown
  confidence: medium
  context: round is I'm the CEO of a company called Foundry. My Foundry, Foundry is
    basically essentially a new cloud bui
  name: My Foundry
  position: 1412
- category: unknown
  confidence: medium
  context: a new cloud built from scratch for my workloads. And I think we've been
    trying to increase the amount of
  name: And I
  position: 1506
- category: tech
  confidence: high
  context: ies to do the types of things that currently only OpenAI and Anthropic
    can do, and we've been approaching
  name: Openai
  position: 1707
- category: tech
  confidence: high
  context: he types of things that currently only OpenAI and Anthropic can do, and
    we've been approaching that primarily
  name: Anthropic
  position: 1718
- category: unknown
  confidence: medium
  context: searcher primarily. I was at DeepMind on the core Deep Learning team, thinking
    a lot about how to scale deep lear
  name: Deep Learning
  position: 2413
- category: unknown
  confidence: medium
  context: nd then also to my PhD work, I'm at Stanford with Matei Haria and Yuri
    Leskovich, kind of also at the intersect
  name: Matei Haria
  position: 2643
- category: unknown
  confidence: medium
  context: my PhD work, I'm at Stanford with Matei Haria and Yuri Leskovich, kind
    of also at the intersection of systems and
  name: Yuri Leskovich
  position: 2659
- category: unknown
  confidence: medium
  context: ally a great method that one of my collaborators, Alex Demakis, came up
    with. And it's, he calls it Leconic Deco
  name: Alex Demakis
  position: 3450
- category: unknown
  confidence: medium
  context: Alex Demakis, came up with. And it's, he calls it Leconic Decoding. It's
    really nice. So in this case, they kind of
  name: Leconic Decoding
  position: 3500
- category: tech
  confidence: high
  context: n for, oh well, there is this space, this kind of meta space of networks
    of networks, and you can think
  name: Meta
  position: 10117
- category: tech
  confidence: high
  context: eneration, invoking implicitly or explicitly this notion of a frontier,
    Gemini is now very explicit about
  name: Notion
  position: 12591
- category: unknown
  confidence: medium
  context: And that's something that when my collaborators, Ling Zhao, got early work
    on stretching back to 2023, 2022,
  name: Ling Zhao
  position: 14418
- category: unknown
  confidence: medium
  context: iting use cases. You see it in languages as well. The QN models from Alibaba
    are better at idiomatic Chine
  name: The QN
  position: 17116
- category: unknown
  confidence: medium
  context: relatively shallow, they're not that high rather. But I think when you
    project into a more agentic world,
  name: But I
  position: 17507
- category: unknown
  confidence: medium
  context: this work that my colleagues, Linjiao, led called LLM Selector, which basically
    was asking the question of if yo
  name: LLM Selector
  position: 19227
- category: unknown
  confidence: medium
  context: a pretty biased way of looking. I think the term Compound Systems is a
    little bit nebulous still, and there's a few
  name: Compound Systems
  position: 23803
- category: tech
  confidence: high
  context: nnected via NVLink, maybe you want an NVL72, like NVIDIA has a new system,
    kind of the big box. There's an
  name: Nvidia
  position: 26065
- category: unknown
  confidence: medium
  context: guous work going on, a lot of contiguous systems. So I've got a lot of
    stranded stuff. So if you have a
  name: So I
  position: 28079
- category: unknown
  confidence: medium
  context: ideas come strongly to mind here. One is in this Armor LLM Calls All You
    Need paper, we were able to characterize what you're d
  name: Armor LLM Calls All You Need
  position: 42485
- category: unknown
  confidence: medium
  context: t talk about kind of parallelizing that judgment. Like I was envisioning,
    you can, turtles all the way dow
  name: Like I
  position: 49176
- category: tech
  confidence: high
  context: . And sometimes just like in the search case with Google, you actually
    have some workloads like MapReduce
  name: Google
  position: 66200
- category: ai_application
  confidence: high
  context: Mentioned as a reasoning model whose longer thinking time can lead to incorrect
    answers (DeepSeek R1 specifically mentioned). Also mentioned as an open-source
    model provider.
  name: DeepSeek
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The company founded by the guest, Jared Quincy Davis. It is described as
    a new cloud built from scratch for AI workloads, focusing on infrastructure and
    compound AI architectures.
  name: Foundry
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a frontier model provider whose capabilities Foundry aims
    to make accessible to more companies. Also mentioned regarding their model selector
    issue and GPT-3.5/GPT-4 options.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned alongside OpenAI as a frontier model provider whose capabilities
    Foundry aims to make accessible to more companies.
  name: Anthropic
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The guest previously worked at DeepMind on the core Deep Learning team,
    focusing on scaling deep learning approaches.
  name: DeepMind
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The guest completed his PhD work at Stanford, focusing on the intersection
    of systems, ML theory, and ML.
  name: Stanford
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as the guest's PhD advisor at Stanford.
  name: Matei Haria
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as the guest's PhD advisor at Stanford.
  name: Yuri Leskovich
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a provider with multiple generations of models (Pro, Flash)
    and explicit about defining a performance frontier.
  name: Gemini
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Likely referring to Google Cloud/Gemini models. Mentioned as starting the
    trend of releasing model families (Pro, Flash) and focusing on agentic coding.
  name: Cloud
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned in the context of model generations alongside Opus (likely referring
    to Anthropic's Claude models, though 'Hiku' is not a standard public name, it's
    grouped with Claude 3 context).
  name: Hiku
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of Claude 3 generation, implying a high-tier model.
  name: Opus
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an open-source model provider, specifically referencing Llama.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as Meta's open-source model family.
  name: Llama
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an open-source model provider.
  name: Grok
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the company behind the Grok model.
  name: XAI
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as a high-cost model (implied to be a frontier model, possibly
    Claude Opus or a high-tier Gemini model, given the context of $150/million tokens).
  name: O1 Pro
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned alongside O1 Pro, likely a typo or shorthand for a specific model
    tier being compared to DeepSeek R1.
  name: R1 Pro
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned for their QN models which are better at idiomatic Chinese, indicating
    domain-specific model development.
  name: Alibaba
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a specific, high-performing model (likely OpenAI's) used in
    comparisons within the LLM Selector work.
  name: GPT-4
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a specific model used in comparisons within the LLM Selector
    work (likely Anthropic's).
  name: Claude 3
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in the context of their hardware systems (NVLink, NVL72) being
    relevant for vertically scaled inference workloads.
  name: NVIDIA
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a classical example of horizontal inference scaling (spinning
    up a million replicas) for coding tasks.
  name: AlphaCode
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced in the context of scaling laws, relating model size and training
    data efficiency.
  name: Chinchilla
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Invoked historically regarding system paradigms and distributed computation,
    relevant to large-scale ML infrastructure.
  name: MapReduce
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a desired frontier model that researchers are aiming for in
    terms of quality and reliability.
  name: GPT-6
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a desired frontier model that researchers are aiming for in
    terms of quality and reliability.
  name: GPT-7
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A research paper discussed, detailing the characterization of ensemble
    methods (best-of-N) and verifier-based judges.
  name: Networks of Networks
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A framework created by the speaker's team to simplify the construction
    and execution of 'networks of networks' architectures.
  name: Ember
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the historical inspiration for Ember, serving as the standard
    framework for searching the neural network space in the 2010s.
  name: Torch
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned because some contributors to Ember were original contributors
    to Ray, linking Ember to distributed computing frameworks.
  name: Ray
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned because some contributors to Ember were original contributors
    to Spark, linking Ember to distributed computing frameworks.
  name: Spark
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a framework whose ergonomics Ember aims to emulate for network-of-network
    constructions.
  name: JAX
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a framework whose ergonomics Ember aims to emulate for network-of-network
    constructions.
  name: PyTorch
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as an entity (university or company group) currently using the
    Ember framework for research.
  name: Clibberter
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A research paper discussed that characterized the 'most common of N' ensemble
    aggregation method.
  name: Armor LLM Calls All You Need
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company pushing the serverless idea in the AI cloud space,
    similar to Foundry.
  name: Modal
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company focusing on a more traditional abstraction layer
    for AI workloads, similar to Foundry.
  name: Fireworks
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Likely referring to Jensen Huang, CEO of NVIDIA, as the context discusses
    different compute regimes (latency vs. throughput) often discussed in hardware/infrastructure
    contexts.
  name: Jensen
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Referenced in the context of search workloads (MapReduce and indexing)
    done explicitly to speed up serving time (inference).
  name: Google
  source: llm_enhanced
date: 2025-07-22 16:00:00 +0000
duration: 73
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: go down, a point around co-design, systems and algorithm co-design, to
    give a little bit of intuition for why it starts to get interesting and why, from
    my perspective, at the cloud and scheduling level, this is really fascinating
  text: we should go down, a point around co-design, systems and algorithm co-design,
    to give a little bit of intuition for why it starts to get interesting and why,
    from my perspective, at the cloud and scheduling level, this is really fascinating.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: the compound systems will be a single model, I think I basically point
    people to at least the CPU versus the GPU to say, at least there'll be a couple
    of different poles. There'll at least be that kind of perhaps small model, highly
    distilled with big models, at least that type of pairing, if not something even
    richer. So yeah, it's a, I think it goes back to this cap theme that I'm going
    to push, which
  text: the future of the compound systems will be a single model, I think I basically
    point people to at least the CPU versus the GPU to say, at least there'll be a
    couple of different poles. There'll at least be that kind of perhaps small model,
    highly distilled with big models, at least that type of pairing, if not something
    even richer. So yeah, it's a, I think it goes back to this cap theme that I'm
    going to push, which is one, basic research is back and there's a lot of work
    to do.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN2657285858.mp3?updated=1753151737
processing_date: 2025-10-05 00:22:19 +0000
quotes:
- length: 274
  relevance_score: 4
  text: And also depending on how long these thinking traces can be when the model
    gets stuck or on average, the distribution of tokens, but if you risk to do early
    stopping over these 10 replicas, you actually might produce fewer total output
    tokens and actually end up saving cost
  topics: []
- length: 253
  relevance_score: 4
  text: And so I would think that a challenge that one might want to address is finding
    ways to use these compositional networks of LLMs to get better average performance
    on all tasks as opposed to doing really well on some benchmark task or something
    like that
  topics: []
- length: 96
  relevance_score: 4
  text: The best-of-N case, I might want a million GPUs for 400 milliseconds to do
    these inference calls
  topics: []
- length: 148
  relevance_score: 4
  text: But essentially, I think there's a lot of things that people talk about, which
    is, hey, will more of the work be inference or more of it be training
  topics: []
- length: 156
  relevance_score: 4
  text: Rather than using that during training data to train a single model, you can
    imagine that system itself being something you just do at inference time, right
  topics: []
- length: 195
  relevance_score: 4
  text: And that we've seen this in the last few years of deep learning where the
    GPU is very much explicitly described as a co-design system where there's elements
    of what level of precision do you need
  topics: []
- length: 201
  relevance_score: 3
  text: It's kind of a bit-or-less-in type notion, it scales a lot of assimilates
    compute so effectively, and so as Moore's law progressed, deep learning is one
    of the methods that could take advantage of that
  topics: []
- length: 184
  relevance_score: 3
  text: I think that for now, the one primitive or the one idea I'd encourage people
    to think of is just ensembles or best-of-N, as a reduction, as the community gets
    more and more intelligent
  topics: []
- length: 190
  relevance_score: 3
  text: I think we're in the early era, it's kind of like 2010 deep learning, where
    a million flowers were blooming and the ideas, the stuff was just starting to
    work, and people were really excited
  topics: []
- impact_reason: Highlights the triple win (speed, accuracy, cost reduction) achievable
    through this simple parallelization/early-stopping heuristic, challenging standard
    assumptions about parallel compute costs.
  relevance_score: 10
  source: llm_enhanced
  text: It's also higher accuracy given the point that we just mentioned, on average.
    But then also, very surprisingly, it can actually be potentially even cheaper
    on average, which is like, wait, how would that be if you're spinning up 10 replicas?
  topic: business
- impact_reason: 'Quantifies the breakthrough: achieving 9%+ gains on hard benchmarks
    via composition, dwarfing the typical 1% generational improvements seen in standard
    model updates.'
  relevance_score: 10
  source: llm_enhanced
  text: with this work in the paper called Networks of Networks, we looked at the
    extent to which you can just combine, compose architectures out of calls to a
    frontier model and push the frontier, push the quality frontier. And we found
    that on tasks that were highly verifiable, you could push the frontier quite dramatically.
    We saw 9% plus gains on these really, really hard sticky benchmarks, where typically
    changes across generations of models were like 1%.
  topic: technical
- impact_reason: A visionary statement suggesting a paradigm shift where model complexity
    is measured not by static parameter count, but by the dynamic number of orchestrated
    inference calls.
  relevance_score: 10
  source: llm_enhanced
  text: you can start to potentially in the distance envision creating architectures
    where when we say 8B is no longer the number of parameters, it's the number of
    inference calls in some kind of inference-guest system.
  topic: predictions
- impact_reason: Provides concrete, dramatic figures illustrating the cost dispersion
    (a 50x difference), which is the core driver for advanced routing and system design
    research.
  relevance_score: 10
  source: llm_enhanced
  text: all-on-pro was $150 per million tokens, whereas DeepSeek R1 was $3 per million
    tokens.
  topic: business/technical
- impact_reason: Moves beyond simple routing to complex model orchestration (e.g.,
    using a cheap model for pre-processing/reframing to save cost on the expensive
    model), signaling the rise of LLM systems engineering.
  relevance_score: 10
  source: llm_enhanced
  text: what if I can combine models? And can I combine calls to R1 Pro? Or can I
    even do kind of funky things where maybe I have some, so on pro, where maybe I
    even do rich things where I send something to R1 initially, get it to reframe
    the question, so I'll use fewer tokens from O1 Pro, something like that?
  topic: technical/strategy
- impact_reason: Introduces the concept of 'model affinity'—that models specialize
    in certain domains (e.g., coding, writing) due to training choices, meaning no
    single model is universally best.
  relevance_score: 10
  source: llm_enhanced
  text: even at the frontier, the things are very jagged. Number one, and number two,
    the affinities of respective providers and models are starting to become clear
    that these different systems have their own affinities.
  topic: technical/strategy
- impact_reason: Presents empirical evidence (from the LLM Selector work) that model
    composition/hybrid systems outperform monolithic systems, even when the monolithic
    system uses the single best available model for every step.
  relevance_score: 10
  source: llm_enhanced
  text: Will you do better by mixing these models to do different parts of that pipeline,
    or by taking the best model, the model that did the best on all steps of the pipeline?
    And it turns out that the hybrid system does better.
  topic: technical/research
- impact_reason: Provides a taxonomy of optimization levers available in complex LLM
    systems (Compound Systems), showing that optimization extends far beyond prompt
    engineering to include routing, architecture, and model weights.
  relevance_score: 10
  source: llm_enhanced
  text: You can optimize prompts over fixed models and a fixed structure, kind of
    DSP-style... You can optimize the weights of a verifier or a router. You can optimize
    model selection. You can optimize the architecture of the non-network of networks.
  topic: technical/strategy
- impact_reason: Illustrates the 'horizontal scaling' approach (best-of-N/sampling)
    using massive parallelism, contrasting sharply with vertical scaling and emphasizing
    the need for filtering protocols.
  relevance_score: 10
  source: llm_enhanced
  text: There's another case, which is kind of more that AlphaCode, kind of more classical
    thing, which was maybe one of the other ways that you can invoke more compute
    from time is horizontal inference time scaling. But I want to instead say, I'm
    going to spin up a million replicas... and I'm going to ask each coding question
    to each replica, right? And I'm going to get a million candidate responses.
  topic: technical
- impact_reason: 'Provides a specific example of co-design feedback loop: inference
    demands drive training strategies (distillation/over-training) to optimize lifetime
    cost, potentially violating standard scaling laws.'
  relevance_score: 10
  source: llm_enhanced
  text: if you're doing a lot of inference, then you might actually want to train
    your model so that they're cheaper to inference, meaning to distill them, meaning
    over-train a small model, kind of pushing the Chinchilla scaling laws somewhere
    they shouldn't go, or they weren't initially intended to go.
  topic: technical/business
- impact_reason: 'Explains why Transformers succeeded over LSTMs: their dense matrix
    operations map perfectly onto existing GPU architectures optimized for graphics.'
  relevance_score: 10
  source: llm_enhanced
  text: The same thing with the methods within deep learning that have been really
    ascended, just a super-dense transformer attention computation versus kind of
    a meter LSTM type of thing that's not as efficient at using the fact that we have
    these really, really dense parallel systems that are initially designed for graphics
    in some sense that are really, really good at these kind of dense matrix operations,
    etc.
  topic: technical/strategy
- impact_reason: 'Defines the critical ''demo-to-real-time'' transition challenge:
    shifting focus from cherry-picked successes to robustness against failure modes
    (bad examples).'
  relevance_score: 10
  source: llm_enhanced
  text: they're also trying to cross that demo-to-real-time chasm where you start
    being judged not by the outlier good examples, but by the outlier bad examples,
    they're trying to make sure that everything works.
  topic: safety/business
- impact_reason: 'This is a major conceptual leap: moving from deep neural networks
    (DNNs) to ''deep networks of networks'' (compound systems/ensembles), suggesting
    a new paradigm for complex AI architecture that mimics hierarchical structures.'
  relevance_score: 10
  source: llm_enhanced
  text: you could say, maybe the judge itself should be a network, or maybe this whole
    little primitive of an ensemble plus judge should be a node within another network
    with a judge. It's kind of trees, right? And so it starts to get pretty rich and
    you start to have deep networks of networks instead of deep neural networks, you
    know, denons, etc.
  topic: technical/predictions
- impact_reason: A bold prediction about the near-future evolution of AI architecture,
    moving beyond single monolithic models to complex, structured, multi-component
    systems.
  relevance_score: 10
  source: llm_enhanced
  text: I think there'll be more and more research and over the next months and years,
    I think it'll start to get wild over high of multi-billion parameter networks
    of networks with very intricate structure.
  topic: predictions
- impact_reason: Strong argument against the 'single monolithic model' future, predicting
    a necessary diversity of architectures, exemplified by the CPU/GPU split, likely
    involving small/distilled vs. large/powerful pairings.
  relevance_score: 10
  source: llm_enhanced
  text: I think when it comes to that question of, well, the future of the compound
    systems will be a single model, I think I basically point people to at least the
    CPU versus the GPU to say, at least there'll be a couple of different poles. There'll
    at least be that kind of perhaps small model, highly distilled with big models,
    at least that type of pairing, if not something even richer.
  topic: predictions/strategy
- impact_reason: 'A critical strategic insight: the next major bottlenecks and breakthroughs
    in AI will likely come from systems engineering (infrastructure, deployment, architecture)
    rather than purely algorithmic innovation.'
  relevance_score: 10
  source: llm_enhanced
  text: the problems that are kind of upstream of deep learning are largely systems
    problems.
  topic: strategy
- impact_reason: Provides a concrete, massive cost-saving metric (12x to 20x) achievable
    by optimizing for flexible, non-immediate batch workloads common in research and
    training.
  relevance_score: 10
  source: llm_enhanced
  text: we've been able to, for certain types of workloads, cut the cost by 12 to
    20 X, particularly for workloads that are amenable to running in a preemptible
    fashion or being checkpointed or running in a heterogeneous way, running in a
    batch mode where they just need six hours within the next 12 hours and they don't
    care which six hours.
  topic: business/technical
- impact_reason: This highlights a critical, non-obvious limitation in current reasoning
    models (like DeepSeek) where excessive 'thinking time' can degrade performance,
    suggesting issues with search paths or backtracking in their inference process.
  relevance_score: 9
  source: llm_enhanced
  text: In this case, they kind of know the quirk of reasoning models, like DeepSeek,
    like DeepSeek R1, which is that all other things being equal, the longer that
    they think for a given problem, the more likely it is that they're going to get
    the answer wrong, which is kind of counterintuitive in some ways.
  topic: technical
- impact_reason: Introduces the core concept of 'compound AI systems' and emphasizes
    the necessity of co-designing infrastructure alongside future model architectures.
  relevance_score: 9
  source: llm_enhanced
  text: we're pretty big believers in AI systems co-design and we've been thinking
    a lot about what type of infrastructure will need in the future for what we think
    the future models and architectures will be. And so we've been talking about the
    theme of compound AI systems, trying to push this and promulgate this in the world.
  topic: technical
- impact_reason: 'Explains the mechanism for cost savings: early stopping prevents
    models from generating long, expensive, and potentially incorrect output traces.'
  relevance_score: 9
  source: llm_enhanced
  text: if you risk to do early stopping over these 10 replicas, you actually might
    produce fewer total output tokens and actually end up saving cost.
  topic: business
- impact_reason: Provides a staggering quantitative claim regarding cost efficiency
    gains achievable by routing/combining cheaper models to match frontier performance.
  relevance_score: 9
  source: llm_enhanced
  text: We've seen gains of over 1,000 X, which sounds kind of absurd. But you can
    have the same performance at a much lower cost.
  topic: business
- impact_reason: 'Clearly defines the scope where composition methods excel: tasks
    where external verification (like running code or checking a proof) is easier
    than generation itself.'
  relevance_score: 9
  source: llm_enhanced
  text: And by highly verifiable, would an example of that be like code generation?
    Code generation, math, these things where the model's pace is better than its
    execution, or where it's easier to check an answer than it has to generate an
    answer.
  topic: technical
- impact_reason: Provides a concrete, aggressive trend line for inference cost reduction
    (10x per year), which underpins the feasibility of inference-heavy compound architectures.
  relevance_score: 9
  source: llm_enhanced
  text: The first is that the cost of inference has been falling precipitously for
    several years, and it's clear that the cost is continuing to go down roughly 10x
    per year for the last three years.
  topic: business
- impact_reason: 'Quantifies the economic opportunity arising from model diversity:
    the cost gap between frontier and efficient models is huge, enabling new optimization
    strategies.'
  relevance_score: 9
  source: llm_enhanced
  text: across both providers, but also across models within a single provider family,
    we've seen this massive dispersion emerge where the difference in cost between
    the most expensive model or system, should I say, and the cheaper system is quite
    wide, there's a massive gulf.
  topic: business/technical
- impact_reason: 'Articulates the fundamental strategy for cost optimization: intelligent
    routing based on task difficulty and model capability saturation.'
  relevance_score: 9
  source: llm_enhanced
  text: routing is a basic idea, you'd say, okay, if it's an easy question, then easy,
    cheap models are already saturating that level of intelligence, so I'll just send
    it there.
  topic: strategy/business
- impact_reason: Suggests that the current era of model cost dispersion and complexity
    is reviving fundamental research areas (like system design and orchestration)
    that were previously overshadowed by pure scaling efforts.
  relevance_score: 9
  source: llm_enhanced
  text: basic research comes back into play, things get exciting again. It's not just
    the most, plus a more farm-up scaling things up. Things start to get rich from
    research, reflective, academics contribute, and it becomes a really dynamic ecosystem
    again.
  topic: strategy/research
- impact_reason: Warns that small performance disparities between models become critically
    important and compound rapidly in complex, multi-step agentic workflows.
  relevance_score: 9
  source: llm_enhanced
  text: If you have many, many steps or long horizon tasks, these small differences
    become big differences. It's like that fun example, one percent difference per
    day over a year, you start to see big, big, big goals emerge.
  topic: predictions/safety
- impact_reason: 'Sets a strategic goal for future AI development: optimizing for
    robust, high *average* performance across diverse tasks using compositional systems,
    rather than maximizing performance on narrow benchmarks.'
  relevance_score: 9
  source: llm_enhanced
  text: I would think that a challenge that one might want to address is finding ways
    to use these compositional networks of LLMs to get better average performance
    on all tasks as opposed to doing really well on some benchmark task or something
    like that.
  topic: strategy
- impact_reason: Points to the necessity of co-designing the AI algorithm (how models
    interact) alongside the underlying cloud/scheduling infrastructure for maximum
    efficiency and performance gains.
  relevance_score: 9
  source: llm_enhanced
  text: a point around co-design, systems and algorithm co-design, to give a little
    bit of intuition for why it starts to get interesting and why, from my perspective,
    at the cloud and scheduling level, this is really fasc[inating].
  topic: technical/strategy
- impact_reason: Highlights the critical importance of co-designing hardware/systems
    infrastructure alongside algorithms, a key strategic consideration for future
    AI scaling.
  relevance_score: 9
  source: llm_enhanced
  text: I'll just note, and this is a broader branch, maybe we should go down, a point
    around co-design, systems and algorithm co-design, to give a little bit of intuition
    for why it starts to get interesting and why, from my perspective, at the cloud
    and scheduling level, this is really fascinating.
  topic: strategy
- impact_reason: Clearly defines the 'vertical scaling' approach, linking long CoT/reasoning
    tasks directly to the need for large, highly interconnected, memory-rich systems
    (like NVL72).
  relevance_score: 9
  source: llm_enhanced
  text: Imagine two very different systems. One system is kind of a vertical inference
    time scaling, like you're basically producing longer and longer chains of thought
    to improve reasoning capabilities... you want a scaled-up system. You want more
    GPU memory so you can keep all that in context.
  topic: technical
- impact_reason: 'Defines the infrastructure requirements for horizontal scaling:
    massive parallelism for short bursts, decoupling from high-speed interconnects,
    which has major implications for cloud scheduling and cost.'
  relevance_score: 9
  source: llm_enhanced
  text: The best-of-N case, I might want a million GPUs for 400 milliseconds to do
    these inference calls. I don't care if they're interconnected. They can be part
    of different, fit-of-end domains.
  topic: technical/business
- impact_reason: 'Direct business/operational insight: Different AI workload patterns
    (vertical vs. horizontal) necessitate differentiated pricing and scheduling strategies.'
  relevance_score: 9
  source: llm_enhanced
  text: One of the things that's cool for us is saying, okay, to what extent like
    these things should probably be priced differently. They should be scheduled differently.
  topic: business
- impact_reason: Draws a powerful historical analogy, suggesting the current shift
    in AI infrastructure mirrors the transition from monolithic supercomputers to
    flexible distributed clusters.
  relevance_score: 9
  source: llm_enhanced
  text: This transition over a prior era from supercomputing to distributed systems...
    And now you've got people running a lot of the things that used to require these
    supercomputing systems on Kubernetes clusters on highly distributed independent
    systems.
  topic: strategy
- impact_reason: 'Offers a fundamental strategic explanation for Deep Learning''s
    dominance: its inherent scalability matched the progression of Moore''s Law and
    dense parallel hardware.'
  relevance_score: 9
  source: llm_enhanced
  text: I think deep learning overall, part of the reason deep learning is the ascended
    paradigm is because it scales so well. It's kind of a bit-or-less-in type notion,
    it scales a lot of assimilates compute so effectively...
  topic: strategy
- impact_reason: Predicts that specialization (vertical vs. horizontal) will lead
    to 'ecological niches' for different workloads, reducing competition and potentially
    lowering costs for niche tasks.
  relevance_score: 9
  source: llm_enhanced
  text: I think we are going to see, to the extent that people all converge on one
    approach, vertical and sometimes scaling... I think we're going to see that actually
    opens up, creates as long as there's a system that can actually manifest this...
    if there's a lot of continuous blocks of work that actually frees up a lot of
    gaps, and if any type of workload that can fill those gaps, it kind of is occupying
    an ecological niche to itself, right, and isn't facing competition in the same
    way, and that can be cheaper.
  topic: predictions/business
- impact_reason: 'Actionable advice for ML engineers: use ensembles/best-of-N as the
    immediate, practical primitive for improving reliability.'
  relevance_score: 9
  source: llm_enhanced
  text: for now, the one primitive or the one idea I'd encourage people to think of
    is just ensembles or best-of-N, as a reduction, as the community gets more and
    more intelligent.
  topic: business/technical
- impact_reason: 'Highlights a key secondary benefit of ensembles: using internal
    disagreement as a heuristic signal for model uncertainty or hallucination.'
  relevance_score: 9
  source: llm_enhanced
  text: You can use the disagreement between ensemble members to get some sense of,
    okay, is this thing hallucinating or not? You can kind of, and that's already
    a pretty good place to start...
  topic: safety/technical
- impact_reason: A strong call to action for the research community, comparing the
    current state to the early, fertile period of deep learning (2010-2012), suggesting
    accessibility for non-frontier labs.
  relevance_score: 9
  source: llm_enhanced
  text: I feel like it's 2012 again, but in this new kind of networks of networks
    setting, not neural networks. And so I encourage people to say, hey, 2010 again,
    like it's, guys, that was a fun time, that was 2010s. Yeah, like, yeah, get excited.
    Do research. If you're an academic, you can push the frontier now. You don't need
    to be at a frontier lab.
  topic: strategy
- impact_reason: 'Highlights a key architectural insight: verification/judgment in
    complex systems (like ensembles or ensembles of ensembles) doesn''t require the
    same computational power as generation, offering significant cost savings if soundness
    can be mathematically guaranteed.'
  relevance_score: 9
  source: llm_enhanced
  text: Yeah, you can definitely use much cheaper models for verification as long
    as that kind of soundness, completeness story is correct.
  topic: technical
- impact_reason: Connects the architectural choice of compound systems directly to
    known LLM limitations (context window constraints and quality degradation), suggesting
    hierarchical decomposition as a solution.
  relevance_score: 9
  source: llm_enhanced
  text: And so you start to think about, oh, there's loss in the middle dynamics,
    right? We know that these models have somewhat limited context, right? And so
    you can't feed an infant context length, or even if you do, quality diminishes.
    So maybe you do want to break things up and have kind of smaller domains where
    fewer generators are going to a single judge...
  topic: technical
- impact_reason: Provides a strong historical analogy, framing the current state of
    compound AI systems research as equivalent to the early perceptron era of deep
    learning, signaling massive potential growth ahead.
  relevance_score: 9
  source: llm_enhanced
  text: I think we're in those very, very, very early days still. A lot of the work
    that's been so far is like those early days in like perceptron research and well,
    peer research, we had like these wide but relatively shallow networks and they
    had relatively few parameters, and then over time, we scaled them up.
  topic: strategy/predictions
- impact_reason: 'Outlines a potential future workflow: Evolve a complex system to
    solve a problem, then distill the learned behavior into a simpler, deployable
    artifact (model or smaller network).'
  relevance_score: 9
  source: llm_enhanced
  text: as opposed to training a model, you evolve a network of networks, and then
    you distill that network of networks down to a simpler either model or network
    of networks.
  topic: strategy/technical
- impact_reason: 'Provides a clear definition and the core benefit of speculative
    decoding: achieving provable speedups without sacrificing output quality by using
    a hybrid small/large model system.'
  relevance_score: 9
  source: llm_enhanced
  text: speculative coding... a small draft or model and then a big verifier model.
    And the idea is you can get greater efficiency... provable that you're not losing
    quality, but you're definitely getting an expected improvement in latency...
  topic: technical
- impact_reason: Describes a form of self-improvement/bootstrapping common in Chain-of-Thought
    (CoT) research, where complex reasoning traces are distilled into a more efficient,
    single-step model.
  relevance_score: 9
  source: llm_enhanced
  text: let me go ahead and do multiple steps, multiple step rollouts of the current
    state-of-the-art model... And then let me take that entire thinking trace and
    use that as my training data to train the next generation of the model to then
    try to produce that whole ten-step raising trace in one step.
  topic: technical/strategy
- impact_reason: Uses the success of ASICs and GPUs as evidence that co-designing
    hardware specifically for an AI task (like parallelism) is crucial for cost efficiency,
    supporting the argument for specialized compound AI systems.
  relevance_score: 9
  source: llm_enhanced
  text: I think that even at the hardware side, I don't think people appreciate how
    all ASICs work, application-specific integrated circuits, how all these, when
    you have a task in mind, if you can design the chip for it and end, how all this
    type of things works. And I think that's why the kind of compound systems or system
    of models or managerive models type of vision. I think the GPU in some sense is
    a proof point that application-specific system built in a co-design way with an
    end use in mind can really bend the curve on cost.
  topic: business/strategy
- impact_reason: Reframes deep learning's success not just as algorithmic superiority,
    but as a fundamental property of its efficient assimilation of massive compute
    resources.
  relevance_score: 9
  source: llm_enhanced
  text: compute is really a root node problem for ML. As it, I was very much kind
    of an algorithms deep learning person, but it's pretty clear that part of the
    reason that deep learning is the ascended paradigm in the first place is because
    of the way it assimilates compute.
  topic: strategy
- impact_reason: Critiques the existing cloud infrastructure model for failing to
    meet the unique demands of modern deep learning workloads, thereby stifling innovation.
  relevance_score: 9
  source: llm_enhanced
  text: the cloud as you currently know it, the algorithms that can model was not
    well suited to deep learning. And it was really going to near our creativity,
    limit the types of things that we could do.
  topic: business/strategy
- impact_reason: 'Pinpoints the specific failure of traditional cloud computing in
    the AI era: the promise of true elasticity and on-demand scaling is broken when
    dealing with large, bursty ML workloads.'
  relevance_score: 9
  source: llm_enhanced
  text: The whole promise of the cloud of elasticity on demand, infinite compute on
    demand, on tap, not needing to capacity plan, that whole notion was not being
    fulfilled in the AI cloud context.
  topic: business
- impact_reason: This succinctly captures the fundamental driver behind deep learning's
    success—its ability to leverage massive computational resources, positioning compute
    assimilation as the core enabler.
  relevance_score: 9
  source: llm_enhanced
  text: deep learning is the ascended paradigm in the first place is because of the
    way it assimilates compute.
  topic: technical/strategy
- impact_reason: Introduces a novel economic model for compute allocation based on
    user-defined value/urgency, moving beyond fixed hourly rates.
  relevance_score: 9
  source: llm_enhanced
  text: you set a price for your workload and you're essentially this is how much
    it's worth to you and run it when you can. Like that's kind of an interesting
    concept and there are a bunch of other marketplace-oriented ideas built into the
    way you're approaching things.
  topic: business/strategy
- impact_reason: 'Quantifies the trade-off: massive cost savings (10x cheaper) for
    batch jobs are highly desirable, even if it means utilizing otherwise idle capacity
    created by others'' flexibility.'
  relevance_score: 9
  source: llm_enhanced
  text: If you can tell me that you could do that 10 times cheaper, then if I need
    it done, just in one shot, I'm like, yeah, sure. That's totally great. If other
    people can do their work and there's 10 times more effective capacity on the cluster
    because of that, that sounds awesome. Why not?
  topic: business/strategy
- impact_reason: Applies the concept of heterogeneous compute needs to AI agents,
    distinguishing between real-time operational tasks (low latency) and background
    research/development tasks (asynchronous).
  relevance_score: 9
  source: llm_enhanced
  text: And so I think that there's this heterogeneous, same thing with these agent
    cases. They're going to be things like operator where I want to be really fast,
    like it's way too slow. I want it to be as quick as possible, go and perform a
    task in real-time, etc. And they have other things that I'll launch async that
    can run in the background, like deep research or like the kind of async coding
    updates, etc., as long as it's high fidelity, right?
  topic: predictions/technical
- impact_reason: 'Defines the mission of Foundry: democratizing advanced AI capabilities
    by focusing on specialized infrastructure, targeting the gap currently held by
    major players like OpenAI and Anthropic.'
  relevance_score: 8
  source: llm_enhanced
  text: Foundry is basically essentially a new cloud built from scratch for my workloads.
    And I think we've been trying to increase the amount of AI progress in the world
    and trying to make it slightly easier for a broader spectrum of companies to do
    the types of things that currently only OpenAI and Anthropic can do, and we've
    been approaching that primarily from an infrastructure perspective.
  topic: business
- impact_reason: Describes a practical, low-complexity technique (parallel sampling
    with early stopping) to leverage model quirks for performance improvement, a key
    element of compound systems.
  relevance_score: 8
  source: llm_enhanced
  text: one very simple method that I came up with to leverage this fact and this
    observation was to basically create 10 replicas of the reasoning model, you know,
    five, 10 replicas, and then just return the response from the first replica to
    complete thinking.
  topic: technical
- impact_reason: Emphasizes that significant performance gains can be achieved through
    inference-time architectural changes (meta-strategies) rather than solely through
    massive model training.
  relevance_score: 8
  source: llm_enhanced
  text: You didn't have to train a new model. You didn't spend a billion dollars.
    No, it didn't take three months. That's kind of inspiring.
  topic: strategy
- impact_reason: Suggests that for verifiable tasks, performance gains via composition
    are theoretically unbounded (though practically limited), reinforcing the power
    of inference-time orchestration.
  relevance_score: 8
  source: llm_enhanced
  text: if you're willing to spend a lot of capital, make many parallel calls under
    some pretty, then you can actually push the frontier almost arbitrarily far on
    tasks that are highly verifiable.
  topic: predictions
- impact_reason: Quantifies the massive historical cost reduction needed (1000x) to
    make current frontier performance economically viable, setting the stage for future
    architectural innovation.
  relevance_score: 8
  source: llm_enhanced
  text: The cost of inference has been falling precipitously for several years, and
    it's clear that the cost is continuing to go down roughly 10x per year for the
    last three years, a thousand X to achieve some kind of baseline level of performance,
    kind of like say GPT-4 level performance on something like a MMLU.
  topic: business
- impact_reason: Describes the 'Cambrian explosion' of available models, moving from
    a duopoly to a diverse ecosystem, which is essential for model routing and compound
    system strategies.
  relevance_score: 8
  source: llm_enhanced
  text: The ecosystem, in addition to this cost piece, the ecosystem is a lot broader
    than it used to be. A couple of years ago, there were really only GPT-3.5 and
    GPT-4 options. Now... there's this whole model selector issue... Gemini has multiple
    generations... Claude 3 generation... open-source model providers, Meta with Llama,
    DeepSeek, it's Grok and XAI.
  topic: strategy
- impact_reason: Highlights the immediate complexity and proliferation of model choices
    even within a single major provider (OpenAI), signaling a shift from monolithic
    models to diverse model families.
  relevance_score: 8
  source: llm_enhanced
  text: Now, even just within OpenAI, there's this whole model selector issue people
    joke about. There's so many different models.
  topic: technical/strategy
- impact_reason: Emphasizes the renewed importance and effectiveness of model distillation
    for achieving extreme cost reduction while maintaining task-specific performance.
  relevance_score: 8
  source: llm_enhanced
  text: things like distillation work extremely well. We've known this for quite a
    while. I think people are starting to appreciate better and better just how much
    you can, if you have a specific task you care about in mind, just how much you
    can reduce the cost and preserve quality and reliability, just how dramatically
    you can push that, how extreme it can be.
  topic: technical/business
- impact_reason: Highlights the use of model hyperparameters (like temperature) as
    an optimization dimension, specifically for generating diverse outputs for ensemble
    decision-making.
  relevance_score: 8
  source: llm_enhanced
  text: You can also change the hyperparameters of the model calls. You can say I'm
    going to change the temperature. Like maybe it's the same ensemble of GPT-4, and
    I'm changing the generator temperature to elicit greater diversity among their
    responses that I can then use.
  topic: technical
- impact_reason: Reiterates the excitement in the research community, noting that
    new system-level research can be conducted effectively at small scale, unlike
    massive pre-training efforts.
  relevance_score: 8
  source: llm_enhanced
  text: there's a rich space here. I think people can see it now a little bit. And
    basic research is back in a new way, and you can do experiments at small scale,
    developing intuition, because some of these approaches, they just are a small
    scale.
  topic: strategy/research
- impact_reason: Indicates that research into model composition is already yielding
    actionable frameworks (LLM Selector) capable of automating the optimization of
    hybrid LLM pipelines.
  relevance_score: 8
  source: llm_enhanced
  text: LLM Selector actually is now a framework that has some algorithms in there
    that can automatically help optimize this. That's right, they can search the space.
  topic: technical/business
- impact_reason: 'Defines the infrastructure requirements for vertical scaling: low
    latency, high contiguity, and single-domain allocation.'
  relevance_score: 8
  source: llm_enhanced
  text: The vertically scaled case, I want a highly interconnected system for 10 continuous
    minutes, and everything should be part of the same domain. It's one continuous
    block of compute.
  topic: technical
- impact_reason: 'Explains the dynamic resource allocation challenge: utilizing stranded
    or underutilized compute capacity by matching it with appropriately structured
    workloads.'
  relevance_score: 8
  source: llm_enhanced
  text: It's like, hey, there's a lot of continuous compute blocks already allocated
    that I can't disrupt. I can fit less contiguity requiring workloads in between,
    etc. So you can nudge to use this broader pool of resources really efficiently.
  topic: business/strategy
- impact_reason: Challenges the binary focus on 'training vs. inference' dominance,
    suggesting they are interdependent and will naturally balance.
  relevance_score: 8
  source: llm_enhanced
  text: I think there's a pendulum, right? And almost it's almost like a competition.
    There's almost like a deeper way to explain this. But essentially, I think there's
    a lot of things that people talk about, which is, hey, will more of the work be
    inference or more of it be training? And I find a lot of these questions, I don't
    think they're the right questions.
  topic: strategy
- impact_reason: Strong encouragement for cross-disciplinary learning between systems
    engineering and ML research, signaling high opportunity at the intersection.
  relevance_score: 8
  source: llm_enhanced
  text: If you're a systems and ML person, there's never been a better time to be
    alive. And I think for people who are on one side or the other, I'd encourage
    them to try to get more familiar with the other side of that.
  topic: business/strategy
- impact_reason: Directly links the desire for frontier performance to the practical
    application of ensemble methods using the best available models.
  relevance_score: 8
  source: llm_enhanced
  text: In that case, they want GPT-6 early, they want GPT-7 early. I think one of
    the really simple ideas if people can try to employ is just calling the frontier
    models multiple times and trying to combine the answer, at least on those verifiable
    paths.
  topic: technical
- impact_reason: 'Quantifies the trade-off: Best-of-N reliably improves performance
    but inherently increases computational cost.'
  relevance_score: 8
  source: llm_enhanced
  text: a very simple ensemble or best-of-N structure with generators and a judge
    will push the frontier pretty reliably. Obviously, it will increase the cost as
    well.
  topic: technical/business
- impact_reason: A practical, cost-driven observation. Simple binary feedback (thumbs
    up/down) is inherently cheaper in terms of inference cost (fewer output tokens)
    than generating complex text responses for judgment.
  relevance_score: 8
  source: llm_enhanced
  text: thumbs up, thumbs down also, it's nice because tokens are cheaper than output
    tokens, you know?
  topic: business/technical
- impact_reason: Introduces the concept of automated design for compound AI systems,
    linking it to Neural Architecture Search (NAS) but applied to the higher level
    of network composition.
  relevance_score: 8
  source: llm_enhanced
  text: neural architecture design is an idea that comes to mind here, like where
    it's network of network design, where it's a highly automated process...
  topic: technical/strategy
- impact_reason: Identifies infrastructure and cost as the primary bottlenecks currently
    preventing deeper research into complex, distributed AI architectures (like networks
    of networks).
  relevance_score: 8
  source: llm_enhanced
  text: partly because I think the infrastructure doesn't support doing richer research,
    like the lanes is going to kill you if you try to make a thousand calls to GPT-4
    with four loops nested four loops or something. It's going to get pretty ugly,
    pretty quickly, and also it's going to get pretty expensive pretty quickly.
  topic: business/strategy
- impact_reason: Explicitly names 'speculative coding' and 'distillation of networks
    of networks' as two existing, albeit nascent, examples of compound AI systems.
  relevance_score: 8
  source: llm_enhanced
  text: One is speculative coding, and the other is what you just mentioned, of kind
    of distillation of networks of networks.
  topic: technical
- impact_reason: A call to action for practitioners, emphasizing that the future of
    AI advancement lies at the intersection of systems engineering, ML research, and
    hardware co-design.
  relevance_score: 8
  source: llm_enhanced
  text: it's never been a more interesting time to do co-design and to have both the
    systems and ML or engineering and research perspective on these things.
  topic: strategy
- impact_reason: A sharp critique of the current 'AI cloud' experience, where users
    are burdened with infrastructure complexity instead of being able to focus purely
    on application development.
  relevance_score: 8
  source: llm_enhanced
  text: Almost all the complexity of infrastructure is being posted onto the users
    themselves as opposed to the cloud handling it for you so you can just focus on
    making your beard taste better.
  topic: business
- impact_reason: A strong concluding statement calling for a complete overhaul of
    cloud infrastructure specifically tailored to the needs of ML/AI workloads.
  relevance_score: 8
  source: llm_enhanced
  text: You need to really fundamentally rethink the way cloud works, reinvigilate
    these systems,
  topic: strategy
- impact_reason: 'Differentiates strategic approaches in the MLOps/Cloud space: Foundry
    focuses on the *economic* model (pricing/scheduling), while others focus on abstraction
    (serverless/API wrapper).'
  relevance_score: 8
  source: llm_enhanced
  text: where I would characterize Foundry as really pushing innovation in the economic
    interface of inference, whereas these other companies are more like Modal is pushing
    the serverless idea, Fireworks is like more traditional, like you were just going
    to abstract the call and you use it in a similar way.
  topic: business/strategy
- impact_reason: Clearly defines the need for low-latency, interactive compute for
    active research/debugging, contrasting it with long-running batch jobs.
  relevance_score: 8
  source: llm_enhanced
  text: In the life of a researcher, I have some kind of verification, training, and
    debugging workloads I'm doing throughout the day as a researcher. And in that
    context, if I have a five-minute, 10-minute, 20-minute, one-hour queuing time
    on the cluster, I'm very unhappy. I want it to be snappy.
  topic: strategy/technical
- impact_reason: Defines the 'asynchronous batch' use case where cost optimization
    through scheduling flexibility is highly valued over speed.
  relevance_score: 8
  source: llm_enhanced
  text: I have a workload that I set off at 8 p.m., it's a typical researcher, and
    I come back the next morning at 10 a.m. I want it done in that 14-hour span. I
    don't really care when you finish the workload.
  topic: strategy/technical
- impact_reason: Uses the Tetris analogy to describe the complexity of efficient resource
    packing across diverse AI workload requirements (latency vs. throughput vs. batch).
  relevance_score: 8
  source: llm_enhanced
  text: I think you need an infrastructure, you need a cloud that can handle these
    inference regimes efficiently, pack them all. It's kind of like playing Tetris.
  topic: strategy/technical
- impact_reason: Provides an accessible analogy for the counterintuitive finding about
    reasoning models, linking model inference time to human cognitive efficiency under
    pressure.
  relevance_score: 7
  source: llm_enhanced
  text: think about a student on an exam. If it's the same set of exam questions and
    one student's taking three hours on the first question and one student wrapped
    up in 20 minutes, probably the person who wrapped up in between then has a better
    score on the exam.
  topic: strategy
- impact_reason: Acknowledges the practical limits to the 'arbitrarily far' claim,
    grounding the theory in the quality of the verifier and the diversity (avoiding
    mode collapse) of the generator outputs.
  relevance_score: 7
  source: llm_enhanced
  text: It is also counterintuitive that you can push this idea arbitrarily far. I
    would think that there would be diminishing returns, right? Yeah, there definitely
    are definitely plateaus at some point, depending on the fidelity of your verifier,
    for example, depending on the fidelity of your verifier, also depending on the
    distribution or variation of responses at your generators.
  topic: technical
- impact_reason: Contrasts early predictions of extreme centralization with the current
    reality of model proliferation, validating the viability of decentralized, composite
    AI solutions.
  relevance_score: 7
  source: llm_enhanced
  text: there was a period of time where we thought that there would be only three
    model providers in the world because it was just such a herculean effort to train
    these, and now we've got this Cambrian explosion of models for every need.
  topic: predictions
- impact_reason: Shows that major providers are explicitly segmenting their offerings
    based on the quality/cost trade-off, reinforcing the need for systems that can
    intelligently navigate these trade-offs.
  relevance_score: 7
  source: llm_enhanced
  text: Gemini is now very explicit about that notion of a frontier, whether you want
    high quality and reliability, or whether you want low latency and low cost, a
    Gemini model will give you the best.
  topic: business
- impact_reason: A cautionary note on the cost and complexity of prompt optimization
    when applied across large, multi-step systems, suggesting other optimization paths
    might be more tractable.
  relevance_score: 7
  source: llm_enhanced
  text: Optimizing prompts can get pretty expensive pretty quickly, even for shallow
    pipelines, if you try to do that over some large network of calls, that might
    be really difficult depending on your approach and how you're searching, right?
  topic: business/technical
- impact_reason: Acknowledges the terminological ambiguity around 'Compound Systems'
    and signals the importance of distinguishing it from 'tool use,' suggesting tool
    use is a separate, critical component of advanced AI systems.
  relevance_score: 7
  source: llm_enhanced
  text: I think the term Compound Systems is a little bit nebulous still, and there's
    a few ideas that are being jumbled into this banner. I think that another idea
    that comes up a lot is kind of tool use, which we haven't talked about.
  topic: technical/strategy
- impact_reason: 'Insight into the current maturity curve of AI adoption: most users
    prioritize reliability/functionality over cost optimization.'
  relevance_score: 7
  source: llm_enhanced
  text: I think most people aren't trying to optimize costs yet in my biased view,
    they're not yet at that state, they're trying to get it to work, they're trying
    to make their reliability good.
  topic: business
- impact_reason: Draws a parallel between compound systems and standard Reinforcement
    Learning (RL) pipelines, where multiple rollouts are used to generate high-quality
    training data for a single policy update.
  relevance_score: 7
  source: llm_enhanced
  text: The other search talking about, which is more this distilling and networking
    networks. I think we're kind of already doing this with RL when we have many,
    many replicas and kind of a standard RL pipeline, you spend at many replicas,
    you ask them to do questions, they all produce candidate responses, maybe one
    of them's correct, and that's the trace, that's the rollout that you end up training
    on...
  topic: technical
- impact_reason: Confirms the willingness of users to pay a premium for guaranteed
    low latency/immediate access when required for interactive tasks.
  relevance_score: 7
  source: llm_enhanced
  text: I'm going to pay you a lot more than the typical rate of a GPU to get it faster,
    for example.
  topic: business
- impact_reason: Shows how strategic upfront batch processing (pre-computation) can
    enable faster, cheaper inference serving later, linking training/batch optimization
    to serving efficiency.
  relevance_score: 7
  source: llm_enhanced
  text: you can have a still model or a really fast model or just a retriever actually
    at inference time because you did all this work upfront pre-computing things.
  topic: technical/strategy
- impact_reason: This unfinished thought implies that true innovation in AI cloud
    scheduling involves dynamic resource manipulation (splitting, deleting, reforming
    compute blocks) to eliminate fragmentation and inefficiency.
  relevance_score: 7
  source: llm_enhanced
  text: It'd be much easier to play Tetris, it'd be a crazy cheat code, if you could
    delete blocks that were kind of in a weird shape and making things hard and have
    them come back later. If you could take a block and split it into sub-blocks or
    make a block into
  topic: technical
- impact_reason: Suggests that architectural challenges solved in traditional DNNs
    (like vanishing gradients) will reappear and require novel solutions in the emerging
    field of 'networks of networks.'
  relevance_score: 5
  source: llm_enhanced
  text: You kind of get back to these neural network, deep neural network concepts
    of vanishing and skip connections, and these come back in cool ways, which I think
    is really exciting to just talk to think about.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: Infrastructure Scaling and Compound AI Systems with
  Jared Quincy Davis - #740


  This episode of the Twimmel AI podcast features host Sam Charrington in conversation
  with **Jared Quincy Davis**, Founder and CEO of **Foundry**, focusing on the concept
  of **Compound AI Systems** and the future of generative AI infrastructure. Davis,
  drawing from his background at DeepMind and Stanford, argues that the next major
  leaps in AI progress will come less from training single, larger models and more
  from intelligently composing existing models into complex architectures.


  ### 1. Focus Area

  The primary focus is on **Compound AI Architectures** (or "Networks of Networks"),
  which leverage the increasing diversity and cost dispersion in the current LLM ecosystem.
  The discussion centers on infrastructure scaling, model composition, inference efficiency,
  and how these systems can push the quality frontier beyond what monolithic models
  can achieve alone, particularly for complex, verifiable tasks.


  ### 2. Key Technical Insights

  *   **Counterintuitive Reasoning Flaw:** Models like DeepSeek exhibit a quirk where
  longer reasoning time (e.g., deep beam search) can *increase* the likelihood of
  error, similar to a student overthinking an exam question.

  *   **Ensemble/Early Stopping Efficacy:** A simple compound technique—running multiple
  replicas of a reasoning model in parallel and stopping at the first successful completion—can
  simultaneously increase accuracy and speed, and potentially *reduce* cost by minimizing
  expensive output tokens.

  *   **Verifiable Task Frontier Pushing:** For highly verifiable tasks (like code
  generation or math proofs), compositional methods can yield dramatic quality gains
  (e.g., 9%+ improvements on sticky benchmarks), theoretically allowing the frontier
  to be pushed "arbitrarily far" with sufficient parallel capital.


  ### 3. Business/Investment Angle

  *   **Cost Dispersion Opportunity:** The massive gulf in cost between frontier models
  (e.g., $150/million tokens) and cheaper models (e.g., $3/million tokens) creates
  significant financial incentives for sophisticated model routing and composition.

  *   **Democratization of Frontier Capabilities:** Compound systems offer a path
  for broader companies to achieve state-of-the-art results without needing the resources
  of OpenAI or Anthropic, primarily through infrastructure and architectural innovation
  rather than massive training budgets.

  *   **Hybrid System Superiority:** Research (like the LLM Selector paper) demonstrates
  that hybrid systems, which mix different models for different steps in a multi-step
  pipeline (e.g., agentic coding tasks like SweetBench), outperform monolithic systems
  using only the single best available model.


  ### 4. Notable Companies/People

  *   **Jared Quincy Davis (Foundry):** Proponent and researcher behind Compound AI
  Systems and infrastructure co-design.

  *   **Alex Demakis:** Collaborator credited with the "Leconic Decoding" intuition
  related to reasoning model quirks.

  *   **OpenAI, Anthropic, Google (Gemini), Meta (Llama), XAI (Grok):** Mentioned
  as key players contributing to the diverse ecosystem of models.

  *   **Matei Zaharia, Yuri Leskovich, Ling Zhao:** Mentioned as collaborators on
  foundational work in model selection and routing.


  ### 5. Future Implications

  The industry is moving toward an era where the number of *inference calls* in a
  system might become a more relevant metric than the number of *parameters*. The
  focus will shift to **architectural efficiency** and **system design** (how to route,
  compose, and distill models) rather than solely on training the next monolithic
  giant. This resurgence of systems-level research is re-engaging the academic community.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Infrastructure Architects,
  AI Product Leaders, and Venture Capitalists** focused on the operational and strategic
  scaling of generative AI applications. It requires a baseline understanding of LLM
  concepts, inference costs, and model performance benchmarks.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- openai
- anthropic
- meta
- nvidia
title: 'Infrastructure Scaling and Compound AI Systems with Jared Quincy Davis - #740'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 94
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 36
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 12
  prominence: 1.0
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 00:22:19 UTC -->
