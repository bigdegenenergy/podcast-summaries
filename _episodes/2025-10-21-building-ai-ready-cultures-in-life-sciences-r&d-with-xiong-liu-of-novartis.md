---
actionable_items:
- action: potentially
  category: investigation
  full_context: 'we could potentially '
  priority: medium
companies:
- category: unknown
  confidence: medium
  context: Welcome everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at
  name: Business Podcast
  position: 31
- category: unknown
  confidence: medium
  context: come everyone to the AI and Business Podcast. I'm Matthew Damello, Editorial
    Director here at Emerge AI Research. T
  name: Matthew Damello
  position: 53
- category: unknown
  confidence: medium
  context: the AI and Business Podcast. I'm Matthew Damello, Editorial Director here
    at Emerge AI Research. Today's guest is Jung
  name: Editorial Director
  position: 70
- category: unknown
  confidence: medium
  context: . I'm Matthew Damello, Editorial Director here at Emerge AI Research. Today's
    guest is Jung Liu, Director of Data Scie
  name: Emerge AI Research
  position: 97
- category: unknown
  confidence: medium
  context: ctor here at Emerge AI Research. Today's guest is Jung Liu, Director of
    Data Science in AI at Novartis. Jung
  name: Jung Liu
  position: 134
- category: unknown
  confidence: medium
  context: Research. Today's guest is Jung Liu, Director of Data Science in AI at
    Novartis. Jung joins us on today's show
  name: Data Science
  position: 156
- category: unknown
  confidence: medium
  context: ecutive thought leaders, everyone from the CIO of Goldman Sachs to the
    head of AI at Raytheon and AI pioneers lik
  name: Goldman Sachs
  position: 1332
- category: unknown
  confidence: medium
  context: o the head of AI at Raytheon and AI pioneers like Yasha Wabengio. With
    nearly a million annual listeners, AI and B
  name: Yasha Wabengio
  position: 1397
- category: unknown
  confidence: medium
  context: eve you can help other leaders move the needle on AI ROI, visit Emerge.com
    and fill out our Thought Leader
  name: AI ROI
  position: 1865
- category: unknown
  confidence: medium
  context: edle on AI ROI, visit Emerge.com and fill out our Thought Leaders submission
    form. That's Emerge.com and click on b
  name: Thought Leaders
  position: 1907
- category: unknown
  confidence: medium
  context: ne. Again, that's emerj.com/expertone. Hey folks, Steven Johnson here,
    co-founder of Notebook LM. As an author, I'
  name: Steven Johnson
  position: 2159
- category: unknown
  confidence: medium
  context: ne. Hey folks, Steven Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 2194
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. Without further
    ado, here's our conversation
  name: Google
  position: 2559
- category: unknown
  confidence: medium
  context: Without further ado, here's our conversation with Zhong Liu. Dr. Liu, thank
    you so much for being with us onc
  name: Zhong Liu
  position: 2621
- category: unknown
  confidence: medium
  context: many, many different aspects, R&D in healthcare. If I can jump in just
    right there very quickly, just t
  name: If I
  position: 5302
- category: unknown
  confidence: medium
  context: sults, you know, as for the healthcare companies. So I think it's a combination,
    it's a hybrid method, r
  name: So I
  position: 17985
- category: unknown
  confidence: medium
  context: rganizations, there are different players in this Gen AI paradigm, right?
    Then how do we ensure, you know,
  name: Gen AI
  position: 22806
- category: unknown
  confidence: medium
  context: s to have some awareness of this history as well. And I think this episode
    and everything we've described
  name: And I
  position: 24105
- category: ai_media_and_research
  confidence: high
  context: The organization hosting the 'AI and Business Podcast' and featuring executive
    thought leaders on AI adoption.
  name: Emerge AI Research
  source: llm_enhanced
- category: ai_user_life_sciences
  confidence: high
  context: The company where the guest (Jung Liu) is the Director of Data Science
    in AI, focusing on applying generative AI and LLMs in R&D and clinical workflows.
  name: Novartis
  source: llm_enhanced
- category: ai_user_finance
  confidence: medium
  context: Mentioned as an organization whose CIO has been featured on the podcast,
    implying their involvement in enterprise AI adoption.
  name: Goldman Sachs
  source: llm_enhanced
- category: ai_user_defense
  confidence: medium
  context: Mentioned as an organization whose head of AI has been featured on the
    podcast, implying their involvement in enterprise AI adoption.
  name: Raytheon
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: An AI-first tool built by Steven Johnson for organizing ideas and making
    sense of complex information by uploading documents.
  name: Notebook LM
  source: llm_enhanced
- category: ai_infrastructure_research
  confidence: high
  context: Mentioned as the originator of the transformer architecture, which is the
    foundation for modern LLMs and foundation models.
  name: Google
  source: llm_enhanced
- category: ai_application_startup
  confidence: high
  context: The specific URL provided for the Notebook LM tool, confirming its association
    with Google.
  name: NotebookLM.google.com
  source: llm_enhanced
- category: consulting_and_research
  confidence: high
  context: Mentioned for conducting a survey regarding life sciences leaders' investment
    in generative AI initiatives.
  name: Deloitte
  source: llm_enhanced
- category: ai_model_general
  confidence: high
  context: Mentioned as a benchmark for the current generation of generative models
    (GPT-like models), contrasting with older models like BERT.
  name: ChatGPT
  source: llm_enhanced
- category: ai_model_research
  confidence: high
  context: Mentioned as an older language model (pre-ChatGPT) based on the transformer
    architecture used for information extraction in clinical trial documents.
  name: BERT
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as an architecture leveraged for generating new sequences, implying
    the technology pioneered by OpenAI.
  name: GPT (Generative Pre-trained Transformer)
  source: llm_enhanced
- category: general_industry_reference
  confidence: high
  context: Mentioned as sources of open public architectures that can be leveraged.
  name: Tech Companies (General)
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as being good at building foundation models and publishing results.
  name: Academics / Academic Institutions
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Referenced as a phenomenon that people are fascinated by, often built by
    major AI labs.
  name: LLMs (Large Language Models)
  source: llm_enhanced
- category: ai_model_concept
  confidence: high
  context: The core subject of discussion regarding data leverage and domain-specific
    application.
  name: Foundation Models
  source: llm_enhanced
date: 2025-10-21 06:00:00 +0000
duration: 31
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc
  text: we should align, you know, between different parties, like scientists, basic
    scientists, or AI modelers, AI professionals, and also leadership teams, AI adoption,
    etc.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://traffic.libsyn.com/secure/techemergence/Business_-_10.21.25_-_Xiong_Liu.mp3?dest-id=151434
processing_date: 2025-10-21 07:39:03 +0000
quotes:
- length: 205
  relevance_score: 8
  text: So you can use this kind of advanced embedding-like features to improve the
    prediction accuracy of your machine learning but now the GPT they also leverage
    the decoding part of the transformer architecture
  topics: []
- length: 244
  relevance_score: 6
  text: So we have witnessed from the early years the machine learning technologies,
    by the supervised learning, unsupervised learning, and then we move to the deep
    learning techniques like convolutional neural networks, RNNs, and graph neural
    networks
  topics: []
- length: 238
  relevance_score: 6
  text: So the benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information, although it may not be specifically to your
    data
  topics: []
- length: 156
  relevance_score: 4
  text: Jung joins us on today's show to explore how generative AI and large language
    models are revolutionizing R&D and clinical workflows throughout life sciences
  topics: []
- length: 289
  relevance_score: 4
  text: Together, we break down the shift from traditional task-based machine learning
    to foundational models that leverage massive data repositories, enabling teams
    to accelerate molecule design, digital twin creation, and protocol optimization
    through deeper automation and improved data quality
  topics: []
- length: 238
  relevance_score: 4
  text: Our conversation also highlights practical challenges in workflow, including
    building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI
  topics:
  - valuation
- length: 152
  relevance_score: 4
  text: But first, are you driving AI transformation at your organization, or maybe
    you're guiding critical decisions on AI investments, strategy, or deployment
  topics:
  - investment
- length: 134
  relevance_score: 4
  text: Obviously, this is a developing story from even a few years ago where we saw
    the explosion of LLMs and generative AI across industries
  topics: []
- length: 233
  relevance_score: 4
  text: So the difference is that previously when we do machine learning, we just
    collect our own data, we label the data, and then apply algorithms to train models
    so that we can get a working model to predict the outputs for the new inputs
  topics: []
- length: 100
  relevance_score: 4
  text: But now people have been developing these new paradigms, which is along with
    LLMs, foundation models
  topics: []
- length: 159
  relevance_score: 4
  text: So we are familiar with LLMs, they trained based on all the human corpus,
    all the human languages you can imagine, all field of settings, large language
    models
  topics: []
- length: 101
  relevance_score: 4
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have, right
  topics: []
- length: 95
  relevance_score: 4
  text: It doesn't mean we retrain the whole, you know, the foundation model, large
    language model, etc
  topics: []
- length: 142
  relevance_score: 4
  text: They could all fit into those kind of transformer architecture to let you
    learn the hidden embedding signatures of those molecules, atoms, etc
  topics: []
- length: 283
  relevance_score: 4
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves, although the goal of the foundation model
    is, it's tried to, you know, leverage all the available data, minimizing, you
    know, the specifics, you know, of specific data
  topics: []
- length: 152
  relevance_score: 4
  text: And we see this built in, it's an advantage that natural language processing
    as a domain became one of the first gigantic generative AI use cases, right
  topics: []
- length: 70
  relevance_score: 3
  text: So we can use the data you have to train to fine-tune the model, right
  topics: []
- length: 105
  relevance_score: 3
  text: So that is based on the breakthrough of the transformer architecture that
    Google proposed a long time ago
  topics: []
- impact_reason: This clearly defines the major paradigm shift occurring in life sciences
    AI, moving from narrow, task-specific ML to broad, data-leveraging foundational
    models, highlighting key application areas.
  relevance_score: 10
  source: llm_enhanced
  text: we break down the shift from traditional task-based machine learning to foundational
    models that leverage massive data repositories, enabling teams to accelerate molecule
    design, digital twin creation, and protocol optimization through deeper automation
    and improved data quality.
  topic: technical/strategy
- impact_reason: This provides a crucial, quantifiable metric illustrating the 'AI
    adoption gap'—high investment interest but low scaling success—a common theme
    in enterprise AI.
  relevance_score: 10
  source: llm_enhanced
  text: 73% of life sciences leaders are investing in generative AI initiatives, fewer
    than 20% have successfully scaled these technologies beyond pilot projects.
  topic: business/strategy
- impact_reason: 'This explains the core value proposition of foundation models: knowledge
    transfer and utility even with limited task-specific data (few-shot/zero-shot
    learning capability).'
  relevance_score: 10
  source: llm_enhanced
  text: The benefit is that even if you have very small data for your own machine
    learning tasks, you can leverage those foundation models because they already
    capture some relevant information... So that's why they are called foundation
    models. They could be useful for many downstream tasks.
  topic: technical
- impact_reason: This is a powerful conceptual analogy, framing genetics as a language
    suitable for LLM processing, which underpins the concept of digital twins in biology.
  relevance_score: 10
  source: llm_enhanced
  text: genetics are just code, they're just a language so we can build a large language
    model to have kind of the same functionality or be able to test a lot of the,
    you know, genetic functions of a compound as an example to maybe take away some
    of the larger heavy work of much of across the drug development process...
  topic: predictions/industry application
- impact_reason: Explains the shift from representation (BERT) to generation (GPT)
    based on the decoder component of the Transformer, a core concept in modern GenAI.
  relevance_score: 10
  source: llm_enhanced
  text: But now the GPT they also leverage the decoding part of the transformer architecture.
    So meaning they are now able to generate new types of data.
  topic: technical
- impact_reason: Details the power of generative models in molecular design, emphasizing
    the crucial ability to incorporate multi-objective constraints (like toxicity
    and solubility) during generation.
  relevance_score: 10
  source: llm_enhanced
  text: So now, certainly, it can allow you to generate new sequence, new graphs.
    And really, yeah, additionally, also based on your constraints, your requirements,
    your domain-specific knowledge. So for example, you can generate many different
    types of new molecules, but you can add constraints like have, you know, better
    toxicity, solubility, etc. It can allow you to, you know, do that simultaneously.
  topic: technical/business
- impact_reason: Pinpoints the severe risk of hallucination in high-stakes domains
    like biomedicine, where outputs cannot be instantly verified like text or code.
  relevance_score: 10
  source: llm_enhanced
  text: a way of phenomenon is about hallucination, right? So, when those models,
    AI models, they generate new outputs, they probably seemingly like answers. But
    if you delve into that... But when it goes to the bio-medical, biochemistry, etc.
    For example, it generates a new set of genes, a new molecule, is this something
    that you cannot be answered or validated instantly?
  topic: safety/limitations
- impact_reason: 'Proposes a crucial solution for model selection in specialized fields:
    hybrid evaluation metrics that combine domain expertise with AI-driven scoring.'
  relevance_score: 10
  source: llm_enhanced
  text: So, I think this domain-driven, domain and AI-driven evaluation is key here,
    because there could be so many versions of found data models. Now, if we, you
    know, have those kind of domain plus AI-driven those evaluation metrics, so we
    can score across the different models, right?
  topic: safety/strategy
- impact_reason: 'This outlines the critical practical challenges for enterprise AI
    adoption in specialized fields like life sciences: measurement, collaboration,
    and proving value.'
  relevance_score: 9
  source: llm_enhanced
  text: building domain-specific benchmarks, enabling tighter alignment between scientists
    and AI teams, and adopting new evaluation standards for AI-driven research ROI.
  topic: business/strategy
- impact_reason: This succinctly contrasts the old supervised learning approach with
    the new foundation model paradigm, which relies on massive pre-training.
  relevance_score: 9
  source: llm_enhanced
  text: Previously when we do machine learning, we just collect our own data, we label
    the data, and then apply algorithms to train models... But now people have been
    developing these new paradigms, which is along with LLMs, foundation models.
  topic: technical
- impact_reason: This provides a concrete example of how foundation models are built
    in life sciences—by incorporating vast public biological datasets (like cell atlases)
    before specific project fine-tuning.
  relevance_score: 9
  source: llm_enhanced
  text: But now with foundation models, it has a lot of pre-trained models based on
    publicly available molecular data. So for example, cell atlases, etc. So there's
    a lot of different cell types that gene expression data already being built into
    those foundation models.
  topic: technical/industry application
- impact_reason: This offers a clear, accessible definition of fine-tuning in the
    context of foundation models, emphasizing weight adjustment for personalization
    rather than full retraining.
  relevance_score: 9
  source: llm_enhanced
  text: Fine-tuning meaning yes, we take the foundation models but we can still take
    the data you have... We just adjust the weights based on the data you have so
    that it can better be personalized for your problems having better prediction
    accuracy or relevancy to the study you have.
  topic: technical
- impact_reason: 'This details a cutting-edge application: using generative AI to
    simulate biological outcomes (phenotypes) via *in silico* testing, accelerating
    early discovery.'
  relevance_score: 9
  source: llm_enhanced
  text: People have been already applying those models. I just give some examples
    about using foundation models, gen AI to generate new cell types and gene expressions
    so that we can do all kinds of in silico, you know, treatment methods to see,
    to observe the predicted phenotypes, etc.
  topic: industry application
- impact_reason: 'Highlights a key application and benefit of GenAI in drug discovery:
    focusing research efforts on high-potential areas.'
  relevance_score: 9
  source: llm_enhanced
  text: So that way can allow us to better focus on discovering, you know, novel targets,
    novel gene pathways, etc.
  topic: predictions/business
- impact_reason: Provides a clear, concise technical distinction between BERT (encoder-only/representation
    model) and its utility in feature extraction.
  relevance_score: 9
  source: llm_enhanced
  text: So the difference is that the previously BERT architecture they use the decoder
    part of the transformer. So you can think it's as a very good representation model.
  topic: technical
- impact_reason: Stresses the indispensable role of domain experts in guiding data
    selection and, critically, in the benchmarking and validation process for domain-specific
    foundation models.
  relevance_score: 9
  source: llm_enhanced
  text: And then this text knowledge about not just AI, computer science, but also
    the knowledge about the data themselves... we still need the domain knowledge
    people, you know, people have those kind of expertise to guide the, you know,
    the selection of the data and also more importantly, then these models are, you
    build, now how do we benchmark with them?
  topic: strategy/safety
- impact_reason: 'Highlights a critical bottleneck: the gap between theoretical AI
    capability (architecture) and the physical limitations/speed of experimental data
    generation in biotech.'
  relevance_score: 9
  source: llm_enhanced
  text: But unfortunately, to get those, you know, single-cell RNA-seq data, all those
    kind of more, you know, experiment-driven data, probably we're not exactly there
    yet, because, you know, we still rely on those traditional biotechnology methods
    to collect those data through experiments on cell lines, on human tissues, etc.
    So, there's, you know, where AI, the architecture, the concept there, but the,
    sometimes we are limited by the, you know, experimental side.
  topic: limitations/strategy
- impact_reason: A cautionary statement emphasizing that current AI technology is
    probabilistic, necessitating rigorous, domain-specific benchmarking rather than
    blind trust.
  relevance_score: 9
  source: llm_enhanced
  text: because right now the technology cannot give you 100% answers, how those nations
    are always there, right? So, it's quite important that we have to do those benchmarking
    and select models accordingly.
  topic: safety/limitations
- impact_reason: Identifies organizational culture and alignment between technical
    builders (AI modelers) and domain users (scientists/leadership) as a primary barrier
    to effective AI adoption.
  relevance_score: 9
  source: llm_enhanced
  text: So, there's a lot of cultural organizational issue. I see the key that really
    we should align, you know, between different parties, like scientists, basic scientists,
    or AI modelers, AI professionals, and also leadership teams, AI adoption, etc.
  topic: business/strategy
- impact_reason: This describes a strategic shift towards centralizing and leveraging
    massive, diverse data lakes to power broad organizational capabilities, moving
    beyond simple automation.
  relevance_score: 8
  source: llm_enhanced
  text: We're seeing a new model where you have a large data repository where everything
    goes, all data is great data. We'll be discriminating about it later... from that
    data repository we can drive more advanced algorithms to help with strategic tasks
    across the entire organization.
  topic: strategy
- impact_reason: This highlights the necessary specialization required for high-stakes
    industries like life sciences, where general models are insufficient without adaptation.
  relevance_score: 8
  source: llm_enhanced
  text: We're talking about generic AI versus the domain-specific gen AI.
  topic: strategy
- impact_reason: 'This grounds the current wave of GenAI/Foundation Models in their
    fundamental technical underpinning: the Transformer architecture.'
  relevance_score: 8
  source: llm_enhanced
  text: all those models, you know, essentially they have the same, I mean, by nature,
    they have the same setup. So that is based on the breakthrough of the transformer
    architecture that Google proposed a long time ago.
  topic: technical
- impact_reason: 'A succinct summary of the organizational requirement for successful
    AI implementation in specialized fields: mandatory cross-disciplinary teamwork.'
  relevance_score: 8
  source: llm_enhanced
  text: So, yeah, so it's basically a cross-discipline collaboration, whether in house
    or externally, yeah.
  topic: strategy
- impact_reason: Identifies a strategic advantage for language-heavy domains (like
    genetics) due to the maturity of NLP/LLM technology.
  relevance_score: 8
  source: llm_enhanced
  text: it's an advantage that natural language processing as a domain became one
    of the first gigantic generative AI use cases, right? That builds in certain advantage
    to genetics, which is more language-based than anything else versus, you know,
    other domains of health and life sciences, the services involved.
  topic: strategy/predictions
- impact_reason: 'Reiterates the perennial challenge in ML: model sophistication does
    not negate the fundamental requirement for high-quality, domain-specific data.'
  relevance_score: 8
  source: llm_enhanced
  text: So, so let's talk through the, you know, the challenges, you know, of building
    and deploying those models. So, the first thing is about the data challenge, right?
    So, so the foundation models, although they take all kinds of available data,
    but still they require high-quality data.
  topic: business/technical
- impact_reason: Highlights the 'digestibility' problem—translating complex model
    capabilities into actionable insights for domain experts who must then validate
    the results.
  relevance_score: 8
  source: llm_enhanced
  text: how do we digest the models, the capabilities? And again, we have to plug
    in, you know, the right scientists to help us together validate those.
  topic: strategy
- impact_reason: 'This raises a key strategic question for enterprises: build vs.
    buy when it comes to specialized foundation models, especially in regulated or
    domain-specific areas.'
  relevance_score: 7
  source: llm_enhanced
  text: I'm just curious for these foundational models, is it more often that the
    organizations themselves are developing them or is there a third-party more robust
    market?
  topic: business
source: Unknown Source
summary: '## Podcast Episode Summary: Building AI-Ready Cultures in Life Sciences
  R&D - with Xiong Liu of Novartis


  This 30-minute episode features Jung Liu, Director of Data Science in AI at Novartis,
  discussing the paradigm shift in Life Sciences R&D driven by Generative AI (GenAI)
  and foundational models, and the cultural and technical challenges of scaling these
  technologies beyond pilot projects.


  ---


  ### 1. Focus Area

  The primary focus is the transition in Life Sciences R&D from traditional, task-specific
  Machine Learning (ML) to leveraging **Foundational Models (FMs)** and **Large Language
  Models (LLMs)**. Key application areas discussed include accelerating **molecule
  design**, creating **digital twins**, and optimizing **clinical protocol design**.
  A significant theme is the necessity of building **AI-ready cultures** to bridge
  the gap between AI development and scientific application.


  ### 2. Key Technical Insights

  *   **Shift to Foundation Models:** The paradigm has moved from training models
  solely on proprietary, labeled data to utilizing massive, pre-trained foundation
  models (like LLMs) that capture broad domain knowledge. This allows for leveraging
  general knowledge even when specific downstream task data is limited, often through
  **fine-tuning** (adjusting weights rather than full retraining).

  *   **Transformer Architecture in Biology:** The underlying **Transformer architecture**
  (pioneered by Google) is being adapted beyond text to model biological data. This
  includes representing molecules as sequences (SMILES strings) or graphs, enabling
  the generation of novel compounds with specified constraints (e.g., better toxicity
  or solubility).

  *   **Evaluation Challenges (Hallucination):** A critical technical hurdle in biomedical
  applications is model **hallucination**. Unlike text or code generation, validating
  novel gene sets or molecules requires extensive, time-consuming **experimental benchmarking**
  against known biological knowledge, necessitating domain-driven evaluation metrics.


  ### 3. Business/Investment Angle

  *   **Scaling Gap:** Despite high investment (73% of leaders investing in GenAI),
  few life sciences organizations (under 20%) have successfully scaled GenAI beyond
  initial pilots, highlighting a significant execution challenge.

  *   **Data Dependency vs. Model Power:** While foundation models reduce the immediate
  need for massive proprietary datasets for every task, the advancement of high-throughput
  experimental data (like single-cell RNA-seq) remains a bottleneck for fully realizing
  the potential of these models in areas like detailed phenotype modeling.

  *   **ROI and Benchmarking:** Successfully demonstrating Return on Investment (ROI)
  requires establishing **domain-specific benchmarks** and objective metrics to score
  and select the optimal models for specific R&D tasks, moving beyond simple technological
  fascination.


  ### 4. Notable Companies/People

  *   **Xiong Liu (Novartis):** The guest, Director of Data Science in AI, provided
  insights from the perspective of a major pharmaceutical company actively integrating
  GenAI into R&D.

  *   **Google (Transformer Architecture):** Mentioned as the originator of the foundational
  technology underpinning modern LLMs and domain-specific FMs.

  *   **Deloitte:** Referenced for a recent survey quantifying the gap between GenAI
  investment and successful scaling in life sciences.


  ### 5. Future Implications

  The industry is moving toward **domain-specific foundation models** tailored for
  drug discovery and clinical operations, leveraging public biological data (like
  cell atlases) alongside proprietary information. Success will hinge on **cross-disciplinary
  collaboration**—integrating AI experts, computer scientists, and domain experts
  (biologists, chemists) to guide data selection, model development, and rigorous
  validation. The future requires building robust evaluation frameworks that incorporate
  deep biological knowledge to mitigate risks like hallucination.


  ### 6. Target Audience

  This episode is highly valuable for **Life Sciences R&D leaders, Chief Digital/Information
  Officers (CDOs/CIOs), Data Science Directors, and AI Strategy professionals** within
  pharma and biotech who are tasked with moving GenAI initiatives from experimentation
  to scaled, impactful deployment. It is valuable for both technical and strategic
  professionals.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- startup
- google
title: Building AI-Ready Cultures in Life Sciences R&D - with Xiong Liu of Novartis
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 121
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 11
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 4
  prominence: 0.4
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-21 07:39:03 UTC -->
