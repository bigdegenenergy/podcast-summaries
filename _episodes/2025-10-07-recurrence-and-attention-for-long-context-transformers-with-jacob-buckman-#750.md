---
companies:
- category: unknown
  confidence: medium
  context: ight, everyone, welcome to another episode of the Two Minute AI podcast.
    I am your host, Sam Charrington. Today,
  name: Two Minute AI
  position: 567
- category: unknown
  confidence: medium
  context: ode of the Two Minute AI podcast. I am your host, Sam Charrington. Today,
    I'm joined by Jacob Buckman. Jacob is co-
  name: Sam Charrington
  position: 606
- category: unknown
  confidence: medium
  context: your host, Sam Charrington. Today, I'm joined by Jacob Buckman. Jacob is
    co-founder and CEO of Manifest AI. Befo
  name: Jacob Buckman
  position: 644
- category: unknown
  confidence: medium
  context: by Jacob Buckman. Jacob is co-founder and CEO of Manifest AI. Before we
    get going, be sure to take a moment to
  name: Manifest AI
  position: 690
- category: unknown
  confidence: medium
  context: ieving long context with transformers and the new Power Retention architecture
    that you recently published. To get
  name: Power Retention
  position: 996
- category: unknown
  confidence: medium
  context: 'tle bit about your background.


    Yeah, absolutely. So I''ve been working on deep learning for the better p'
  name: So I
  position: 1155
- category: unknown
  confidence: medium
  context: e now. I started maybe in 2016 as an undergrad at Carnegie Mellon. I did
    my master's thesis on deep learning for au
  name: Carnegie Mellon
  position: 1273
- category: unknown
  confidence: medium
  context: ing I anticipated when I started getting into it. But I am telling you,
    it turns out as it turns out. Yea
  name: But I
  position: 1620
- category: tech
  confidence: high
  context: ns out as it turns out. Yeah. And so I worked for Google Brain for a while,
    got lots of exposure to differ
  name: Google
  position: 1698
- category: unknown
  confidence: medium
  context: ns out as it turns out. Yeah. And so I worked for Google Brain for a while,
    got lots of exposure to different ar
  name: Google Brain
  position: 1698
- category: unknown
  confidence: medium
  context: digm was going to take us to unbelievable places. And I started thinking
    together with my co-founder, Col
  name: And I
  position: 2159
- category: unknown
  confidence: medium
  context: consequences of, "Hey, here's a particular input. Do I get the output I
    want?" And ultimately, it's such
  name: Do I
  position: 6510
- category: unknown
  confidence: medium
  context: ction on the time axis. Using something like GQA, Grouped Query Attention,
    this is a reduction on the heads axis because yo
  name: Grouped Query Attention
  position: 21986
- category: unknown
  confidence: medium
  context: 'you think about the relationship between those?


    So RNNs, including all state space models, have the oppos'
  name: So RNNs
  position: 25145
- category: unknown
  confidence: medium
  context: ht FLOPs there. But then there's the state FLOPs. State FLOPs are multiplications
    between some state elementâ€”wh
  name: State FLOPs
  position: 29149
- category: unknown
  confidence: medium
  context: iting and retesting in order to make that happen. So Vigil is a framework
    that gives you just a set of very
  name: So Vigil
  position: 38000
- category: unknown
  confidence: medium
  context: 'ric patterns that you want and then gives a clean Python JIT system for
    doing any empirical sweep.


    And just t'
  name: Python JIT
  position: 38960
- category: unknown
  confidence: medium
  context: 'ematically.


    Exactly. And I''m very, very proud of Power Retention I would put together here
    because it is exactly wha'
  name: Power Retention I
  position: 40310
- category: unknown
  confidence: medium
  context: ntations are. We rely heavily on something called Cute Layout, which is
    a sort of recent addition to the Cutlas
  name: Cute Layout
  position: 40785
- category: tech
  confidence: high
  context: nt addition to the Cutlass library by the team at Nvidia. It's an absolutely
    brilliant innovation, and I t
  name: Nvidia
  position: 40871
- category: unknown
  confidence: medium
  context: 'n Cute.


    Interesting. Have you been tracking what Chris Lattner and Modular are doing with
    Mojo? They, I think, r'
  name: Chris Lattner
  position: 41252
- category: ai_startup
  confidence: high
  context: The company founded by Jacob Buckman and his co-founder to research and
    solve the problem of synthesizing extremely large inputs (context length) using
    the Power Retention architecture.
  name: Manifest AI
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Where Jacob Buckman worked for a period, gaining exposure to deep learning
    areas like GANs and deep reinforcement learning.
  name: Google Brain
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The research institute in Montreal where Jacob Buckman pursued his PhD,
    focusing mostly on deep reinforcement learning.
  name: Mila
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of its large context window (millions of tokens)
    and its 'needle in a haystack' utility metric, implying a major LLM release (Google/Alphabet).
  name: Gemini
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as a state space model architecture that the host has interviewed
    people working on.
  name: Mamba
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A specific version of the Mamba architecture, noted as being extremely
    similar to a transformer.
  name: Mamba 2
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned alongside Mamba and RetNet as a sub-quadratic transformer variant.
  name: DeltaNet
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Recurrent Neural Network mentioned as a sub-quadratic transformer variant
    and part of the 'retention' umbrella.
  name: RetNet
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The speaker mentions their work at Manifest regarding concurrent arrival
    at the chunked algorithm for retention in 2023-2024, and that a main focus of
    their research is making RNN/SSM states bigger.
  name: Manifest
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned in relation to 'latent attention,' which is described as a reduction
    on the feature dimension of the state size.
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A three-billion-parameter coding model from the BigCode project used as
    the base for tuning the Power Retention architecture (resulting in PowerCoder).
  name: StarCoder
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The project responsible for training the StarCoder model.
  name: BigCode project
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in relation to the Cutlass library and Cute Layout, which the
    speaker relies heavily upon for memory manipulation in their CUDA framework.
  name: Nvidia
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned in connection with Chris Lattner and their work on Mojo.
  name: Modular
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned in relation to his work with Modular on Mojo.
  name: Chris Lattner
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A highly optimized attention kernel implementation. The speaker compares
    their custom CUDA kernels against it, noting they achieve speedups over it.
  name: FlashAttention
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned as the source of the FlashAttention implementation they benchmarked
    against.
  name: ThreeDot
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A library from Nvidia that includes Cute Layout, which the speaker uses
    extensively in their kernel framework.
  name: Cutlass library
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The resulting model after tuning StarCoder weights onto the Power Retention
    architecture.
  name: PowerCoder
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Specific hardware (NVIDIA GPUs) used for the two-hour retraining/metamorphosis
    period.
  name: H100s
  source: llm_enhanced
date: 2025-10-07 17:37:00 +0000
duration: 57
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: having a state that's really big or really small fundamentally
  text: The problem with having a state that's really big or really small fundamentally
    is that it's imbalanced.
  type: problem_identification
- actionable: false
  confidence: medium
  extracted: having a state that's really big or really small fundamentally
  text: the problem with having a state that's really big or really small fundamentally
    is that it's imbalanced.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN7068202936.mp3?updated=1759858524
processing_date: 2025-10-08 03:25:06 +0000
quotes:
- length: 168
  relevance_score: 7
  text: It just shows that, hey, if there is a model that you care about and it's
    pre-trained as an exponential transformer, all you have to do is do a little bit
    of retraining
  topics: []
- length: 136
  relevance_score: 4
  text: Pre-training evaluation is much cleaner than downstream task evaluation because
    you're always just focused on minimizing the loss, right
  topics:
  - valuation
- length: 247
  relevance_score: 4
  text: You see scaling curves on the negative log likelihood that are completely
    analogous to the ones that you see in parameter scaling or dataset size scaling
    or steps of training scaling or any of these other scaling axes that we know in
    deep learning
  topics: []
- length: 215
  relevance_score: 4
  text: So the crazy ratioâ€”I mean, even training transformers at size a thousand,
    training transformers at context size like, you know, 8,000, 16,000, 100,000,
    like 100,000â€”this is five orders of magnitude gap in state size
  topics: []
- length: 270
  relevance_score: 4
  text: If you're doing, say, the forward pass of a transformer, what you're doing
    is you're starting with some tokens, you embed them into activations, and then
    those activations get transformed and transformed and transformed again until
    they're finally giving you some output
  topics: []
- length: 300
  relevance_score: 4
  text: And so this means that like the only architectures for which it makes sense
    to train a transformer on a long context have this unfathomably huge number of
    weights, beyond even the very largest models that people are commonly training
    today, to get to a context length of a million or something, right
  topics: []
- length: 116
  relevance_score: 4
  text: And in the context of the modern era of transformers, every good training
    run starts from some great initial weights
  topics: []
- length: 242
  relevance_score: 3
  text: But I do believe that just like we've seen in every other sort of piece of
    the field of AI, if you can find a way to add pre-training time squeeze more intelligence
    into the model, it's going to come out on downstream tasks one way or another
  topics: []
- length: 150
  relevance_score: 3
  text: And when you're doing recurrent architecture, you have to unroll many, many
    sequential steps, so you can't express this as a big matrix multiplication
  topics: []
- length: 164
  relevance_score: 3
  text: All these things can have lots of different values, and you have to basically
    just do a huge empirical sweep to find out which ones are the best for any given
    setup
  topics: []
- length: 111
  relevance_score: 3
  text: The sort of realignment period, what we call the metamorphosis, took around
    two hours on a cluster of 128 H100s
  topics: []
- impact_reason: Identifies the context length as the primary bottleneck preventing
    further scaling success within the existing transformer paradigm, setting the
    stage for their research focus.
  relevance_score: 10
  source: llm_enhanced
  text: Do we see any technical bottlenecks that will actually prevent us from getting
    there? And the conclusion that we reached was that although several of the major
    axes of scale are handled wellâ€”you know, we know how to scale weights, we know
    how to scale the datasetâ€”there's one crucial axis which scaling does not work
    so well on, and in this case, it's the context axis.
  topic: technical
- impact_reason: A crucial critique of current industry benchmarks, emphasizing that
    raw context length does not equate to functional model utility.
  relevance_score: 10
  source: llm_enhanced
  text: A lot of the time people confuse the top-line number of contextâ€”oh, a million
    tokensâ€”for actual utility. And not all context is created equal here, right?
  topic: predictions/business
- impact_reason: 'Proposes a rigorous, fundamental metric for context scaling: the
    marginal reduction in negative log likelihood (loss) per added token during pre-training.'
  relevance_score: 10
  source: llm_enhanced
  text: 'Pre-training evaluation is much cleaner than downstream task evaluation because
    you''re always just focused on minimizing the loss, right? The negative log likelihood.
    And you can actually measure it. And we do measure this: given each additional
    token of context, how much marginally easier does predicting the subsequent token
    become?'
  topic: technical
- impact_reason: Confirms that context length adheres to predictable scaling laws
    (log-linear improvement in loss), solidifying its status as a fundamental scaling
    axis alongside parameters and data.
  relevance_score: 10
  source: llm_enhanced
  text: You get these beautiful log-linear pluses that show that each additional token
    on average improves your model ability to predict the next token by a small, like
    a, you know, a log-linear amount, basically. And so I basically view appropriately
    trained, like appropriately situated models' inference time predictive ability
    as downstream of the chosen context length. So in this sense, context is just
    another axis of scale, and they can deeply improve performance.
  topic: technical
- impact_reason: Provides a concise definition of their core innovation ('retention')
    as a hybrid approach combining the efficiency of recurrence (linear cost) with
    the benefits of attention.
  relevance_score: 10
  source: llm_enhanced
  text: 'Retention is a form of linear attention that mixes the current most, you
    know, successful paradigm with the best parts of the previous most successful
    paradigm: recurrence.'
  topic: technical
- impact_reason: 'Explains the fundamental hardware bottleneck of recurrence: sequential
    dependency prevents the massive parallelization that GPUs excel at via large matrix
    multiplications.'
  relevance_score: 10
  source: llm_enhanced
  text: The limitation, of course, is that all this computation must be done sequentially.
    GPUs love big matrix multiplications. There's nothing they like more, right? And
    when you're doing recurrent architecture, you have to unroll many, many sequential
    steps, so you can't express this as a big matrix multiplication.
  topic: technical
- impact_reason: 'Explains *why* attention dominates in wall-clock time: its structure
    maps perfectly to highly parallel matrix multiplication operations favored by
    GPUs, despite its theoretical quadratic cost.'
  relevance_score: 10
  source: llm_enhanced
  text: Attention, the advantage of attention is that it's super parallel. It's possible
    something that's basically like you multiply two matricesâ€”it's basically like
    two matrices together... Very fat matrices, very parallelizable, very GPU friendly.
  topic: technical
- impact_reason: Introduces the core concept of Retention/SSMs having dual mathematical
    forms (recurrent vs. attention), which is the foundation for achieving hybrid
    efficiency.
  relevance_score: 10
  source: llm_enhanced
  text: Retention is a family of architectures that has both forms. The computation
    can be either expressed in a recurrent form... or it can be expressed in an equivalent
    attention form... And these two are mathematically equivalent.
  topic: technical
- impact_reason: Details the 'chunk algorithm' strategy, which is a major architectural
    breakthrough for combining the parallel efficiency of attention locally with the
    linear scaling of recurrence globally.
  relevance_score: 10
  source: llm_enhanced
  text: What's really cool is that you can combine them to get the best of both worlds.
    People call it the chunk algorithm. Basically, what you do is if you have a very
    long input sequence, you break it up into reasonably small chunks. Now, within
    each chunk, you use the attention calculation... But then you use recurrence between
    chunks.
  topic: technical
- impact_reason: 'Provides a precise, causal explanation for quadratic scaling in
    Transformers: it stems directly from the linear growth of the KV cache (state
    size) interacting with per-token computation.'
  relevance_score: 10
  source: llm_enhanced
  text: The quadratic cost is downstream of the fact that the state size itself is
    growing linearly. So if you're doing a linear number of operations on each token,
    and the size of each operation is growing linearly, right, then that comes out
    to quadratic overall. So the reason why the quadratic scaling of a transformer
    sort of is a thing is because the state of a transformer is growing linearly.
  topic: technical
- impact_reason: 'Provides a powerful comparative summary: Transformers suffer from
    state *bloat* (too large), while RNNs/SSMs suffer from state *poverty* (too small/fixed),
    framing the entire long-context debate around state size.'
  relevance_score: 10
  source: llm_enhanced
  text: RNNs, including all state space models, have the opposite problem of transformers.
    Transformers have a stateâ€”at long context, at leastâ€”transformers have a state
    that's way too large. And so all these levers exist to make that state small.
    Now, RNNs do not have this problem. RNN states are way too small.
  topic: technical
- impact_reason: Provides a stark, quantitative comparison illustrating why modern
    Transformers outperform LSTMs/RNNs, attributing the gap primarily to state size
    differences rather than just parameter count.
  relevance_score: 10
  source: llm_enhanced
  text: RNN states are way too small. And you can kind of just napkin map this out,
    and it becomes actually crazy that people ever trained an LSTM because for an
    LSTM, the state size at each layer is, let's say, it's equivalent to the feature
    size, so it's like let's say 64.
  topic: technical
- impact_reason: A powerful, concrete metric demonstrating the massive state size
    disparity between Transformers and RNNs, explaining the performance gap.
  relevance_score: 10
  source: llm_enhanced
  text: So a transformer with even just a small context of length like a thousand,
    right, has a 2,000 times larger state than an LSTM that is equivalent in every
    way except for the time axis.
  topic: technical
- impact_reason: Provides a concrete, quantifiable metricâ€”the Weight-State FLOP Ratioâ€”for
    determining compute optimality in sequence models.
  relevance_score: 10
  source: llm_enhanced
  text: So a way to think about this is as the ratio between the FLOPs spent on parameter-based
    calculations and on state-based calculations.
  topic: technical
- impact_reason: 'Explains the ''free lunch'' principle of scaling: when imbalanced,
    increasing the smaller component (state or weights) yields performance gains without
    significant cost until balance is approached.'
  relevance_score: 10
  source: llm_enhanced
  text: If one of these things is much larger than the other, double the smaller thing,
    free. And then you apply that same argument again. You can still double it and
    double it and double it and double it, and eventually you'll get into a regime
    where you notice the doubling in terms of wall-clock time. But that won't happen
    until weights and states are doing roughly the same number of FLOPs...
  topic: technical/strategy
- impact_reason: 'A major strategic implication: achieving very long context in Transformers
    requires parameter counts orders of magnitude larger than current state-of-the-art,
    due to the dominance of state FLOPs (attention cost).'
  relevance_score: 10
  source: llm_enhanced
  text: if you have a transformer with very long context, attention is so, so expensive,
    you're doing all those state FLOPs, that it is basically free to double the weights.
    You might as well do it, right? And so this means that like the only architectures
    for which it makes sense to train a transformer on a long context have this unfathomably
    huge number of weights, beyond even the very largest models that people are commonly
    training today...
  topic: predictions/technical
- impact_reason: Provides empirical validation for the weight-state FLOP ratio theory,
    showing the 'Goldilocks' balanced model outperforms the extremes in controlled
    experiments.
  relevance_score: 10
  source: llm_enhanced
  text: We have one model that has a huge, huge context and a very small number of
    weights, another model that has a huge number of weights and a very small context,
    and finally, the goldilocks model, right, in between, perfectly balanced. And
    you see that, yeah, that's the one that has the best performance.
  topic: technical
- impact_reason: Reveals the extreme complexity of optimizing novel, complex operations
    on GPUs, forcing the creation of a custom framework rather than relying on existing
    CUDA patterns.
  relevance_score: 10
  source: llm_enhanced
  text: We had to drop down a level of abstraction and just program directly into
    CUDA. And it was so complicated that even typical CUDA programming patterns didn't
    really work for our application. There were just there were too many moving pieces,
    too many like workarounds that we needed to do. So we realized that actually the
    best way to write these kernels is to build a whole CUDA framework from the ground
    up.
  topic: technical
- impact_reason: 'Highlights the key feature of Vigil: abstracting low-level kernel
    logic into generic patterns while using Python JIT for empirical search over hardware
    configurations (tile shapes, pipelining, etc.).'
  relevance_score: 10
  source: llm_enhanced
  text: Vigil lets you just write the generic patterns that you want and then gives
    a clean Python JIT system for doing any empirical sweep.
  topic: technical
- impact_reason: Demonstrates massive performance gains (2x to 4x) over Triton for
    their specific operation, showcasing the value of dropping to the lowest level
    of abstraction when necessary.
  relevance_score: 10
  source: llm_enhanced
  text: And of course, on Power Retention itself, we're now able to get like two to
    four X speedups over the Triton implementation because we're really squeezing
    what can be squeezed as hard as possible.
  topic: technical
- impact_reason: 'A crucial strategic insight: well-designed abstractions can *enable*
    optimization by facilitating systematic search, counteracting the typical trade-off
    between abstraction and performance.'
  relevance_score: 10
  source: llm_enhanced
  text: usually think about going up a level of abstraction as introducing inefficiencies,
    but in this case, it actually allows us to search the space of possible low-level
    implementations to get the best one.
  topic: strategy/technical
- impact_reason: 'Defines the first core principle: maximizing static analysis (compile-time
    inference) of memory movement based on shape and configuration, independent of
    the actual data.'
  relevance_score: 10
  source: llm_enhanced
  text: one is this separation of static and dynamic computation, where you at static
    time, like at compile time, learn everything you can about what memory is moving
    where for any given configuration.
  topic: technical
- impact_reason: 'Identifies a key practical advantage: achieving massive speedups
    in real-world, irregular workloads (like variable document pre-fill) where standard,
    fixed-size kernels underperform.'
  relevance_score: 10
  source: llm_enhanced
  text: But where we get the huge speedups is on the problem shapes that are less
    typical, like if you want to have some weird number of tokens in your sequence
    length. For example, if you're doing pre-fill on a real document, that document
    isn't going to have a nice even 1,024 token length. It's going to have some crazy
    number.
  topic: business/technical
- impact_reason: 'This is the core technical insight: using Power Retention architecture
    initialized with pre-trained weights (like StarCoder) rather than random initialization
    for rapid adaptation or fine-tuning.'
  relevance_score: 10
  source: llm_enhanced
  text: ining run like any other with the Power Retention architecture, but instead
    of initializing randomly, initialize to the known good StarCoder weights.
  topic: technical
- impact_reason: 'Details the exact architectural swap required: replacing FlashAttention
    with Power Retention while keeping the original weights intact, demonstrating
    model modularity.'
  relevance_score: 10
  source: llm_enhanced
  text: If you download the StarCoder weights and download the StarCoder architecture,
    but then replace the call to FlashAttention in the StarCoder architecture with
    a call to Power Retention, you now have a new model with weights that are equivalent
    to the StarCoder weights.
  topic: technical
- impact_reason: Directly addresses the context length limitation, identifying RoPE
    as the key parameter that needs modification to extend context.
  relevance_score: 10
  source: llm_enhanced
  text: Yeah, but you can just change. So there are some parameters like the RoPE,
    the rotational positional embedding parameters, that do sort of fix the context
    length, but you can work around it.
  topic: technical
- impact_reason: 'This provides a core strategic insight: achieving optimal performance
    (compute-optimal models) requires balancing different scaling dimensions (weights,
    data, context/state), not just maximizing one.'
  relevance_score: 9
  source: llm_enhanced
  text: All axes of scale are important. I don't want to say that state is the only
    important one at all. What you really want is an architecture that's balanced,
    right? Just like I was saying before, in this field, it's all about balance.
  topic: strategy
- impact_reason: Generalizes the context length problem beyond text (LLMs) to a fundamental
    architectural challenge applicable to images, video, and other sequential data
    processing.
  relevance_score: 9
  source: llm_enhanced
  text: Or more generally, I think context is a very context size is a very specific
    instance of a more general problem, which is how do you make a network that can
    synthesize a really big input?
  topic: technical
- impact_reason: Defines true context utility as the model's *ability to utilize*
    the information, not just its capacity to hold it.
  relevance_score: 9
  source: llm_enhanced
  text: It's about the interplay between model learning, the final model being able
    to effectively use within its context. That's what makes the context valuable.
    It's not that you can just shove more stuff in there, it's that the model can
    use the additional shoved stuff to improve its prediction and its abilities.
  topic: technical
- impact_reason: Highlights the inadequacy of simple retrieval tests ('needle in a
    haystack') as a measure of comprehensive context utilization across the entire
    input span.
  relevance_score: 9
  source: llm_enhanced
  text: Needle in a haystack doesn't necessarily translate to the ability to use all
    the context or the ability to, you know, prioritize something that's in the middle
    equally to what's at the beginning or the end.
  topic: safety/metrics
- impact_reason: Illustrates the multi-dimensional nature of context utility, showing
    that performance must be evaluated across diverse tasks (retrieval, summarization,
    reasoning).
  relevance_score: 9
  source: llm_enhanced
  text: A model can be great at finding needles in a haystack and horrible at computing
    summaries of books of equivalent length to the haystack, right? Or can be great
    at needle in a haystack and great at summarizing books, but terrible at performing
    long mathematical derivations that are many, many tokens long.
  topic: technical
- impact_reason: Demystifies the landscape of 'transformer alternatives,' suggesting
    that many new architectures (like Mamba) are essentially modified transformers
    rather than entirely new paradigms.
  relevance_score: 9
  source: llm_enhanced
  text: Actually, something that might be a bit surprising is that many of the other
    architectures are very similar to transformers. Mamba, Mamba 2 in particular,
    extremely similar to a transformer, DeltaNet, RetNet, any other sub-quadratic
    transformer variant.
  topic: technical
- impact_reason: 'Details their specific architectural strategy: retaining the successful
    transformer backbone but replacing the quadratic bottleneck (attention/time mixing)
    with a more efficient mechanism (retention).'
  relevance_score: 9
  source: llm_enhanced
  text: And our approaches do different. Where our approach is also to basically start
    from this really solid architectural backbone and intervene directly on the one
    piece that we feel is basically bottlenecking the whole thing, and that's the
    time mixing. Transformers use attention, and we swap in this thing called retention...
  topic: technical
- impact_reason: 'Clearly explains the computational advantage of recurrence: fixed
    state size leads to linear cost scaling, contrasting it with the growing state/cost
    of attention.'
  relevance_score: 9
  source: llm_enhanced
  text: The advantage of recurrence is that you get linear cost with respect to context
    length. The other way to view this is that the state of a recurrent model is of
    fixed size. It never grows.
  topic: technical
- impact_reason: 'Articulates why attention dominated despite its quadratic cost:
    its inherent structure allows for massive parallelization on modern hardware,
    leading to better wall-clock time.'
  relevance_score: 9
  source: llm_enhanced
  text: But attention changes this, right? Attention, the advantage of attention is
    that it's super parallel... Very fat matrices, very parallelizable, very GPU friendly.
  topic: technical
- impact_reason: Provides a high-level synthesis, unifying SSMs and other long-context
    models under the 'retention' umbrella, signaling a major shift in architectural
    understanding.
  relevance_score: 9
  source: llm_enhanced
  text: So you basically just get a best-of-both-worlds situation. And these are the
    really, really powerful, like basically architecture families. And all state space
    models that you're familiar with fall under the retention umbrella...
  topic: technical
- impact_reason: 'A powerful operational insight: the optimal chunk size is dictated
    by hardware saturation, not subjective model tuning, simplifying deployment optimization.'
  relevance_score: 9
  source: llm_enhanced
  text: '...that chunking size is not a hyperparameter that the developer needs to
    fiddle with? It''s strictly determined by the size of the GPU.'
  topic: technical/strategy
- impact_reason: Offers a clear, unifying definition of the Transformer's 'state'
    as the KV cache, which is essential for comparing it structurally to RNNs.
  relevance_score: 9
  source: llm_enhanced
  text: I think a really good way to frame it is to consider the KV cache to be the
    state of the network. And indeed it is Markovian with respect to the KV cache.
  topic: technical
- impact_reason: Deconstructs the state size into its constituent dimensions (time,
    heads, feature dimension), providing a roadmap for where architectural modifications
    (like GQA or windowing) reduce complexity.
  relevance_score: 9
  source: llm_enhanced
  text: 'The state size, at each layer of the network... is going to have one dimension
    that is proportional to the time. And it''s also going to have one for each head.
    And finally, the last dimension is going to be sort of like the head dimension...
    And reductions on any of these axes do the same thing: they make the overall state
    smaller.'
  topic: technical
- impact_reason: 'Describes the common ''hybrid'' architecture strategy: using a few
    expensive full-attention layers interspersed with many cheaper, reduced-state
    layers (like windowed attention) to gain efficiency without losing too much capacity.'
  relevance_score: 9
  source: llm_enhanced
  text: This is basically saying, 'Okay, assuming that the size of the windowed layers
    is negligible compared to the giant size of the full attention layers,' there's
    a gain saying, 'Okay, like only one out of every eight things on the layers dimension
    is going to be meaningful.'
  topic: strategy/technical
- impact_reason: 'Clearly defines the fundamental trade-off and challenge of the Transformer
    architecture at scale: the KV cache (state) grows too large for long contexts.'
  relevance_score: 9
  source: llm_enhanced
  text: Transformers have a stateâ€”at long context, at leastâ€”transformers have a state
    that's way too large. And so all these levers exist to make that state small.
  topic: technical
- impact_reason: Highlights a critical, underappreciated axis of model scaling (state
    size) that researchers should focus on, moving beyond the standard parameter count
    metric.
  relevance_score: 9
  source: llm_enhanced
  text: State size is just as important, is just much less understood and much less
    studied because not many people have yet adopted this perspective of viewing the
    KV cache of a transformer as its state.
  topic: technical/strategy
- impact_reason: 'Offers a counter-intuitive insight: techniques designed to shrink
    Transformer state are counterproductive for SSMs/RNNs, which need *larger* states
    to compete.'
  relevance_score: 9
  source: llm_enhanced
  text: All these interventions that are about reducing the state of a transformer,
    that'll apply for RNNs, including state space models, Mamba, all those good architectures,
    because their states are way too small already, you want to make those states
    bigger.
  topic: technical
- impact_reason: Introduces the core concept of 'balance' as the key to compute optimality,
    framing architectural design as an aesthetic pursuit leading to efficiency.
  relevance_score: 9
  source: llm_enhanced
  text: What you really want is an architecture that's balanced, right? Just like
    I was saying before, in this field, it's all about balance. That's how you know
    when a model is beautiful and balanced and elegant. That's how you know it's going
    to be a good compute-optimal model.
  topic: strategy
- impact_reason: A perfect illustration of the 'free win' concept applied to an LSTM,
    demonstrating how increasing the underutilized state component offers performance
    gains at near-zero computational cost.
  relevance_score: 9
  source: llm_enhanced
  text: if you think about an architecture that's wildly imbalanced, let's say you're
    using like an LSTM, which has just this tiny little state and a huge amount of
    weights relatively speaking... Doubling the state size is negligible. Your weights
    are a hundred times larger than your state. So doubling the state, you don't even
    notice the difference in runtime. But you do notice the difference in performance.
    So essentially, it's a free win, right?
  topic: technical
- impact_reason: 'Summarizes the two core findings for achieving optimal performance:
    processing the entire input (not sparse attention) and maintaining a balanced
    weight-state FLOP ratio.'
  relevance_score: 9
  source: llm_enhanced
  text: scaling laws, process the whole input and do so with a balanced weight-state
    FLOP ratio. And that's the first piece that we explained in the paper...
  topic: strategy/technical
- impact_reason: Reveals the extreme lengths (building a custom framework, Vigil)
    required to optimize novel, complex architectures efficiently on GPUs, suggesting
    limitations in existing low-level tools for cutting-edge designs.
  relevance_score: 9
  source: llm_enhanced
  text: So we realized that actually the best way to write these kernels is to build
    a whole CUDA framework from the ground up. So we built something that we call
    Vigil, which is basically a way of writing non-spaghetti CUDA...
  topic: implementation
- impact_reason: Highlights the critical need for co-design between novel AI architectures
    and their hardware implementation for achieving real-world performance gains (wall-clock
    speedups).
  relevance_score: 9
  source: llm_enhanced
  text: something that you can actually run and have wall-clock speedups. And this
    actually, there's a little bit of an aspect of co-design between this part of
    things and the mathematical thing I mentioned a moment ago, because we would only
    be happy with an architecture if we knew that it could be given a very efficient,
    GPU-friendly implementation.
  topic: strategy/technical
- impact_reason: Provides a scathing critique of the current state of hand-written
    CUDA kernel development, emphasizing its fragility, lack of modularity, and high
    maintenance cost.
  relevance_score: 9
  source: llm_enhanced
  text: Everyone who writes CUDA kernels does so. There's very little code reuse,
    no testable components, a bunch of if statements everywhere with different compilation
    paths, hundreds and hundreds of lines of very complicated, nuanced pointer arithmetic
    in order to make all the memory movements happen correctly.
  topic: technical/strategy
- impact_reason: Introduces 'Vigil' as a solution to the CUDA mess, focusing on testability
    and efficiency through clean patterns, which is a major goal for high-performance
    computing libraries.
  relevance_score: 9
  source: llm_enhanced
  text: So Vigil is a framework that gives you just a set of very clean patterns for
    writing beautiful, testable, and very, very efficient CUDA.
  topic: technical
- impact_reason: Provides a concrete, competitive benchmark result (20% speedup over
    FlashAttention) using the generalized framework, validating its performance claims.
  relevance_score: 9
  source: llm_enhanced
  text: we actually get, you know, 20% speedups over ThreeDot's FlashAttention on
    a whole bunch of problem shapes, which we thought was pretty cool, pretty good
    sort of proof point for the framework.
  topic: technical
- impact_reason: Clarifies that the 'learning' is static inference/deduction by the
    compiler/framework, not machine learning, which is crucial for predictable, low-latency
    performance.
  relevance_score: 9
  source: llm_enhanced
  text: Really, it's just you just infer it just at static time. But profiling kind
    of similar to this. Yeah, you can say, 'Okay, like if the problem shape is this
    and the layout is this, and we're going to want to use a tile size of that and
    break it up into this many threads, then, okay, that means this thread needs to
    take this set of memory addresses and move them to here.' And this can be known
    statically, right?
  topic: technical
- impact_reason: Highlights the importance of Python integration and JIT compilation
    for making hyper-specialized, low-level kernels accessible and usable for end-users.
  relevance_score: 9
  source: llm_enhanced
  text: And the other is this integration with Python. So we want it to be very easily
    usable, very easily so we have this JIT system. So you can just in-time compile
    on the fly brand new, hyper-specialized Vigil kernel for any given problem shape,
    any given hardware platform.
  topic: business/technical
- impact_reason: 'Outlines a three-pillar strategy for successful architectural innovation
    in AI: theoretical soundness, hardware efficiency, and trainability/adoption feasibility.'
  relevance_score: 9
  source: llm_enhanced
  text: The first one was it needs to be mathematically and theoretically optimal.
    The next is it needs to actually run fast on hardware. And the final one is you
    need to want to train this thing.
  topic: strategy
- impact_reason: 'Explains the challenge and solution for adopting new attention mechanisms:
    weights are incompatible, but a short ''metamorphosis'' retraining phase can bridge
    the gap cheaply.'
  relevance_score: 9
  source: llm_enhanced
  text: By switching the architecture from using exponential attention to using Power
    Retention, this is going to completely mess up the outputs, right? The existing
    weights produce like basically garbage. But by doing a little bit of additional
    retraining, you can get right back to the original performance.
  topic: technical/business
- impact_reason: Quantifies the economic benefit of weight transfer/fine-tuning over
    new pre-training when switching architectures.
  relevance_score: 9
  source: llm_enhanced
  text: It's way, way cheaper to start from the bad predictions of a good set of weights
    on a Power Retention architecture and train it to match the performance that those
    weights were able to get on an exponential architecture than it is to train from
    scratch.
  topic: business
- impact_reason: Provides a tangible, relatively low cost (2 hours on 128 H100s) for
    the retraining required to switch architectures, making adoption feasible for
    smaller entities.
  relevance_score: 9
  source: llm_enhanced
  text: The sort of realignment period, what we call the metamorphosis, took around
    two hours on a cluster of 128 H100s. It's not nothing, but it's not massive. It's
    not something that you need big lab pre-training scale in order to pull off.
  topic: business
- impact_reason: Provides a concrete, extremely fast benchmark for the effectiveness
    of this initialization/adaptation technique (recovering 30% performance in just
    2 hours).
  relevance_score: 9
  source: llm_enhanced
  text: And after two hours of training, you've recovered 30% score on HumanEval.
  topic: technical
- impact_reason: Highlights the simplicity and ease of adoption for converting established
    models to leverage the Power Retention mechanism.
  relevance_score: 9
  source: llm_enhanced
  text: So it's actually just a really, really easy way to turn any existing model
    into the Power Retention variant.
  topic: technical
- impact_reason: Reinforces the speed of recovery, suggesting that the Power Retention
    adaptation preserves the base knowledge efficiently.
  relevance_score: 9
  source: llm_enhanced
  text: And if you take this model and train it for just two hours, it'll recover
    the performance that StarCoder initially had.
  topic: technical
- impact_reason: 'Provides the solution for context extension: tweaking RoPE parameters
    combined with further training.'
  relevance_score: 9
  source: llm_enhanced
  text: You can just tweak those too and do additional training, and that also works
    fairly well.
  topic: technical
- impact_reason: Connects architectural 'beauty' and 'balance' directly to the practical
    goal of achieving compute-optimality, suggesting that imbalance leads to inefficiency.
  relevance_score: 8
  source: llm_enhanced
  text: That's how you know when a model is beautiful and balanced and elegant. That's
    how you know it's going to be a good compute-optimal model. The problem with having
    a state that's really big or really small fundamentally is that it's imbalanced.
  topic: technical
- impact_reason: Clearly states the company's mission, focusing on architectural design
    for large input synthesis, which is a key area of current AI research.
  relevance_score: 8
  source: llm_enhanced
  text: And we started Manifest AI essentially to crack this problem. So we've been
    working for the past couple years mostly doing focused research on the question
    of what is the right way to design a neural architecture with extremely large
    inputs synthesis of those.
  topic: business
- impact_reason: Frames context scaling as an extension of in-context learning principles,
    measurable during pre-training.
  relevance_score: 8
  source: llm_enhanced
  text: We call it in-context learning curves. So I think people are familiar with
    in-context learning in general... But the more general principle is, as you give
    it more information in a particular format, how does the resulting ability improve?
  topic: technical
- impact_reason: Reinforces the mathematical equivalence across chunk sizes, separating
    the *computational efficiency* goal (GPU utilization) from the *output quality*
    goal (mathematical result).
  relevance_score: 8
  source: llm_enhanced
  text: No matter what you set the chunk size to, you're producing the same numbers.
    Right. No, no need to work to optimize GPU utilization, exactly.
  topic: technical
- impact_reason: Explains how local windowing techniques directly solve the quadratic
    problem by constraining the state size to be constant, effectively turning the
    scaling linear.
  relevance_score: 8
  source: llm_enhanced
  text: Windowed attention is sort of an obvious way... We say, 'Okay, is this attending
    over the entire history? We're just going to be attending over a small local window
    of it?'... In fact, it'll make the state constant size with respect to time, and
    so the overall cost will be linear and not quadratic again.
  topic: technical
- impact_reason: Gives a concrete example of how a specific optimization (GQA) maps
    onto the state-size reduction framework (reducing the 'heads' axis).
  relevance_score: 8
  source: llm_enhanced
  text: Using something like GQA, Grouped Query Attention, this is a reduction on
    the heads axis because you have the same number of queries but now fewer keys
    and values along the head dimension because they're being shared.
  topic: technical
- impact_reason: Identifies latent attention as a technique that specifically targets
    the feature dimension of the state, offering another lever for complexity reduction.
  relevance_score: 8
  source: llm_enhanced
  text: Something like DeepSeek's latent attention, this is going to be a reduction
    on the feature dimension. Instead of keeping around a whole key or query of a
    particular size, you just keep around a smaller thing.
  topic: technical
- impact_reason: 'Offers pragmatic, experience-based advice on model design: architectural
    optimization is a balancing act, and empirical testing remains the final arbiter.'
  relevance_score: 8
  source: llm_enhanced
  text: If you lean too heavily on any one axis, it'll generally have worse performance.
    But ultimately, this is a heuristic, and I think the real answer is, yeah, you
    just fiddle with the knobs until you find the one that seems to work the best.
  topic: strategy
- impact_reason: A bold, direct statement of market ambition for their new architecture
    in the long-context domain.
  relevance_score: 8
  source: llm_enhanced
  text: Basically, anyone who's doing long context anything should be training their
    models using our architecture. That's our goal.
  topic: business
- impact_reason: Illustrates the necessary engineering depth required to achieve peak
    performance in modern AI hardware acceleration, moving beyond high-level frameworks
    like Triton.
  relevance_score: 8
  source: llm_enhanced
  text: We started off with Triton, which works fairly well... But that's not the
    most you can do. So we actually went a level deeper... We had to drop down a level
    of abstraction and just program directly into CUDA.
  topic: technical/implementation
- impact_reason: Confirms that the developed framework (Vigil) is a generalized tool
    for CUDA optimization, not just a one-off solution, suggesting broader applicability
    in the ML infrastructure space.
  relevance_score: 8
  source: llm_enhanced
  text: It's completely generic. So we built it because we wanted to write kernels
    for Power Retention, but in order to write those kernels, we realized we just
    needed to make a good general CUDA framework, which we did.
  topic: technical/strategy
- impact_reason: Highlights the importance of cutting-edge Nvidia libraries (Cute
    Layout) for managing complex memory movement in highly optimized kernels, suggesting
    this is the future direction for low-level GPU programming.
  relevance_score: 8
  source: llm_enhanced
  text: We rely heavily on something called Cute Layout, which is a sort of recent
    addition to the Cutlass library by the team at Nvidia. It's an absolutely brilliant
    innovation, and I think we're probably the most advanced users of Cute out there
    because our entire system for basically computing which memory needs to be moved
    where across basically any different possible configuration you can imagine relies
    on these sort of layout manipulation operations in Cute.
  topic: technical
- impact_reason: 'A critical observation on modern LLM development: transfer learning
    from existing checkpoints is the standard, not training from scratch, which dictates
    architectural adoption strategy.'
  relevance_score: 8
  source: llm_enhanced
  text: Unless you're one of the big labs, you're very rarely going to want to initialize
    your model randomly. You're always going to want to start off from some pre-trained
    weights that already do something useful.
  topic: business/strategy
- impact_reason: Provides a concrete example (PowerCoder) demonstrating that the new
    architecture can yield a superior, readily available product (faster inference)
    by leveraging existing models.
  relevance_score: 8
  source: llm_enhanced
  text: PowerCoder is this tune of StarCoder. And it's sort of two things. One is
    it's just a useful model. If you want a model that is a coding assistant at the
    three-billion-parameter scale and you want it to have really fast inference, use
    PowerCoder. It's just better than anything else out there.
  topic: business
- impact_reason: Suggests that the barrier to entry for adopting novel, high-performance
    architectures might be lower than expected, requiring only a minor modification
    to standard training pipelines.
  relevance_score: 8
  source: llm_enhanced
  text: Surprisingly, no complex recipe is required. We did do various explorations
    with the recipe to see if we could squeeze a little bit more out, and you can.
    But the basic recipe is like 99% of the way there. Literally just set it up like
    a training run, a training run like any other with the Power Retention architecture,
    but instead of initializing randomly, initialize to the known good StarCoder weights.
  topic: business/technical
- impact_reason: Suggests that the initial rapid recovery is just the start, implying
    significant potential for further performance gains using this method.
  relevance_score: 8
  source: llm_enhanced
  text: And with more training, you can push it even beyond.
  topic: technical
- impact_reason: Confirms that the described method (Power Retention adaptation +
    RoPE tweaking) is the actual methodology employed by the speakers/researchers.
  relevance_score: 8
  source: llm_enhanced
  text: And so, yeah, this is basically exactly what we do.
  topic: strategy
- impact_reason: Reflects the prevailing sentiment in the AI community leading up
    to recent scaling breakthroughs, validating the core belief in scaling laws.
  relevance_score: 7
  source: llm_enhanced
  text: For a long time, I feel like the writing was on the wall that just actually
    pushing the current paradigm was going to take us to unbelievable places.
  topic: strategy
- impact_reason: A strong statement of faith in the transferability of intelligence
    gained through fundamental pre-training improvements, even if the direct link
    to downstream tasks isn't immediately obvious.
  relevance_score: 7
  source: llm_enhanced
  text: But I do believe that just like we've seen in every other sort of piece of
    the field of AI, if you can find a way to add pre-training time squeeze more intelligence
    into the model, it's going to come out on downstream tasks one way or another.
  topic: strategy
- impact_reason: Provides historical context, showing that the underlying mathematical
    operation predates the Transformer era, suggesting a cyclical nature in architectural
    innovation.
  relevance_score: 7
  source: llm_enhanced
  text: The first retention algorithm was actually proposed under the name linear
    attention... This operation was actually used even before attention existed.
  topic: technical
- impact_reason: 'Defines the practical, user-focused mission of the research lab:
    delivering implementable, superior architectures, not just theoretical proofs.'
  relevance_score: 7
  source: llm_enhanced
  text: We aren't satisfied with just getting a theoretical win. We want to actually
    have the best architecture, the one that is most worthwhile for almost everyone
    to train.
  topic: business/strategy
- impact_reason: States the portability of their core optimization principles, suggesting
    that the framework's success isn't tied solely to CUDA, but could be ported to
    newer backends like Mojo.
  relevance_score: 7
  source: llm_enhanced
  text: Ultimately, the core principles that we're leveraging are completely agnostic
    to what's going on underneath. It's done in CUDA and in C++ via C++ template metaprogramming
    because that's the current sort of like, that's the practical way to do it.
  topic: strategy/technical
- impact_reason: 'Manages expectations: standard, well-optimized use cases see only
    marginal gains, emphasizing that the true value lies in specialized or irregular
    workloads.'
  relevance_score: 7
  source: llm_enhanced
  text: Our gains over FlashAttention on normal problem shapes, the ones that the
    authors of FlashAttention expected people to be using, our gains there are minimal.
    They're in the like one to two percent.
  topic: technical
- impact_reason: A common assumption in LLM architecture regarding fixed context length,
    setting up the subsequent discussion on overcoming this limitation.
  relevance_score: 7
  source: llm_enhanced
  text: I don't think that works because the model architecture kind of fixes the
    context, right?
  topic: technical
source: Unknown Source
summary: '## Podcast Episode Summary: Recurrence and Attention for Long-Context Transformers
  with Jacob Buckman - #750


  This episode of the Two Minute AI podcast, hosted by Sam Charrington, features Jacob
  Buckman, co-founder and CEO of Manifest AI, discussing the critical challenge of
  achieving effective **long context** in Transformer models and introducing their
  proposed solution, the **Power Retention architecture**.


  ### 1. Focus Area

  The primary focus is on overcoming the computational bottlenecks of standard Transformer
  attention mechanisms when dealing with extremely long input sequences (context length).
  The discussion centers on architectural innovations that aim to achieve **compute-optimal
  scaling** by balancing the axes of scale, specifically focusing on the context axis
  through a mechanism called **Retention**.


  ### 2. Key Technical Insights

  *   **Context as an Axis of Scale:** Effective long context is framed as another
  crucial axis of scaling, analogous to parameters or dataset size. Improvements in
  context utilization lead to measurable, log-linear improvements in pre-training
  negative log-likelihood, suggesting that better context handling directly translates
  to better predictive ability.

  *   **The Power of Retention (Recurrence + Attention):** Retention is presented
  as a family of architectures (related to Mamba 2 and DeltaNet) that mathematically
  unifies recurrent and attention-based computation. It allows for computation to
  be expressed either sequentially (recurrent form, linear cost, fixed state size)
  or in parallel (attention form, quadratic cost, GPU-friendly).

  *   **Chunking for Optimal Performance:** The key innovation is the **chunked algorithm**
  for Retention. By breaking long sequences into small chunks, attention is used within
  the chunk for hardware efficiency (parallelization), and recurrence is used *between*
  chunks. This yields the parallelization benefits of attention while maintaining
  an overall **linear cost** relative to context length, determined by the fixed size
  of the GPU memory.


  ### 3. Business/Investment Angle

  *   **Context Utility vs. Context Length:** There is a growing realization that
  simply having a large context window (e.g., millions of tokens) does not guarantee
  utility. Value is derived from the model''s ability to *effectively use* that context,
  which requires architectural improvements.

  *   **The Bottleneck of Context Scaling:** The high cost associated with scaling
  context length is identified as a major current bottleneck preventing models from
  reaching their full potential across various domains (text, video, etc.). Companies
  solving this problem, like Manifest AI, are targeting a fundamental architectural
  limitation.

  *   **Unifying Architectural Design:** The discussion provides a framework for understanding
  various long-context solutions (like Mamba, windowed attention, GQA) by analyzing
  how they reduce the size or growth rate of the model''s "state" (analogous to the
  KV cache in Transformers). This offers strategic insight into architectural trade-offs.


  ### 4. Notable Companies/People

  *   **Jacob Buckman (Manifest AI):** Co-founder and CEO, presenting the research
  on the Power Retention architecture.

  *   **Sam Charrington (Host):** Host of the Two Minute AI podcast.

  *   **Mila / Google Brain:** Institutions where Jacob Buckman conducted prior research
  in deep learning and RL.

  *   **Mamba:** Mentioned as a prominent example of a state-space model that falls
  under the broader "retention" umbrella.


  ### 5. Future Implications

  The industry is moving toward architectures that achieve **compute optimality**
  by balancing all axes of scale, not just parameters. The future of long-context
  models likely involves hybrid architectures like chunked Retention, which leverage
  hardware capabilities (parallel matrix multiplication) while maintaining linear
  scaling costs, potentially making extremely long-context processing economically
  viable.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Researchers, Deep Learning Engineers,
  and Technical Product Managers** involved in building or optimizing large language
  models, especially those focused on efficiency, scaling laws, and novel sequence
  modeling architectures.'
tags:
- artificial-intelligence
- ai-infrastructure
- startup
- investment
- google
- nvidia
title: 'Recurrence and Attention for Long-Context Transformers with Jacob Buckman
  - #750'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 74
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 32
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 2
  prominence: 0.2
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 2
  prominence: 0.2
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-08 03:25:06 UTC -->
