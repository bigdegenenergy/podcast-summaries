---
companies:
- category: unknown
  confidence: medium
  context: Hey folks, Stephen Johnson here, co-founder of Notebook LM. As an author,
    I'
  name: Stephen Johnson
  position: 11
- category: unknown
  confidence: medium
  context: Hey folks, Stephen Johnson here, co-founder of Notebook LM. As an author,
    I've always been obsessed with how
  name: Notebook LM
  position: 47
- category: tech
  confidence: high
  context: and helping you brainstorm. Try it at notebooklm.google.com. If I'm an
    application developer, if five, 10
  name: Google
  position: 413
- category: unknown
  confidence: medium
  context: you brainstorm. Try it at notebooklm.google.com. If I'm an application
    developer, if five, 10 years ago
  name: If I
  position: 425
- category: unknown
  confidence: medium
  context: s a gold mine, it's becoming an actual liability. In Europe, for example,
    there's GDPR and data privacy. We h
  name: In Europe
  position: 537
- category: unknown
  confidence: medium
  context: nd data privacy. We have California data privacy. In China, you actually
    need to pay a data tax if you're us
  name: In China
  position: 625
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the Twombo AI podcast. I
    am your host, Sam Charrington. Today I
  name: Twombo AI
  position: 998
- category: unknown
  confidence: medium
  context: episode of the Twombo AI podcast. I am your host, Sam Charrington. Today
    I'm joined by Ilia Polosuchen. Ilia is a c
  name: Sam Charrington
  position: 1033
- category: unknown
  confidence: medium
  context: ombo AI podcast. I am your host, Sam Charrington. Today I'm joined by Ilia
    Polosuchen. Ilia is a co-founder
  name: Today I
  position: 1050
- category: unknown
  confidence: medium
  context: m your host, Sam Charrington. Today I'm joined by Ilia Polosuchen. Ilia
    is a co-founder of Nira AI, but is perhaps
  name: Ilia Polosuchen
  position: 1070
- category: unknown
  confidence: medium
  context: oined by Ilia Polosuchen. Ilia is a co-founder of Nira AI, but is perhaps
    best known as a co-author of the
  name: Nira AI
  position: 1111
- category: unknown
  confidence: medium
  context: haps best known as a co-author of the now famous "Attention Is All You
    Need" paper, which introduced the transformer. Before
  name: Attention Is All You Need
  position: 1180
- category: unknown
  confidence: medium
  context: uch in machine learning and AI research. I joined Google Research because
    I saw the cat neuron paper, if folks reme
  name: Google Research
  position: 1833
- category: unknown
  confidence: medium
  context: e. And so in 2017, I left, and with my co-founder Alex Kudanaf, we started
    Nira AI. Was the idea that how do we
  name: Alex Kudanaf
  position: 2761
- category: unknown
  confidence: medium
  context: aced was the students were in China, they were in Eastern Europe, they
    were in South Estonia. In all of these coun
  name: Eastern Europe
  position: 4131
- category: unknown
  confidence: medium
  context: China, they were in Eastern Europe, they were in South Estonia. In all
    of these countries, there's some kind of
  name: South Estonia
  position: 4160
- category: unknown
  confidence: medium
  context: China, people don't have bank accounts, they have WeChat Pay. In Ukraine,
    for example, you need to sell half o
  name: WeChat Pay
  position: 4304
- category: unknown
  confidence: medium
  context: e don't have bank accounts, they have WeChat Pay. In Ukraine, for example,
    you need to sell half of your dolla
  name: In Ukraine
  position: 4316
- category: unknown
  confidence: medium
  context: already in the pocket. And so that's kind of how Near Protocol was born.
    We launched it in 2020. It's one of the
  name: Near Protocol
  position: 5372
- category: tech
  confidence: high
  context: e interesting thing happened is the hardware. So, Nvidia hardware and Intel,
    both kind of at a similar tim
  name: Nvidia
  position: 8890
- category: tech
  confidence: high
  context: happened is the hardware. So, Nvidia hardware and Intel, both kind of at
    a similar time, enabled this mod
  name: Intel
  position: 8910
- category: unknown
  confidence: medium
  context: assembly, like C list, like special instructions. And Intel, in 2024, mid-2024,
    released on the new fifth-gen
  name: And Intel
  position: 9843
- category: unknown
  confidence: medium
  context: ', but also private to the same level as local AI. Am I parsing that correctly?
    I mean, I run some of the'
  name: Am I
  position: 12791
- category: unknown
  confidence: medium
  context: So what's a specific example of that happening? So Mistral gave its weights
    to Hugging Face and it didn't do
  name: So Mistral
  position: 16055
- category: tech
  confidence: high
  context: of that happening? So Mistral gave its weights to Hugging Face and it didn't
    doubt one for chat. Oh, well, I had
  name: Hugging Face
  position: 16086
- category: tech
  confidence: high
  context: nd then joined in network. But yeah, I mean, like Amazon, you know, data
    center can repurpose itself to be
  name: Amazon
  position: 18379
- category: unknown
  confidence: medium
  context: ngs that you may not even have on the local host. Because I mean, local
    host, you still can access the hard d
  name: Because I
  position: 18754
- category: tech
  confidence: high
  context: uropean right now, I'm in Lisbon right now. I use OpenAI. OpenAI, I train
    on my data and then I go and I i
  name: Openai
  position: 20568
- category: tech
  confidence: high
  context: y their fine, like I mean, similar how Google and Facebook has paid, you
    know, billions of dollars in fines.
  name: Facebook
  position: 21081
- category: unknown
  confidence: medium
  context: let's go back to like Google, right, a Facebook. Like Google and Facebook
    learned from user behavior, like dir
  name: Like Google
  position: 23753
- category: unknown
  confidence: medium
  context: 'then dealing with all the repercussions of that.


    So I had asked about the, you know, this like creating'
  name: So I
  position: 25838
- category: unknown
  confidence: medium
  context: t's payable. Like, for example, for payable data, New York Times can contribute
    their data fully privately into se
  name: New York Times
  position: 28845
- category: unknown
  confidence: medium
  context: a more interesting kind of ecosystem of results. Like I think, you know,
    there are people that do that ki
  name: Like I
  position: 32467
- category: unknown
  confidence: medium
  context: . It is relatively small. It is relatively small. But I think the idea
    here and kind of I think everybody
  name: But I
  position: 33465
- category: unknown
  confidence: medium
  context: like links you like effectively told it relevant Docker GitHubs and other
    things you need to know if you want to
  name: Docker GitHubs
  position: 39736
- category: tech
  confidence: high
  context: built? Like you're, you clearly have at least the notion of a user interface
    of not actual user interface,
  name: Notion
  position: 40068
- category: unknown
  confidence: medium
  context: s inertia that we kind of need to address, right? And I mean, not to, I
    mean, I work at Google, so there'
  name: And I
  position: 44604
- category: ai_application
  confidence: high
  context: An AI-first tool for organizing ideas and making connections, built by
    Stephen Johnson.
  name: Notebook LM
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned via Notebook LM (notebooklm.google.com) and as the former employer
    of Ilia Polosuchen where he worked at Google Research.
  name: Google
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The division where Ilia Polosuchen worked, focusing on question answering
    and machine translation, leading to the Transformer architecture.
  name: Google Research
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The company co-founded by Ilia Polosuchen and Alex Kudanaf, focused on
    teaching machines to code, and currently approaching private AI.
  name: Nira AI
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A blockchain project co-founded by Ilia Polosuchen, initially to solve
    global payment issues for data labeling, now used for various financial use cases
    and AI workloads.
  name: Near Protocol
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a hardware provider whose recent hardware enabled confidential
    computing modes necessary for their decentralized confidential machine learning
    approach.
  name: Nvidia
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a hardware provider whose new fifth-generation Xeons enabled
    confidential computing modes necessary for their decentralized confidential machine
    learning approach.
  name: Intel
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: Mentioned as a model developer that gave its weights to Hugging Face, leading
    to a leak.
  name: Mistral
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as the recipient of Mistral's model weights, where a leak occurred.
  name: Hugging Face
  source: llm_enhanced
- category: historical_tech_analogy
  confidence: high
  context: Used as an analogy for a centralized entity controlling access to a fundamental
    technology (the internet), contrasting with the desired decentralized AI future.
  name: AOL
  source: llm_enhanced
- category: media_organization
  confidence: high
  context: The podcast hosting the discussion, implying an interest in the AI/ML space.
  name: Twombo AI podcast
  source: llm_enhanced
- category: potential_mention
  confidence: low
  context: Not explicitly mentioned by name, but the context of 'hot mic that records
    everything we say, although that is becoming popularized by some AI companies'
    might subtly refer to Microsoft's Copilot/Cortana integration, though this is
    weak.
  name: Microsoft
  source: llm_enhanced
- category: potential_mention
  confidence: low
  context: Not explicitly mentioned by name, but implied as one of the large labs
    developing frontier AI models.
  name: Meta
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as one of the largest AI labs and in relation to GDPR compliance
    issues regarding user data.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as an example of a model developer whose first model was state-of-the-art
    among open-rate models and did not use explicit user queries for tuning.
  name: DeepSeek
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned in comparison to OpenAI regarding differences in model 'vibe'
    or output characteristics.
  name: Claude
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned regarding its history of paying billions in fines and learning
    from user behavior/clicks for ML signal processing.
  name: Facebook
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as the product based on GPT 3.5, which used human labeling rather
    than direct user feedback for initial training.
  name: ChatGPT
  source: llm_enhanced
- category: content_provider/data_owner
  confidence: medium
  context: Used as an example of a content provider that could contribute data privately
    into secure enclaves for retrieval-time use and be compensated via tokens.
  name: New York Times
  source: llm_enhanced
- category: individual/leader
  confidence: medium
  context: Mentioned via a post where he discussed the high cost of building models
    and the need for new business models.
  name: Dario
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as an example of a traditional data center that could repurpose
    itself to become a member of the decentralized cloud discussed.
  name: Amazon
  source: llm_enhanced
- category: data_platform
  confidence: medium
  context: Mentioned in the context of the difficulty of compensating rights holders
    for content crawled from the internet.
  name: Tenor
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a platform that experienced data leaks where chat logs became
    visible and indexed.
  name: Grok
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The platform/infrastructure the speaker is working on, which forms an MPC
    network for secure enclaves.
  name: Near blockchain
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A serving engine (or a customized version thereof) used to serve models
    within the secure enclave environment.
  name: VLLM
  source: llm_enhanced
date: 2025-09-30 16:22:00 +0000
duration: 65
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: do that, but for text and really figure out how to learn
  text: we should do that, but for text and really figure out how to learn.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: solve this problem, right? We have this, you know, we talked to other
    people, other people have this as well
  text: we should solve this problem, right? We have this, you know, we talked to
    other people, other people have this as well.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/d2c5cdae21d34cd2be093f0ce7ec7765/
processing_date: 2025-10-06 05:03:19 +0000
quotes:
- length: 277
  relevance_score: 4
  text: And would you say that that is because, you know, we're just learning how
    to manipulate, you know, the vibe or or output characteristics of a model based
    on more kind of curated, you know, training data or feedback, or is there are
    there techniques that are enabling this shift
  topics: []
- length: 158
  relevance_score: 4
  text: And so you can also do that, or you can say, hey, I want it to be used at
    inference time as part of your search like retrieval index, but not not for training
  topics: []
- length: 81
  relevance_score: 4
  text: And then, I mean, the fine-tuning and kind of training, that's coming a bit
    later
  topics: []
- length: 48
  relevance_score: 3
  text: And the reality is back then, it wasn't possible
  topics: []
- length: 131
  relevance_score: 3
  text: And so, so the reality is actually like, if before this was like really valuable,
    for many use cases, now it's actually a liability
  topics: []
- length: 161
  relevance_score: 3
  text: I mean, and you know, maybe OpenAI has the money to pay their fine, like I
    mean, similar how Google and Facebook has paid, you know, billions of dollars
    in fines
  topics: []
- length: 292
  relevance_score: 3
  text: And you we started off talking about the fact that, you know, encryption is
    computationally complex, like it brings along its own costs, you know, relative
    to, you know, the per-token inference costs that, you know, someone might see,
    you know, whether it's OpenAI or open router or something
  topics: []
- impact_reason: Directly attributes the invention of the Transformer architecture
    to the practical need for efficient context processing and low latency in real-world
    Google products.
  relevance_score: 10
  source: llm_enhanced
  text: As part of that, due to even actual requirements and latency for Google.com,
    we were trying to figure out how to actually build a deep learning model that
    can consume lots of context and really reason about it without taking time and
    time to process that. And so that's where the Transformer architecture comes.
  topic: technical/history
- impact_reason: A strong philosophical warning about the societal danger of centralized
    AI control, equating monopolistic AI access to thought control, similar to Orwell's
    *1984*.
  relevance_score: 10
  source: llm_enhanced
  text: If we have only kind of a handful of kind of closed-source, like profit-driven
    companies dominating the space, we may end up in a 1984 type situation, right?
    Where you effectively have, you know, a company that, that, you know, can be very
    much not intentionally then, effectively deciding how everybody thinks, right?
  topic: safety/ethics
- impact_reason: Identifies the convergence of hardware advancements (Confidential
    Computing/Secure Enclaves) as the key enabler that made private, large-scale cloud
    AI feasible, bypassing the overhead of pure cryptographic methods.
  relevance_score: 10
  source: llm_enhanced
  text: The interesting thing happened is the hardware. So, Nvidia hardware and Intel,
    both kind of at a similar time, enabled this mode called confidential computing.
  topic: technical/breakthroughs
- impact_reason: 'Perfectly summarizes the core technological goal: achieving local-level
    privacy guarantees while leveraging the superior intelligence and scale of cloud-based
    models.'
  relevance_score: 10
  source: llm_enhanced
  text: Sounds like what you're trying to do is more, like, create a system that would
    allow, like, remote and cloud-based, but also private to the same level as local
    AI.
  topic: technical/predictions
- impact_reason: A strong statement on the shifting perception of user data from an
    asset to a liability due to regulatory and security burdens.
  relevance_score: 10
  source: llm_enhanced
  text: The other thing is for developers, actually, like if I'm an application developer,
    if five, 10 years ago, data was, you know, a gold mine, it's becoming an actual
    liability.
  topic: business/safety
- impact_reason: 'Explains the technical mechanism (secure enclaves) used to solve
    the dual privacy problem: protecting both the model weights (from the cloud/host)
    and the user data (from the model provider).'
  relevance_score: 10
  source: llm_enhanced
  text: because of secure enclaves, you can actually encrypt the model weights and
    they only get decrypted inside the secure enclave. And then user data is also
    private, right? So you effectively bring kind of privacy from both sides.
  topic: technical
- impact_reason: 'Defines the new paradigm: moving from noisy user feedback to ''verifiable
    results'' requiring specific, controlled human supervision or synthetic data.'
  relevance_score: 10
  source: llm_enhanced
  text: We're talking about like this transition from user feedback to verifiable
    results. It's a combination of data labeling, like indeed human, but like you
    actually want a very specific supervision and you want to control kind of what
    feedback you get.
  topic: technical/trends
- impact_reason: 'Presents a novel business model: tokenizing individual models to
    distribute value and reward contributors based on the model''s success.'
  relevance_score: 10
  source: llm_enhanced
  text: So we actually do, I mean, we talked about this like last year, like where
    effectively every model gets its own token.
  topic: business/monetization
- impact_reason: Detailed explanation of the tokenization model, directly linking
    data contribution to ownership/reward share in the resulting AI asset.
  relevance_score: 10
  source: llm_enhanced
  text: whoever contributes data gets a token of this model that's trained on their
    data. And then the revenue is distributed to the stockholders, right, as a model
    like for the model's lifetime.
  topic: business/monetization
- impact_reason: 'Proposes a new paradigm: ''Open Process'' where data and weights
    are encrypted/controlled, allowing for monetization while maintaining accessibility
    for research (via secure enclaves/DCML).'
  relevance_score: 10
  source: llm_enhanced
  text: So the training, I mean, the data either fully open or this like kind of encrypted
    data, right, that you can run over. So available but not necessarily transparent.
    Yeah. Yeah. And you need to pay to access it... And then the resulting weights
    are actually also encrypted and and only run in this kind of DCML model. So you
    can actually monetize it.
  topic: technical/business strategy
- impact_reason: 'Shifts the focus of AI value proposition: once general intelligence
    is achieved, the differentiator becomes the depth and breadth of context the user
    allows the model to access.'
  relevance_score: 10
  source: llm_enhanced
  text: The more you give, the better. Like it actually we're getting to the stage
    where models are generally like there's sufficiently intelligent and actually
    it's all becomes about context, like about context management, about tool management,
    about all the pieces, right?
  topic: AI technology trends/predictions
- impact_reason: 'Highlights the core security mechanism: using secure enclaves to
    protect the model weights and data during inference from external interception.'
  relevance_score: 10
  source: llm_enhanced
  text: It's the secure enclave that ensures that there's no kind of man-in-the-middle
    attack between VLLM and like the model weights and the customer data.
  topic: safety/technical
- impact_reason: Details the integration of MPC (Multi-Party Computation) with the
    blockchain to manage decryption keys securely within the enclave, a complex but
    powerful security primitive.
  relevance_score: 10
  source: llm_enhanced
  text: The Near blockchain itself kind of right now, part of our nodes, form this
    multi-party computation network which allows inside the secure enclave effectively
    have its own private key to decrypt things.
  topic: technical
- impact_reason: 'Crucial data point: Confidential computing overhead for inference
    is minimal (1-5%), addressing a major historical concern about performance degradation.'
  relevance_score: 10
  source: llm_enhanced
  text: The overhead on the computation side is one to 5%. Yeah, so it's very minimal
    and it's mostly just, I mean, it's like encryption decryption on a boundary, yeah,
    and kind of constrained by that, not by computation.
  topic: technical
- impact_reason: Uses specific, recent examples (OpenAI human review) to illustrate
    the concrete risks of non-private AI systems, serving as a strong adoption driver.
  relevance_score: 10
  source: llm_enhanced
  text: There is, I mean, we've seen this with OpenAI, right? There's news that the
    fact that scanning all the chat logs and then the ones that are flagged are sent
    to human evaluation and then to police, right? So you're actually have potentially
    humans looking at your chat logs.
  topic: safety
- impact_reason: This is a significant shift in perspective on data, moving from an
    asset to a liability due to increasing regulatory complexity (GDPR, CCPA, data
    taxes). It frames the need for privacy-preserving AI solutions.
  relevance_score: 9
  source: llm_enhanced
  text: If I'm an application developer, if five, 10 years ago data was a gold mine,
    it's becoming an actual liability.
  topic: business/strategy
- impact_reason: A critical observation on the shift in the AI research ecosystem
    from open collaboration to proprietary secrecy, driving the need for decentralized
    alternatives.
  relevance_score: 9
  source: llm_enhanced
  text: It went from, you know, this was open research, everybody was contributing,
    the papers published, you know, transformer code was out, you know, for everybody
    to build on top to like kind of everybody was keeping secrets, things are starting
    to like close up.
  topic: safety/strategy
- impact_reason: Provides a concise distinction between the impact of the internet
    (information access) and the impact of AI (processing and decision-making), underscoring
    why AI centralization is more dangerous.
  relevance_score: 9
  source: llm_enhanced
  text: The internet is information, but this is actually like the processing, the
    decision-making.
  topic: safety/strategy
- impact_reason: Quantifies the immense computational cost associated with traditional
    privacy-preserving techniques (HE, ZKPs), explaining why they were impractical
    for large-scale ML until recently.
  relevance_score: 9
  source: llm_enhanced
  text: All the methods that people use, for example, for privacy, for verifiability,
    are extremely expensive, right? There's like homomorphic encryption, there's ZK
    proof, etc. All of them have, you know, like 10,000 to 100,000 times overhead.
  topic: technical/limitations
- impact_reason: 'Defines the value proposition of their technology: lowering the
    user''s internal ''trust threshold'' by guaranteeing that no party (developer,
    operator, model owner) can access the data.'
  relevance_score: 9
  source: llm_enhanced
  text: So there's always a threshold where you kind of get all like, um, maybe I
    shouldn't do that, right? And so what we offer is effectively removing that threshold
    and say, hey, actually, it's so confidential, all enter and encrypted for you.
  topic: business/value proposition
- impact_reason: 'This clearly defines the core value proposition: achieving local-level
    privacy in a remote/cloud setting, which addresses a major adoption barrier for
    sensitive use cases.'
  relevance_score: 9
  source: llm_enhanced
  text: And so it's the idea that, oh, there's like so many questions I'm trying to
    ask here. So like you're describing a system that, you know, many people say,
    like, if I don't, if I can't run this locally on my machines, I'm not going to
    run it. But it sounds like what you're trying to do is more, like, create a system
    that would allow, like, remote and cloud-based, but also private to the same level
    as local AI.
  topic: technical/strategy
- impact_reason: Provides concrete examples of the regulatory landscape (GDPR, China
    data tax) driving the shift in data liability.
  relevance_score: 9
  source: llm_enhanced
  text: And it's become liability, both like in Europe, for example, there's GDPR
    data privacy. We have California data privacy. There's all this kind of different
    data privacy laws that popping up. In China, you actually need to pay a data tax
    if you're using consumer data.
  topic: safety/business
- impact_reason: 'Identifies a critical trust issue for model developers: the risk
    of proprietary model weights leaking when using third-party cloud infrastructure.'
  relevance_score: 9
  source: llm_enhanced
  text: The challenge is actually this model developers don't trust third parties.
    Because they're afraid that their model will leak. And this has happened where
    the model weights have leaked from third parties.
  topic: technical/safety
- impact_reason: Provides a specific, high-profile example (Mistral/Hugging Face)
    illustrating the risk of model weight leakage, validating the security concerns.
  relevance_score: 9
  source: llm_enhanced
  text: So Mistral gave its weights to Hugging Face and it didn't doubt one for chat.
  topic: technical/safety
- impact_reason: Provides a clear topological definition for this novel infrastructure—a
    'decentralized, residential cloud'—distinguishing it from traditional centralized
    clouds.
  relevance_score: 9
  source: llm_enhanced
  text: Topologically, this is this is compute hardware, let's say GPUs and CPUs that
    live in this decentralized, residential cloud.
  topic: technical/strategy
- impact_reason: Precisely maps out the data flow and access rights in this new architecture,
    confirming that neither the cloud operator nor the model provider sees the raw
    user data.
  relevance_score: 9
  source: llm_enhanced
  text: As a user, as an end user, like I'm contributing my data in some way because
    I want some processing on my data or to access intelligence. And the cloud can't
    access my data. And presumably the model provider can't access my data, but the
    model provider, like it's providing the model into this cloud and it can access
    my data and return some results back to me. Correct.
  topic: technical/safety
- impact_reason: Signals a major shift in model improvement methodology away from
    relying solely on raw user interaction data (like clicks/queries) towards more
    curated feedback.
  relevance_score: 9
  source: llm_enhanced
  text: why I think the space is transitioning from user feedback is so let's use
    a DeepSeek example. So when DeepSeek releases their first model, right, which
    was, you know, at least at the time from the open-rate models was state-of-the-art.
    And like ours, for example, for our one, it did not use the explicit like user
    queries, right?
  topic: technical/trends
- impact_reason: 'Explains *why* the shift is happening: foundational models have
    absorbed generic knowledge, pushing innovation toward specialized, verifiable,
    and expert-driven fine-tuning.'
  relevance_score: 9
  source: llm_enhanced
  text: So part of it is we've collected enough data and we've kind of baked the generic
    stuff into the foundation model. So now where the innovation is happening is,
    you know, bringing in more subject matter expertise or specialized skills or indeed
    like a verifiable thing or combining like, you know, synthesizing.
  topic: strategy/technical
- impact_reason: 'Highlights the shift in data strategy for LLMs: moving from heavy
    reliance on explicit user interaction data (like clicks) to massive amounts of
    unsupervised data supplemented by targeted, specialized human labeling.'
  relevance_score: 9
  source: llm_enhanced
  text: I think what LLMs did is kind of transition that to like, hey, we actually
    just pass a lot of unsupervised data, right? And not not like user click data.
    And then we add a little bit of a human, like a very specific human label data.
  topic: technical/data strategy
- impact_reason: Suggests that blockchain technology is the key enabler for creating
    sustainable, equitable data contribution models that reward users economically.
  relevance_score: 9
  source: llm_enhanced
  text: The underlying blockchain has the mechanism to make that tenable in a way
    that it's not tenable today [for data contribution rewards].
  topic: business/technology integration
- impact_reason: Shows how tokenization can solve the seemingly intractable problem
    of compensating original content creators whose data is used in large-scale web
    crawls.
  relevance_score: 9
  source: llm_enhanced
  text: But just you describing this token model, it's kind of like, oh, well, you
    maybe you could do that [compensate rights holders for crawled content]. Like
    you're crawling a site, you know that site, you know, you reserve a token for
    that site.
  topic: safety/ethics/business
- impact_reason: Illustrates granular control over data usage (training vs. inference)
    enabled by new architectures, crucial for enterprise adoption and privacy.
  relevance_score: 9
  source: llm_enhanced
  text: So you can contribute data privately. So you can say, hey, I want this data
    to be used in the model training, but I don't want anyone to see it, right? For
    example, right? And so you can also do that, or you can say, hey, I want it to
    be used at inference time as part of your search like retrieval index, but not
    not for training.
  topic: safety/privacy/technical
- impact_reason: 'Identifies the primary bottleneck for open-source AI development:
    the lack of a viable monetization path for model creators.'
  relevance_score: 9
  source: llm_enhanced
  text: I was talking about like, hey, open source is going to catch up. Because yeah,
    I mean, I think I think the challenges like with pure open source right now, and
    again, this is something we're solving, is that I built a model, I released it,
    and it's like, cool, you know, here is here's the stars on GitHub, stars on Hugging
    Face, but then you don't make any money.
  topic: business/open source strategy
- impact_reason: 'The ultimate goal for personalized AI: achieving holistic life management
    by gaining access to sensitive, comprehensive user data due to trust established
    via privacy guarantees.'
  relevance_score: 9
  source: llm_enhanced
  text: And so it's able to manage your whole life, not just like some aspects that
    you were willing to share.
  topic: predictions/business
- impact_reason: Connects the open/monetized research process directly to the proliferation
    of high-quality, specialized models (e.g., finance, healthcare).
  relevance_score: 9
  source: llm_enhanced
  text: The other side of this is actually because of this open research process what
    we are aiming for is to have people again fine-tuning specialized models for specialized
    use cases, right?
  topic: business/AI adoption
- impact_reason: Highlights how secure, localized deployment (running the app/model
    within the user's trusted data enclave) removes the liability hurdle for third-party
    application developers.
  relevance_score: 9
  source: llm_enhanced
  text: liability, and also, you know, now it's a hurdle for everybody else to adopt
    it, or you can just say, actually, I'm going to build my app and deploy it into
    this cloud where it runs on it on your side, right? You know, and saves context
    there as well in your data store.
  topic: business/strategy
- impact_reason: Articulates a powerful network effect loop centered on the user's
    data store, where context, applications, and AI utility mutually reinforce each
    other.
  relevance_score: 9
  source: llm_enhanced
  text: The idea is like you have network effects of kind of more context, more data,
    more applications building around the user.
  topic: strategy
- impact_reason: Highlights the benefit of data portability and identity ownership,
    contrasting with current platform lock-in models (like ChatGPT/Google).
  relevance_score: 9
  source: llm_enhanced
  text: everybody who logs in will get all their messages, older history, older memory,
    all the ups with them. So it's also like kind of detaching your identity from
    a specific application but.
  topic: strategy/predictions
- impact_reason: Identifies 'inertia' (user comfort with existing giants like Google/OpenAI)
    as the primary barrier to adoption, rather than purely technical feasibility.
  relevance_score: 9
  source: llm_enhanced
  text: What do you say to the skeptic that says, you know, it all sounds too good
    to be true? ... I mean, the hard part right now is inertia, right?
  topic: business/strategy
- impact_reason: Cites specific data leak incidents (Grok) to reinforce the reality
    of security failures in incumbent AI platforms.
  relevance_score: 9
  source: llm_enhanced
  text: We had obviously data leaks from Grok and others that like, you know, your
    chat logs got visible and indexed.
  topic: safety
- impact_reason: Presents a future vision where decentralized, localized confidential
    computing could *reduce* latency compared to centralized cloud providers by bringing
    compute closer to the user.
  relevance_score: 9
  source: llm_enhanced
  text: Ideally, actually latency is less because ideally right now, yeah, you actually
    should be accessing the closest GPU ideally in your city.
  topic: predictions/strategy
- impact_reason: 'Summarizes the end-to-end verifiable process: local encryption,
    on-chain verification, and a signed response confirming the exact model ran on
    the user''s data.'
  relevance_score: 9
  source: llm_enhanced
  text: And so this is where that doesn't kind of this NPC network enables that. So
    like effectively, you know, you encrypt locally, you upload it, checkpoint it.
    And now when a user calls, they know that like effectively the secure enclave
    will respond that this model hash was run on your data.
  topic: technical
- impact_reason: Highlights a potential business model shift where developers can
    offer intelligence without directly handling sensitive user data, relying on secure
    computation platforms.
  relevance_score: 8
  source: llm_enhanced
  text: This actually creates a platform where you as a developer don't need to deal
    with the use of data, pushing software to them.
  topic: business/strategy
- impact_reason: Captures the skepticism surrounding large-scale AI capabilities just
    a few years ago, emphasizing the critical role of compute scaling in enabling
    today's breakthroughs.
  relevance_score: 8
  source: llm_enhanced
  text: Back then, nobody thought what's happening right now is possible, right? So
    it was like, whatever pitching was kind of somewhere between science fiction and
    fusion. The reality is back then, it wasn't possible. The compute wasn't there,
    right?
  topic: predictions/strategy
- impact_reason: Illustrates how real-world operational friction (global micro-payments
    for data labeling) directly led to the exploration and eventual founding of a
    blockchain solution (Near Protocol).
  relevance_score: 8
  source: llm_enhanced
  text: In China, people don't have bank accounts, they have WeChat Pay. In Ukraine,
    for example, you need to sell half of your dollars on the rival if it's in a foreign
    currency... And so we started looking at blockchain as like, hey, this global
    payment network that people are talking about, you know, we can just use it right
    to pay people.
  topic: business/strategy
- impact_reason: Defines the core mission of 'user-owned AI' as a direct countermeasure
    to corporate centralization, emphasizing user agency over corporate control.
  relevance_score: 8
  source: llm_enhanced
  text: The realization was that if we have only kind of a handful of kind of closed-source,
    like profit-driven companies dominating the space... create the AI that actually
    is on the user side, on, you know, your side, not their side.
  topic: strategy
- impact_reason: 'Provides a clear, high-level explanation of Confidential Computing:
    isolating computation so that even the cloud provider cannot view the data or
    the process.'
  relevance_score: 8
  source: llm_enhanced
  text: Inside the chips itself, there's a, like you can enable it in such a way that
    even the owner of the hardware of the compute is not able to access what computation
    is happening inside.
  topic: technical
- impact_reason: Highlights the enduring need for cloud infrastructure even with powerful
    local models, specifically for continuous background processing, agents, and workflows.
  relevance_score: 8
  source: llm_enhanced
  text: I mean, obviously they're not as intelligent as what you can have in the cloud.
    They know this fast. But importantly, also like, you know, even if we have like
    a smarter model, you still have a lot of things that are happening in the background
    that you want to, like, keep happening, right? You want, you know, set up an agent
    that runs and re-dolls and uses and summarizes it and processes it or workflows,
    etc. So there's always going to be a need for background work and analysis and
    kind of surfacing it even as local models improve.
  topic: strategy/predictions
- impact_reason: Describes a new business model for developers where they ship software/logic
    without inheriting the data compliance burden.
  relevance_score: 8
  source: llm_enhanced
  text: And so this, this actually creates a platform where you as a developer don't
    need to deal with the user data. You're actually pushing software to them. Again,
    similar how local works, right? You push the application, you know, to the user
    and then translate their device, you don't need to deal with whatever data.
  topic: business/strategy
- impact_reason: Raises a critical, unresolved legal/compliance question regarding
    the 'right to be forgotten' (GDPR) when data is incorporated into model training
    sets.
  relevance_score: 8
  source: llm_enhanced
  text: I use OpenAI. OpenAI, I train on my data and then I go and I invoke my GDPR
    law and say, hey, remove all my data. I'm assuming that that that is not a resolved
    issue either because I can't believe it's because no one has asked yet.
  topic: safety/legal
- impact_reason: Warns that data liability risks are existential for smaller model
    developers, unlike large players who can absorb fines.
  relevance_score: 8
  source: llm_enhanced
  text: But if you're a smaller model developer, that's why I was kind of using, you
    know, other examples. This effect, it can be like existential.
  topic: business/safety
- impact_reason: Illustrates the increasing specialization required for high-quality
    labeling as models become more capable; generic labelers are insufficient for
    advanced tasks.
  relevance_score: 8
  source: llm_enhanced
  text: And so that we also need like the better the foundational model, almost like
    the more complex things we want people to label. And so kind of like against for
    our example, we were finding computer science students because we needed people
    who code.
  topic: technical/business
- impact_reason: A strong cautionary note on data acquisition, emphasizing the legal
    and ethical risks of using passively collected user data versus proactively compensating
    contributors for rights.
  relevance_score: 8
  source: llm_enhanced
  text: It's better to just pay people to contribute their audio and like sign off
    the rights.
  topic: safety/ethics/data strategy
- impact_reason: Points out the inefficiency in the current 'open weights' paradigm
    where lack of process transparency forces widespread, redundant experimentation.
  relevance_score: 8
  source: llm_enhanced
  text: And so in result, we're actually wasting a lot of resources because everybody
    kind of redoing experiments because we actually don't know what were the things
    people did to get to these results.
  topic: strategy/technical
- impact_reason: Argues that their proposed encrypted/monetized model retains the
    core benefit of open source (accessibility/usability) while solving the monetization
    problem.
  relevance_score: 8
  source: llm_enhanced
  text: The benefit is you still get all the properties of open source, right? Everybody
    can use it. There's no like way to stop using it. You can even run it on your
    hardware if you have the modern like Blackwell or Hopper. You need to set it up
    in this confidential mode.
  topic: strategy/open source
- impact_reason: A counter-argument suggesting that the lack of truly state-of-the-art
    open models stifles deeper, more fundamental research ('brain surgery') into model
    architectures.
  relevance_score: 8
  source: llm_enhanced
  text: I, you know, though, I like the pushback that I would offer is that like if
    the best models were open, you know, maybe we'd see a lot more brain surgery and
    maybe we'd have a more interesting kind of ecosystem of results.
  topic: technical/strategy
- impact_reason: Describes the specific mechanism for securing model provenance and
    data integrity using blockchain hashing and decentralized storage before execution
    in a secure enclave.
  relevance_score: 8
  source: llm_enhanced
  text: You checkpoint your, you know, model weights on-chain, so you know the hash,
    the hash of the model weights, and encrypted hash as well. You know, the encrypted
    data is uploaded kind of decentralized storage.
  topic: technical/security
- impact_reason: 'Clearly defines the trust boundary for custom models: the user must
    trust the integrity of the Docker container running the model inside the enclave.'
  relevance_score: 8
  source: llm_enhanced
  text: The trust boundary is the container, and if the container is doing what you
    say it's doing, then the signature certifies that that was the container that
    was actually used.
  topic: safety/technical
- impact_reason: Identifies a specific consumer-facing product ('private judge GPT')
    built on their confidential infrastructure, demonstrating a clear use case.
  relevance_score: 8
  source: llm_enhanced
  text: as well as we have a kind of consumer product which is, you know, private
    judge. Yes, you know, private judge GPT effectively, which indeed provides you
    all the kind of certification and replication information if you want while using
    it.
  topic: business/predictions
- impact_reason: 'Articulates the core product strategy: achieving feature parity
    or superiority with existing, non-private solutions.'
  relevance_score: 8
  source: llm_enhanced
  text: The goal is to make it everything is like either the same or better, right?
    Like you either don't, I mean, it looks exactly, I mean, very similar experience,
    right?
  topic: strategy
- impact_reason: Perfectly summarizes the mindset of the segment of users who are
    already deeply integrated into large ecosystems and feel they have already traded
    privacy for convenience.
  relevance_score: 8
  source: llm_enhanced
  text: Google has my email and my calendar anyway, why do I care if it's private?
  topic: business/safety
- impact_reason: Reiterates that the fundamental value proposition is shifting from
    pure capability to verifiable trust in AI interactions.
  relevance_score: 8
  source: llm_enhanced
  text: I guess another question that I have is that I think it, you know, when you
    boil it down, a lot of the value proposition here is, you know, around trust and
    the user being able to trust the interactions they're having with AI.
  topic: strategy
- impact_reason: 'A clear warning about the limits of the secure enclave model: while
    the enclave protects against external snooping, the user still relies on the integrity
    of the software image they choose to run.'
  relevance_score: 8
  source: llm_enhanced
  text: If somebody builds a custom Docker container, you can do that, but then the
    user needs to trust your Docker container.
  topic: safety
- impact_reason: Provides historical context on the motivation behind early large-scale
    language model research, linking back to seminal computer vision work (AlexNet/Cat
    paper) and the drive to apply similar concepts to NLP.
  relevance_score: 7
  source: llm_enhanced
  text: I joined Google Research because I saw the cat neuron paper, if folks remember
    that, and I was like, okay, we should do that, but for text and really figure
    out how to learn.
  topic: technical/history
- impact_reason: Reveals the specific early application focus (code generation) that
    motivated the co-founder of Nira AI to leave Google immediately after the Transformer
    concept emerged.
  relevance_score: 7
  source: llm_enhanced
  text: I was excited about the idea of machines writing code. And so in 2017, I left,
    and with my co-founder Alex Kudanaf, we started Nira AI.
  topic: business/history
- impact_reason: 'Coined term summarizing the speaker''s approach: combining decentralized
    architecture (blockchain) with hardware-level privacy guarantees (confidential
    computing) for ML.'
  relevance_score: 7
  source: llm_enhanced
  text: We call it decentralized confidential machine learning.
  topic: technical/strategy
- impact_reason: Connects abstract privacy concepts to relatable human behavior (not
    walking around with a hot mic), emphasizing that trust thresholds exist even for
    users who claim not to care about privacy.
  relevance_score: 7
  source: llm_enhanced
  text: Confidentiality is a combination of things, right? First of all, for the user...
    But there's important kind of interesting effects that there's still something
    that you're not going to trust. Like, you know, we don't normally walk around
    with like a hot mic that records everything we say...
  topic: safety/ethics
- impact_reason: Suggests that this decentralized cloud environment can offer *stronger*
    security guarantees than a purely local setup by adding network-level controls.
  relevance_score: 7
  source: llm_enhanced
  text: And then you can potentially even add additional, like, you know, pin code
    to FA, etc. Like you can actually restrict things that you may not even have on
    the local host. Because I mean, local host, you still can access the hard drive
    physically here.
  topic: safety/technical
- impact_reason: 'Frames the central non-technical challenge for privacy-preserving
    AI: overcoming user inertia and perceived inconvenience to monetize privacy features.'
  relevance_score: 7
  source: llm_enhanced
  text: Historically, you know, there's always been this big barrier that's not at
    all technical, and that is, will people pay for privacy?
  topic: business/privacy
- impact_reason: Technical insight confirming that their secure deployment environment
    is compatible with existing, standard serving frameworks (like VLLM), minimizing
    migration friction.
  relevance_score: 7
  source: llm_enhanced
  text: We actually run, you know, VLLM and a customized version of the LLM. So everything
    that you know normally served already works.
  topic: technical/deployment
- impact_reason: Analogizes the complex verification process to familiar web security
    indicators (HTTPS), aiming for user trust through intuitive UI feedback.
  relevance_score: 7
  source: llm_enhanced
  text: And in our UI, we have effectively like, you know, like a green shield that
    you can click, similar like like HTTPS works to go there and gives you like it
    gives you like it's all correct.
  topic: business/strategy
- impact_reason: 'Pinpoints the central business challenge for privacy-focused AI:
    convincing users that the added security justifies any perceived friction or cost.'
  relevance_score: 7
  source: llm_enhanced
  text: I think the kind of as we just discussed, right? Like are people willing to
    pay for it?
  topic: business
- impact_reason: Suggests that privacy enables new product capabilities, moving beyond
    just security to feature differentiation.
  relevance_score: 7
  source: llm_enhanced
  text: the idea that because it's kind of private, you can also have additional features
    that you wouldn't have in in the public.
  topic: strategy
- impact_reason: Acknowledges the massive network effect and user base of competitors,
    setting a high bar for product quality required for switching.
  relevance_score: 7
  source: llm_enhanced
  text: OpenAI already has whatever half a billion users or a billion users. So we
    need to have a product that indeed can deliver on on, you know, if people want
    to like switch and and use.
  topic: business
- impact_reason: Foreshadows a trend toward highly distributed, potentially mobile,
    infrastructure tailored for low-latency, localized inference.
  relevance_score: 7
  source: llm_enhanced
  text: there's like you, I mean, I've talked to people who like, you know, they're
    going to be data centers everywhere, people build like mobile data centers, etc.
  topic: predictions
- impact_reason: 'Describes a classic technology adoption curve strategy: targeting
    privacy-conscious early adopters first, then expanding as the ecosystem matures.'
  relevance_score: 7
  source: llm_enhanced
  text: And so yes, like it starts with early adopters who care about privacy and
    kind of layers on as more and more applications and things become available on
    this cloud.
  topic: strategy
- impact_reason: Details a practical, early-stage data acquisition strategy involving
    crowdsourcing specialized data (code/descriptions) from students globally.
  relevance_score: 6
  source: llm_enhanced
  text: We found kind of a niche of people around the world who would do this for
    reasonably cheap... we build like a crowdsourcing platform for computer science
    students where they can go and, you know, effectively practice their coding and
    get paid.
  topic: business/practical lessons
- impact_reason: Indicates the project is past the purely theoretical stage and is
    actively being tested by early adopters.
  relevance_score: 6
  source: llm_enhanced
  text: we have a product that we can, I mean, that we're in testing and alpha testing
    with, you know, kind of a cohort of users.
  topic: business
- impact_reason: Suggests that hardware advancements are mitigating the historical
    performance penalty associated with encryption.
  relevance_score: 6
  source: llm_enhanced
  text: Is a lot of that stuff getting pushed into the hardware? Like, is it an issue
    for you? Not really.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: The Decentralized Future of Private AI with Illia Polosukhin
  - #749


  This episode of the Twombo AI podcast features Sam Charrington in conversation with
  **Illia Polosukhin**, co-founder of **Nira AI** and co-author of the seminal "Attention
  Is All You Need" paper that introduced the Transformer architecture. The discussion
  centers on Polosukhin''s journey from foundational AI research to co-founding **Near
  Protocol** (a major blockchain) and his current focus on building a **decentralized,
  private AI ecosystem** using emerging hardware capabilities.


  ### 1. Focus Area

  The primary focus is the **Decentralized Future of Private AI**. The conversation
  bridges Polosukhin''s history in large language models (LLMs) and distributed systems
  (blockchain) to address the growing centralization and privacy risks associated
  with proprietary, cloud-based AI. The core solution proposed involves leveraging
  **Confidential Computing** hardware to enable private, scalable AI computation outside
  of traditional centralized cloud providers.


  ### 2. Key Technical Insights

  *   **Confidential Computing as the Enabler:** The convergence of recent hardware
  advancements (Intel''s 5th-gen Xeons and specific Nvidia modes) enabling **secure
  enclaves** is the critical technical breakthrough. This allows computation to occur
  in an encrypted state, inaccessible even to the hardware owner or cloud operator,
  providing "local-level" privacy for cloud-based workloads.

  *   **Decentralized Confidential Machine Learning (DCML):** This framework combines
  user data privacy (via enclaves) with model weight security (encrypting model weights
  until they are loaded into the enclave). This solves the trust issue for both users
  (data privacy) and model developers (IP protection).

  *   **Shifting Data Dynamics:** Data, once a "gold mine," is becoming a **liability**
  for application developers due to increasing global regulations (GDPR, etc.) and
  data taxes (China). This incentivizes pushing computation and software execution
  toward the user, rather than centralizing data collection.


  ### 3. Business/Investment Angle

  *   **Data Liability for Developers:** The regulatory burden and cost associated
  with handling consumer data are making centralized data collection a significant
  business risk, favoring models where software is pushed to the user.

  *   **Solving Model Developer Trust:** By encrypting model weights within secure
  enclaves, the platform removes the risk of model IP leakage when developers rent
  compute resources from third parties (a major barrier to decentralized deployment).

  *   **The Residential Cloud:** The concept opens up a new market for utilizing distributed,
  residential/non-traditional GPU/CPU resources as a trusted, private cloud layer,
  moving beyond the major hyperscalers.


  ### 4. Notable Companies/People

  *   **Illia Polosukhin:** Central figure, tracing his path from the Transformer
  paper to solving global payment issues (leading to Near Protocol) and now focusing
  on private AI infrastructure.

  *   **Near Protocol:** Mentioned as a highly utilized blockchain (50M MAUs) born
  from the need for cheap, global microtransactions, which now serves as a foundation
  for AI workloads like data labeling.

  *   **Nira AI:** Polosukhin''s current venture focused on private AI.

  *   **Hugging Face/Mistral:** Mentioned as an example where model weight leakage
  occurred after weights were shared with a third party, illustrating the need for
  model-side confidentiality.


  ### 5. Future Implications

  The conversation suggests a future where **intelligence as a technology** is not
  monopolized by a few centralized, profit-driven entities (likened to an "AOL internet").
  Instead, AI will become a user-owned, sovereign technology, accessible via a decentralized,
  confidential cloud layer. This shift is necessary to prevent a "1984 type situation"
  where a handful of companies control how information is processed and perceived.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Blockchain Developers, Infrastructure
  Architects, and Technology Strategists** concerned with data governance, regulatory
  compliance, and the long-term decentralization of foundational technologies.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- investment
- google
- nvidia
- openai
title: 'The Decentralized Future of Private AI with Illia Polosukhin - #749'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 119
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 16
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 9
  prominence: 0.9
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 3
  prominence: 0.3
  topic: startup
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 05:03:19 UTC -->
