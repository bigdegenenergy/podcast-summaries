---
companies:
- category: unknown
  confidence: medium
  context: 0 million people worldwide that want to watch it. When I first joined,
    the goal really was about data unif
  name: When I
  position: 95
- category: unknown
  confidence: medium
  context: We did that by building out what we now call the Televisa Univision Household
    Graph, the representation of our best guess of who we t
  name: Televisa Univision Household Graph
  position: 324
- category: unknown
  confidence: medium
  context: s interested in Spanish language media across the United States. We take
    what data is available and make either i
  name: United States
  position: 464
- category: unknown
  confidence: medium
  context: four years that we've been a part of it. That was Sergey Fogelson, VP of
    Data Science at Televisa Univision, the wo
  name: Sergey Fogelson
  position: 796
- category: unknown
  confidence: medium
  context: een a part of it. That was Sergey Fogelson, VP of Data Science at Televisa
    Univision, the world's largest Spanis
  name: Data Science
  position: 819
- category: unknown
  confidence: medium
  context: . That was Sergey Fogelson, VP of Data Science at Televisa Univision, the
    world's largest Spanish language media compa
  name: Televisa Univision
  position: 835
- category: unknown
  confidence: medium
  context: 300 million people worldwide. In this episode of High Signal, I talk with
    Sergey about what it means to build
  name: High Signal
  position: 1002
- category: unknown
  confidence: medium
  context: ks are in the show notes. Let's now check in with Duncan Gilchrist from
    Delphina before we jump into the interview.
  name: Duncan Gilchrist
  position: 1793
- category: unknown
  confidence: medium
  context: from Delphina before we jump into the interview. Hey Duncan. Hey Hugo,
    how are you? So before we jump into th
  name: Hey Duncan
  position: 1859
- category: unknown
  confidence: medium
  context: na before we jump into the interview. Hey Duncan. Hey Hugo, how are you?
    So before we jump into the conversa
  name: Hey Hugo
  position: 1871
- category: unknown
  confidence: medium
  context: re up to at Delphina and why we make High Signal. At Delphina, we're building
    AI agents for Data Science, and t
  name: At Delphina
  position: 2046
- category: unknown
  confidence: medium
  context: o much for having me, Hugo. A real pleasure, man. So Televisa Univision
    is the largest Spanish language media company in
  name: So Televisa Univision
  position: 2986
- category: unknown
  confidence: medium
  context: e basically is the largest customer of the other. So I think it made a
    lot of sense, especially in the c
  name: So I
  position: 3780
- category: unknown
  confidence: medium
  context: fore, I went to a soccer match in Argentina, or a Boca Juniors match. And
    this was one of the wildest experience
  name: Boca Juniors
  position: 5410
- category: unknown
  confidence: medium
  context: tches are definitely an interesting experience in South America. So I am
    interested, you're VP of Data Science. A
  name: South America
  position: 5738
- category: unknown
  confidence: medium
  context: a. So I am interested, you're VP of Data Science. And I'm wondering if
    you just tell us a bit about the d
  name: And I
  position: 5800
- category: unknown
  confidence: medium
  context: .com website on a few of our legacy applications. So Univision back then
    had, for example, apps that were hyper-
  name: So Univision
  position: 6804
- category: unknown
  confidence: medium
  context: le, apps that were hyper-local. So if you were in Los Angeles, there was
    a Los Angeles local Univision app for
  name: Los Angeles
  position: 6891
- category: unknown
  confidence: medium
  context: ications, etc. We built a CDP, an in-house CDP, a Customer Data Platform,
    that allows us to basically manage those relatio
  name: Customer Data Platform
  position: 11364
- category: unknown
  confidence: medium
  context: stem looked like, and really before the launch of Paramount Plus. Back
    then, that ecosystem was basically very sim
  name: Paramount Plus
  position: 12750
- category: tech
  confidence: high
  context: mbedding, whatever those entities are. And I know Anthropic has done some
    really fascinating work with doing
  name: Anthropic
  position: 20072
- category: unknown
  confidence: medium
  context: I'm wondering if we could just tie it back to the Household Graph, so if
    you could remind us what this flagship ass
  name: Household Graph
  position: 24336
- category: unknown
  confidence: medium
  context: ties it up to the specific person's PII, so their Personally Identifiable
    Information, to any component of our graph, and that's really
  name: Personally Identifiable Information
  position: 25358
- category: unknown
  confidence: medium
  context: for us, they lead to increases in revenue, right? The CPMs that you can
    get for delivering content on a smar
  name: The CPMs
  position: 30263
- category: unknown
  confidence: medium
  context: oing some interesting personalization work there. The Clips product is
    basically a combination of short-form
  name: The Clips
  position: 36749
- category: unknown
  confidence: medium
  context: different highlights of different goals from the Champions League or wherever.
    So we're starting to do some more in
  name: Champions League
  position: 37674
- category: unknown
  confidence: medium
  context: show notes, link to an episode that we have with Roberto Mejri, who's VP
    of Data at Instagram, who he launched R
  name: Roberto Mejri
  position: 39275
- category: tech
  confidence: high
  context: ls, and how it was really existential for him and Meta as well, of course,
    because of TikTok, and how th
  name: Meta
  position: 39436
- category: unknown
  confidence: medium
  context: to be clear, a lot of the successes in generative AI I've seen, for example,
    in people building customer
  name: AI I
  position: 44370
- category: unknown
  confidence: medium
  context: rovider. So we told it explicitly that we were on Cloud Provider A, doing
    XYZ, and it basically gave us a step—so it
  name: Cloud Provider A
  position: 47207
- category: unknown
  confidence: medium
  context: ng XYZ, and it basically gave us a step—so it was Cloud Provider 1 for
    steps one through four, and then part of st
  name: Cloud Provider
  position: 47278
- category: ai_application
  confidence: high
  context: The company hosting the podcast ('High Signal') that is building AI agents
    for Data Science.
  name: Delphina
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Mentioned as a system where data (like true logs) was stored at ViacomCBS/Paramount,
    likely referring to their analytics/marketing cloud products.
  name: Adobe
  source: llm_enhanced
- category: media_tech
  confidence: high
  context: Sergey's previous employer, where he worked on their digital ecosystem
    and data modeling before Paramount Plus launched.
  name: Paramount (ViacomCBS)
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: The streaming service launched by Paramount, mentioned in the context of
    Sergey's previous work.
  name: Paramount Plus
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned in the context of doing fascinating work with text and using
    sparse autoencoders to analyze embeddings.
  name: Anthropic
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as an ancient algorithm used to generate embeddings from unstructured
    text.
  name: Word2Vec
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned alongside Word2Vec as an algorithm used to generate embeddings
    from unstructured text.
  name: GloVe
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as large-scale neural network architectures used for generating
    embeddings, contrasting with smaller models like Word2Vec.
  name: Transformer architectures
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Mentioned as a slightly larger neural network architecture than Word2Vec/GloVe,
    containing embedding components.
  name: BERT
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned generally as the context for modern, huge, and deep neural network
    architectures.
  name: LLMs
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a product/service where the speaker's team works on personalization
    and recommendation systems, utilizing embeddings.
  name: ViX
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in reference to Roberto Mejri, VP of Data at Instagram, and the
    existential pressure from TikTok regarding the launch of Reels.
  name: Meta
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: A Meta platform where Roberto Mejri launched Reels, a short-form video
    product competing with TikTok.
  name: Instagram
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Frequently mentioned as the benchmark for short-form video personalization
    and a major competitor driving Meta's Reels strategy.
  name: TikTok
  source: llm_enhanced
date: 2025-10-02 04:00:00 +0000
duration: 56
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/000ae3bc3e8d4aca92a70c3adb8bba11/
processing_date: 2025-10-06 04:07:56 +0000
quotes:
- length: 287
  relevance_score: 10
  text: '" And the problem is, you know, and as I think everyone that''s actually
    a practitioner in the field knows, is you have to check almost everything as an
    output, especially when it comes to code generation, documentation, really any
    kind of output that you have there when it comes to text'
  topics: []
- length: 265
  relevance_score: 8
  text: And that makes it so that it actually takes, in some cases, longer leveraging
    the outputs of the LLMs than it would have been for the user to just do the thing
    themselves, which frankly, it was the whole point of why everyone's so excited
    about generative AI, right
  topics: []
- length: 215
  relevance_score: 7
  text: But now, we're using a much more modern architecture, we're using deep neural
    networks, which, and we have to train them at scale, which means that you have
    to start using GPUs, which are not fun to work with at all
  topics: []
- length: 183
  relevance_score: 7
  text: I would just say more like you have to be—I am much more currently in the
    camp where LLMs, just in general, are really, really good at making their outputs
    appear incredibly confident
  topics: []
- length: 274
  relevance_score: 7
  text: The problem is, so as a person that has been doing what I would call classic
    data science and machine learning for the vast majority of my career, and I'm
    sure you're the same thing with you, Hugo, we really like understanding the level
    of uncertainty with a piece of output
  topics: []
- length: 296
  relevance_score: 6
  text: And to be clear, a lot of the successes in generative AI I've seen, for example,
    in people building customer service chatbots have been conversational AI customer
    service agents that the customer service human users to then get information to
    serve to a customer as opposed to directly serving it
  topics: []
- length: 209
  relevance_score: 5
  text: So I think the biggest, the biggest opportunity I had when I was there was
    to get really into the weeds with what the now Paramount digital ecosystem looked
    like, and really before the launch of Paramount Plus
  topics:
  - opportunity
- length: 265
  relevance_score: 4
  text: We explore the role of data science in serving bilingual and multicultural
    communities, the trade-offs between internal data products and revenue-driving
    ones, and how to bring modern machine learning practices into a company where
    creativity and content still lead
  topics:
  - revenue
- length: 142
  relevance_score: 4
  text: And something I love about the work you've done, a lot of generative AI applications
    these days rest upon the supreme importance of embeddings
  topics: []
- length: 293
  relevance_score: 4
  text: We don't talk enough about embeddings in the space at the moment, and your
    work and your team links heavily on embeddings across projects, so maybe you could
    just give us a 30-second refresher on embeddings and then how they fit into your
    machine learning stack and what quick wins they enable
  topics: []
- length: 231
  relevance_score: 4
  text: And what we've done is we've basically taken some of the algorithms that I
    would say at this point now these algorithms are ancient news from the perspective
    of LLMs and training those models, but they're still incredibly effective
  topics: []
- length: 170
  relevance_score: 4
  text: And we have seen in the past couple of years a lot of excitement and a lot
    of possibility of LLMs and generative AI possibility as opposed to capability
    is how I frame it
  topics: []
- length: 101
  relevance_score: 4
  text: And currently, there are no error bars that you can see when you have outputs
    from LLM infrastructure
  topics: []
- length: 224
  relevance_score: 4
  text: There's just—it's just you get some output, and there's no way to say that
    the likelihood that this output, whatever it is, especially when it comes to text
    or code or whatever, is in the LLM itself has some certainty around
  topics: []
- length: 192
  relevance_score: 4
  text: And that is very—and it's not only from the perspective, I mean, it's from
    the perspective of the way that the LLM actually communicates, the output is just
    incredibly like, "This is how it is
  topics: []
- length: 269
  relevance_score: 4
  text: '" So I would just say that at this point, I think the way to really take
    LLMs into, I would say, like the next level of usability, there has to be some
    way that the—for specific use cases—that there is some kind of a confidence assigned
    to the output that the LLM makes'
  topics: []
- length: 283
  relevance_score: 3
  text: It did outperform their model, which was, I think, validating for our data
    scientists, but also showed that we could actually in-house the intellectual property
    around this system, because in many cases, it could be the secret sauce for the
    continued growth of your streaming service
  topics:
  - growth
- length: 95
  relevance_score: 3
  text: They are really challenging, especially when you have to string the principles
    of them together
  topics: []
- length: 214
  relevance_score: 3
  text: So last time we spoke, you told me how you tested LLM-generated metadata against
    existing features, and I'm wondering if you could explain what the motivation
    was here, and what lift you saw, and what surprised you
  topics: []
- length: 229
  relevance_score: 3
  text: So what we wound up doing is we wound up cleaning a little bit the metadata
    tags and then applying them and running an A/B test between our classic tagging
    solution that we had and then the generative AI basically passed solution
  topics: []
- impact_reason: 'Provides crucial strategic advice for data modernization in large
    enterprises: focus on incremental, clear wins and avoid over-engineering solutions
    (like blindly applying LLMs).'
  relevance_score: 10
  source: llm_enhanced
  text: Sergey's data modernization and innovation story at a massive traditional
    business, Televisa Univision, is chock full of real examples of practical wisdom,
    focusing on landing small, clear improvements, no single points of failure, making
    sure you know what problem to solve, and not just bringing an LLM hammer to every
    text nail.
  topic: strategy
- impact_reason: A strong statement prioritizing embeddings as the foundational layer
    for modern AI applications, contrasting with the current hype cycle focused only
    on LLMs.
  relevance_score: 10
  source: llm_enhanced
  text: a lot of generative AI applications these days rest upon the supreme importance
    of embeddings. We don't talk enough about embeddings in the space at the moment.
  topic: technical
- impact_reason: 'Shows a creative application of sequential modeling algorithms (like
    those used for text) to non-textual data: user behavior histories, treating actions
    as ''tokens''.'
  relevance_score: 10
  source: llm_enhanced
  text: What's critical about them is that they be sequential, meaning that there's
    some meaning behind token or token is brought up. So we've done for us is we treat
    our, if you will, sentences or our documents as user histories or really histories
    of—it could be histories of actions, it could be histories of consumption behavior...
  topic: technical
- impact_reason: Provides a clear architectural progression showing how embeddings
    fit into increasingly complex models (Word2Vec -> BERT -> LLMs), illustrating
    the concept of intermediate representations.
  relevance_score: 10
  source: llm_enhanced
  text: You can think of Word2Vec or GloVe as very, very small neural networks where
    the component just sits right in the middle, and then you have BERT, which is
    a slightly larger neural network architecture where those embedding components
    might sit slightly higher up, or then you have the LLMs where you have incredibly
    huge, incredibly deep architectures.
  topic: technical
- impact_reason: 'A crucial strategic pivot in recommendation systems: prioritizing
    consumption behavior embeddings over purely content-based feature embeddings for
    driving engagement.'
  relevance_score: 10
  source: llm_enhanced
  text: We care a lot about not necessarily what the content is about, but how it's
    being consumed.
  topic: strategy
- impact_reason: Illustrates the power of behavioral embeddings to uncover surprising,
    high-value cross-category recommendations that content metadata alone would miss.
  relevance_score: 10
  source: llm_enhanced
  text: if there is a random sports league for whatever reason that has a really high
    affinity or a high embedding-based similarity from the perspective of content
    consumption behavior to a telenovela or to an original ViX movie, we would rather
    leverage that, right?
  topic: technical
- impact_reason: A powerful example of using graph context (household composition)
    to enrich device-level embeddings, linking device features to household media
    consumption capabilities.
  relevance_score: 10
  source: llm_enhanced
  text: You can also do, I would say, context-level expansion within the household
    to inform device-level embeddings. So, for example, you can take knowing that
    device A, device B, device C—device A being an iOS tablet, device B being an Android
    mobile phone, and then device C being a smart TV—you can, as part of your embedding
    process for those devices, you can say, 'Hey, this device belongs to a household
    that has at least one smart-enabled television.'
  topic: technical/AI
- impact_reason: Documents a key architectural evolution in recommender systems, moving
    from static factorization models to sequential models to solve the cold-start
    problem for new users.
  relevance_score: 10
  source: llm_enhanced
  text: So we moved away from that factorization machine model, and we moved to something
    that was a little more modern, more flexible, and didn't require—didn't require
    that the user were in the training set. And we moved to a sequential recommendation
    architecture.
  topic: technical/AI
- impact_reason: 'Details a specific, high-value application of generative AI: enriching
    legacy metadata using multimodal models to improve downstream recommendation performance.'
  relevance_score: 10
  source: llm_enhanced
  text: we tested LLM-generated metadata against existing features... we made a bet
    that if we could extract useful data basically automatically by passing the content
    through single-modal, initially LLMs, now multi-modal models, models that take
    both video and audio, and basically prompt them to generate high-level moods,
    themes, things of that nature, apply them to and tag our content with them, we
    would be able to see some incremental lift in the performance of our models.
  topic: AI technology trends
- impact_reason: 'Offers key strategic advice: AI infrastructure use cases (like metadata
    enrichment) are lower risk and easier to adopt than direct user-facing applications.'
  relevance_score: 10
  source: llm_enhanced
  text: I would say in this case, it was a good place to use a generative AI because
    it didn't require—there was nothing at the end of the day that was directly surfaced
    to the user. It was behind, I would say, it was part of the infrastructure for
    our algorithm as opposed to something that was just directly being surfaced to
    the user.
  topic: business advice
- impact_reason: 'This is the core critique of current LLMs from a data science perspective:
    the lack of inherent uncertainty quantification (error bars).'
  relevance_score: 10
  source: llm_enhanced
  text: LLMs, just in general, are really, really good at making their outputs appear
    incredibly confident. The problem is... we really like understanding the level
    of uncertainty with a piece of output. I really like seeing error bars on things,
    right?
  topic: limitations
- impact_reason: 'Presents a clear roadmap/requirement for the next generation of
    usable LLMs: integrated confidence scoring.'
  relevance_score: 10
  source: llm_enhanced
  text: at this point, I think the way to really take LLMs into, I would say, like
    the next level of usability, there has to be some way that the—for specific use
    cases—that there is some kind of a confidence assigned to the output that the
    LLM makes.
  topic: predictions
- impact_reason: 'This highlights a fundamental, unsolved, or poorly solved problem
    in current LLM deployment: the necessity of quantifying output certainty to enable
    trust and automation.'
  relevance_score: 10
  source: llm_enhanced
  text: There has to be some way to provide probabilities against the outputs.
  topic: technical/deployment
- impact_reason: 'This clearly articulates the consequence of missing confidence scores:
    the failure of the automation promise, requiring human oversight for every output.'
  relevance_score: 10
  source: llm_enhanced
  text: ultimately, if you can't assign those [probabilities], you basically have
    to have your end users check everything.
  topic: deployment/business impact
- impact_reason: This is a powerful statement on the current state of AI ROI. If verification
    time exceeds creation time, the entire value proposition of generative AI collapses.
  relevance_score: 10
  source: llm_enhanced
  text: And that makes it so that it actually takes, in some cases, longer leveraging
    the outputs of the LLMs than it would have been for the user to just do the thing
    themselves...
  topic: business impact/ROI
- impact_reason: 'Highlights the foundational, non-glamorous first step in any large-scale
    data modernization effort: establishing a unified, standardized data layer.'
  relevance_score: 9
  source: llm_enhanced
  text: When I first joined, the goal really was about data unification, standardization,
    and then building out some core infrastructure that would allow us to serve our
    digital audience.
  topic: strategy
- impact_reason: Introduces a core, proprietary data asset—the Household Graph—which
    is central to their personalization and targeting efforts.
  relevance_score: 9
  source: llm_enhanced
  text: We did that by building out what we now call the Televisa Univision Household
    Graph, the representation of our best guess of who we think is interested in Spanish
    language media across the United States.
  topic: technical
- impact_reason: Quantifies the massive business impact achieved, linking data science
    efforts directly to digital transformation success.
  relevance_score: 9
  source: llm_enhanced
  text: At this point, the digital business has grown at least 10x in the four years
    that we've been a part of it.
  topic: business
- impact_reason: Details the nuance of defining the Total Addressable Market (TAM)
    in specialized media—focusing on engagement rather than demographics (like ethnicity),
    which presents unique modeling challenges.
  relevance_score: 9
  source: llm_enhanced
  text: We're really just looking for people that have engaged with that content,
    and that allows us to cast a slightly wider net. But at the same time, it makes
    our job a little bit harder, I think, than your standard broad-based, traditional
    media company where your TAM, your total addressable market, at least in the United
    States, is literally every person that lives in the United States.
  topic: business
- impact_reason: 'Provides a clear roadmap of data science maturity stages: Infrastructure
    -> Core Modeling (Profiling/Look-alike) -> Measurement (Attribution).'
  relevance_score: 9
  source: llm_enhanced
  text: Initially, like I said, it was all about data unification, building out data
    infrastructure, and building out core components. So that's the graph, some ancillary
    components on top of the graph, things like audience profiling, look-alike modeling,
    and then some basic attribution analyses around ad delivery, things like that.
  topic: technical
- impact_reason: Details the application of data science to the entire customer lifecycle,
    including in-app experience analysis and cross-channel personalized messaging
    (omnichannel personalization).
  relevance_score: 9
  source: llm_enhanced
  text: To that end, we've built some algorithms—the team has built some algorithms
    for analyzing the experience on ViX, as well as infrastructure to tailor the messaging
    that we send to people, either within the app or outside of the app, to their
    inboxes or in push notifications, etc.
  topic: technical
- impact_reason: Provides insight into the emerging trend of 'AI agents for Data Science'—automation
    tools aimed at augmenting or replacing parts of the data science workflow.
  relevance_score: 9
  source: llm_enhanced
  text: At Delphina, we're building AI agents for Data Science, and through the nature
    of our work, we speak with the very best in the field. And so with the podcast,
    we're sharing that high signal.
  topic: AI technology trends
- impact_reason: 'Details a core challenge in large media/tech companies: unifying
    messy, disparate first-party data with valuable third-party licensed data through
    schema standardization.'
  relevance_score: 9
  source: llm_enhanced
  text: I really got to understand how to take disparate data sources and put them
    into one kind of general schema that we could then use, leverage, and ultimately
    connect to external data sources that we would license.
  topic: technical
- impact_reason: Emphasizes the necessity of strong cross-functional partnership (Data
    Engineering, Product) to allow specialized teams (Data Science/Modeling) to focus
    on core value creation.
  relevance_score: 9
  source: llm_enhanced
  text: We have a really incredible data engineering organization at Univision, and
    we have really great product folks that I've been working with. One set the overall
    vision, the other really got data to me and shipped data out for me—things that
    I didn't necessarily care about having to do.
  topic: business
- impact_reason: Provides a concise, accessible definition of embeddings, emphasizing
    their mathematical utility (vector math) across diverse data types.
  relevance_score: 9
  source: llm_enhanced
  text: an embedding is a low or high-dimensional representation of some object. That
    object can really be anything in a mathematical space that allows you to do interesting
    things with them. Specifically, you can do vector math with those embeddings,
    right?
  topic: technical
- impact_reason: 'Frames the core challenge in applied ML: the transformation process
    (encoding) from raw, heterogeneous data into usable vector representations.'
  relevance_score: 9
  source: llm_enhanced
  text: Now, the secret sauce is how do you get to an embedding? How do you go from
    data that exists, whether it is text on a web page or a picture or an audio file
    or even just tabular data in a single spreadsheet... into one of these things?
  topic: technical
- impact_reason: Demystifies older embedding techniques (Word2Vec/GloVe) by classifying
    them correctly as fundamental algorithms rather than just abstract infrastructure
    pieces.
  relevance_score: 9
  source: llm_enhanced
  text: things like Word2Vec or GloVe or Word2Vec—these are people talk about that
    as infrastructure or architectural components, but they're really just algorithms
    that you can take and take, I would say, unstructured text or unstructured data
    or what you think is unstructured data and then create embeddings out of them,
    right?
  topic: technical
- impact_reason: 'Directly links the abstract concept of embedding similarity (finding
    similar users) to a concrete business outcome: targeted acquisition/retention
    marketing.'
  relevance_score: 9
  source: llm_enhanced
  text: we can then try to find them in the wider world and try to get them to come
    back to our platform, come to our platforms, and to subscribe.
  topic: business
- impact_reason: Shows advanced application of embeddings beyond content items (shows)
    to entities within the content (actors), enabling richer similarity mapping.
  relevance_score: 9
  source: llm_enhanced
  text: We're also exploring getting embeddings of things even deeper than that. So,
    for example, embeddings of core actors that exist across our library and finding
    interesting similarities or differences among them and then using that to our
    advantage.
  topic: technical
- impact_reason: Highlights the critical importance of privacy-preserving design (anonymization,
    decoupling PII) when building large-scale identity/behavioral graphs.
  relevance_score: 9
  source: llm_enhanced
  text: The graph itself is actually fully anonymized, so we have no—there's nothing
    that ties it up to the specific person's PII, so their Personally Identifiable
    Information, to any component of our graph, and that's really critical because
    the whole point of this is to make it s[afe/scalable].
  topic: safety
- impact_reason: Illustrates the application of embeddings beyond content to demographic
    and geographic targeting at the household level, showing how latent features are
    derived from the graph structure.
  relevance_score: 9
  source: llm_enhanced
  text: We can generate household-level embeddings, which are going to be coarse,
    but they are going to give you some understanding of the geography and the household-level
    demography of that household, right? So you can embed the household in a space
    that tells you, 'Okay, these are higher-income households, middle-income households,
    lower-income households living in these specific parts of the United States.'
  topic: technical/AI
- impact_reason: Provides a concrete business justification (higher CPMs on CTV vs.
    mobile) for prioritizing screen optimization, linking technical goals to revenue
    generation.
  relevance_score: 9
  source: llm_enhanced
  text: The CPMs that you can get for delivering content on a smart TV are significantly
    higher than the CPMs—this is basically the revenue that you can generate from
    a thousand impressions, so a thousand ads served—significantly higher than what
    you can get from a mobile device.
  topic: business
- impact_reason: 'A strong business case for building proprietary AI/ML systems: achieving
    better performance than paid third-party solutions while securing core intellectual
    property.'
  relevance_score: 9
  source: llm_enhanced
  text: Our first test was really just about our homegrown system that we built to
    outperform the third-party model that we were paying quite a bit of money for
    to support. Thankfully, it did, which was great. It did outperform their model,
    which was, I think, validating for our data scientists, but also showed that we
    could actually in-house the intellectual property around this system, because
    in many cases, it could be the secret sauce for the continued growth of your streaming
    service.
  topic: business/strategy
- impact_reason: 'Clearly articulates the classic limitation of matrix factorization
    models: poor handling of new users (cold start) due to reliance on pre-existing
    user embeddings.'
  relevance_score: 9
  source: llm_enhanced
  text: There were some drawbacks with the system, typically around being able to
    generate, I would say, cold-start recommendations or recommendations for users
    that weren't in our original, I would say, training set or in the set of data
    that we used to build the model against.
  topic: technical
- impact_reason: Directly contrasts the MLOps overhead between classic ML models (like
    FM) and modern deep learning sequential models, emphasizing the infrastructure
    cost of advanced AI.
  relevance_score: 9
  source: llm_enhanced
  text: There's just lots of, there's lots of, I would say, more like MLOps that's
    involved with these kinds of infrastructure than the simpler, I would say, more
    classic like factorization-based models.
  topic: technical/MLOps
- impact_reason: Highlights the critical difference in data quality/frequency between
    long-form and short-form content for ML training.
  relevance_score: 9
  source: llm_enhanced
  text: the reason that TikTok is such a boon for personalization, or these other
    short-form video platforms are a boon for personalization, is that the amount
    of interactivity and the amount of signal that you get is much, much higher, the
    frequency of the signal, right?
  topic: technical
- impact_reason: Defines the superior feedback loop available in short-form video
    (explicit negative signals), which is crucial for rapid model iteration.
  relevance_score: 9
  source: llm_enhanced
  text: But with the short-form content, you have equivalent both positive and negative
    feedback, right? A user swipes away a video, it means they didn't want it, they
    didn't like it...
  topic: technical
- impact_reason: 'Provides a crucial limitation/caveat for using LLMs/multimodal models
    for content tagging: modality bias (performing poorly on animation vs. live-action).'
  relevance_score: 9
  source: llm_enhanced
  text: we had to actually do some post-cleaning against that. For example, one thing
    we found was that the automated, this automated approach did really well for content
    with actual people in it, right, so for non-animated, but it was actually really
    not great with animated content.
  topic: limitations
- impact_reason: Quantifies the business impact (10-15% engagement lift) achieved
    by using LLM-generated metadata, validating the effort.
  relevance_score: 9
  source: llm_enhanced
  text: We found actually some significant improvement. It was on the order of 10
    to 15% increases in overall engagement against the original model...
  topic: business
- impact_reason: Directly calls out the missing feature in current LLM APIs necessary
    for reliable, production-grade ML engineering.
  relevance_score: 9
  source: llm_enhanced
  text: And currently, there are no error bars that you can see when you have outputs
    from LLM infrastructure. There's just—it's just you get some output, and there's
    no way to say that the likelihood that this output... is in the LLM itself has
    some certainty around.
  topic: technical
- impact_reason: A concrete example of hallucination leading to operational failure
    (mixing documentation from different systems), illustrating the risk of unverified
    output.
  relevance_score: 9
  source: llm_enhanced
  text: it just completely invented something... it basically gave us documentation
    from a completely different cloud provider.
  topic: limitations
- impact_reason: 'Articulates the potential productivity paradox if confidence scoring
    is not solved: LLMs become a net negative on time-to-ship.'
  relevance_score: 9
  source: llm_enhanced
  text: if you can't assign those [probabilities], you basically have to have your
    end users check everything. And that makes it so that it actually takes, in some
    cases, longer leveraging the outputs of the LLMs than it would have been for the
    user to just do the thing themselves...
  topic: business
- impact_reason: It suggests that even major AI labs are actively struggling with
    or prioritizing the development of confidence scoring mechanisms, indicating a
    major industry bottleneck.
  relevance_score: 9
  source: llm_enhanced
  text: I don't know what they are, but the companies themselves, they have hundreds
    of people working on this stuff, and I think that there has to be a way to be
    able to do that...
  topic: technical/industry challenge
- impact_reason: This succinctly captures the core promise of generative AI (speed/efficiency)
    and frames the preceding problem (manual checking) as a direct threat to that
    promise.
  relevance_score: 9
  source: llm_enhanced
  text: which frankly, it was the whole point of why everyone's so excited about generative
    AI, right? It's supposed to shrink our time to ship.
  topic: strategy/value proposition
- impact_reason: Establishes the massive scale and global reach of the media company,
    setting the context for the data and AI challenges they face.
  relevance_score: 8
  source: llm_enhanced
  text: Our content is available for at least 250, 300 million people worldwide that
    want to watch it.
  topic: strategy
- impact_reason: 'Defines a clear, tiered value proposition for the data science function:
    operational efficiency, internal enablement, and direct revenue generation.'
  relevance_score: 8
  source: llm_enhanced
  text: We take what data is available and make either infrastructural products or
    data products that will either make our own lives easier, our internal clients'
    lives easier, or make the company more money.
  topic: business
- impact_reason: Illustrates the complex, multicultural nature of the target audience,
    emphasizing that content consumption patterns define the market more than simple
    linguistic or ethnic labels.
  relevance_score: 8
  source: llm_enhanced
  text: So it could be those hardcore soccer fans that don't speak any Spanish, but
    they like to watch it on our platforms, or it could be those first-generation
    Spanish speakers, fully acculturated Americans, or people that live in the United
    States that came from a Spanish-speaking country but that still want to have some
    kind of a connection to their culture or what have you.
  topic: strategy
- impact_reason: Identifies the launch of a major DTC product (ViX) as the catalyst
    for shifting focus from foundational infrastructure to advanced modeling (personalization,
    churn prediction).
  relevance_score: 8
  source: llm_enhanced
  text: But that really happened after the second, I would say, evolution, big evolution
    after I came, which was the launch of our fully owned, fully created Spanish language
    direct-to-consumer streaming service called ViX.
  topic: business
- impact_reason: Highlights the strategic decision to build a proprietary Customer
    Data Platform (CDP) to manage complex customer relationships and messaging orchestration.
  relevance_score: 8
  source: llm_enhanced
  text: We built a CDP, an in-house CDP, a Customer Data Platform, that allows us
    to basically manage those relationships and to be able to message them effectively.
  topic: technical
- impact_reason: Describes the transition from building core infrastructure to focusing
    on advanced, iterative improvements in personalization and retention mechanics.
  relevance_score: 8
  source: llm_enhanced
  text: And now we're, we're kind of in maintenance mode with some of these things,
    like with the graph, but then on the flip side, we're also doing some new and
    more interesting work with deeper personalization on the application, as well
    as deeper and more interesting work around messaging, tailoring personalized messaging
    experiences so that we can bring them back [when they're not on the app].
  topic: strategy
- impact_reason: Reinforces the critical technical skill of schema unification and
    integration of first-party data with licensed third-party data for enriched modeling.
  relevance_score: 8
  source: llm_enhanced
  text: So I really got to understand how to take disparate data sources and put them
    into one kind of general schema that we could then use, leverage, and ultimately
    connect to external data sources that we would license.
  topic: technical
- impact_reason: Highlights the critical, often overlooked journey from foundational
    data infrastructure to delivering tangible product value, a common challenge in
    data-heavy organizations.
  relevance_score: 8
  source: llm_enhanced
  text: ve gone from the basics of data unification infrastructure to building out
    all these incredible products that deliver so much value while continuing to make
    sure the infrastructure and all the data is working as is, is an amazing story.
  topic: strategy
- impact_reason: 'Defines the clear mandate for a data science team: focus on creating
    internal/external products that drive efficiency or revenue, rather than managing
    undifferentiated heavy lifting.'
  relevance_score: 8
  source: llm_enhanced
  text: That allowed me and my team to really just focus on the stuff that we can
    do well, which is take what data is available and make either infrastructural
    products or data products that will either make our own lives easier, our internal
    clients' lives easier, or make the company more money.
  topic: strategy
- impact_reason: Provides a quantifiable success metric (10x growth) directly attributable
    (in part) to the data/modeling efforts, demonstrating ROI.
  relevance_score: 8
  source: llm_enhanced
  text: the digital business has grown at least 10x in the four years that we've been
    a part of, and my team can take some amount of, I would say, credit for that success.
  topic: business
- impact_reason: 'A realistic perspective for most companies: acknowledging the resource
    gap compared to major AI labs and focusing on leveraging existing, smaller, effective
    techniques.'
  relevance_score: 8
  source: llm_enhanced
  text: We are a small but mighty team. We have these much smaller architectures.
    We don't have the resources to be able to do those kinds of things [like Anthropic's
    sparse autoencoders].
  topic: business
- impact_reason: 'Confirms the primary, high-value application of embeddings in media:
    powering content recommendation engines.'
  relevance_score: 8
  source: llm_enhanced
  text: We find that embeddings in our recommendations systems, right? We have embeddings
    of all of our shows, of all of our content.
  topic: technical
- impact_reason: Defines the core concept of persistence and stability required for
    building durable identity graphs, crucial for long-term personalization and measurement.
  relevance_score: 8
  source: llm_enhanced
  text: The building blocks of the graph are just a few components that are reasonably
    persistent and stable, right? So they're persistent in the sense that they exist
    from session A to session B to session C, and they're stable meaning they usually
    refer to the same entity in our case, either a household or a device over a long
    enough period of time, where long enough means usually weeks or months.
  topic: technical/strategy
- impact_reason: Lists the foundational, persistent identifiers used to map digital
    activity back to a physical household unit, covering mobile, desktop (implied
    via IP), and Connected TV (CTV).
  relevance_score: 8
  source: llm_enhanced
  text: And so those components are IP addresses, which are basically usually tied—there's
    a single IP address that is associated with a single household, if you will, in
    the United States—and then digital identifiers that exist in the digital ecosystem.
    So that is device IDs, advertising identifiers on mobile phones, and then CTV
    identifiers.
  topic: technical
- impact_reason: 'States a core business objective in streaming: maximizing screen
    size for better engagement and experience, which is then operationalized via data
    science.'
  relevance_score: 8
  source: llm_enhanced
  text: What we ultimately want to do, in general, when you're at your house, what
    we always want people to do is to watch our content on the largest possible screen
    that we can find, right? Because ultimately, larger screens almost always lead
    to more engagement, and then ultimately they lead to better consumer experience.
  topic: business/strategy
- impact_reason: Details the initial, robust recommendation architecture (Factorization
    Machines combining CF and CBF), a common and effective baseline in recommender
    systems.
  relevance_score: 8
  source: llm_enhanced
  text: Our initial system was really a, I would say it was like a hybrid system.
    We used a factorization machine that was a combination of what I would call collaborative
    filtering and or with most people called both collaborative and content-based
    filtering, where we used user features, item features, but we also used basically
    matrix factorization, I would say, on steroids, to generate our recommendations.
  topic: technical
- impact_reason: 'Provides a concise, conceptual definition of how sequential models
    treat user history: as a stream of tokens (items) with associated metadata.'
  relevance_score: 8
  source: llm_enhanced
  text: They're really just streams of, again, tokens with some added side information,
    either against the token or the stream itself, where the stream itself is the
    user, and then the tokens or the items, the pieces of content that they interacted
    with in some meaningful or interesting way.
  topic: technical/AI
- impact_reason: A candid acknowledgment of the operational complexity (MLOps burden)
    introduced by scaling deep learning models, specifically mentioning the difficulty
    of managing GPU infrastructure.
  relevance_score: 8
  source: llm_enhanced
  text: But now, we're using a much more modern architecture, we're using deep neural
    networks, which, and we have to train them at scale, which means that you have
    to start using GPUs, which are not fun to work with at all. They are really challenging,
    especially when you have to string the principles of them together.
  topic: technical/MLOps
- impact_reason: Confirms the current state-of-the-art in their production recommendation
    system, moving beyond simpler models.
  relevance_score: 8
  source: llm_enhanced
  text: we're using these sequential recommendation models to generate our recommendations
    that power various components of our application.
  topic: technical
- impact_reason: Clearly articulates the challenge of implicit negative feedback in
    traditional VOD recommendation systems.
  relevance_score: 8
  source: llm_enhanced
  text: From the perspective of the long-form content that we have, the shows or the
    movies or whatever, all we get is positive feedback, right? What people clicked,
    and the positive feedback is explicit; the negative feedback is more implicit.
  topic: technical
- impact_reason: Contrasts the low-stakes A/B testing environment for backend AI improvements
    versus the high-stakes nature of direct user interaction with generative models.
  relevance_score: 8
  source: llm_enhanced
  text: I think there, I think it's a lot, you have to be much more careful in leveraging
    something like this [direct user-facing LLM]. Here, I would say the barrier to
    success was significantly lower because basically it was like, 'Hey, either metrics
    go up or they don't.'
  topic: strategy
- impact_reason: Reinforces the 'AI as an assistant/copilot' pattern for high-stakes
    applications, rather than full automation.
  relevance_score: 8
  source: llm_enhanced
  text: a lot of the successes in generative AI I've seen, for example, in people
    building customer service chatbots have been conversational AI customer service
    agents that the customer service human users to then get information to serve
    to a customer as opposed to directly serving it.
  topic: business advice
- impact_reason: Describes the dangerous conversational confidence that masks potential
    hallucinations or errors.
  relevance_score: 8
  source: llm_enhanced
  text: it's 100% certain about 100% of everything. And that is very—and it's not
    only from the perspective, I mean, it's from the perspective of the way that the
    LLM actually communicates, the output is just incredibly like, 'This is how it
    is. This is what you do. Oh, it's super easy. Here you go.'
  topic: safety/limitations
- impact_reason: 'A practical warning for practitioners: the overhead of verification
    can negate the speed benefits of LLM usage.'
  relevance_score: 8
  source: llm_enhanced
  text: you have to check almost everything as an output, especially when it comes
    to code generation, documentation, really any kind of output that you have there
    when it comes to text.
  topic: practical lessons
- impact_reason: Explains the core economic driver behind the merger, relevant for
    understanding media consolidation and vertical integration strategies.
  relevance_score: 7
  source: llm_enhanced
  text: Televisa is actually the company that creates the vast majority of the content
    that Univision would then license and put on all of its broadcast channels. And
    so it really didn't make sense, I think, economically for them both to be separate
    entities, right? One basically is the largest customer of the other.
  topic: strategy
- impact_reason: 'A classic, relatable description of data chaos in legacy media companies:
    disparate sources (Adobe, ad servers) with varying levels of data quality.'
  relevance_score: 7
  source: llm_enhanced
  text: There was data that was stored in basically Adobe on the true logs. There
    was data that was stored in ad servers in various states of, I would say, cleanliness.
  topic: technical
- impact_reason: 'Defines the purpose of their flagship asset, the Household Graph:
    identifying the addressable universe for Spanish-language media consumers.'
  relevance_score: 7
  source: llm_enhanced
  text: The graph is, like I said, it's a representation of who we think a household
    in the United States that we think are very likely to interact with Spanish language
    media broadly construed...
  topic: business
- impact_reason: 'Describes the core mechanism of graph construction: stitching together
    disparate data sources (first-party and licensed) using a proprietary matching
    algorithm.'
  relevance_score: 7
  source: llm_enhanced
  text: What we do is we have a proprietary algorithm that takes sightings of these
    IP addresses and device identifiers from all corners of wherever we can basically
    get this data, whether it's our own first-party data or data that we license from
    elsewhere, and we have the same account that has been out in my household.
  topic: technical/business
- impact_reason: 'Simplifies the complex process of identity resolution into three
    key steps: stitching, filtering, and thresholding to define the final graph entity
    (the household).'
  relevance_score: 7
  source: llm_enhanced
  text: We call that a household. We do some filtering on it, we do some thresholding
    on it, and we get what we call our graph.
  topic: technical
- impact_reason: Defines the company's entry into the short-form, swipeable video
    format, signaling a strategic shift mirroring dominant social media trends.
  relevance_score: 7
  source: llm_enhanced
  text: The Clips product is basically a combination of short-form video content.
    It's a combination of news, sports, and entertainment, but all in a single, basically,
    swipeable feed that you'd be familiar with, obviously, if you consume any TikTok
    content or content on Instagram or any of these other large-scale video applications.
  topic: business/strategy
- impact_reason: Points out a key differentiator in their personalization challenge
    compared to major social media platforms (content source control).
  relevance_score: 7
  source: llm_enhanced
  text: a lot of this work has been done with user-generated content. So doing it
    from this perspective [in-house content] is fascinating.
  topic: strategy
- impact_reason: Signals the shift in industry focus from traditional ML to Generative
    AI, even for established recommendation systems.
  relevance_score: 7
  source: llm_enhanced
  text: I would be dishonest if we didn't talk about a bit about generative AI. So
    last time we spoke, you told me how you tested LLM-generated metadata against
    existing features...
  topic: AI technology trends
- impact_reason: 'Crucial distinction for content strategy: they control the entire
    supply chain (no UGC), which simplifies moderation and potentially standardizes
    data quality for personalization.'
  relevance_score: 6
  source: llm_enhanced
  text: This content that we have, none of it is user-generated; it's all basically
    created in-house.
  topic: business
source: Unknown Source
summary: '## Podcast Episode Summary: Episode 25: How Data-Driven Growth Redefined
  a Media Giant


  This episode features an in-depth conversation with **Sergey Fogelson, VP of Data
  Science at Televisa Univision**, the world''s largest Spanish-language media company.
  The discussion centers on the massive data modernization effort undertaken following
  the merger of Televisa and Univision, focusing on how data science infrastructure
  and advanced modeling—particularly the use of embeddings—fueled a tenfold growth
  in their digital business and supported the launch of their streaming service, ViX.


  ### 1. Focus Area

  The primary focus is the **practical application of data science and machine learning
  infrastructure within a legacy, massive traditional media organization** to drive
  digital transformation and audience growth. Key themes include data unification,
  building proprietary audience graphs, personalization algorithms, and leveraging
  sequential modeling techniques (embeddings) for content recommendation and audience
  targeting, all while navigating the complexities of serving multicultural and bilingual
  communities.


  ### 2. Key Technical Insights

  *   **Televisa Univision Household Graph:** The team built a proprietary graph representing
  their best guess of households in the U.S. likely to engage with Spanish-language
  media. This graph is foundational for audience profiling, look-alike modeling, and
  targeted advertising, moving beyond simple demographic targeting to focus on content
  consumption behavior.

  *   **Embeddings for Sequential Behavior:** The team heavily utilizes embedding
  techniques (like Word2Vec/GloVe, adapted from NLP) not just for text, but to represent
  sequential user histories (e.g., the order in which a user watched shows). These
  embeddings capture latent relationships between content, actors, regions, and user
  behavior, enabling powerful, non-obvious recommendations.

  *   **Content vs. Consumption Similarity:** The data science strategy prioritizes
  **consumption-based similarity** derived from embeddings over traditional content-based
  similarity (e.g., genre matching). If a sports league''s consumption pattern embeds
  closely to a telenovela''s consumption pattern, they leverage that behavioral link
  for personalization, even if the content seems disparate.


  ### 3. Business/Investment Angle

  *   **Digital Growth Multiplier:** The data infrastructure build-out directly correlated
  with significant business success; the digital business has grown at least **10x**
  in the four years since the modernization began.

  *   **Internal vs. Revenue Products:** The strategy involved a phased approach:
  first, building core infrastructural data products (like the Graph) to simplify
  internal operations, and second, developing revenue-driving products like personalization
  algorithms for the ViX streaming service.

  *   **The Value of First-Party Data:** The focus on building the Household Graph
  highlights the critical need for media companies to unify and leverage their first-party
  data to define their Total Addressable Market (TAM) within specific cultural segments,
  rather than relying solely on broad market definitions.


  ### 4. Notable Companies/People

  *   **Sergey Fogelson (VP of Data Science, Televisa Univision):** The featured expert
  who detailed the journey from data unification to advanced ML implementation.

  *   **Televisa Univision:** The subject company, representing the merger of the
  largest Spanish-language content creator (Televisa) and distributor (Univision)
  in the Americas.

  *   **ViX:** The company''s highly successful, fully owned, Spanish-language direct-to-consumer
  streaming service, which necessitated the rapid development of personalization infrastructure.

  *   **ViacomCBS/Paramount:** Sergey’s previous role provided him with crucial experience
  in unifying disparate data sources (Adobe logs, ad servers) before joining Televisa
  Univision.

  *   **Delphina (Duncan Gilchrist):** The sponsoring company, which builds AI agents
  for Data Science, emphasizing practical, non-LLM-centric applications of data science.


  ### 5. Future Implications

  The conversation suggests that for large, established media entities, the future
  of data science lies in **deep behavioral modeling** using sequential data (embeddings)
  rather than simply adopting the latest large language models (LLMs). The focus will
  remain on creating tailored cultural experiences and driving monetization through
  highly personalized engagement loops, especially in niche but massive markets like
  Spanish-language media. The need for robust data engineering and cross-functional
  collaboration remains paramount for any data science function to succeed.


  ### 6. Target Audience

  This episode is highly valuable for **Data Science Leaders, Machine Learning Engineers,
  Media/Entertainment Executives, and Digital Transformation Professionals**. It offers
  practical wisdom on scaling data science within large, complex organizations and
  provides technical depth on applying classic ML techniques (embeddings) to user
  behavior data.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- anthropic
- meta
title: 'Episode 25: How Data-Driven Growth Redefined a Media Giant'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 90
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 10
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 7
  prominence: 0.7
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-06 04:07:56 UTC -->
