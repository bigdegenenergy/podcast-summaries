---
actionable_items:
- action: democratize
  category: investigation
  full_context: 'opportunity to democratize '
  priority: medium
companies:
- category: tech
  confidence: high
  context: Basically, look at OpenAI's Deep Research Agent. Right? So they have been
    t
  name: Openai
  position: 19
- category: unknown
  confidence: medium
  context: Basically, look at OpenAI's Deep Research Agent. Right? So they have been
    telling us to just prom
  name: Deep Research Agent
  position: 28
- category: unknown
  confidence: medium
  context: ight, everyone. Welcome to another episode of the Twimble AI podcast. I
    am your host, Sam Charrington. Today I
  name: Twimble AI
  position: 467
- category: unknown
  confidence: medium
  context: pisode of the Twimble AI podcast. I am your host, Sam Charrington. Today
    I'm joined by Mahesh Satya Merti. Mahesh i
  name: Sam Charrington
  position: 503
- category: unknown
  confidence: medium
  context: mble AI podcast. I am your host, Sam Charrington. Today I'm joined by Mahesh
    Satya Merti. Mahesh is co-foun
  name: Today I
  position: 520
- category: unknown
  confidence: medium
  context: m your host, Sam Charrington. Today I'm joined by Mahesh Satya Merti. Mahesh
    is co-founder and CEO of Bespoke Labs. Be
  name: Mahesh Satya Merti
  position: 540
- category: unknown
  confidence: medium
  context: hesh Satya Merti. Mahesh is co-founder and CEO of Bespoke Labs. Before
    we get going, be sure to hit that subscri
  name: Bespoke Labs
  position: 592
- category: tech
  confidence: high
  context: Bespoke Labs, and before this, I spent a while at Google. Most recently,
    I was at Google DeepMind, basical
  name: Google
  position: 1433
- category: unknown
  confidence: medium
  context: spent a while at Google. Most recently, I was at Google DeepMind, basically
    training LLMs and recommendation syste
  name: Google DeepMind
  position: 1465
- category: unknown
  confidence: medium
  context: rk published, so we came up with something called Generative Retrieval,
    which is now used across the industry. What from
  name: Generative Retrieval
  position: 1810
- category: unknown
  confidence: medium
  context: as been involved in what's called data companies. Alex Timarkis is a faculty
    member at UC Berkeley. And there aga
  name: Alex Timarkis
  position: 2942
- category: unknown
  confidence: medium
  context: a companies. Alex Timarkis is a faculty member at UC Berkeley. And there
    again, the goal is all about how do yo
  name: UC Berkeley
  position: 2979
- category: unknown
  confidence: medium
  context: ike that. There was a move, I think spurred on by Andrew Ng a few years
    ago, focused on data-centric AI that
  name: Andrew Ng
  position: 3332
- category: unknown
  confidence: medium
  context: al model I use is like there is this book called *The Good Strategy/Bad
    Strategy* by Richard Rumelt, I think. And the
  name: The Good Strategy
  position: 7354
- category: unknown
  confidence: medium
  context: like there is this book called *The Good Strategy/Bad Strategy* by Richard
    Rumelt, I think. And there he talks a
  name: Bad Strategy
  position: 7372
- category: unknown
  confidence: medium
  context: s book called *The Good Strategy/Bad Strategy* by Richard Rumelt, I think.
    And there he talks about how you have a
  name: Richard Rumelt
  position: 7389
- category: unknown
  confidence: medium
  context: ts use in association with large language models. DeepSeek RL comes to
    mind as really illustrating the growing
  name: DeepSeek RL
  position: 10676
- category: unknown
  confidence: medium
  context: portant? For many people, DeepSeek RL happened on ML Day, I think, in January,
    and we all woke up to the p
  name: ML Day
  position: 10911
- category: unknown
  confidence: medium
  context: model that was trained on this. So, this was the Bespoke Status model,
    which we pulled off in like 48 hours. So,
  name: Bespoke Status
  position: 11793
- category: unknown
  confidence: medium
  context: of this, and both have been pretty well received. The OpenThinker model,
    the 32B model, actually beats the DeepSeek
  name: The OpenThinker
  position: 12702
- category: unknown
  confidence: medium
  context: t runtime. I think this also connects back to the Bitter Lesson that we
    are all familiar with, which is at the en
  name: Bitter Lesson
  position: 16396
- category: unknown
  confidence: medium
  context: is what you should do." So, that is very brittle. The Bitter Lesson actually
    says, in a way, don't do this, but you s
  name: The Bitter Lesson
  position: 16850
- category: unknown
  confidence: medium
  context: we don't want to fight that train of improvement. But I think the other
    thing to think about is what's in
  name: But I
  position: 18165
- category: unknown
  confidence: medium
  context: hat I asked earlier, which was essentially like, "Why RL? Why now?" What
    is different? Correct. RL has bee
  name: Why RL
  position: 20325
- category: unknown
  confidence: medium
  context: long time, right? Maybe 30 years or so, I think. And RL has also been effective,
    like in AlphaGo, AlphaZe
  name: And RL
  position: 20449
- category: unknown
  confidence: medium
  context: 'things that we wanted to do. I think this is what David Silver and Rich
    Sutton wrote in *The Era of Experience*:'
  name: David Silver
  position: 23827
- category: unknown
  confidence: medium
  context: 'nted to do. I think this is what David Silver and Rich Sutton wrote in
    *The Era of Experience*: eventually, we'
  name: Rich Sutton
  position: 23844
- category: unknown
  confidence: medium
  context: 'is is what David Silver and Rich Sutton wrote in *The Era of Experience*:
    eventually, we will move to Alpha'
  name: The Era
  position: 23866
- category: unknown
  confidence: medium
  context: of applying RL to foundation models? So, I think Andrej Karpathy had this
    tweet sometime back saying, "Programming
  name: Andrej Karpathy
  position: 24223
- category: unknown
  confidence: medium
  context: es to applying RL to address this problem? Right. In RL, I think what you
    need is you have a prompt, and
  name: In RL
  position: 27716
- category: unknown
  confidence: medium
  context: ify if the answer is right or not. This is in RL. In SFT, all you need
    is, "Okay, here is my prompt, here
  name: In SFT
  position: 27908
- category: unknown
  confidence: medium
  context: I think the other example would be in the case of Deep Research, basically,
    you can give a reward based on whethe
  name: Deep Research
  position: 29713
- category: unknown
  confidence: medium
  context: d. The most famous is VRL, which people use to do RL GRPO training. Now,
    a bunch of others like Unsloth, Hu
  name: RL GRPO
  position: 30528
- category: tech
  confidence: high
  context: PO training. Now, a bunch of others like Unsloth, Hugging Face's TRL, all
    of them support this. And then you nee
  name: Hugging Face
  position: 30583
- category: unknown
  confidence: medium
  context: on math reasoning and AIME and things like that, Math Olympiad, right?
    There, you have a lot more data, and you
  name: Math Olympiad
  position: 32821
- category: unknown
  confidence: medium
  context: 'ut this is another way to showcase that it works. And MiniChart is kind
    of also similar: it does Q&A on charts, a'
  name: And MiniChart
  position: 41672
- category: ai_company
  confidence: high
  context: Mentioned for their Deep Research Agent, which they built using RL fine-tuning
    instead of just prompting.
  name: OpenAI
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: The company co-founded by the guest (Mahesh Satya Merti), focused on improving
    AI models using reinforcement learning and data curation tools.
  name: Bespoke Labs
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The previous employer of the guest, where he worked on training LLMs and
    recommendation systems.
  name: Google
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The specific division within Google where the guest worked on LLMs and
    post-training work.
  name: Google DeepMind
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The predecessor to Gemini, which the guest worked on; its improvement was
    attributed heavily to data curation.
  name: Bard
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The current name for the model formerly known as Bard.
  name: Gemini
  source: llm_enhanced
- category: research_institution
  confidence: high
  context: Where the co-founder (Alex Timarkis) is a faculty member, focusing on data
    curation.
  name: UC Berkeley
  source: llm_enhanced
- category: ai_model
  confidence: medium
  context: Mentioned as an example where improvements are primarily driven by data
    quality.
  name: Llama
  source: llm_enhanced
- category: individual_influence
  confidence: high
  context: Credited with spurring the focus on data-centric AI a few years prior.
  name: Andrew Ng
  source: llm_enhanced
- category: ai_tooling
  confidence: medium
  context: Mentioned as an example of a direction focused on optimizing prompting
    (though Bespoke Labs chose RL fine-tuning instead).
  name: DSP
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A specific RL application/paper that gained attention, prompting Bespoke
    Labs to release data based on it.
  name: DeepSeek RL
  source: llm_enhanced
- category: ai_company
  confidence: high
  context: The organization/project associated with the DeepSeek RL paper and the
    32B and 7B models.
  name: DeepSeek
  source: llm_enhanced
- category: ai_organization
  confidence: high
  context: An organization/consortium created by Bespoke Labs and the open-source
    community to curate reasoning data.
  name: OpenThoughts
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: The model trained by the OpenThoughts consortium on curated reasoning data,
    which outperformed DeepSeek 32B.
  name: OpenThinker
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used as an example agent for discussing the design of rewards in RL applications.
  name: Deep Research Agent
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a famous library used by people to do RL PPO training.
  name: VRL
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as one of the libraries supporting RL GRPO training.
  name: Unsloth
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a library supporting RL GRPO training.
  name: Hugging Face's TRL
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Historical RL application used as an analogy for models starting with SFT/human
    intuition.
  name: AlphaGo
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Historical RL application used as an analogy for models trained completely
    from scratch.
  name: AlphaZero
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Co-author mentioned regarding the book *The Era of Experience*, relevant
    to RL research.
  name: David Silver
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Co-author mentioned regarding the book *The Era of Experience*, relevant
    to RL research (The Bitter Lesson).
  name: Rich Sutton
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned via their TRL (Transformer Reinforcement Learning) library, which
    supports RL GRPO training.
  name: Hugging Face
  source: llm_enhanced
- category: ai_startup
  confidence: high
  context: Mentioned as an example of an autonomous software engineer, suggesting
    a parallel goal for an autonomous ML engineer.
  name: Devin
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Used as a benchmark for comparison against the custom-trained MiniCheck
    model for hallucination detection; implied to be an OpenAI product.
  name: GPT-4o
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Used as a benchmark for comparison against the custom-trained MiniChart
    model.
  name: Claude 3.5
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Used as a benchmark for comparison against the custom-trained MiniChart
    model.
  name: Gemini 1.5
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The base model used by the speakers for RL fine-tuning in their tool-use
    example.
  name: Qwen-1.5-Instruct model 7B
  source: llm_enhanced
date: 2025-05-13 22:10:00 +0000
duration: 61
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN2707985480.mp3?updated=1747175429
processing_date: 2025-10-05 17:56:50 +0000
quotes:
- length: 91
  relevance_score: 5
  text: Most recently, I was at Google DeepMind, basically training LLMs and recommendation
    systems
  topics: []
- length: 94
  relevance_score: 5
  text: So, again, you have to take your corpus and curate some data and create the
    evaluation dataset
  topics:
  - valuation
- length: 277
  relevance_score: 4
  text: You should be able to use structured output to kind of guide in different
    stages of doing the data curation, and then you don't want to worry about retries
    and failures and so on, especially when you have like a million examples that
    you want to curate for the fine-tuning data
  topics: []
- length: 187
  relevance_score: 3
  text: And this is something you can to some extent automate, but most of the time
    you have to just read what's happening, and then you realize, "Oh, it's actually
    making these kinds of mistakes
  topics: []
- length: 244
  relevance_score: 3
  text: You can work on making the rockets faster or all sorts of things, but if you
    think through what the crux of the problem is, it's like you want reusable rockets
    because the cost is the limiting factor in having a system that sends people to
    Mars
  topics: []
- length: 186
  relevance_score: 3
  text: So, for SFT, what happens is you have to get all sorts of data, maybe tens
    of thousands of examples of saying, "Okay, this is my question, this is how the
    tool call should be," and so on
  topics: []
- length: 96
  relevance_score: 3
  text: What we found is that RL is actually much nicer to work with because you don't
    need as much data
  topics: []
- length: 77
  relevance_score: 3
  text: They don't have to rely on OpenAI training the best model and then using that
  topics: []
- impact_reason: 'Highlights a key contradiction: industry leaders advocating for
    pure prompting are internally relying on advanced techniques like RL fine-tuning
    for complex agents, suggesting prompting alone has significant limitations for
    agentic behavior.'
  relevance_score: 10
  source: llm_enhanced
  text: Basically, look at OpenAI's Deep Research Agent. Right? So they have been
    telling us to just prompt models, but they ended up using fine-tuning, RL fine-tuning,
    to actually build this agent, right?
  topic: technical/strategy
- impact_reason: 'Provides a clear rationale for using RL: it''s essential for teaching
    models nuanced concepts like ''good'' vs. ''bad'' and improving complex reasoning,
    going beyond the capabilities of simple prompting.'
  relevance_score: 10
  source: llm_enhanced
  text: The reason they do this is because objectively, yes, I think prompting only
    gets you so far. You want to use RL as a way to teach the model what's good, what's
    bad, and how to reason through things, and so on.
  topic: technical/agent
- impact_reason: 'Outlines a clear roadmap for advancing model capability: Phase 1
    was SFT for basic reasoning; Phase 2 is RL for advanced reasoning and agentic
    behavior.'
  relevance_score: 10
  source: llm_enhanced
  text: What we did earlier was show how you can do this just using SFT to unlock
    reasoning in non-reasoning models. And now the next phase is how do you use RL
    to further augment this?
  topic: technical/roadmap
- impact_reason: Identifies tool use as the critical next frontier for RL application
    in LLMs, directly linking RL to the development of functional AI agents.
  relevance_score: 10
  source: llm_enhanced
  text: And basically, where I'm going with this is ultimately we were targeting tool
    use because that's one of the key components of building agents.
  topic: agent/technical
- impact_reason: Articulates a major pain point in current LLM application development—the
    fragility and complexity of prompt engineering ('prompt hell')—setting the stage
    for why RL-based solutions are necessary.
  relevance_score: 10
  source: llm_enhanced
  text: 'Right now people are just prompting away models, and they have these complex
    series of prompts. They end up in prompt hell. That''s basically what''s happening:
    you put together all these prompts, and then something works or doesn''t, and
    you don''t know why. It''s very fragile.'
  topic: business/strategy
- impact_reason: Uses OpenAI's own Deep Research Agent as evidence that even leading
    labs resort to RL fine-tuning for complex agentic behavior, undermining the 'just
    prompt it' narrative.
  relevance_score: 10
  source: llm_enhanced
  text: They could have built that agent using by prompting GPT-4 or GPT-4 Mini or
    GPT-4, but they ended up using RL fine-tuning, right? So, they have been telling
    us to just prompt models, but they ended up using fine-tuning, RL fine-tuning,
    to actually build this agent, right?
  topic: technical/strategy
- impact_reason: 'Explains the fundamental advantage of RL: it bakes runtime feedback
    into the model weights permanently, contrasting with interactive, fragile runtime
    prompting.'
  relevance_score: 10
  source: llm_enhanced
  text: So, you can use RL to—it's actually like you're essentially what the process
    is doing is baking that feedback into the model that you can then run, as opposed
    to trying to do that interactively at runtime.
  topic: technical
- impact_reason: 'Provides a crucial strategic insight for enterprise AI adoption:
    frontier models are excellent in-distribution (general world knowledge) but fail
    out-of-distribution (specific enterprise context/tools). RL fine-tuning bridges
    this gap.'
  relevance_score: 10
  source: llm_enhanced
  text: It's the latter, actually. So, models are actually just in general getting
    better, right? So, we don't want to fight that train of improvement. But I think
    the other thing to think about is what's in-distribution and what's out-of-distribution.
    In the sense that for all the frontier models, what they are in-distribution is
    what's happening around the world, but they don't know what's happening inside
    an enterprise, right? So, that is out-of-distribution for these frontier models.
  topic: business/strategy
- impact_reason: 'Provides a powerful analogy: RL/SFT fine-tuning is the ''onboarding
    training'' for an LLM to adapt it to a specific company''s ecosystem and tools.'
  relevance_score: 10
  source: llm_enhanced
  text: So, basically, what we are trying to do is help enterprises be able to bake
    that knowledge into the model. So, you can take any open-source model, and then
    the model—it's like any you are hiring a human, right? So, I can go in and take
    a look at what's happening. You can tell me all the things, but maybe I need some
    training to get used to an enterprise when I start doing work at a company, right?
    So, that's the process you can think of as a combination of reinforcement learning
    and SFT to kind of get the model trained in your system so that it becomes adapted
    using your tools.
  topic: business/strategy
- impact_reason: 'Explains why RL is suddenly viable for LLMs: the models possess
    massive prior knowledge (like a ''trained human''), drastically reducing the compute
    needed for trial-and-error learning compared to training from scratch (the ''monkey''
    analogy).'
  relevance_score: 10
  source: llm_enhanced
  text: What's happening now with the LLMs is that these LLMs are like you don't need
    a monkey; you can actually train a human, right? They come with built-in knowledge.
    So, you don't need to throw in lots of compute; you can put a little less compute,
    a lot less compute.
  topic: technical/predictions
- impact_reason: Clearly defines the 'bootstrapping problem' in traditional RL and
    explains how LLMs' world knowledge solves it, leading to compute efficiency.
  relevance_score: 10
  source: llm_enhanced
  text: Essentially, in prior applications of RL, there was a bootstrapping problem.
    You're starting with this agent that knows nothing; it needs to bounce around
    on all the walls and kind of do a lot of things just to start to gain a foundational
    understanding. But now, due to the models being more powerful and incorporating
    world knowledge... we've got this synergy that allows you to make a lot of progress
    on less compute.
  topic: technical
- impact_reason: 'Offers a powerful conceptual shift: if prompting is ''Programming
    in English,'' then reward shaping is the ''new prompting'' or the new programming
    language for controlling RL agents.'
  relevance_score: 10
  source: llm_enhanced
  text: Andrej Karpathy had this tweet sometime back saying, 'Programming in English
    is the hottest new programming language.' So, he was just saying, 'Now, in order
    to control these models, you just specify in English.' Right? So, now I think
    what's happening is—the way I think is—reward shaping is the new prompting, the
    new programming language, right? It's about how you shape the reward.
  topic: strategy/technical
- impact_reason: This is a core thesis statement, equating the complex task of designing
    effective reward functions in RL with the current practice of prompt engineering,
    suggesting RL fine-tuning is the next frontier of 'programming' LLMs.
  relevance_score: 10
  source: llm_enhanced
  text: Reward shaping is the new prompting, the new programming language, right?
    It's about how you shape the reward.
  topic: technical
- impact_reason: 'This is a major practical finding: RL can significantly reduce data
    requirements for complex tasks like multi-turn tool use compared to Supervised
    Fine-Tuning (SFT), making agent development more accessible.'
  relevance_score: 10
  source: llm_enhanced
  text: What we found is that RL is actually much nicer to work with because you don't
    need as much data. In that particular case, all we needed were about a hundred
    good-quality examples instead of tens of thousands.
  topic: business/technical
- impact_reason: 'Describes a powerful hybrid training loop: using RL to bootstrap
    capability, then using the resulting RL model to generate high-quality synthetic
    data for subsequent SFT, potentially combining the strengths of both methods.'
  relevance_score: 10
  source: llm_enhanced
  text: Is it possible to capture the data that's generated in the RL process and
    use that as prompts for SFT? ... they had DeepSeek RL 0, but they didn't do any
    SFT; they did just the RL-based fine-tuning, and using that, they got a model,
    and they used that model to generate some data, which then went into the SFT.
  topic: technical
- impact_reason: Provides a concrete, large-scale example of using RL to generate
    high-quality synthetic data for subsequent SFT, demonstrating a powerful self-improvement
    loop in model training.
  relevance_score: 10
  source: llm_enhanced
  text: DeepSeek RL does something like that. So, in this case, things worked fine,
    but in the large-scale DeepSeek, what they do is they had DeepSeek RL 0, but they
    didn't do any SFT; they did just the RL-based fine-tuning, and using that, they
    got a model, and they used that model to generate some data, which then went into
    the SFT.
  topic: technical
- impact_reason: 'Introduces a specialized, highly relevant application: a dedicated
    model for grounding/hallucination detection within RAG systems, addressing a major
    pain point in enterprise LLM deployment.'
  relevance_score: 10
  source: llm_enhanced
  text: MiniCheck is a model that is trained to detect hallucinations. It's a specific
    problem called groundedness, basically. If you take, especially in RAG systems,
    you have a context which the model looks at and produces an answer or a claim.
    This particular model is good at checking if this claim is supported by this context
    or not.
  topic: technical
- impact_reason: A powerful claim of superior performance (beating GPT-4o on a specific
    task) combined with massive cost and latency advantages (7B model), demonstrating
    the power of specialized, distilled models.
  relevance_score: 10
  source: llm_enhanced
  text: The advantage of using the MiniCheck model is it's actually—we measure it
    on a benchmark—and it's actually better than GPT-4o in this task. And then it's
    actually way cheaper because it's a 7B model.
  topic: business
- impact_reason: Demonstrates that highly complex multimodal tasks (chart Q&A) can
    be mastered by smaller, specialized models (7B) to reach state-of-the-art performance
    levels previously reserved for frontier models.
  relevance_score: 10
  source: llm_enhanced
  text: 'MiniChart is kind of also similar: it does Q&A on charts, and we are able
    to train a 7B model, and it''s much better than existing 7B models. It also achieves
    the level of Claude 3.5 and Gemini 1.5.'
  topic: predictions
- impact_reason: A direct insight from a major product development cycle (Bard/Gemini)
    emphasizing that data quality and curation, not just model architecture, drive
    significant performance gains.
  relevance_score: 9
  source: llm_enhanced
  text: 'So, I was on the extended Bard team, now called Gemini, and one thing was
    very clear: the role of data. So, there was a huge difference between where Bard
    started after the 100-day sprint and then eventually as the model kept improving.
    There was, of course, some modeling improvements, but everything is all about
    data.'
  topic: business/data strategy
- impact_reason: Strong advocacy for the 'Data-Centric AI' paradigm, asserting that
    data improvement yields higher returns ('alpha') than traditional model tuning
    in the current LLM landscape.
  relevance_score: 9
  source: llm_enhanced
  text: Looking at the data has so much alpha, and working on the data, improving
    the data, rather than spending time on, "Okay, what's my hyperparameter? How do
    I change the learning rate?" and so on.
  topic: strategy/business
- impact_reason: 'Crucial advice for practitioners: moving beyond simple accuracy
    metrics to deep error analysis is necessary for iterative improvement, especially
    for agents.'
  relevance_score: 9
  source: llm_enhanced
  text: But the second thing you need to do is error analysis. So, again, I don't
    think a lot of people are intimately familiar when I talk about this. People just
    stop at evaluation, but what you really need is evaluation and error analysis,
    where you see what kind of mistakes the agent or the model is making.
  topic: technical/methodology
- impact_reason: A powerful anecdote showcasing rapid iteration and model deployment
    capability enabled by strong data infrastructure (48 hours to train and release
    a competitive model).
  relevance_score: 9
  source: llm_enhanced
  text: We organized the dataset and we kind of released this, as well as the 32B
    model that was trained on this. So, this was the Bespoke Status model, which we
    pulled off in like 48 hours.
  topic: business/speed
- impact_reason: 'Provides concrete evidence of success: outperforming a baseline
    model (DeepSeek) and demonstrating massive community impact (200+ models trained
    on their dataset).'
  relevance_score: 9
  source: llm_enhanced
  text: The OpenThinker model, the 32B model, actually beats the DeepSeek 32B model.
    So, that was a good achievement for all of us who worked together, and the dataset
    has also been used for training at least more than 200 models that we know of.
  topic: business/impact
- impact_reason: 'Summarizes the core value proposition: high-quality data unlocks
    reasoning capabilities, which is the necessary precursor or complement to RL-based
    agent refinement.'
  relevance_score: 9
  source: llm_enhanced
  text: The reason this is all popular and useful is that it's unlocking reasoning,
    and it's kind of tightly connected with RL.
  topic: technical/strategy
- impact_reason: Presents RL as the direct, more robust replacement for brittle prompting
    workflows when building complex, reasoning-capable AI agents.
  relevance_score: 9
  source: llm_enhanced
  text: So, all of this we want to replace with using RL because that makes it a much
    more flexible mechanism to build agents, especially agents that can use reasoning.
  topic: strategy/technical
- impact_reason: 'Defines the core function of RL in this context: reinforcing the
    internal reasoning process (''thought trace'') rather than just enforcing surface-level
    instruction adherence.'
  relevance_score: 9
  source: llm_enhanced
  text: Basically you want to use RL as a way to teach the model what's good, what's
    bad, and how to—it's not about following instructions, but basically the model
    learns to figure out, 'Okay, these are the parts to follow, this is how to reason
    through things,' and so on, again, in the reasoning and kind of reinforcing that
    thought trace process, and also punishing the bad behavior, right?
  topic: technical
- impact_reason: 'Identifies the critical failure of prompting for iterative improvement:
    the lack of a closed, actionable feedback loop for runtime failures.'
  relevance_score: 9
  source: llm_enhanced
  text: I think what's happening with prompting is people prompt models and build
    agents, but they have no way of completing the feedback loop, right? So, you have
    all these prompts, and your agent either worked or it didn't work, and now what
    do you do with that information?
  topic: strategy/technical
- impact_reason: Frames the shift from prompt engineering to RL as aligning with the
    'Bitter Lesson'—minimizing reliance on human-encoded intuition in favor of automated
    search and learning.
  relevance_score: 9
  source: llm_enhanced
  text: This also connects back to the Bitter Lesson that we are all familiar with,
    which is at the end of the day, what the Bitter Lesson says is you don't want
    to rely too much on human intuition. You just let the model search and learn,
    right?
  topic: strategy
- impact_reason: 'Defines the business value proposition for using RL/fine-tuning
    on open-source models: deep adaptation to proprietary enterprise environments
    and specific tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: Our hope and what we are pitching is, 'Okay, we can build all this tooling
    that can be used so that the models—you can take open-source models—and they understand
    your environment, your ecosystem, and they get better at the tasks that you care
    about.'
  topic: business
- impact_reason: 'Summarizes the current sweet spot for RL application: leveraging
    highly capable base models (''nudge it in the right direction'') with modern algorithms
    (like GRPO) and environment setup.'
  relevance_score: 9
  source: llm_enhanced
  text: 'I think that''s what has changed: these models are now generally very capable,
    and you just nudge it in the right direction, and you use GRPO, you set up the
    environment, and you use a reasonably good reward model or reward shaping, and
    you kind of get this working.'
  topic: technical
- impact_reason: 'Uses the AlphaGo/AlphaZero comparison to perfectly analogize the
    current LLM situation: we are in the ''AlphaGo moment'' where pre-training provides
    the necessary foundation, avoiding the massive compute cost of AlphaZero.'
  relevance_score: 9
  source: llm_enhanced
  text: 'The other example here is AlphaZero and AlphaGo. AlphaZero is trained completely
    from scratch, so it uses a lot more compute, whereas AlphaGo starts from some
    sort of SFT, some sort of human intuition that''s baked in. So, that''s like what
    we have now: the AlphaGo moment, which is you take these LLMs, they kind of understand
    the world, so they have been SFT''d and whatnot.'
  topic: strategy/technical
- impact_reason: Warns about the classic RL pitfall of 'reward hacking,' where agents
    optimize for the metric rather than the underlying goal, a crucial consideration
    for safety and alignment.
  relevance_score: 9
  source: llm_enhanced
  text: But what we have found—so, basically, you are giving it either a positive
    reward, say +1, when it gets the answer right, or a -1. And sometimes we do see
    that the model does what's called reward hacking, so the agent kind of finds a
    shortcut to get the reward that we are promising it by not doing actually the
    things it needs to do.
  topic: safety/technical
- impact_reason: This quote, attributed to Andrej Karpathy, succinctly captures the
    paradigm shift where natural language (English) is becoming the primary interface
    for controlling complex AI models, replacing traditional code.
  relevance_score: 9
  source: llm_enhanced
  text: Programming in English is the hottest new programming language.
  topic: strategy
- impact_reason: 'Highlights a critical and persistent challenge in Reinforcement
    Learning (RL) and agent design: the misalignment between the specified reward
    function and the desired complex behavior.'
  relevance_score: 9
  source: llm_enhanced
  text: Sometimes we do see that the model does what's called reward hacking, so the
    agent kind of finds a shortcut to get the reward that we are promising it by not
    doing actually the things it needs to do.
  topic: safety/technical
- impact_reason: 'Summarizes the key advantage of RL for agent training: the reliance
    on verifiable outcomes (rewards) rather than exhaustive, correct demonstration
    sequences (SFT data).'
  relevance_score: 9
  source: llm_enhanced
  text: You just need the prompts, and you need a way to verify if the answers are
    good. So, RL made it much easier for us to basically train these agents compared
    to SFT.
  topic: technical
- impact_reason: Articulates a strategic goal of democratizing advanced AI tooling
    (like RL fine-tuning) so that organizations can build specialized, high-quality
    models without relying solely on frontier labs.
  relevance_score: 9
  source: llm_enhanced
  text: To a large extent, our goal is to actually democratize access to these tools...
    So, they should be able to train good models for their use cases, and it's actually
    cheaper and better quality, and so on.
  topic: strategy
- impact_reason: Offers a rapid, near-term prediction for the emergence of autonomous
    AI engineering roles, paralleling the development of autonomous software engineers
    like Devin.
  relevance_score: 9
  source: llm_enhanced
  text: Maybe in a year or so, right? Things are moving rapidly. We have Devin, that's
    an autonomous software engineer. Maybe we have something that's an autonomous
    ML engineer or AI engineer.
  topic: predictions
- impact_reason: 'Provides empirical evidence of RL''s benefit: it moves models beyond
    simple, single-step actions to complex, sequential reasoning required for multi-turn
    agentic behavior.'
  relevance_score: 9
  source: llm_enhanced
  text: We did see that the reasoning actually improves. So, the non-fine-tuned model
    would just make some kind of silly mistakes, or it was mostly doing just single-turn.
    After you did RL, it became—it was able to do multi-turn tool calling...
  topic: technical
- impact_reason: 'Reiterates the key value proposition of RL here: achieving complex,
    multi-step capabilities without the prohibitive cost of collecting labeled demonstration
    data for those complex sequences.'
  relevance_score: 9
  source: llm_enhanced
  text: We did not have to curate any of the data [for multi-turn tool calling]. So,
    that was our way of evaluating.
  topic: business/technical
- impact_reason: Directly contrasts the capability gap between a base model and an
    RL-tuned model regarding complex, sequential reasoning (multi-turn tool calling),
    highlighting RL's effectiveness in teaching procedural logic.
  relevance_score: 9
  source: llm_enhanced
  text: So, the non-fine-tuned model would just make some kind of silly mistakes,
    or it was mostly doing just single-turn. After you did RL, it became—it was able
    to do multi-turn tool calling, so it would actually call one tool, look at the
    response, and then decide, "Okay, based on this, I need to call a different tool,"
    right?
  topic: technical
- impact_reason: Provides a clear, relatable, real-world example of multi-turn tool
    use (using `pwd` before `cd` and then deleting), illustrating necessary procedural
    dependency in agentic workflows.
  relevance_score: 9
  source: llm_enhanced
  text: 'So, maybe let''s take an example: you ask the model to delete a file in a
    specific folder. It might just attempt to delete directly, or it may say, "Let
    me first see where I am." So, it might execute `pwd`, and then based on that,
    it''s, "Okay, now I need to go to that folder," so `cd` to that folder, and then
    delete, right? So, this is multi-turn.'
  topic: technical
- impact_reason: Crucially links RL fine-tuning to improved robustness against failure
    modes (giving up, hallucinating) when dealing with multi-step tasks, making the
    agent more reliable.
  relevance_score: 9
  source: llm_enhanced
  text: RL produces models that recognize a multi-turn path to solving a problem,
    as opposed to either giving up or hallucinating an API that doesn't really exist
    that only uses the information they have in a single turn.
  topic: technical
- impact_reason: Identifies a critical bottleneck in synthetic data pipelines—quality
    filtering—and presents a solution (MiniCheck) for automated data curation.
  relevance_score: 9
  source: llm_enhanced
  text: Anyway, the reason we trained this is it's actually useful in data curation,
    where you are curating data and you are generating a lot of synthetic data, and
    so you want to be able to filter out bad-quality data.
  topic: strategy
- impact_reason: Provides strong evidence that distillation can lead to performance
    gains over the teacher model, directly challenging the reluctance of practitioners
    to engage in fine-tuning.
  relevance_score: 9
  source: llm_enhanced
  text: This model is kind of distilled, and so it actually outperforms the teacher
    models. So, it's also a proof of concept that people are very reluctant to fine-tune
    models, but this is another way to showcase that it works.
  topic: technical
- impact_reason: Introduces the concept of 'data recipes,' suggesting that the methodology
    of data preparation is a critical, proprietary asset in LLM development.
  relevance_score: 8
  source: llm_enhanced
  text: So, it's all about what data recipes we use, how we curate the data, how we
    filter the data, and so on.
  topic: technical/data strategy
- impact_reason: Reiterates the enduring importance of data-centric AI, suggesting
    the industry briefly forgot this principle while chasing large model releases.
  relevance_score: 8
  source: llm_enhanced
  text: Those same ideas are exactly still important, it's just that people somehow
    seem to have kind of lost the train of thought there. So, if you look at all the
    improvements in Llama or everywhere, it's all about the data, right?
  topic: strategy
- impact_reason: 'Points out a critical bottleneck in the prompting workflow: evaluation
    requires high-quality, curated datasets, even if initial work is done via prompting.'
  relevance_score: 8
  source: llm_enhanced
  text: A lot of people are prompting models to get things done, but the other component
    is how do you evaluate? And there, you still need to curate data.
  topic: technical/evaluation
- impact_reason: Identifies data curation as the primary failure point for post-training/fine-tuning
    projects, justifying the focus on building specialized tooling.
  relevance_score: 8
  source: llm_enhanced
  text: The place where they usually fail is in the data part, and that's the reason
    we started with data and we kind of built some tooling around like visualization
    and also curation.
  topic: business/strategy
- impact_reason: Confirms that modern data curation heavily involves synthetic data
    generation, whether through direct generation or using LLMs for filtering/labeling
    existing data.
  relevance_score: 8
  source: llm_enhanced
  text: When we're talking about data curation here, are we primarily talking about
    synthetic data generation? Basically, yeah, any time you use an LLM to do something.
    So, you could be using an LLM to filter your existing human data, or you could
    be generating more human-LLM-generated data.
  topic: technical/data generation
- impact_reason: Demonstrates the speed and capability of specialized data tooling
    (Curator) in rapidly addressing community needs, specifically the lack of high-quality
    reasoning datasets following the DeepSeek announcement.
  relevance_score: 8
  source: llm_enhanced
  text: What they did was, given that we had actually built Curator, which is good
    at data creation, we were able to quickly get started and start generating a reasoning
    dataset, which there weren't many such datasets available, especially high-quality
    ones.
  topic: business/tooling impact
- impact_reason: Establishes reasoning as a key focus area for the community and explicitly
    links high-quality reasoning data to RL applications.
  relevance_score: 8
  source: llm_enhanced
  text: The goal has been to curate reasoning data that people can then use to train
    reasoning models. I'll kind of touch upon why reasoning is important and how it's
    related to RL and so on.
  topic: technical/reasoning
- impact_reason: Directly contrasts brittle, human-guided prompting (violating the
    Bitter Lesson) with the self-discovery enabled by RL.
  relevance_score: 8
  source: llm_enhanced
  text: Here, with prompting, we are actually giving the model all sorts of this human
    intuition, saying, 'Okay, at this stage, this is what you should do.' So, that
    is very brittle. The Bitter Lesson actually says, in a way, don't do this, but
    you should let the model figure that out by itself.
  topic: strategy
- impact_reason: Highlights tool/function calling as a prime example of adapting a
    general model to a specific environment (the toolset) using RL.
  relevance_score: 8
  source: llm_enhanced
  text: One of the ways, in fact, you recently published some work on using RL to
    improve tool calling or function calling in models. That's a particular way of
    adapting a model, a generic model, to a particular environment—the environment
    in this case being a set of tools that are available.
  topic: technical/business
- impact_reason: Details the high cost (complexity, latency, expense) associated with
    trying to integrate internal APIs/tools via prompt engineering.
  relevance_score: 8
  source: llm_enhanced
  text: What happens is they basically have to specify, 'Okay, these are my tools,'
    and all sorts of definitions around it. And before you know it, the prompt is
    very complex, and latency adds up like anything, and it also becomes very expensive.
  topic: business
- impact_reason: Provides a long-term strategic outlook, suggesting that while we
    are currently in the 'AlphaGo' phase (leveraging SFT), the ultimate goal remains
    the 'AlphaZero' phase (pure self-learning, unconstrained by human data).
  relevance_score: 8
  source: llm_enhanced
  text: 'I think this is what David Silver and Rich Sutton wrote in *The Era of Experience*:
    eventually, we will move to AlphaZero, where we are not bounded by human intuition,
    you don''t need so much SFT and human-curated data, and it just kind of learns
    itself what it needs to do.'
  topic: predictions/strategy
- impact_reason: 'Provides actionable advice regarding reward design: simplicity often
    outperforms complex, multi-faceted reward structures, suggesting a practical heuristic
    for RL practitioners.'
  relevance_score: 8
  source: llm_enhanced
  text: But at least in our recent work, we found that the simpler the reward, the
    better it is. Maybe this is some form of Occam's Razor.
  topic: technical/business
- impact_reason: Draws an analogy between the 'Bitter Lesson' (general methods leveraging
    massive computation beat specialized human knowledge) and the finding that simpler,
    general rewards outperform complex, hand-crafted ones in this context.
  relevance_score: 8
  source: llm_enhanced
  text: I think it sounds like the Bitter Lesson applied to reward shaping.
  topic: strategy
- impact_reason: Clearly outlines the two primary methods (SFT vs. RL) for teaching
    LLM agents complex, structured behaviors like tool use.
  relevance_score: 8
  source: llm_enhanced
  text: Tool use is useful for agents. So, how do you do tool use? One, you can do
    SFT, or use reinforcement learning as RL-based fine-tuning.
  topic: technical
- impact_reason: 'Provides a concise definition of the necessary components for applying
    RL to LLM agents: Prompt, Environment, and Verifier/Reward mechanism.'
  relevance_score: 8
  source: llm_enhanced
  text: In RL, I think what you need is you have a prompt, and you need an environment
    where the agent is trying things out, and you need a way to verify if the answer
    is right or not.
  topic: technical
- impact_reason: Illustrates the flexibility in designing reward signals for agents,
    ranging from purely linguistic evaluation (LLM as judge) to functional execution
    verification.
  relevance_score: 8
  source: llm_enhanced
  text: You can also have a reward function that just looks at the text, in which
    case you can use an LLM to say, 'Is this function making sense?' Or you can just
    execute and get a response back and use that as the reward.
  topic: technical
- impact_reason: 'Identifies a significant infrastructure gap in the AI ecosystem:
    the lack of generalized, plug-and-play libraries for creating and instrumenting
    realistic simulation environments for agent training.'
  relevance_score: 8
  source: llm_enhanced
  text: 'There are no good libraries out there, actually. That''s one of the opportunities:
    basically building something that makes it easy for people to plug and play different
    environments.'
  topic: business/strategy
- impact_reason: Expresses a strong preference for fine-tuning/RL methods over prompt
    engineering for complex tasks, viewing it as a more robust and intellectually
    satisfying engineering path.
  relevance_score: 8
  source: llm_enhanced
  text: 'For me, that''s my personal goal: not how people spend so much time on prompting,
    but actually fine-tuning and doing all these things.'
  topic: strategy
- impact_reason: Foreshadows the automation of the agent creation pipeline itself,
    suggesting that the processes of data curation and reward shaping are becoming
    structured enough to be automated by other AI systems.
  relevance_score: 8
  source: llm_enhanced
  text: 'Maybe we build an agent that builds an agent. That''s kind of what you are
    mentioning: how much we can automate. I think there are well-defined data recipes,
    well-defined ways of reward shaping, and so on. So, a lot of this could be codified...'
  topic: predictions
- impact_reason: Suggests that Reinforcement Learning (RL) can induce complex behaviors
    (like multi-turn reasoning) without explicit Supervised Fine-Tuning (SFT) data
    curation, implying efficiency gains in training.
  relevance_score: 8
  source: llm_enhanced
  text: The model is actually picking this up without us having to curate any of the
    data.
  topic: technical
- impact_reason: 'Defines the core requirement for advanced reasoning agents: the
    ability to chain multiple, distinct tool calls sequentially to solve complex problems
    where a single tool is insufficient.'
  relevance_score: 8
  source: llm_enhanced
  text: A strong reasoner should recognize that maybe using a calculator, then looking
    up the exchange rate, then doing something else with that will get you to an answer,
    even if a single tool is close but not correct.
  topic: technical
- impact_reason: 'Highlights the dual utility of specialized small models: real-time
    guardrails (low latency) and offline quality assurance/metric generation (high
    throughput).'
  relevance_score: 8
  source: llm_enhanced
  text: So, it can be used as a guardrail because of its latency, or if you have a
    lot of data, it can go through it and get like a faithfulness metric and things
    like that.
  topic: business
- impact_reason: 'Identifies the next logical step for specialized models: integrating
    tool use (e.g., calling a calculator API) to mitigate inherent model weaknesses
    like mathematical errors.'
  relevance_score: 8
  source: llm_enhanced
  text: The next thing we were thinking is also it can—it should be able to use tools.
    In this case, the model itself is doing the math, which can sometimes be error-prone,
    but we can have a situation where...
  topic: technical
- impact_reason: Broadens the definition of data curation to include essential tooling
    for visualization and qualitative inspection, not just automated processing.
  relevance_score: 7
  source: llm_enhanced
  text: Data curation actually means tooling to, of course, munch the data, filter,
    and so on, but also being able to just visualize, read, and annotate, and those
    kind of things that you need to do.
  topic: technical/tooling
- impact_reason: Highlights the need for abstraction layers (like Curator) to handle
    the complex infrastructure of large-scale, batch-mode data curation for fine-tuning.
  relevance_score: 7
  source: llm_enhanced
  text: Curator is now an open-source library, which makes it pretty easy or seamless
    to actually curate data. So, if you want to fine-tune a model, you want to basically
    not worry about the infrastructure around data curation.
  topic: technical/tooling
- impact_reason: 'Defines the requirements for effective LLM data pipelines: high-throughput,
    batch processing, and integrated quality visualization.'
  relevance_score: 7
  source: llm_enhanced
  text: Here, it's all about large-scale batch mode offline working, and you want
    the data, you want to make sure the throughput is high, and then also ways to
    visualize if the data quality is good and so on.
  topic: technical/infrastructure
- impact_reason: Provides historical context on RL's reputation—aspirational but brittle—setting
    the stage for discussing why it's now succeeding with LLMs.
  relevance_score: 7
  source: llm_enhanced
  text: RL has been around for years, and historically, it's been thought of as this
    kind of aspirational but kind of very brittle approach to building out ML models
    or agents.
  topic: technical/history
- impact_reason: Acknowledges the current cost trade-off (RL is more expensive than
    SFT) but expresses optimism based on ongoing infrastructural and algorithmic improvements
    mitigating this gap.
  relevance_score: 7
  source: llm_enhanced
  text: The compute requirement for RL is kind of higher. But I do see that there
    are a lot of improvements coming in terms of maybe the algorithmic changes, but
    also how you do the rollouts, how you optimize the infra, so that's being helpful.
  topic: business
- impact_reason: Provides a concrete, albeit high-level, example of the compute cost
    difference between RL fine-tuning (using 4 H200s) and the expected lower cost
    of SFT for the same task.
  relevance_score: 7
  source: llm_enhanced
  text: In the one we tried, it's like a small dataset. We had four H200s. I don't
    remember the number of hours, but I think if we did the same thing with SFT, it'd
    be a lot cheaper.
  topic: business
- impact_reason: Highlights the flexibility in modern LLM alignment pipelines, suggesting
    that the order of SFT and RL is a tunable hyperparameter with significant experimental
    potential.
  relevance_score: 7
  source: llm_enhanced
  text: The other thing you can also do is you do some SFT and then start RL. So,
    there is a lot of wiggle room and a lot of experiments that we can run.
  topic: strategy
- impact_reason: 'Reiterates a fundamental principle in modern AI development: the
    quality and curation of data often trump raw model size or architecture complexity.'
  relevance_score: 7
  source: llm_enhanced
  text: So, again, the goal there is to show that good-quality data and data curation
    help.
  topic: strategy
- impact_reason: Highlights the necessity for specialized vision-language models to
    perform quantitative reasoning (arithmetic) over visual data, moving beyond simple
    description.
  relevance_score: 7
  source: llm_enhanced
  text: It's able to answer questions. It's able to do math. If you ask it, "What's
    the average across whatever many quarters?" it does math.
  topic: technical
source: Unknown Source
summary: '## Podcast Summary: From Prompts to Policies: How RL Builds Better AI Agents
  with Mahesh Sathiamoorthy - #731


  This episode of the Twimble AI podcast, hosted by Sam Charrington, features Mahesh
  Sathiamoorthy, Co-founder and CEO of Bespoke Labs, focusing on the critical role
  of **Reinforcement Learning (RL)** and **data curation** in moving beyond brittle
  prompting techniques to build robust, high-performing AI agents.


  ### 1. Focus Area

  The discussion centers on advanced post-training techniques for Large Language Models
  (LLMs), specifically contrasting the limitations of prompt engineering with the
  power of **RL Fine-Tuning** for developing sophisticated AI agents capable of complex
  reasoning and tool use. A strong secondary focus is the **Data-Centric AI** philosophy,
  emphasizing the necessity of high-quality, curated data (including reasoning traces)
  for model improvement.


  ### 2. Key Technical Insights

  *   **RL as the Policy Builder:** RL is presented as the necessary mechanism to
  teach models "what''s good and what''s bad," moving beyond static instructions provided
  via prompting. This is exemplified by OpenAI’s Deep Research Agent, which utilized
  RL fine-tuning despite the general industry push toward prompting.

  *   **The AlphaGo Moment in LLMs:** The current application of RL to LLMs is analogous
  to AlphaGo, not AlphaZero. LLMs possess significant prior knowledge (world understanding
  from pre-training/SFT), meaning RL only requires nudging this knowledge in the right
  direction (fewer rollouts/less compute) rather than learning everything from scratch.

  *   **Reward Shaping as the New Prompting:** The method of defining success and
  failure through reward functions in RL is becoming the new "programming language"
  for controlling model behavior, replacing complex, brittle prompt chains.


  ### 3. Business/Investment Angle

  *   **Enterprise Adaptation is Key:** The primary business value lies in adapting
  powerful, general open-source models to specific enterprise environments, ecosystems,
  and proprietary tools (e.g., internal APIs). RL/SFT combinations allow models to
  become "hired" and trained for specific organizational contexts.

  *   **Data Curation as the Bottleneck:** The failure point for most custom model
  projects is not the model architecture but the data curation pipeline. Tools that
  systematize large-scale, batch-mode data curation and visualization unlock significant
  alpha.

  *   **Agent Development Fragility:** The current reliance on complex, multi-step
  prompting for agents leads to "prompt hell"—fragile, expensive, and difficult-to-debug
  systems. RL offers a more flexible, baked-in policy mechanism for agent behavior.


  ### 4. Notable Companies/People

  *   **Mahesh Sathiamoorthy (Bespoke Labs):** Discussed his background at Google
  DeepMind (working on Bard/Gemini) and the inspiration for Bespoke Labs—the realization
  that data recipes drive most model improvement.

  *   **OpenAI Deep Research Agent:** Cited as a prime example of using RL fine-tuning
  for agent construction.

  *   **DeepSeek RL:** Mentioned as a catalyst for community focus on reasoning data,
  prompting Bespoke Labs to release their own trained model (Bespoke Status) and contribute
  to the **OpenThoughts/OpenThinker** consortium.

  *   **Andrej Karpathy:** Quoted regarding "Programming in English" to frame reward
  shaping as the new control mechanism.


  ### 5. Future Implications

  The industry is moving toward **RL-driven policy creation** for agents, especially
  those requiring tool use and complex reasoning. This shift will allow enterprises
  to deeply customize foundation models for proprietary tasks, moving away from runtime
  prompt manipulation toward baked-in, robust behaviors that adhere to the "Bitter
  Lesson" by letting the model learn optimal policies through experience rather than
  explicit human instruction.


  ### 6. Target Audience

  AI/ML Engineers, Research Scientists, AI Product Managers, and Technology Leaders
  focused on deploying custom, robust AI agents and understanding the next frontier
  beyond standard prompt engineering.'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- investment
- startup
- openai
- google
title: 'From Prompts to Policies: How RL Builds Better AI Agents with Mahesh Sathiamoorthy
  - #731'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 92
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 13
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 7
  prominence: 0.7
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 7
  prominence: 0.7
  topic: investment
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 3
  prominence: 0.3
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 17:56:50 UTC -->
