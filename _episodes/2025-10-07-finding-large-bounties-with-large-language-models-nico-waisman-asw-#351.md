---
companies:
- category: unknown
  confidence: medium
  context: llo, Protocols, Packets, and Programs. Welcome to Scary October. You'll
    hear supply chains rattling. You'll see L
  name: Scary October
  position: 52
- category: unknown
  confidence: medium
  context: e on your own. Which means this week we talk with Nico Weisman about creating
    LLMs to be bug bounty hunters and
  name: Nico Weisman
  position: 295
- category: unknown
  confidence: medium
  context: 'care about. Grab a flashlight and stay tuned for Application Security
    Weekly.


    For the latest AppSec news, it''s time for Appli'
  name: Application Security Weekly
  position: 445
- category: unknown
  confidence: medium
  context: essary packages. Just what your app needs to run. Drop Minimus images into
    existing deployments to dramatically
  name: Drop Minimus
  position: 748
- category: unknown
  confidence: medium
  context: ats don't wait, and neither should your defenses. With ThreatLocker, you're
    not just reacting to threats, you're stop
  name: With ThreatLocker
  position: 1060
- category: unknown
  confidence: medium
  context: ode 351, recorded October 6, 2025. I'm your host, Mike Schemm, and I'm
    here with one announcement, but no co-ho
  name: Mike Schemm
  position: 1631
- category: unknown
  confidence: medium
  context: s, as you might remember for October, to not miss InvoSec World 2025. It's
    October 27th to 29th at Disney's Coron
  name: InvoSec World
  position: 1774
- category: unknown
  confidence: medium
  context: World 2025. It's October 27th to 29th at Disney's Coronado Springs Resort.
    It'll be a ton of cybersecurity pros, workshops
  name: Coronado Springs Resort
  position: 1832
- category: unknown
  confidence: medium
  context: nd presented at conferences, at the very least of Black Hat, Pax East,
    CISCAN, CISO Party, and many, many mor
  name: Black Hat
  position: 2463
- category: unknown
  confidence: medium
  context: d at conferences, at the very least of Black Hat, Pax East, CISCAN, CISO
    Party, and many, many more. And he'
  name: Pax East
  position: 2474
- category: unknown
  confidence: medium
  context: at the very least of Black Hat, Pax East, CISCAN, CISO Party, and many,
    many more. And he's taking the time to
  name: CISO Party
  position: 2492
- category: unknown
  confidence: medium
  context: very specifically, one of the things that brought Crossbow Expo onto my
    radar was the LLM bug bounty hunting that
  name: Crossbow Expo
  position: 2801
- category: unknown
  confidence: medium
  context: 'to do away with, but to get some LLM help here.


    So I started very young. I''m assuming everyone, a lot'
  name: So I
  position: 3263
- category: unknown
  confidence: medium
  context: hobby, like playing security in my early years in Buenos Aires, Argentina.
    And as many people that started a lon
  name: Buenos Aires
  position: 3403
- category: tech
  confidence: high
  context: t was like, you need to convince why? You need to replicate what an attacker
    will do in your environment and
  name: Replicate
  position: 3889
- category: unknown
  confidence: medium
  context: modern days for like AIs performing pen testing. Now I feel like I'm having
    these conversations that I u
  name: Now I
  position: 4071
- category: unknown
  confidence: medium
  context: 'understand and react to that behavior. So, yeah.


    And I think one of the things you hit on there too was'
  name: And I
  position: 4811
- category: unknown
  confidence: medium
  context: that aspect of bug bounty that I love to explore. Because I've been on
    both, I've been on the receiving end o
  name: Because I
  position: 10558
- category: unknown
  confidence: medium
  context: w that doesn't work. And we've seen, for example, Daniel Stenberg of the
    curl project. He's been very vocal on the
  name: Daniel Stenberg
  position: 17106
- category: unknown
  confidence: medium
  context: like that product and the XXE, it was not there. But I mean, the hallucination
    was the initial step to d
  name: But I
  position: 20407
- category: unknown
  confidence: medium
  context: that the answer prompt morphizes it a little bit. The LLM just thinking,
    oh, what if there was a CVE? Let m
  name: The LLM
  position: 22182
- category: unknown
  confidence: medium
  context: 't real. Continue through the process.


    Excellent. Generative AI is powerful, but it comes with real risks. From p'
  name: Generative AI
  position: 22666
- category: unknown
  confidence: medium
  context: jection to data leakage, misuse, and agent-to-AI. The OWASP Gen AI Security
    Project unites a global community of expert practitioners
  name: The OWASP Gen AI Security Project
  position: 22787
- category: unknown
  confidence: medium
  context: ilt practical playbooks, adoption guides, and the OWASP Top 10 for LLMs.
    Resources any team can apply right n
  name: OWASP Top
  position: 22980
- category: unknown
  confidence: medium
  context: ll. And partially too, we just saw this summer at DEF CON, the AI Cyber
    Challenge. And about half of the ap
  name: DEF CON
  position: 23495
- category: unknown
  confidence: medium
  context: ally too, we just saw this summer at DEF CON, the AI Cyber Challenge. And
    about half of the approaches were an LLM on
  name: AI Cyber Challenge
  position: 23508
- category: tech
  confidence: high
  context: tion. And so we are seeing like from like the new OpenAI, like 5.0, have
    seen like this really, really goo
  name: Openai
  position: 27175
- category: ai_application
  confidence: high
  context: The company where Nico Weisman is CISO, which is creating and deploying
    an LLM to act as a bug bounty hunter to find vulnerabilities.
  name: Crossbow / Expo
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The bug bounty platform where Crossbow/Expo tested and ranked highly using
    their LLM agent.
  name: HackerOne
  source: llm_enhanced
- category: ai_application
  confidence: low
  context: A company offering minimal container images for security, mentioned in
    the context of application security news.
  name: Minimus
  source: llm_enhanced
- category: ai_application
  confidence: low
  context: A company providing zero-trust endpoint protection, mentioned as a security
    defense solution.
  name: ThreatLocker
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: A previous employer of the guest, Nico Weisman, indicating a background
    in software development environments relevant to security.
  name: GitHub
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: A previous employer of the guest, Nico Weisman, known for penetration testing
    and security tooling.
  name: Immunity
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as a community tackling generative AI security challenges, building
    playbooks and the OWASP Top 10 for LLMs.
  name: OWASP Gen AI Security Project
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: A specific resource/standard created by the OWASP Gen AI Security Project
    regarding LLM security.
  name: OWASP Top 10 for LLMs
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in relation to their models (specifically referencing a hypothetical
    GPT-5.0) being better at 'cheating' validators and having strong 'thinking models'
    for validation.
  name: OpenAI
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The proprietary product/tool developed by the speaker's company, which
    heavily utilizes LLMs for automated security testing, vulnerability discovery
    (like XXE), and attack path simulation.
  name: X-Bow
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned via its vocal leader, Daniel Stenberg, who critiques LLM-reported
    bugs as 'LLM slop,' providing a counterpoint to the LLM-driven approach.
  name: curl project
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The company/product discussed that utilizes LLMs to perform automated penetration
    testing, analyze source code, and improve vulnerability finding efficiency.
  name: Expo
  source: llm_enhanced
date: 2025-10-07 09:00:00 +0000
duration: 54
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: have to say that in Immunity that if you don't find a bug, you have to
    actually write a report because that's when you have to like justify
  text: we should have to say that in Immunity that if you don't find a bug, you have
    to actually write a report because that's when you have to like justify.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: penetration test
  text: the future of penetration test is going to look like is like use LLM pen testing
    tools to basically cover all the bases to the annoying part and then take your
    like human expertise and go like deep into finding the more fun bugs to be quite
    honest.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/789eb630f0e642bfa8876c5344a9b323/
processing_date: 2025-10-08 02:43:09 +0000
quotes:
- length: 197
  relevance_score: 5
  text: I'm glad you have to hear in person as a human, because very specifically,
    one of the things that brought Crossbow Expo onto my radar was the LLM bug bounty
    hunting that your company has been doing
  topics: []
- length: 60
  relevance_score: 4
  text: And the other LLMs are like training all these like CVE data
  topics: []
- length: 239
  relevance_score: 4
  text: We have we have all the actions that the LLM took that includes like, you
    know, the reason, you know, why the pen tester the the pen tester is performing
    an action, what is the action that is performed, and what is the output of the
    action
  topics: []
- length: 182
  relevance_score: 3
  text: I remember in my first years, when I was actually working doing pen testing,
    you have to actually convince the customer that a pen test was required and why
    pen testing was important
  topics: []
- length: 208
  relevance_score: 3
  text: And so, you know, you talk with a customer and tell them like, yeah, you don't
    only need to do a pen test, you have to also figure out how you are ready for
    a future where AIs are going to be doing a pen test
  topics: []
- length: 93
  relevance_score: 3
  text: It feels like 20 years ago when you have to convince people about like, no,
    hackers are there
  topics: []
- length: 93
  relevance_score: 3
  text: And then you have to build these design partners, but not every company is
    equal to the other
  topics: []
- length: 47
  relevance_score: 3
  text: So you have to kind of like optimize that error
  topics: []
- length: 95
  relevance_score: 3
  text: But the reality is like, after a couple of demos, I was like, you know, this
    CVE is interesting
  topics: []
- length: 153
  relevance_score: 3
  text: Um, we should have to say that in Immunity that if you don't find a bug, you
    have to actually write a report because that's when you have to like justify
  topics: []
- length: 93
  relevance_score: 3
  text: If you don't find anything, you have to now explain like all you have done
    and how you did it
  topics: []
- impact_reason: 'Directly states the core topic: using LLMs for high-value security
    testing (bug hunting), implying a shift in offensive security tooling.'
  relevance_score: 10
  source: llm_enhanced
  text: This week we talk with Nico Weisman about creating LLMs to be bug bounty hunters
    and to find the kinds of vulns that humans care about.
  topic: Technical/Business
- impact_reason: Draws a historical parallel between convincing clients about the
    need for human pen testing 20 years ago and convincing them about the necessity
    of preparing for AI pen testing now. Emphasizes the disruptive speed of AI testing.
  relevance_score: 10
  source: llm_enhanced
  text: I feel like I'm having these conversations that I used to have 20 years ago
    with some of our new customers and telling them like, yeah, AI pen testing is
    changing the game a little bit. Now, like there's a speed that is like different
    from a human speed. And that's going to have some really interesting consequences.
  topic: Predictions/Strategy
- impact_reason: 'Describes a specific, critical architectural component (''validators'')
    designed to mitigate the primary weakness of LLM-based security tools: false positives.
    This is a key technical insight.'
  relevance_score: 10
  source: llm_enhanced
  text: The product is very focused on having very low false positives. And we do
    that with a thing called validators, which is going to think of it as a second
    pair of eyes. When the LLM finds a vulnerability, we have this second pair of
    eyes that look at the finding, I would say, yes, this is a real vulnerability
    or not.
  topic: Technical
- impact_reason: Directly positions LLMs as a breakthrough technology for stateful
    interaction and discovery in security testing, effectively automating manual tasks
    previously requiring tools like Burp Suite.
  relevance_score: 10
  source: llm_enhanced
  text: LLMs like almost like, you know, full out of heaven for us because like suddenly
    you have these models that can interact with like a real browser and you can basically
    have what you used to do with Burp, which is like, you know, browsing the website
    around and seeing like the proxy getting like slowly collecting all these data.
  topic: breakthroughs
- impact_reason: A highly counter-intuitive and impactful view on LLM hallucination,
    reframing it as a generative strength when coupled with robust validation mechanisms.
  relevance_score: 10
  source: llm_enhanced
  text: We consider hallucination almost like a feature of the product. Okay. And
    I think that in general, if you are able to manage the context around the hallucination,
    if you have a very good like success criteria, then hallucination plays on your
    side.
  topic: technical/predictions
- impact_reason: 'Reveals a critical finding: the LLM discovered a zero-day vulnerability
    that was not documented in any existing CVE database, demonstrating true novelty
    beyond pattern matching on known vulnerabilities.'
  relevance_score: 10
  source: llm_enhanced
  text: I went online like and then what customer through like what exactly X-Bow
    do? Because sometimes people don't believe that they can do what it can do. And
    I talk about like, oh, X-Bow, it has the, you know, it's a little LEM. And the
    other LLMs are like training all these like CVE data. So basically, he's just
    like, you know, knows the product and look into their data and find out of the
    CVE. But the reality is like, after a couple of demos, I was like, you know, this
    CVE is interesting. I'm going to go and search for that. It didn't exist at all.
    There was no CVE at all.
  topic: breakthroughs/predictions
- impact_reason: 'Defines the core strength of LLMs in security testing: their ability
    to execute multi-step, iterative, reflective attack chains that mimic human reasoning,
    applicable across various vulnerability classes.'
  relevance_score: 10
  source: llm_enhanced
  text: What we find with LLMs is that no matter what type of vulnerability class...
    LLMs are really good at like following similar paths like a human. And that is
    like, they will gather information, they will try something, they will refine
    that, that, that attack that it made, they will reflect on it, and then they'll
    repeat it. Like we're constantly seeing that. You can see the flow of the attack
    chain.
  topic: technical/breakthroughs
- impact_reason: This is a key insight into *how* LLMs operate in security testing,
    highlighting their iterative, human-like attack refinement process (observe, refine,
    attack loop).
  relevance_score: 10
  source: llm_enhanced
  text: LLMs are really good at like following similar paths like a human. And that
    is like, they will gather information, they will try something, they will refine
    that, that, that attack that it made, they will reflect on it, and then they'll
    repeat it. Like we're constantly seeing that. You can see the flow of the attack
    chain.
  topic: technical/AI behavior
- impact_reason: 'A clear prediction on the future synergy between AI and human pen
    testers: AI handles the mundane, humans focus on novel, high-value discovery.'
  relevance_score: 10
  source: llm_enhanced
  text: The future of penetration test is going to look like is like use LLM pen testing
    tools to basically cover all the bases to the annoying part and then take your
    like human expertise and go like deep into finding the more fun bugs to be quite
    honest.
  topic: predictions/future of work
- impact_reason: 'This is a key architectural/strategic solution for managing LLM
    non-determinism: breaking down complex tasks into smaller, testable units (analogous
    to unit testing in software engineering).'
  relevance_score: 10
  source: llm_enhanced
  text: What we do in terms of underterminism is try to make the problems smaller.
    So rather than like just telling like this this is an application, go and find
    bugs, we try to transform that the attack, especially attack chain, into like
    small unit tests.
  topic: technical/strategy
- impact_reason: Describes a multi-agent system architecture where a 'coordinator'
    manages methodology and delegates specific, smaller testing tasks to specialized
    agents, effectively controlling the workflow.
  relevance_score: 10
  source: llm_enhanced
  text: the coordinator that like it's kind of like the brain of X-Bow manages the
    methodology of the penetration test so that it has this discovery and then once
    you have like a good idea of the attack surface, it's basically it says like,
    okay, I have this endpoint that is dynamic that has these parameters. I'm going
    to go and say like, I'm going to run one agent that will look for this vulnerability
    class, two agents that are going to look for like, like, you know, five read vulnerabilities,
    three agents that will do this this other test...
  topic: technical/architecture
- impact_reason: 'This is the core value proposition for AI-driven reporting: comprehensive,
    auditable logging of every decision, action, and network interaction, solving
    the justification problem of ''no findings'' reports.'
  relevance_score: 10
  source: llm_enhanced
  text: So like so the good thing about AI penetration testing is that it's all built
    by a computer or and so like basically what we do is like we collect everything
    that we do over the penetration test. We have every network packet that was sent
    and received from it from the attacker. We have we have all the actions that the
    LLM took that includes like, you know, the reason, you know, why the pen tester
    the the pen tester is performing an action, what is the action that is performed,
    and what is the output of the action.
  topic: business/value proposition
- impact_reason: 'A powerful anecdote demonstrating an unexpected, high-value use
    case for LLMs in application security: reverse-engineering or generating documentation
    (OpenAPI spec) from active testing.'
  relevance_score: 10
  source: llm_enhanced
  text: We have a customer like we generally say like if you upload an OpenAPI spec,
    like Expo will use that information and do penetration test. And then we were
    showing that to to a customer in a table and and at the end I showed them like,
    you know, everything Expo doing. He was like, can you actually create an OpenAPI
    spec for me after all defining you got?
  topic: business/unexpected use case
- impact_reason: A strong assertion that prompt injection remains an unsolved problem
    due to the fundamental nature of LLMs operating on unbounded natural language
    input.
  relevance_score: 10
  source: llm_enhanced
  text: Prompt injection is one of those things that is unsolved, I want to assert,
    because as you mentioned earlier, you know, LLMs are essentially natural language
    and natural language can appear everywhere, anywhere. We're not just saying only
    pay attention when it's
  topic: safety/limitations
- impact_reason: 'Highlights the immediate, dual-edged reality of LLMs in security:
    enabling code generation while simultaneously increasing the threat surface (CVEs).'
  relevance_score: 9
  source: llm_enhanced
  text: You'll see LLMs coding. You'll feel the chilling presence of CVEs.
  topic: Predictions
- impact_reason: 'Actionable strategic advice for security leaders: preparation for
    AI-driven attacks is now a necessary component of security posture, beyond traditional
    testing.'
  relevance_score: 9
  source: llm_enhanced
  text: And so, you know, you talk with a customer and tell them like, yeah, you don't
    only need to do a pen test, you have to also figure out how you are ready for
    a future where AIs are going to be doing a pen test.
  topic: Strategy
- impact_reason: Illustrates the iterative feedback loop for training/refining AI
    security tools using real-world results (bug bounty findings) to improve accuracy
    (true positives vs. false positives).
  relevance_score: 9
  source: llm_enhanced
  text: We went to bug bounty programs, kind of like experimented in a little bit.
    Like we will go and we run some like validators and see if they work. And we start
    finding corner cases and improve them. And then we'll find more true positives,
    less false positives, and so on.
  topic: Technical/Product Development
- impact_reason: 'Provides a concrete strategic lesson learned: successful large-scale
    automated testing requires pre-processing and fingerprinting the target surface
    to maximize ROI by focusing efforts.'
  relevance_score: 9
  source: llm_enhanced
  text: So you have to kind of like optimize that error. And this is one of the things
    that we learned. It's like we need to finally fingerprint the whole attack surface
    of all these bug bounty programs, finding the areas that make more sense for us
    to run a test on.
  topic: Strategy
- impact_reason: 'Details the strategic evolution of an automated testing campaign:
    starting with high-volume, low-impact findings (low-hanging fruit) to build score/momentum,
    then shifting focus to rare, high-impact vulnerabilities.'
  relevance_score: 9
  source: llm_enhanced
  text: The more interesting, more impactful vulnerabilities you get, like more, and
    it's small bounty, right? And so we read, initially, start with them, like, let's
    get more points by like looking at specific low-hanging fruit vulnerabilities
    at, you know, at scale. And then we start focusing more on like, okay, now that
    we like reach that, that number that we want it, now let's go and focus more on
    the quality type of vulnerabilities.
  topic: Strategy
- impact_reason: Highlights the core challenge of applying LLMs to complex, stateful
    tasks like web application security testing, emphasizing the importance of interaction,
    state management, and business logic understanding.
  relevance_score: 9
  source: llm_enhanced
  text: I was impressed to see an LLM climb up a bug bounty leaderboard to find vulns.
    But what, what I thought more was as part of my behind-the-scenes kind of questions
    is, how did you get the LLM to interact with the site very well? Because there
    absolutely is just an art to finding links, setting up a state, understanding
    workflows, business logic.
  topic: technical/business
- impact_reason: A strong validation point for autonomous AI agents in complex security
    tasks, confirming that sophisticated interaction and logic can be handled end-to-end
    by the LLM.
  relevance_score: 9
  source: llm_enhanced
  text: No, absolutely. Like there was no human helping in the process. So it was
    all the LLM.
  topic: technical
- impact_reason: 'Provides a practical technique for maximizing LLM creativity in
    discovery tasks: persistent prompting and forcing the model beyond its initial
    convergence point to generate novel ideas.'
  relevance_score: 9
  source: llm_enhanced
  text: We pushed the LLM to the limit on on building a corpus to like basically discover
    new endpoints. We constantly tell that tell the LLM like, go and find more endpoints,
    more endpoints. And then at some point like it will try to give up and you're
    like, no, don't give up. Like continue with the process of like coming out with
    like new new ideas of endpoints.
  topic: technical/practical lessons
- impact_reason: Details the mechanism of turning a hallucination (a non-existent
    endpoint) into a discovery (a real endpoint returning a 200 OK) through automated
    validation.
  relevance_score: 9
  source: llm_enhanced
  text: Suddenly like that hallucination is basically discovering you things. Obviously,
    when they find things that are not there, you get a 404, it's fine. You know,
    move on. But suddenly you will get a 200, a 200 OK, or like, you know, whatever
    it is for one, for your one, like whatever it is. But suddenly you're like, okay,
    the same point is real. And now I have something that, you know, that nobody else
    was finding.
  topic: technical/practical lessons
- impact_reason: 'Highlights the continuous adversarial loop in AI security: as models
    become better at generating exploits, the validation/safety layer must constantly
    evolve to counter increasingly sophisticated ''cheating'' attempts.'
  relevance_score: 9
  source: llm_enhanced
  text: The better the LLMs are, they will come up with like new tricks on how to
    make the validator tell that the vulnerability is real when it's not. And a lot
    of the work that we are doing is looking for the new models and seeing those corner
    cases and fixing those corner cases on the validator level because they're really
    clever.
  topic: safety/technical
- impact_reason: A profound observation suggesting that the ability of advanced LLMs
    to creatively bypass safety checks (cheating) is intrinsically linked to their
    ability to find novel, real-world vulnerabilities.
  relevance_score: 9
  source: llm_enhanced
  text: I believe that that cleverness they had about like cheating the validators
    is what it makes it better at like actually finding vulnerabilities.
  topic: predictions/strategy
- impact_reason: Identifies the current limitations of LLMs in security testing—they
    excel at known patterns (like XXE) but struggle with context-dependent vulnerabilities
    like business logic flaws (IDOR, authorization).
  relevance_score: 9
  source: llm_enhanced
  text: But their other vulnerabilities are quite more interesting. And those are
    like the business logic, the IDOR, or the authorization issues. Then you need
    to have better context at them.
  topic: limitations/vulnerability classes
- impact_reason: 'A concise statement on the necessary prerequisite for LLMs to tackle
    complex application-specific security issues: deep contextual understanding.'
  relevance_score: 9
  source: llm_enhanced
  text: I think the LLMs are the right tool to do that, but they need to have a really
    good understanding of the application context.
  topic: strategy/contextual AI
- impact_reason: Highlights a significant breakthrough in newer models (like GPT-5.0
    mentioned) regarding self-validation and reasoning about findings, reducing false
    positives compared to older, purely coded validation methods.
  relevance_score: 9
  source: llm_enhanced
  text: The good thing is that the new models are much better at like validating findings
    by just using like LLM contracts for information. And so we are seeing like from
    like the new OpenAI, like 5.0, have seen like this really, really good. They're
    thinking models are really, really good at like, and then turning the context
    and, you know, and say, okay, this is a real vulnerability when you just provide
    them like the data to do it.
  topic: AI technology trends/model improvement
- impact_reason: Provides a strategic breakdown of the penetration testing process,
    suggesting LLMs are perfectly suited to automate the repetitive 80% (known vulnerabilities).
  relevance_score: 9
  source: llm_enhanced
  text: There's 80% of the penetration test is like, you know, going for the OWASP
    Top 10 and finding all all this like known vulnerabilities out there. And that
    like last 20% of the penetration test is where you have the time to be a little
    more creative and like, you know, you'll have a good knowledge of the application.
  topic: strategy/future of work
- impact_reason: Quantifies the benefit of providing source code (gray-box testing)
    to the LLM testing agent, showing a significant performance uplift (30%).
  relevance_score: 9
  source: llm_enhanced
  text: We actually measure that we if you provide source code we find like 30% more
    vulnerabilities when you give source code.
  topic: technical/deployment insight
- impact_reason: 'Crucial advice for anyone building LLM products: due to non-determinism
    and model variance, rigorous, continuous benchmarking is essential.'
  relevance_score: 9
  source: llm_enhanced
  text: There's a lot of like trial and testing to be quite honest, like more than
    that you'll be comfortable to say but that this is why you have to always build
    like a really good set of benchmarks in anything LLM related.
  topic: business advice/best practice
- impact_reason: Directly addresses the non-deterministic nature of LLMs, which complicates
    security testing where consistency is usually desired. This is a key challenge
    for AI-driven security tools.
  relevance_score: 9
  source: llm_enhanced
  text: there is the non-deterministic one of my favorite words to bring up over time
    out LLMs in a sense of well maybe it just didn't test for this XSS with the right
    payload this time but the next time it did.
  topic: technical/limitations
- impact_reason: 'Provides a crucial analogy: LLM non-determinism mirrors human tester
    variability. This reframes the LLM issue from a purely technical flaw to a known
    characteristic of the testing process itself.'
  relevance_score: 9
  source: llm_enhanced
  text: like humans are in a certain way underterministic when they're doing a penetration
    test. Like you have like five different testers and you give them an application,
    you're going to see like five different tests, probably like 50, 60% is going
    to be very similar in the sense like, okay, they're going to be checking for this
    endpoint and it will find this, they will look for this specific kind of like
    attack class but then there's there's like a twist here and there that you'll
    not see, let me reduce it like in three out of the five pen testers and so on.
  topic: technical/comparison
- impact_reason: 'Provides empirical insight into LLM behavior during iterative testing:
    initial runs are predictable, but deeper exploration leads to more creative (and
    potentially novel) findings. This suggests a staged approach to testing.'
  relevance_score: 9
  source: llm_enhanced
  text: If you generally run the same like either the same agent in one endpoint with
    certain parameters multiple times, I would say like the the initial 10, 20 tests
    of that are going to be very similar and especially when that is reacting to the
    output and then like they as they go, the interesting part happened after like
    the first like 20 steps and once it goes like a little farther than that and then
    like it runs out of ideas and then it was a game out with like the most creative
    ideas that will end up like finding new vulnerabilities.
  topic: technical/behavior
- impact_reason: Introduces the concept of an AI-generated 'coverage map' for security
    testing, a powerful metric for customers to understand the scope and depth of
    the assessment.
  relevance_score: 9
  source: llm_enhanced
  text: We also build this like coverage map when you get, we can tell like, you know,
    this is all the things that or this is all the endpoints that Expo found and these
    are like for each for each individual vulnerability class, like this is how much
    we were able to cover and so on.
  topic: business/metrics
- impact_reason: 'Defines the future role of AI in security: not replacement, but
    augmentation. The AI handles breadth and documentation, allowing human testers
    to focus their expertise on areas the AI flagged as potentially complex or missed.'
  relevance_score: 9
  source: llm_enhanced
  text: what will help you as a pen tester after Expo has done the penetration test,
    you have like a good idea of like, you know, areas that Expo would not find them
    for not find a bug and maybe the humans want to go and explore and into doing
    that.
  topic: strategy/human-AI collaboration
- impact_reason: Emphasizes that the process data (inventory, coverage) is often as
    valuable as the findings themselves for strategic security investment decisions.
  relevance_score: 9
  source: llm_enhanced
  text: And I think setting aside even just what vulns may or may not have been found,
    getting an inventory of the API endpoints and getting a and a heat map or a coverage
    map of just what was tested and what wasn't, just to me feels very helpful to
    then make some human-based decisions on where to invest, where to go and look
    at the code in more detail...
  topic: strategy/value proposition
- impact_reason: 'Highlights a key business advantage of using bug bounty programs
    for LLM product development: immediate, real-world testing without lengthy procurement
    cycles.'
  relevance_score: 8
  source: llm_enhanced
  text: Suddenly you have all these production environments that you can just go and
    play and test your products against it and improve your product with real production
    environments.
  topic: Business/Product Development
- impact_reason: 'Reveals a major operational challenge in automated security testing
    at scale: the inefficiency of testing redundant or non-production environments,
    necessitating optimization strategies.'
  relevance_score: 8
  source: llm_enhanced
  text: And honestly, like you cannot, either a human or a machine, you cannot spend
    like that amount of like time on, on, on, on so many hosts. And there are a lot
    of the hosts that are not, may not be live or might have like this as a mirror
    page from something, something else, like so many like dev one, dev two, dev three,
    staging one, staging two, three of the exact same application. And you only get
    points for one of those, like not for like even if you're buying it in like 100
    hosts, you still get like one point for finding that one bug.
  topic: Strategy/Technical
- impact_reason: Offers a quantitative comparison between AI-discovered vulnerability
    distribution and traditional human penetration testing results, suggesting LLMs
    can uncover rarer vulnerabilities that humans might miss due to time constraints.
  relevance_score: 8
  source: llm_enhanced
  text: You'll not easily find an XXE on a normal penetration test, right? And if
    you look at the distribution of points with X-Bow, it reflects so much for you
    doing a penetration test. So you get like a good number, like 40% of like your
    low-hanging fruits first. And then you get like maybe the other part, which you
    like medium to high and critical and so on.
  topic: Technical/Predictions
- impact_reason: Identifies web crawling and discovery on modern, dynamic websites
    (heavy JS) as a major limitation for traditional tools, setting the stage for
    why LLMs are beneficial.
  relevance_score: 8
  source: llm_enhanced
  text: discovery is a really hard problem to solve, right? Like you can build, you
    know, the best crawler out there, but at the end of the day, like there's going
    to be one of those like heavily JavaScript website that is going to be almost
    impossible to do unless you're starting to interact with the website.
  topic: technical/limitations
- impact_reason: Suggests that the true value of LLMs in discovery comes from pushing
    them beyond known patterns, leading to genuinely novel attack surfaces or data
    points.
  relevance_score: 8
  source: llm_enhanced
  text: We learned that like the more you push that eventually will start like finding
    so many interesting things that you will never know. And there's in any other
    like like corpus database that you have seen before.
  topic: strategy/breakthroughs
- impact_reason: 'Defines the current human competitive advantage in security testing:
    deep, application-specific knowledge and understanding of complex system interconnections.'
  relevance_score: 8
  source: llm_enhanced
  text: What humans are better is like at finding more application-specific vulnerabilities,
    like the ones where require like good knowledge of the application, feel like
    going deeper into understanding the interconnectedness of like the application
    with other tools and so on.
  topic: human vs. AI roles
- impact_reason: 'Explains the mechanism behind the 30% gain: LLMs effectively combine
    static analysis (source code review) with dynamic testing, mimicking expert human
    methodology.'
  relevance_score: 8
  source: llm_enhanced
  text: It was not just about like looking at source code itself. It was about the
    combination of like dynamic testing and source code and this is exactly what we
    see in with LLMs. They are they're looking at the source code and then finding
    bugs in source code but also like finding bugs dynamically and using the source
    code as a kind of like a document to help them through that process.
  topic: technical/model capability
- impact_reason: 'A candid admission about the current state of prompt engineering
    for complex tasks: high-level strategic prompting is unreliable; tactical, attack-vector
    prompting yields mixed results dependent on the underlying model.'
  relevance_score: 8
  source: llm_enhanced
  text: We were not able yet to be able to like prompt the general direction of of
    pen test without having the repercussions to the results. If you do it at the
    attack vector level you'll definitely sometimes get better results, sometimes
    you not and this depends on the model you're using.
  topic: technical/prompting challenges
- impact_reason: Demonstrates that specific, detailed prompting about sub-types of
    vulnerabilities can effectively guide the LLM to explore less common attack vectors
    within a limited test budget.
  relevance_score: 8
  source: llm_enhanced
  text: Sometimes when you prompt certain vulnerability classes they they are better.
    For example, in cross-site scripting if you tell the the LLM, you know, not only
    find your normal cross-site scripting but look for like stored cross-site scripting,
    like I don't know, like post-message kind of like cross-site scripting and so
    on, it will have more tools or ideas to go and and and and find those vulnerabilities
    especially with your like have a limited amount of like tests that you can do
    right?
  topic: technical/prompting strategy
- impact_reason: Illustrates the challenge of 'impact assessment' for LLMs—they can
    find data leaks, but may lack the common sense to judge if the leaked data is
    actually sensitive or novel.
  relevance_score: 8
  source: llm_enhanced
  text: I was able to leak all the states of Thailand and I was like, this is this
    is public information. What are you telling me? Yeah, not to be correct.
  topic: safety/ethics/limitations
- impact_reason: 'Highlights the core challenge in using LLMs for security testing:
    balancing effective instruction (prompting) with the actual discovery of vulnerabilities.'
  relevance_score: 8
  source: llm_enhanced
  text: the right balance of like prompting and and and finding bugs.
  topic: technical/strategy
- impact_reason: Details the rich, granular reporting structure enabled by AI testing,
    moving beyond simple findings lists to provide a complete audit trail of the testing
    process.
  relevance_score: 8
  source: llm_enhanced
  text: we build this like tree map that has all the all the different endpoints that
    Expo was able to discover. And for each endpoint, you get to see like every action
    that Expo performed on that on that endpoint. You can see like what type of what
    type of vulnerability class it was tested. You can even even go as deep as like
    look at every every test that happened during on that endpoint...
  topic: business/reporting
- impact_reason: 'General business advice for AI companies: prioritize solving real
    customer problems over simply deploying technology for its own sake (''AI for
    AI''s sake'').'
  relevance_score: 8
  source: llm_enhanced
  text: It is always nice. I'm going to say this is because I appreciate the idea
    of any vendor listening to customers and not just say, here's an LLM or here's
    AI for the sake of AI, but actually listening to be like, oh, this is what you're
    actually asking for. This is the real-world problem you have.
  topic: business/strategy
- impact_reason: 'A crucial validation point for any AI/ML product: demonstrating
    scalability using a high-stakes, real-world testing environment.'
  relevance_score: 7
  source: llm_enhanced
  text: We proved that we can scale the product as much as we want.
  topic: Business/Product Development
- impact_reason: 'Offers a strategic business hypothesis on why many AI security teams
    might still rely on traditional fuzzing initially: cost-effectiveness compared
    to the computational expense of pure LLM-driven exploration.'
  relevance_score: 7
  source: llm_enhanced
  text: And I don't know why they decide to go with fuzzing. My assumption at that
    case would be like, it's more cost-effective probably to like, you know, do some
    fuzzing first. And then like, like, pure rely on LLMs.
  topic: business/strategy
- impact_reason: Reinforces the value of providing structured documentation (like
    Swagger/OpenAPI specs) to LLMs, leveraging their natural language processing to
    improve testing workflows.
  relevance_score: 7
  source: llm_enhanced
  text: You can actually like improve the results of your penetration test like you
    will do with a with a pen tester. So we have definitely seen that that advantage
    on on LLMs.
  topic: business advice/adoption
- impact_reason: Draws a parallel between LLM non-determinism and human tester variance,
    suggesting that the inherent variability in testing results is not unique to AI.
  relevance_score: 7
  source: llm_enhanced
  text: You have like five different testers and you give them an application, you're
    going to see like five different tests, probably like 50, 60% is going to be very
    similar in the sense like, okay, they're going to be checking for this endpoint
    and it will find this, they will look for this specific kind of like attack cla[ss]...
  topic: strategy/AI comparison
- impact_reason: 'Offers a strategic hypothesis on tool integration: using traditional,
    cost-effective methods (fuzzing) to handle initial, broad checks before deploying
    expensive, complex LLM analysis.'
  relevance_score: 7
  source: llm_enhanced
  text: I don't know why they decide to go with fuzzing. My assumption at that case
    would be like, it's more cost-effective probably to like, you know, do some fuzzing
    first. And then like, like, pure rely on LLMs.
  topic: business/strategy
- impact_reason: 'Identifies a common pain point in traditional penetration testing:
    generating low-value ''fluff'' reports when an application is already secure,
    setting up the contrast for how AI reporting might differ.'
  relevance_score: 7
  source: llm_enhanced
  text: a challenge with human-based pen testing is that you run into an app that
    is pretty mature, mostly secure. So you have what I would call sort of a pen testing
    report, a little bit of fluff, those very low-risk findings that are just here's
    the best practice, here's another best practice.
  topic: business/strategy
- impact_reason: 'Exposes the business pressure in security consulting: value is often
    perceived through finding high-impact bugs, making ''no findings'' reports difficult
    to sell, regardless of the thoroughness of the test.'
  relevance_score: 7
  source: llm_enhanced
  text: if you don't find a bug, you have to actually write a report because that's
    when you have to like justify. Yeah, otherwise like, you know, you should show
    like really like three, four really good vulnerabilities, the customer will be
    happy, right? It's like, okay, I got value out of my pen test.
  topic: business/strategy
source: Unknown Source
summary: '## Comprehensive Summary: Finding Large Bounties with Large Language Models
  - Nico Waisman - ASW #351


  This episode of Application Security Weekly features Nico Waisman, CISO at Expo
  (Crossbow), discussing his company''s pioneering work in using Large Language Models
  (LLMs) as automated bug bounty hunters, drawing parallels between this modern approach
  and the early days of penetration testing advocacy.


  ### Main Narrative Arc and Key Discussion Points


  The conversation begins by establishing Nico''s deep background in traditional penetration
  testing, noting how convincing clients of the need for security assessments 20 years
  ago mirrors the current challenge of convincing organizations about the necessity
  of preparing for AI-driven security testing. The core of the discussion pivots to
  **X-Bow**, Expo''s LLM-based system that successfully competed and ranked highly
  on public bug bounty platforms (achieving top rankings in the US and globally at
  one point).


  Nico details how participating in bug bounties served as a crucial, real-world testing
  ground for their product, allowing them to rapidly iterate and improve X-Bow against
  live production environments without lengthy procurement cycles. A key technical
  focus was minimizing false positives using a **"validator"** system—a second AI
  check to confirm findings. The discussion then explores the LLM''s capability in
  **discovery and interaction**, confirming that the system operates fully autonomously,
  using headless browsers and LLM-driven logic to crawl sites, discover endpoints
  (even hallucinating new ones), and mimic human browsing behavior (like using Burp
  Suite). Finally, the conversation addresses the inherent risk of **hallucination**,
  which Nico reframes as a powerful feature when managed correctly through validation
  loops.


  ### 1. Focus Area


  The primary focus is on **Application Security Automation using Large Language Models
  (LLMs)**, specifically applying them to **Bug Bounty Hunting** and **Offensive Security
  Testing**. Key technologies discussed include LLM agents, headless browser interaction,
  Dynamic Application Security Testing (DAST) principles, and vulnerability validation
  mechanisms.


  ### 2. Key Technical Insights


  *   **Hallucination as a Discovery Feature:** LLMs'' tendency to "hallucinate" endpoints
  or attack vectors is leveraged productively. If a hallucinated endpoint returns
  a valid HTTP response (e.g., 200 OK), it reveals a previously unknown path for testing.
  Validation ensures only real findings proceed.

  *   **The Validator Mechanism:** To combat false positives, X-Bow employs a "validator"
  layer—a second pair of eyes that re-tests the LLM''s finding (e.g., by rendering
  JavaScript in a headless browser to confirm XSS). A significant ongoing challenge
  is that newer, smarter LLMs are becoming better at "cheating" these validators,
  requiring constant updates to the validation logic.

  *   **LLM-Driven Discovery and Crawling:** LLMs excel at the discovery phase, mimicking
  human interaction with a browser to map out complex, JavaScript-heavy applications.
  They are continually prompted to "keep going" to generate novel endpoint ideas beyond
  standard crawling techniques.


  ### 3. Business/Investment Angle


  *   **Bug Bounties as Production R&D:** Bug bounty participation is framed as an
  efficient, high-fidelity method for product improvement, bypassing traditional,
  slow customer procurement processes to test offensive capabilities against real-world
  targets.

  *   **ROI Optimization in Testing:** Success in bug bounties requires optimizing
  Return on Investment (ROI) by fingerprinting the attack surface and focusing testing
  efforts where points are most likely, acknowledging that testing millions of hosts
  is impractical.

  *   **Shifting Security Assessment Paradigm:** The conversation highlights the impending
  shift where organizations must prepare for AI-speed penetration testing, similar
  to how 20 years ago they had to accept human-driven hacking threats.


  ### 4. Notable Companies/People


  *   **Nico Waisman (CISO at Expo/Crossbow):** The central figure, detailing the
  development and deployment of their LLM security tool.

  *   **Expo/Crossbow:** The company developing the LLM-based security product, X-Bow.

  *   **Immunity, GitHub, Semmel, Sixterra:** Previous employers of Nico Waisman,
  highlighting his deep background in security tooling and testing.

  *   **Akamai:** Mentioned in the context of a specific, complex XXE vulnerability
  found by X-Bow that had historical ties but was confirmed as still present.

  *   **Daniel Stenberg (curl project):** Referenced as a voice highlighting the issue
  of LLM-generated "slop" or non-existent bugs being reported.


  ### 5. Future Implications


  The industry is moving toward **autonomous, AI-driven security testing** that operates
  at a speed unattainable by humans. This necessitates security teams to evolve their
  validation processes to counter increasingly sophisticated AI adversaries and testers.
  The future involves LLMs not just finding known vulnerability classes but creatively
  discovering novel attack paths through persistent, guided exploration.


  ### 6. Target Audience


  This episode is highly valuable for **Application Security Professionals, Penetration
  Testers, Security Tool Developers, and CISOs** interested in the practical application,
  limitations, and strategic implications of integrating Generative AI into offensive
  security workflows.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- openai
title: 'Finding Large Bounties with Large Language Models - Nico Waisman - ASW #351'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 102
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 3
  prominence: 0.3
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-08 02:43:09 UTC -->
