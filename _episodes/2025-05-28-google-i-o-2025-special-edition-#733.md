---
companies:
- category: tech
  confidence: high
  context: of the main things that makes what we're doing at Google with Gemini different
    from what a lot of the othe
  name: Google
  position: 54
- category: unknown
  confidence: medium
  context: 'together, and 2.5 Pro.


    Hey, what''s up everyone? Today I''m super excited to share a special crossover
    edit'
  name: Today I
  position: 694
- category: unknown
  confidence: medium
  context: ossover edition of the podcast recorded live from Google I/O 2025. In this
    episode, I joined Swix from the L
  name: Google I
  position: 789
- category: unknown
  confidence: medium
  context: I/O 2025. In this episode, I joined Swix from the Latent Space podcast
    to interview Logan Kilpatrick and Shrasta
  name: Latent Space
  position: 846
- category: unknown
  confidence: medium
  context: d Swix from the Latent Space podcast to interview Logan Kilpatrick and
    Shrasta Basu Malik, PMs at Google DeepMind wo
  name: Logan Kilpatrick
  position: 880
- category: unknown
  confidence: medium
  context: t Space podcast to interview Logan Kilpatrick and Shrasta Basu Malik, PMs
    at Google DeepMind working on AI Studio and
  name: Shrasta Basu Malik
  position: 901
- category: unknown
  confidence: medium
  context: w Logan Kilpatrick and Shrasta Basu Malik, PMs at Google DeepMind working
    on AI Studio and the Gemini API, and Quin
  name: Google DeepMind
  position: 928
- category: unknown
  confidence: medium
  context: sta Basu Malik, PMs at Google DeepMind working on AI Studio and the Gemini
    API, and Quinn LaKramer, CEO at Da
  name: AI Studio
  position: 955
- category: unknown
  confidence: medium
  context: s at Google DeepMind working on AI Studio and the Gemini API, and Quinn
    LaKramer, CEO at Daily, who tags in fo
  name: Gemini API
  position: 973
- category: unknown
  confidence: medium
  context: Mind working on AI Studio and the Gemini API, and Quinn LaKramer, CEO at
    Daily, who tags in for Logan midway. It's
  name: Quinn LaKramer
  position: 989
- category: unknown
  confidence: medium
  context: 'from the event and more. Check it out.


    Hey, I''m Sarah Trankton. Welcome to another episode of the Toronto AI pod'
  name: Sarah Trankton
  position: 1158
- category: unknown
  confidence: medium
  context: Sarah Trankton. Welcome to another episode of the Toronto AI podcast, and
    I'm Swix. This is a special episode
  name: Toronto AI
  position: 1208
- category: unknown
  confidence: medium
  context: t, and I'm Swix. This is a special episode of the Latent Space Pod with
    Twi Mo at Google I/O. Welcome. Thanks for be
  name: Latent Space Pod
  position: 1275
- category: unknown
  confidence: medium
  context: is a special episode of the Latent Space Pod with Twi Mo at Google I/O.
    Welcome. Thanks for being here. Th
  name: Twi Mo
  position: 1297
- category: unknown
  confidence: medium
  context: ore accessible with the rest of the Google suite. And Shrasta, you've been
    actually... I don't know you well. I
  name: And Shrasta
  position: 1698
- category: unknown
  confidence: medium
  context: . That's hard work, so thank you for that, Logan. But I think everyone
    knows who really runs the show. Th
  name: But I
  position: 2092
- category: unknown
  confidence: medium
  context: nother one, I'd say we released a new tool called URL Context, and the
    idea is that you can use it by yourself
  name: URL Context
  position: 4599
- category: unknown
  confidence: medium
  context: respectful of our publisher ecosystem, of course. And I think this will
    unlock new use cases, like if peo
  name: And I
  position: 4802
- category: unknown
  confidence: medium
  context: 'those conversations.


    Yeah, my underrated pick is Gemini Diffusion. Yes, yeah, yeah, yeah, yeah. I''m
    getting... So,'
  name: Gemini Diffusion
  position: 7605
- category: unknown
  confidence: medium
  context: ults of a diffusion language model generative UI. Generative UI is the
    way generative UIs happen is through this
  name: Generative UI
  position: 7777
- category: tech
  confidence: high
  context: ed on what a user does." You have no pre-compiled notion of what your website
    is, and as a user goes throu
  name: Notion
  position: 7987
- category: unknown
  confidence: medium
  context: I was referring to the famous demo at Next where Shopee Pie showed how
    to set up a DNS using Cloudflare, righ
  name: Shopee Pie
  position: 12278
- category: unknown
  confidence: medium
  context: whose shoes are boss's boss, the CTO of DeepMind. And Coro had a really
    interesting take, which is just arou
  name: And Coro
  position: 13479
- category: unknown
  confidence: medium
  context: 'capabilities together. So, I think that is like a North Star for Gemini
    models.


    Makes a ton of sense. I agree'
  name: North Star
  position: 15190
- category: unknown
  confidence: medium
  context: . It's great to actually run the voice meet-up in San Francisco. You are
    basically consistently the leading commu
  name: San Francisco
  position: 16355
- category: unknown
  confidence: medium
  context: ive into a little bit on the cascade of models in Gemini Live. I mean,
    I think Shrasta is taking a really inter
  name: Gemini Live
  position: 17200
- category: unknown
  confidence: medium
  context: ve audio in, but then it's a separate text model. The NotebookLM models
    audio out. What was the driver for that or
  name: The NotebookLM
  position: 17566
- category: unknown
  confidence: medium
  context: hit a certain quality bar, a certain latency bar. And NotebookLM was already
    out, and the TTS models that were pow
  name: And NotebookLM
  position: 17724
- category: unknown
  confidence: medium
  context: ', exactly, at pretty demanding real-time latency. As Shrasta is saying,
    human beings expect you to do this. An'
  name: As Shrasta
  position: 21550
- category: unknown
  confidence: medium
  context: mean by Gemini 5? What do you want in Gemini 5.0? Then I'll let Quinn go.
    I'll just put on my hat as repre
  name: Then I
  position: 24460
- category: big_tech
  confidence: high
  context: The host organization, heavily featured for developing and releasing Gemini,
    AI Studio, and various related APIs and models.
  name: Google
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: The specific research division within Google working on Gemini, AI Studio,
    and the Gemini API.
  name: Google DeepMind
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A startup/company whose CEO (Quinn LaKramer) was interviewed. They are
    partners with Google on the live API and focus on voice/audio applications.
  name: Daily
  source: llm_enhanced
- category: media/community
  confidence: high
  context: The podcast organization/show that is co-producing this crossover episode.
  name: Latent Space podcast
  source: llm_enhanced
- category: media/community
  confidence: high
  context: The primary podcast hosting this episode.
  name: Toronto AI podcast
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: An open-source framework for voice orchestration, created by Quinn LaKramer
    (CEO of Daily), which supports Gemini models.
  name: PipeCat
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned in reference to a famous demo at Next showing how to set up DNS
    using Cloudflare, implying they are a user or developer showcasing complex workflows.
  name: Shopee Pie
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned in relation to a demo (by Shopee Pie) where DNS setup was shown,
    suggesting they are involved in infrastructure or services that AI agents interact
    with.
  name: Cloudflare
  source: llm_enhanced
- category: big_tech
  confidence: low
  context: Implied when discussing 'other labs' or model providers in the general
    AI landscape, though not explicitly named as a competitor in the context of Gemini's
    singular model approach.
  name: Microsoft
  source: llm_enhanced
- category: ai_company
  confidence: low
  context: Implied when discussing 'other labs' or model providers, though not explicitly
    named as a competitor in the context of Gemini's singular model approach.
  name: OpenAI
  source: llm_enhanced
- category: ai_company
  confidence: low
  context: Implied when discussing 'other labs' or model providers, though not explicitly
    named as a competitor in the context of Gemini's singular model approach.
  name: Anthropic
  source: llm_enhanced
- category: ai_company
  confidence: low
  context: Implied when discussing the broader ecosystem of model providers, though
    not explicitly named.
  name: Hugging Face
  source: llm_enhanced
- category: ai_infrastructure
  confidence: low
  context: Implied when discussing infrastructure problems related to caching and
    performance, though not explicitly named.
  name: NVIDIA
  source: llm_enhanced
- category: ai_infrastructure
  confidence: low
  context: Implied when discussing the broader ecosystem of infrastructure providers,
    though not explicitly named.
  name: Databricks
  source: llm_enhanced
- category: big_tech
  confidence: low
  context: Implied when discussing 'other labs' or model providers, though not explicitly
    named.
  name: Meta AI
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as the central model family being discussed, implying the developer
    is Google/DeepMind.
  name: Gemini models
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a model for image generation, distinct from the main Gemini
    model.
  name: Imagen models
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as a model for image generation and editing, often used by developers.
  name: Imagine
  source: llm_enhanced
- category: ai_developer/team
  confidence: medium
  context: Mentioned alongside Quinn, likely a key figure or company/team involved
    in the voice meet-up and the development of the live API architecture.
  name: Shrasta
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned as models whose TTS capabilities were initially used for audio
    out in the live API architecture.
  name: NotebookLM models
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned as a partner (along with Shrasta) in developing the open-source
    framework for production voice systems.
  name: DeepMind
  source: llm_enhanced
- category: person/host
  confidence: high
  context: The host or interviewer, who is asking questions about the architecture
    and features.
  name: Sam
  source: llm_enhanced
- category: person/guest
  confidence: high
  context: A guest on the podcast, founder of PipeCat, and involved in community building
    for voice/audio.
  name: Quinn
  source: llm_enhanced
date: 2025-05-28 20:59:00 +0000
duration: 26
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: do a deep dive episode
  text: we should do a deep dive episode.
  type: recommendation
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/traffic.megaphone.fm/MLN4033526197.mp3?updated=1748466377
processing_date: 2025-10-05 14:11:38 +0000
quotes:
- length: 213
  relevance_score: 5
  text: Yeah, transcription, actually, even before we released native audio, now of
    course you get text and audio interleaved in the output, but transcription used
    to be one of the biggest use cases we had on the live API
  topics: []
- impact_reason: Implicit caching is a major infrastructure win for developers, directly
    translating to cost savings without requiring manual configuration. This addresses
    a key pain point in high-volume API usage.
  relevance_score: 10
  source: llm_enhanced
  text: implicit context caching, which I know a lot of folks are waiting for. We
    made implicit caching happen. I think there was lots of feedback that people are
    looking for, explicit caching is nice, there are definitely use cases where it
    makes sense, but people want implicit caching, so I'm happy passing the cost saving
    on to developers. You don't have to do anything, it just works right now, and
    you're saving money.
  topic: technical/business
- impact_reason: 'This is a critical business/ecosystem insight: live/multimodal APIs
    currently suffer from high vendor lock-in due to bespoke infrastructure, making
    the commitment decision much riskier for developers compared to standard text
    APIs.'
  relevance_score: 10
  source: llm_enhanced
  text: I think the level of commitment you need to make to the model provider in
    the world of the live API. I do think for developers, it's a higher bar. ... There's
    it's not easily interoperable between different model providers. Everyone's infrastructure
    is all bespoke and different, so it is a different level of commitment that you
    need to have to really bet your company or your business or your product on the
    live API.
  topic: business/strategy
- impact_reason: This is a core strategic insight into Google's unified model philosophy
    (Gemini) versus splintered capabilities, emphasizing the necessary but temporary
    nature of research forks.
  relevance_score: 10
  source: llm_enhanced
  text: 'We''re here to make one model, and that model is Gemini. And I think you
    do need to trust this point: to make the capabilities work, in some cases, you
    do need to have these forks that go off to make that capability, harden it, and
    then find a way to bring it back into the mainline model.'
  topic: strategy
- impact_reason: Provides a concrete example of emergent capabilities arising from
    merging distinct model strengths (reasoning + multimodality), which is a major
    theme in modern LLM development.
  relevance_score: 10
  source: llm_enhanced
  text: But the really exciting thing is what happens when you bring the capability
    together. And 2.5 Pro with reasoning is a great example of this for like multimodal
    with video understanding ended up having this huge... It's having this beautiful
    moment. The model is sort of out of the box because of all the reasoning capabilities
    that were baked in.
  topic: technical
- impact_reason: Quantifies the extreme engineering challenge of achieving human-competitive
    latency (sub-second) for real-time voice agents, emphasizing infrastructure difficulty
    over model capability alone.
  relevance_score: 10
  source: llm_enhanced
  text: I think the larger point that you're touching on, Sam, that I do want to mention,
    is it is really, really hard to bring all these components together and get latency
    down to where it needs to be, in the 500 to 700 millisecond range. Like, it's
    one of the hardest things we've had to do with the live API.
  topic: technical
- impact_reason: Introduces 'proactive audio' or 'semantic VAD,' a sophisticated feature
    addressing context switching and background noise interruption, moving beyond
    simple silence detection.
  relevance_score: 10
  source: llm_enhanced
  text: It's trained not to respond to irrelevant audio. It's like a refusal kind
    of... Yeah, or you could call it directionally like semantic voice activity detection,
    right? So, basically, yeah, let's say I'm talking to the AI, and then Quinn comes
    and asks me a question, and I respond to Quinn. It knows when not to respond.
  topic: technical
- impact_reason: Reveals an emergent, powerful capability (speaker identification/diarization)
    within the new models that is exceeding official documentation, signaling a major
    leap in multimodal context awareness.
  relevance_score: 10
  source: llm_enhanced
  text: 'The other is part of the magic there is this semi-separate feature, but I
    think they''re multiplicative: of now your models can actually recognize two different
    people just based on their voices. You and I reply to that. Yeah, this is not
    officially supported yet. The world just does it, but just try it, right?'
  topic: technical
- impact_reason: This is a strong strategic statement emphasizing Google's commitment
    to a unified, singular flagship model (Gemini) rather than fragmented capabilities,
    which is a key differentiator in the competitive LLM landscape.
  relevance_score: 9
  source: llm_enhanced
  text: One of the main things that makes what we're doing at Google with Gemini different
    from what a lot of the other labs are doing is we're here to make one model, and
    that model is Gemini.
  topic: strategy
- impact_reason: This is a significant developer control feature. Offering the ability
    to disable the 'thinking' (reasoning/chain-of-thought) process allows developers
    to optimize for speed and cost when reasoning isn't required, directly impacting
    deployment economics.
  relevance_score: 9
  source: llm_enhanced
  text: thinking budgets coming to 2.5 Pro. So, and you can also disable thinking
    as well, so if you just want 2.5 Pro as a raw, non-reasoning model, we'll have
    that hopefully in early June.
  topic: technical/business
- impact_reason: 'A bold prediction about the future of UI development: models like
    Gemini Diffusion could enable on-the-fly UI generation, but current token generation
    speed is the primary bottleneck.'
  relevance_score: 9
  source: llm_enhanced
  text: 'I do think that''s going to be the killer use case: this generative UI experience
    that doesn''t exist today because the model just takes too long to generate tokens.'
  topic: predictions
- impact_reason: Highlights the necessity of dynamic system instruction modification
    for complex, multi-step agentic workflows (like gaming agents or customer support),
    which is a key challenge in current agent development.
  relevance_score: 9
  source: llm_enhanced
  text: one of the things is being able to change the system instructions at every
    step of your workflow. So, onboarding some of the more complex use cases with
    the live API has been a work in progress as we release more features.
  topic: technical/product
- impact_reason: Acknowledges the tension between the future trend of unified audio
    models and the current developer preference for modular, componentized pipelines,
    indicating a transitional phase in audio AI adoption.
  relevance_score: 9
  source: llm_enhanced
  text: I do think perhaps eventually for most use cases, as these audio architecture
    models get better, a lot of use cases will probably transition to that. But when
    we talk to our developers, they still very much like those componentized components.
  topic: predictions/technical
- impact_reason: Shows the necessary evolution from centralized, black-box infrastructure
    components (like VAD) to providing developer controls for customization in real-time
    applications.
  relevance_score: 9
  source: llm_enhanced
  text: The first thing that comes to mind is, of course, the voice activity detection
    models that we have. And we've done a lot of work like penning that model server-side,
    but we've also learned that we need to provide some knobs to developers.
  topic: technical
- impact_reason: 'A crucial distinction for developers: real-time voice introduces
    unique infrastructure and latency challenges distinct from standard text-based
    LLM interactions.'
  relevance_score: 9
  source: llm_enhanced
  text: What we see is that the shape of building these real-time voice agents is
    a different set of developer problems than the shape of non-real-time or text-mode
    things.
  topic: strategy
- impact_reason: 'Describes the symbiotic relationship between open-source frameworks
    (like PipeCat) and core APIs: successful framework features get productized into
    the API, while new use cases drive framework evolution.'
  relevance_score: 9
  source: llm_enhanced
  text: As the models get better, as the use cases get more clear, some of those features
    migrate from the framework, so we work into the APIs, which makes life easier
    for developers. The use cases at the same time continue to broaden out, and so
    there are more things for the framework to do.
  topic: business
- impact_reason: Highlights the critical, often overlooked, networking infrastructure
    layer (packet routing, WebRTC/WebSockets) required to support modern, low-latency
    media AI.
  relevance_score: 9
  source: llm_enhanced
  text: What we saw from my perspective when we started to see the possibilities of
    voice AI was you need this packet routing down underneath the inference layer.
    There's the AI inference stuff, but then there's just how do you move the audio
    and increasingly video around the internet?
  topic: technical
- impact_reason: 'A significant technical improvement for agentic workflows: asynchronous
    function calling unlocks parallel processing, drastically improving perceived
    responsiveness and efficiency for complex tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: One thing that we launched on the cascaded architecture that we hope to eventually
    bring to the native audio as well is asynchronous function calling. So, earlier,
    the way it used to work is if you wanted the model to do a function call, you
    had to wait for the response. And now you can set a non-blocking parameter, and
    the model can go off and execute the function in the background.
  topic: technical
- impact_reason: 'This is the key actionable wish: a demand for expanded multilingual
    support in foundational models, crucial for global AI adoption and development.'
  relevance_score: 9
  source: llm_enhanced
  text: more and more languages, because AI is going on
  topic: technical
- impact_reason: 'This reveals the internal engineering reality of developing a unified
    model: specialized forks are necessary for capability development before reintegration,
    highlighting the complexity of maintaining a single, powerful model.'
  relevance_score: 8
  source: llm_enhanced
  text: To make the capabilities work, in some cases, you do need to have these forks
    that go off to make that capability, harden it, and then find a way to bring it
    back into the mainline model.
  topic: technical
- impact_reason: Highlights the increasing importance of high-quality, multilingual
    native audio generation as a key feature differentiating modern multimodal models.
  relevance_score: 8
  source: llm_enhanced
  text: native audio output is... a personal highlight. I actually... Quinn and I
    have been playing with it together for a bit as well. I think especially with
    all the voices sounding great, the fact that it can switch in and out of languages
    is amazing.
  topic: predictions/technical
- impact_reason: Details a specific RAG-like capability (URL Context) and explicitly
    mentions the critical consideration of respecting the publisher ecosystem, touching
    on ethical/business constraints in web data usage.
  relevance_score: 8
  source: llm_enhanced
  text: we released a new tool called URL Context, and the idea is that you can use
    it by yourself or pair it with search to retrieve more in-depth information from
    web pages in a way that's respectful of our publisher ecosystem, of course.
  topic: business/safety
- impact_reason: Highlights Gemini Diffusion as a potentially transformative technology,
    linking diffusion models with language modeling to enable novel generative interfaces.
  relevance_score: 8
  source: llm_enhanced
  text: My underrated pick is Gemini Diffusion. Yes, yeah, yeah, yeah, yeah. I'm getting...
    So, like a preference speed, I wonder what the potential results of a diffusion
    language model generative UI.
  topic: technical/predictions
- impact_reason: 'Identifies a crucial limitation in early multimodal/live APIs: constraints
    on session length for continuous audio/video processing, which is vital for real-world
    applications like long customer service calls.'
  relevance_score: 8
  source: llm_enhanced
  text: What are you seeing as the challenges for folks getting started with live?
    ... one of the areas where we've been getting a lot of feedback is in session
    length. Anybody who's been trying to put this in production, when we started,
    you could do like 50 to 20 minutes of audio, I'm sorry, and about five minutes
    of video.
  topic: technical/product
- impact_reason: 'Poses the fundamental architectural question for voice applications:
    monolithic end-to-end audio models vs. modular componentized pipelines (STT ->
    LLM -> TTS).'
  relevance_score: 8
  source: llm_enhanced
  text: When you're thinking about building voice-based applications, is speech-to-text
    and then processing with a standard LLM a precursor to the live API, or are these
    two distinct paths that are still viable, and you still see being viable going
    forward?
  topic: technical
- impact_reason: Illustrates the ongoing process of integrating specialized model
    capabilities (like diffusion-based image generation) into the unified multimodal
    foundation model (Gemini).
  relevance_score: 8
  source: llm_enhanced
  text: But then slowly but surely, we're bringing those capabilities into Gemini.
    Whoever's watching this, we had a mid-I/O switch because obviously there's a lot
    going on here, not a shape-shifting. I know, I know. But we also have Quinn, actually,
    who made this podcast happen. There are founders of Daily. Welcome. I'm a big
    fan of all things voice and audio, so things voice.
  topic: technical
- impact_reason: 'A key insight into user expectation management: users hold real-time
    voice AI to the same strict, low-latency standards as human conversation.'
  relevance_score: 8
  source: llm_enhanced
  text: As Shrasta is saying, human beings expect you to do this. And if we're talking
    to an AI, we don't relax that assumption. We bring our assumptions about human
    conversation into that experience of interacting with an AI.
  topic: safety/ethics
- impact_reason: Indicates a perceived gap between current releases (Gemini 1.0/2.0
    implied) and user expectations, and sets a high bar for future versions (5.0).
  relevance_score: 8
  source: llm_enhanced
  text: Well, I was hoping for Gemini 3 out of this I/O, so maybe Gemini 5.0 next
    time.
  topic: predictions
- impact_reason: Shows Google is actively iterating on how to present complex model
    reasoning (Chain-of-Thought) to users/developers, balancing the desire for transparency
    with the need for concise output.
  relevance_score: 7
  source: llm_enhanced
  text: We have thought summaries right now as a step in that direction. It'll be
    really interesting to find out and get the feedback around what works with thought
    summaries, what doesn't work with thought summaries.
  topic: technical/product
- impact_reason: Provides insight into the engineering trade-offs (latency vs. cost
    vs. cache size) inherent in optimizing infrastructure for large-scale model serving.
  relevance_score: 7
  source: llm_enhanced
  text: I think there's a trade-off between all the dimensions of caching, which is
    around the latency, because in some cases you're getting latency gains, in other
    cases it's how much, what's the cost for Google, how much stuff do you want to
    cache altogether?
  topic: technical
- impact_reason: Expresses a hope for standardization in complex AI infrastructure
    (like live APIs) to reduce vendor lock-in and foster a more flexible ecosystem.
  relevance_score: 7
  source: llm_enhanced
  text: hopefully there'll be some level of similarity, and you'll get some model-agnostic
    infrastructure to help make developers feel a little bit easier about being able
    to move between models potentially.
  topic: strategy
- impact_reason: Provides concrete examples (gaming agents, customer support) of complex,
    long-running, stateful applications that demand advanced live API capabilities.
  relevance_score: 7
  source: llm_enhanced
  text: We have people who are building say gaming agents, but we have multi-states,
    for example, in them. We have a lot... I mean, this was a famous demo at Next,
    but we have folks who want to... Customer support agents, of course. Their sessions
    can last for hours, right?
  topic: predictions/strategy
- impact_reason: Indicates that even as end-to-end audio models advance, Google is
    still investing heavily in improving the discrete components (like high-quality
    TTS) for use in existing or componentized workflows.
  relevance_score: 7
  source: llm_enhanced
  text: we also put out two new text-to-speech models at I/O, not available through
    the live API yet, but really high-performing, controllable, promptable text-to-speech
    models.
  topic: technical
- impact_reason: Highlights the direct impact of user feedback and market demand (specifically
    multilingual support) driving model development priorities.
  relevance_score: 7
  source: llm_enhanced
  text: We had a lot of users asking us for better German language support for something,
    which hopefully now we've delivered on with these models.
  topic: business
- impact_reason: A direct wish list item from the community, emphasizing that global
    adoption hinges on robust, high-quality support for a wider variety of languages.
  relevance_score: 7
  source: llm_enhanced
  text: 'I''ll just put on my hat as representative of a big community of people building
    this stuff: more and more languages, because AI is going on.'
  topic: business
- impact_reason: Directly references the Gemini model, framing the discussion around
    desired future capabilities for Google's flagship AI.
  relevance_score: 7
  source: llm_enhanced
  text: What would be one thing that you would wish? It doesn't have to come true,
    but you know, which happens with Gemini.
  topic: predictions
- impact_reason: Sets the context for future product expectations and roadmap speculation,
    relevant to the tech community following major developer conferences (like Google
    I/O).
  relevance_score: 6
  source: llm_enhanced
  text: So, I think one fun thing that we can do to wrap up would be a wish list for
    next year's I/O.
  topic: strategy
- impact_reason: This hints at a technical capability being discussed, likely related
    to asynchronous processing or tool use within an AI model (like Gemini).
  relevance_score: 5
  source: llm_enhanced
  text: execute the function in the background.
  topic: technical
- impact_reason: A direct prompt seeking clarification on desired features for the
    next major iteration of the AI model.
  relevance_score: 5
  source: llm_enhanced
  text: What do you mean by Gemini 5?
  topic: technical
source: Unknown Source
summary: '## Podcast Episode Summary: Google I/O 2025 Special Edition - #733


  This episode is a special crossover recording from Google I/O 2025, featuring hosts
  from the Toronto AI podcast and Latent Space podcast interviewing key Google DeepMind
  personnel—Logan Kilpatrick and Shrasta Basu Malik (PMs for AI Studio and Gemini
  API)—alongside Quinn LaKramer (CEO of Daily). The discussion centers on the latest
  announcements surrounding the **Gemini** model family, the **Gemini API**, and the
  rapidly evolving landscape of real-time, multimodal AI applications, particularly
  focusing on the **Live API**.


  ---


  ### 1. Focus Area

  The primary focus is the **Google Gemini Ecosystem** as showcased at I/O 2025. Key
  areas include:

  *   **Gemini Model Development Philosophy:** Emphasizing the goal of creating a
  single, unified Gemini model rather than splintered capabilities.

  *   **Gemini API Enhancements:** New developer controls, performance improvements,
  and feature rollouts for Gemini 2.5 Pro.

  *   **Gemini Live API (Real-Time Multimodality):** Deep dive into the infrastructure,
  challenges, and new features for building low-latency voice and video agents.

  *   **Generative UI and Diffusion Models:** Speculation on the future impact of
  models like Gemini Diffusion.


  ### 2. Key Technical Insights

  *   **Unified Model Strategy:** Google DeepMind''s North Star is to build one core
  model (Gemini), integrating specialized capabilities (like reasoning, which was
  initially forked) back into the mainline to unlock emergent, powerful behaviors
  (e.g., improved multimodal understanding).

  *   **Developer Control in 2.5 Pro:** New features like **Thought Summaries** (a
  step toward managing the visibility of internal reasoning) and the upcoming **Thinking
  Budget** control for 2.5 Pro give developers granular control over model execution
  and cost.

  *   **Native Audio Output & Multilinguality:** The release of native audio output,
  capable of seamlessly switching between languages (even unsupported ones like Klingon
  in demos), signifies a major step in making multimodal experiences feel more natural
  and accessible.


  ### 3. Business/Investment Angle

  *   **Live API Commitment Barrier:** Building products on the Live API requires
  a higher commitment from developers because the underlying infrastructure (for real-time
  audio/video processing) is currently bespoke and not easily interoperable between
  different model providers.

  *   **URL Context Tool:** This new tool, designed to respect the publisher ecosystem,
  unlocks new use cases like building custom research agents by retrieving in-depth,
  contextual information from web pages.

  *   **Caching Cost Savings:** The introduction of **implicit context caching** passes
  cost savings directly to developers without requiring manual management, incentivizing
  high-volume, repetitive chat use cases.


  ### 4. Notable Companies/People

  *   **Logan Kilpatrick & Shrasta Basu Malik (Google DeepMind PMs):** Provided insider
  details on API features, model integration philosophy, and the challenges of scaling
  real-time infrastructure.

  *   **Quinn LaKramer (CEO, Daily):** Contributed expertise on the infrastructure
  challenges of real-time networking (WebSockets vs. WebRTC) and the necessity of
  specialized frameworks (like PipeCat) to manage voice orchestration complexity.

  *   **Daily & PipeCat:** Highlighted as key partners in building production-ready,
  low-latency voice systems, with PipeCat serving as an open-source framework bridging
  framework-level solutions with API capabilities.


  ### 5. Future Implications

  The industry is rapidly moving toward **deeply integrated, real-time multimodal
  agents**. The focus is shifting from simple text-in/text-out to complex, stateful
  interactions involving audio, video, and dynamic UI generation. The tension between
  componentized architecture (e.g., separate TTS models) and the unified model approach
  will continue, though the long-term vision favors the unified Gemini model for emergent
  capabilities. The expectation for sub-second latency in human-AI interaction is
  becoming the standard, driving innovation in networking protocols alongside model
  inference.


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, Product Managers, and Technical
  Leaders** involved in building or integrating cutting-edge AI features, especially
  those focusing on **real-time conversational AI, voice applications, and API strategy.**


  ---


  ### Comprehensive Summary


  The Google I/O 2025 special edition podcast provided an in-depth look at the latest
  advancements in the Gemini ecosystem, featuring key product managers from Google
  DeepMind and industry partners. The central narrative revolved around Google''s
  commitment to a **single, unified Gemini model**, contrasting with the splintering
  seen elsewhere, where specialized research capabilities are strategically forked
  and then merged back into the mainline to create emergent power.


  **Technical Deep Dives:** Discussions covered granular controls for developers using
  **Gemini 2.5 Pro**, including the rollout of **Thought Summaries** and forthcoming
  **Thinking Budgets**, designed to manage reasoning costs. A major highlight was
  the **native audio output** feature, praised for its high quality and seamless language
  switching, underscoring the growing importance of audio/video modalities. The team
  also detailed the technical challenges of the **Live API**, noting that achieving
  human-expected latency (500-700ms) for real-time voice agents requires solving complex
  infrastructure problems beyond just inference, such as Voice Activity Detection
  (VAD) tuning and managing network protocols like WebRTC.


  **Architectural Philosophy and Challenges:** A significant portion addressed the
  **component vs. unified model** debate, particularly in voice. While cascaded architectures
  (using separate high-quality TTS models) are currently viable, the long-term goal
  is to integrate these'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- startup
- google
- microsoft
- openai
- anthropic
title: 'Google I/O 2025 Special Edition - #733'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 45
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 3
  prominence: 0.3
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 2
  prominence: 0.2
  topic: generative ai
- keywords:
  - startup
  - entrepreneur
  - founder
  - venture
  mentions: 1
  prominence: 0.1
  topic: startup
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 14:11:38 UTC -->
