---
companies:
- category: unknown
  confidence: medium
  context: At Sierra, discover top workout gear at incredible prices,
  name: At Sierra
  position: 0
- category: unknown
  confidence: medium
  context: t's get moving. This episode is brought to you by State Farm. Listening
    to this podcast is a smart move. Being
  name: State Farm
  position: 271
- category: tech
  confidence: high
  context: ach a language model a skill, right? But it's the meta-skill—the skill
    to create the skills—that is AGI.
  name: Meta
  position: 1842
- category: unknown
  confidence: medium
  context: press. Many of you would have seen last week that Jeremy Berman, who is
    a research scientist at Reflection AGI, i
  name: Jeremy Berman
  position: 2329
- category: unknown
  confidence: medium
  context: hat Jeremy Berman, who is a research scientist at Reflection AGI, is now
    the winner of the ArcAGI V2 leaderboard,
  name: Reflection AGI
  position: 2375
- category: unknown
  confidence: medium
  context: 'e reason this is important is the dirty secret of Silicon Valley: the
    extent to which human data is used to evalua'
  name: Silicon Valley
  position: 3458
- category: unknown
  confidence: medium
  context: 200 per task thing and knocked you off the board? But Jeremy, can you tell
    the audience a little bit about you
  name: But Jeremy
  position: 4936
- category: unknown
  confidence: medium
  context: "tart with your first talk solution? \n\nYeah, sure. So I have only been\
    \ working in research for about eigh"
  name: So I
  position: 5073
- category: unknown
  confidence: medium
  context: I had a company right out of college. I got into Y Combinator and ran a
    company for the last four and a half ye
  name: Y Combinator
  position: 5199
- category: unknown
  confidence: medium
  context: interested in reasoning in the brain. I picked up Jeff Hawkins' book, "A
    Thousand Brains," and I read that. At t
  name: Jeff Hawkins
  position: 5345
- category: unknown
  confidence: medium
  context: ng in the brain. I picked up Jeff Hawkins' book, "A Thousand Brains," and
    I read that. At the same time, I was coming
  name: A Thousand Brains
  position: 5366
- category: unknown
  confidence: medium
  context: in touch with Mike and Francois because I thought Arc AGI was such an elegant
    way of describing the problem
  name: Arc AGI
  position: 5818
- category: unknown
  confidence: medium
  context: er that, I got recruited into Francois and Mike's AGI Lab in India, where
    I was working on program synthesi
  name: AGI Lab
  position: 6120
- category: unknown
  confidence: medium
  context: final test example. I approached this inspired by Ryan Greenblatt, who
    had a solution earlier that generated a ton
  name: Ryan Greenblatt
  position: 7853
- category: unknown
  confidence: medium
  context: is a fundamental change from when ARKV1 existed. When I started in the
    field, I think I'm a bit embarrass
  name: When I
  position: 10631
- category: unknown
  confidence: medium
  context: eff Hawkins. I interviewed him, and he's amazing. His HTM algorithm is
    computationally stronger than a neur
  name: His HTM
  position: 13230
- category: tech
  confidence: high
  context: algorithm that is not reversible with stochastic gradient descent. The
    rough argument is that there is a di
  name: Gradient
  position: 13588
- category: unknown
  confidence: medium
  context: ly correct that it's not happening at the moment. But I still think, fundamentally,
    I don't think there i
  name: But I
  position: 14535
- category: unknown
  confidence: medium
  context: and then it was frozen, and they did some kind of Monte Carlo tree search.
    They achieved adaptivity through exh
  name: Monte Carlo
  position: 15175
- category: tech
  confidence: high
  context: n't think that's so intractable. My guess is that Nvidia just put in a
    hundred billion dollars into OpenAI
  name: Nvidia
  position: 15768
- category: tech
  confidence: high
  context: Nvidia just put in a hundred billion dollars into OpenAI. Sam Altman's
    plan is to produce a gigawatt of co
  name: Openai
  position: 15818
- category: unknown
  confidence: medium
  context: ust put in a hundred billion dollars into OpenAI. Sam Altman's plan is
    to produce a gigawatt of compute a week
  name: Sam Altman
  position: 15826
- category: unknown
  confidence: medium
  context: o hire the people your company desperately needs? Use Indeed sponsor jobs
    to hire top talent fast, and even be
  name: Use Indeed
  position: 17526
- category: unknown
  confidence: medium
  context: "itions apply. \n\nThis episode is brought to you by White Claw Surge.\
    \ Nice choice, hitting up this podcast. No surpris"
  name: White Claw Surge
  position: 17781
- category: unknown
  confidence: medium
  context: vors and 8% alcohol by volume. Unleash the night. Unleash White Claw Surge.
    Please drink responsibly. Hard seltzer with flav
  name: Unleash White Claw Surge
  position: 18107
- category: unknown
  confidence: medium
  context: "Hard seltzer with flavors, 8% alcohol by volume. White Claw Seltzer Works,\
    \ Chicago, Illinois. \n\nThat was on a Kaggle notebo"
  name: White Claw Seltzer Works
  position: 18208
- category: unknown
  confidence: medium
  context: d about there being a Docker for language models. In Docker, you can freeze
    the state of, let's say, a Linux
  name: In Docker
  position: 19713
- category: unknown
  confidence: medium
  context: ed area, and I think we're going to go through an RL S-curve, and then
    I think the next S-curve is figur
  name: RL S
  position: 21154
- category: unknown
  confidence: medium
  context: "g at his company. \n\nThere was a famous guy called Jerry Fodor in 1988\
    \ who had this connectionism critique. He h"
  name: Jerry Fodor
  position: 22183
- category: unknown
  confidence: medium
  context: earning and transfer. I'm not sure if you've seen Eric Pang's solution.
    I'm speaking to him in Hong Kong in a
  name: Eric Pang
  position: 23654
- category: unknown
  confidence: medium
  context: seen Eric Pang's solution. I'm speaking to him in Hong Kong in a couple
    of weeks. Rather than the dream code
  name: Hong Kong
  position: 23699
- category: unknown
  confidence: medium
  context: e powerful. I'm just regurgitating my co-host Dr. Doug Arcos; this is his
    favorite point. He always likes to m
  name: Doug Arcos
  position: 24485
- category: unknown
  confidence: medium
  context: runs in our brain is a Turing machine algorithm. A Turing machine has a
    codebook, which is a finite state a
  name: A Turing
  position: 25576
- category: unknown
  confidence: medium
  context: ce. I don't know if you saw that amazing paper by Kenneth Stanley, the
    fractured and tangled representations paper.
  name: Kenneth Stanley
  position: 26285
- category: unknown
  confidence: medium
  context: "trained. \n\nI think this is the same with humans. If I told you to sell\
    \ arc with Python programs, you wo"
  name: If I
  position: 28896
- category: unknown
  confidence: medium
  context: "ther that is a huge component of your solution. \n\nOn Eric's solution,\
    \ he's still predicting programs and do"
  name: On Eric
  position: 30845
- category: unknown
  confidence: medium
  context: re are basically no programs that RQV2 won't get. When Grok 6 or GPT-7
    comes out, you can use my V2 solution,
  name: When Grok
  position: 31853
- category: tech
  confidence: high
  context: teresting interview at NeurIPS last year with the Google guys. They were
    talking about adaptive temperatur
  name: Google
  position: 33428
- category: unknown
  confidence: medium
  context: ggesting they were performing a search algorithm. The OpenAI guy said on
    Twitter that they were just doing ver
  name: The OpenAI
  position: 35451
- category: unknown
  confidence: medium
  context: "cy was $200 per task. What was your efficiency? \n\nOn Arc V1, maybe $10,\
    \ something like that. I need to che"
  name: On Arc
  position: 36499
- category: unknown
  confidence: medium
  context: h. To give you a few examples, we interviewed the Alpha Evolved team, which
    was fascinating. Maybe you can contra
  name: Alpha Evolved
  position: 37939
- category: unknown
  confidence: medium
  context: cinating. Maybe you can contrast with those guys. Sikana AI released this
    shrinker, a kind of similar evoluti
  name: Sikana AI
  position: 38022
- category: unknown
  confidence: medium
  context: agree. Shole just released revision three of his "Deep Planning with Python"
    book. I recommend folks read chapter
  name: Deep Planning
  position: 40520
- category: unknown
  confidence: medium
  context: knowledge that is memorized, like the capital of New York or the Spanish
    language, and knowledge that is de
  name: New York
  position: 41426
- category: unknown
  confidence: medium
  context: r type of knowledge, like "What is the capital of North Dakota?" That is
    a knowledge network; it's not deductive
  name: North Dakota
  position: 41658
- category: unknown
  confidence: medium
  context: rsion of your blog post that you were inspired by Yann LeCun's JEPA, these
    joint embedding prediction architec
  name: Yann LeCun
  position: 42923
- category: unknown
  confidence: medium
  context: membership that backs your business journey with American Express Business
    Platinum. When you pay with membership rewards points for
  name: American Express Business Platinum
  position: 49186
- category: unknown
  confidence: medium
  context: e flight booked with a qualifying airline through Amex Travel, you can
    get 35% of those points back, up to one
  name: Amex Travel
  position: 49345
- category: unknown
  confidence: medium
  context: "hing you want to say to the audience? \n\nFor sure. At Reflection, we're\
    \ building open intelligence models. We're h"
  name: At Reflection
  position: 51023
- category: unknown
  confidence: medium
  context: rested in pre-training or post-training, we're in San Francisco, New York,
    and London. Check out our site, or you
  name: San Francisco
  position: 51248
- category: ai_research
  confidence: high
  context: AI research company where Jeremy Berman works as a research scientist,
    focused on reinforcement learning with verifiable feedback and building frontier
    foundation models
  name: Reflection AGI
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in context of receiving $100 billion investment from Nvidia and
    Sam Altman's compute scaling plans
  name: OpenAI
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: AI infrastructure company mentioned as investing $100 billion into OpenAI
    for compute infrastructure
  name: Nvidia
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced through their Sonnet 3.5 model used in Jeremy's evolutionary
    approach for the Arc AGI challenge
  name: Anthropic
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Startup accelerator mentioned as where Jeremy got accepted with his previous
    company before switching to AI research
  name: Y Combinator
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Company focused on human data contributions in AI, conducting research
    on how human data is used to evaluate and fine-tune AI models
  name: Prolific
  source: llm_enhanced
- category: ai_application
  confidence: low
  context: Sponsor of the MLST podcast, though specific AI/ML focus unclear from context
  name: Cyberpunk
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Research lab in India where Jeremy worked on program synthesis, led by
    Francois (likely Francois Chollet) focusing on AGI research
  name: Francois and Mike's AGI Lab
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Implied reference through AlphaZero and MuZero - their reinforcement learning
    systems that used training loops with value networks, policy networks, and Monte
    Carlo tree search.
  name: DeepMind
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Platform mentioned where AI competitions take place, specifically referenced
    in context of notebook solutions and ARC challenge competitions.
  name: Kaggle
  source: llm_enhanced
- category: ai_infrastructure
  confidence: low
  context: Referenced as inspiration for creating composable, immutable layers for
    language models - 'Docker for language models' concept.
  name: Docker
  source: llm_enhanced
- category: ai_research
  confidence: medium
  context: Referenced indirectly when discussing Francois (Chollet) and his work on
    intelligence measurement and what 'he's building at his company' regarding symbolic
    compositional processes.
  name: Francois Chollet's Company
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: Mentioned in context of Google researchers discussing adaptive temperature
    in language models for reasoning at NeurIPS
  name: Google
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Referenced through their Grok models (Grok 4, Grok 6) which showed special
    capabilities on grid reasoning tasks and outperformed other models on ARC leaderboard
  name: xAI
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as a team that was interviewed, working on evolutionary program
    approaches to AI problems
  name: Alpha Evolved
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Released a 'shrinker' tool with evolutionary program features including
    bandits and UCB (Upper Confidence Bound) algorithms
  name: Sikana AI
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Indirectly referenced through mention of 'Clem and Bonne' who appear to
    be Clement Delangue and other Hugging Face leadership discussing ML terminology
  name: Hugging Face
  source: llm_enhanced
- category: big_tech
  confidence: medium
  context: Indirectly referenced through discussion of Yann LeCun's JEPA (Joint Embedding
    Prediction Architectures) and energy-based models
  name: Meta AI
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Jeremy's company that is building open intelligence models, actively hiring
    across the stack for pre-training and post-training of large language models,
    with significant GPU resources and offices in San Francisco, New York, and London
  name: Reflection
  source: llm_enhanced
date: 2025-10-03 14:18:03 +0000
duration: 68
has_transcript: false
insights:
- actionable: true
  confidence: medium
  extracted: save that bit for later
  text: we should save that bit for later.
  type: recommendation
- actionable: true
  confidence: medium
  extracted: get to that in a minute
  text: we should get to that in a minute.
  type: recommendation
- actionable: false
  confidence: medium
  extracted: going deep and not going broad
  text: The problem with going deep and not going broad is there are some edge solutions
    you may never reach.
  type: problem_identification
layout: episode
llm_enhanced: true
original_url: https://traffic.megaphone.fm/APO8526044538.mp3
processing_date: 2025-10-03 14:18:03 +0000
quotes:
- length: 115
  relevance_score: 6
  text: The problem is you have to verify whether the instructions are correct; you
    can't run natural language on arc grids
  topics: []
- length: 259
  relevance_score: 5
  text: This was the fundamental problem with the solution, which made iteration challenging,
    especially because for each grid, for each training example, you have to run the
    natural language instructions, and it takes a long time, especially with this
    thinking model
  topics: []
- length: 108
  relevance_score: 4
  text: I believe that artificial general intelligence will be the most important
    invention of hopefully my lifetime
  topics: []
- length: 115
  relevance_score: 4
  text: I'm currently working on reasoning and post-training at Reflection, where
    we're building frontier foundation models
  topics: []
- length: 154
  relevance_score: 4
  text: I'm very excited about an active inference version of that, like an agentic
    version, where we're doing transductive, active fine-tuning in an agential way
  topics: []
- length: 80
  relevance_score: 4
  text: Fine-tuning is relatively trivial compared to the entire process of pre-training
  topics: []
- length: 220
  relevance_score: 4
  text: 'They said this is a curious oddity with transformers: if you start with a
    "virgin" 8 billion transformer, it almost doesn''t matter what it knew about before;
    you could start training it from scratch on the art challenges'
  topics: []
- length: 158
  relevance_score: 4
  text: I used Python programs because they are deterministic, and it's easy to verify
    whether or not it runs and produces the correct outputs on the training samples
  topics: []
- length: 80
  relevance_score: 4
  text: We're hiring across the stack—pre-training, post-training, large language
    models
  topics: []
- length: 141
  relevance_score: 3
  text: He's trying to address the biggest gap in AI at the moment, which is that
    we want systems that can synthesize new knowledge and understanding
  topics: []
- length: 29
  relevance_score: 3
  text: 5, "Here's what you got wrong
  topics: []
- length: 95
  relevance_score: 3
  text: When you fine-tune a model on OpenAI, they're not just fine-tuning it on the
    data you give them
  topics: []
- length: 87
  relevance_score: 3
  text: They did a lot of augmentation and active fine-tuning and built an intelligent
    artifact
  topics: []
- length: 47
  relevance_score: 3
  text: The problem is that Python programs are brittle
  topics: []
- length: 126
  relevance_score: 3
  text: You can express programs you want to run more concisely, but they're not runnable
    programs; you have to check them inductively
  topics: []
- length: 84
  relevance_score: 3
  text: The problem is that for my V1 solution, you also have to generate code, and
    Sonnet 3
  topics: []
- length: 109
  relevance_score: 3
  text: I spoke to the OpenAI guys; they did include the training data in that O3
    model, but I think that's fair game
  topics: []
- length: 70
  relevance_score: 3
  text: He says intelligence is simply the efficiency of knowledge acquisition
  topics:
  - acquisition
- length: 100
  relevance_score: 3
  text: I agree with what you said; it's not enough to have the right proof; you have
    to understand the tree
  topics: []
- length: 31
  relevance_score: 3
  text: You have to have logical chains
  topics: []
- impact_reason: Provides a clear definition of AGI as meta-learning capability and
    positions reasoning as the fundamental breakthrough needed
  relevance_score: 10
  source: llm_enhanced
  text: You can always teach a language model a skill, right? But it's the meta-skill—the
    skill to create the skills—that is AGI. To me, that's reasoning. Reasoning is
    that meta-skill.
  topic: predictions
- impact_reason: Identifies a critical limitation in current AI systems - the inability
    to efficiently learn new skills without catastrophic forgetting or extensive retraining
  relevance_score: 9
  source: llm_enhanced
  text: The ideal system would be we have a set of data. Our language model is bad
    at a certain thing. We can just give it this data, and then all of a sudden it
    keeps all of its knowledge and also gets really good at this new thing. We are
    not there yet, and that to me is a fundamental missing part.
  topic: technical
- impact_reason: Articulates the path to AGI through reasoning alignment, suggesting
    this is the key architectural challenge to solve
  relevance_score: 9
  source: llm_enhanced
  text: If you fundamentally learn the skill of reasoning, you should be able to apply
    that skill to learn all the other skills. That is the meta-skill. You need to
    align the model to reason, and then from there, you have a foundation from which
    you can actually build general targets.
  topic: technical
- impact_reason: Describes the paradigm shift that reasoning-trained models like O1
    brought to AI capabilities and problem-solving approaches
  relevance_score: 9
  source: llm_enhanced
  text: Everything about 01 changed how I think about these things. Before, you could
    simulate or emulate thinking, to quote you, with the parrot analogy. This is a
    classic parrot. Before you had reinforcement learning, you were doing stochastic
    guessing, which was not a very efficient internal revision loop.
  topic: technical
- impact_reason: Identifies a fundamental limitation of current AI training methods
    compared to human intelligence - the inability to achieve Turing completeness
    through SGD
  relevance_score: 9
  source: llm_enhanced
  text: Our brains, even though they are finite, run a Turing complete algorithm,
    which means our brains know how to expand their memory. We can write things on
    a whiteboard and get another notebook. That is a special type of algorithm that
    is not reversible with stochastic gradient descent.
  topic: technical
- impact_reason: Defines a clear benchmark for what true AI adaptability should look
    like and acknowledges current limitations
  relevance_score: 9
  source: llm_enhanced
  text: The ideal system would be we have a set of data; our language model is bad
    at a certain thing. We can give it this data, and all of a sudden, it keeps all
    of its knowledge and gets really good at this new thing. We are not there yet,
    and that to me is a fundamental missing part of general intelligence.
  topic: technical
- impact_reason: Suggests that hybrid AI systems combining neural networks with programming
    capabilities could achieve human-level intelligence
  relevance_score: 9
  source: llm_enhanced
  text: I think if you have a large enough neural network, almost everything can represent
    a symbolic system. But of course, it's not complete. Given a neural network plus
    the ability to write programs, I think we're at the human brain equivalent.
  topic: predictions
- impact_reason: Highlights a fundamental limitation of current AI training methods
    and the mystery of how biological intelligence achieved Turing completeness
  relevance_score: 9
  source: llm_enhanced
  text: Stochastic gradient descent does not find the algorithms that allow the systems
    to behave as if they are Turing machines. God knows how it happened in our brains;
    there was some hint of evolution where it suddenly got the merge operator or something,
    and we've got this incredible Turing complete algorithm in our finite brain.
  topic: technical
- impact_reason: Identifies a core limitation of current training methods - that SGD
    inherently finds shortcuts rather than true algorithmic solutions
  relevance_score: 9
  source: llm_enhanced
  text: When we do SGD, because there are all these shortcuts, it will always just
    find the wrong thing.
  topic: technical
- impact_reason: Identifies a fundamental limitation in current LLMs - lack of transferable
    reasoning capabilities across domains, which is crucial for AGI development
  relevance_score: 9
  source: llm_enhanced
  text: 'Shole hits on a core problem with language models: their reasoning is domain-specific.
    When you train a language model to reason about math, most of the reasoning circuits
    it gains live in the math weights. You try to train it on science, and it''s some
    generalization, but not as much as you would want.'
  topic: technical
- impact_reason: Defines the core challenge for achieving AGI - developing meta-learning
    capabilities rather than just domain-specific skills
  relevance_score: 9
  source: llm_enhanced
  text: He says you can always teach a language model a skill, but it's the meta-skill—the
    skill to create the skills—that is AGI. To me, that's reasoning. Reasoning is
    that meta-skill.
  topic: predictions
- impact_reason: Suggests that current pre-training approaches are fundamentally flawed
    for developing deep understanding, advocating for entirely new training paradigms
  relevance_score: 9
  source: llm_enhanced
  text: Forcing the language models to develop these deep trees from the ground up
    can only be developed from the ground up. We need to come up with new techniques
    and environments to grow the trees instead of pre-training, which is pre-filling
    random—it's not random, but it's a web; it's not a tree.
  topic: technical
- impact_reason: Reveals a counterintuitive insight that natural language can be more
    expressive than code for certain AI tasks, challenging conventional programming
    approaches
  relevance_score: 8
  source: llm_enhanced
  text: Really, what you want is a more expressive program. That's why I switched
    from Python to English, which is a much more expressive program.
  topic: technical
- impact_reason: Describes a novel approach to AI problem-solving that uses natural
    language descriptions instead of code generation, showing promising results on
    ArcAGI
  relevance_score: 8
  source: llm_enhanced
  text: His new architecture generates descriptions of algorithms rather than code,
    and iteratively, in an evolutionary sense, refines those and discards the ones
    that don't work.
  topic: technical
- impact_reason: Highlights the fundamental limitation of current AI systems in creative
    reasoning and knowledge synthesis
  relevance_score: 8
  source: llm_enhanced
  text: Current systems just get trained with a whole bunch of data, and they only
    know what they've been trained on. They can't think outside the box by creatively
    synthesizing new knowledge.
  topic: technical
- impact_reason: Explains why human oversight remains critical in AI systems and identifies
    the grounding problem as a core limitation
  relevance_score: 8
  source: llm_enhanced
  text: Current AI does not understand the world in a grounded way. It doesn't have
    a deep abstract understanding of the world, which is why the only way we can make
    AI work effectively is by grounding the generation and supervising the training
    of AI models with human data.
  topic: technical
- impact_reason: Prediction about the technical path to improved AI capabilities through
    RL-enhanced language models
  relevance_score: 8
  source: llm_enhanced
  text: Over time, I've become more convinced that language modeling with reinforcement
    learning will yield generalization far beyond what we see today.
  topic: predictions
- impact_reason: Quantifies the massive gap between human and AI reasoning capabilities
    on abstract pattern recognition tasks
  relevance_score: 8
  source: llm_enhanced
  text: The art challenge is kind of like an IQ test for machines. It's a set of input-output
    grids, and the whole point is to figure out how to transform input grids into
    output grids, given a common transformation rule. These are really easy for humans;
    the average human gets around 75% accuracy on RKV1. At the time, the best language
    model, GPT-4, Sonnet 3.5, was getting maybe 5%.
  topic: technical
- impact_reason: Describes a practical technique for improving AI performance through
    iterative refinement with specific error feedback
  relevance_score: 8
  source: llm_enhanced
  text: By taking the top-performing programs and running them into a revision loop—asking
    Sonnet 3.5, 'Here's what you got wrong. Here are the cells you got wrong. Here's
    your original Python program, and prove it'—that started to work well.
  topic: technical
- impact_reason: Shows how reasoning-trained models change the optimal strategy for
    problem-solving, requiring less external iteration
  relevance_score: 8
  source: llm_enhanced
  text: Interestingly, I found that for ARKV2, it was more important to be broad.
    This is surprising to a lot of people. This is because the models now think, which
    is great. The models do a lot of the deep revision for you in their thinking block.
  topic: technical
- impact_reason: Explains how reinforcement learning from reasoning creates internal
    thinking capabilities in AI models
  relevance_score: 8
  source: llm_enhanced
  text: You could think of RRL-trained models as having built-in revision loops. They
    are trained to explore the space in a deep way, thinking generally for themselves.
  topic: technical
- impact_reason: Raises fundamental questions about the limitations of current neural
    architectures for achieving true general reasoning
  relevance_score: 8
  source: llm_enhanced
  text: I feel that these models, because they're not Turing complete and not symbolic,
    similar to what Francois believes, will always be templated and not think in a
    very general sense.
  topic: technical
- impact_reason: Argues for the possibility of AGI based on biological existence proof,
    suggesting neural networks can achieve brain-like reasoning
  relevance_score: 8
  source: llm_enhanced
  text: I think fundamentally, taking a step back, the fact that our brains can do
    it and our brains are generally running similar algorithms means that we will
    eventually be able to inject general reasoning into language models.
  topic: predictions
- impact_reason: Reveals the hidden dependence of AI systems on human oversight and
    data curation
  relevance_score: 8
  source: llm_enhanced
  text: 'The dirty secret of Silicon Valley: the extent to which human data is used
    to evaluate and fine-tune AI models.'
  topic: business
- impact_reason: Provides a formal definition of intelligence and critiques current
    LLMs against this standard, highlighting a key limitation
  relevance_score: 8
  source: llm_enhanced
  text: According to Shole, intelligence is simply the ability to search through the
    space of Turing programs. I don't think that's what's happening with these LLMs
    at the moment.
  topic: technical
- impact_reason: Describes a vision for truly adaptive AI systems that can update
    their weights in real-time based on environmental feedback
  relevance_score: 8
  source: llm_enhanced
  text: I'm very excited about an active inference version of that, like an agentic
    version, where we're doing transductive, active fine-tuning in an agential way.
    I take an action, get new information from the environment, and update my weights.
    That would be truly adaptive and intelligent.
  topic: technical
- impact_reason: Explains the hidden computational costs and technical challenges
    behind AI fine-tuning services
  relevance_score: 8
  source: llm_enhanced
  text: The reason fine-tuning is so expensive is that we have this continual learning
    problem. When you fine-tune a model on OpenAI, they're not just fine-tuning it
    on the data you give them. To stop the catastrophic forgetting problem, they presumably
    have to sample in a bunch of the original training data and maintain the distribution.
  topic: technical
- impact_reason: Identifies a counterintuitive problem in AI training where more data
    can actually hurt performance
  relevance_score: 8
  source: llm_enhanced
  text: If you have the perfect weights for a certain problem and then fine-tune that
    model on more examples of that problem, the weights will start to drift, and you
    will actually drift away from the correct solution.
  topic: technical
- impact_reason: Proposes an innovative approach to AI model management using containerization
    concepts for better composability
  relevance_score: 8
  source: llm_enhanced
  text: I've long dreamed about there being a Docker for language models. In Docker,
    you can freeze the state of, let's say, a Linux operating system with an application
    with security updates. You have these immutable layers, and the composability
    we often talk about could actually happen at the architectural level.
  topic: technical
- impact_reason: Clearly articulates the fundamental advantage of symbolic AI over
    neural networks in terms of knowledge retention
  relevance_score: 8
  source: llm_enhanced
  text: The benefit of symbolic systems is that this doesn't happen. Symbolic systems
    are deterministic. When you get the right answer, you can be sure you have the
    right answer; you stash it away into your library of correct solutions. This is
    the problem with continuous structures.
  topic: technical
- impact_reason: Predicts the next major breakthrough area in AI development after
    the current scaling phase
  relevance_score: 8
  source: llm_enhanced
  text: I think the next S-curve is figuring out how to make language models composable.
  topic: predictions
- impact_reason: Explains key advantages of symbolic AI that current neural networks
    struggle to achieve
  relevance_score: 8
  source: llm_enhanced
  text: Symbolic systems have systematicity and productivity. Systematicity is this
    compositional thing; it's the ability to generalize between 'Mary loves John'
    and 'Mary loves Jane.' You have semantics, symbolic relations, and certain computational
    properties like variable binding and quantification over potentially infinite
    domains.
  topic: technical
- impact_reason: Provides evidence that adaptive AI systems with real-time weight
    updates are possible and effective
  relevance_score: 8
  source: llm_enhanced
  text: Intelligence is domain-specific, as per Shole, and they built this system
    that was per task adapting and solving the tasks, updating the weights. It was
    beautiful. That was an existence proof, if nothing else, that this could work.
  topic: technical
- impact_reason: Identifies catastrophic forgetting, not computational power, as the
    primary bottleneck for AI advancement
  relevance_score: 8
  source: llm_enhanced
  text: I think fundamentally compute is not the issue; I think it's this catastrophic
    forgetfulness.
  topic: technical
- impact_reason: Challenges fundamental assumptions about the gap between human cognition
    and AI, suggesting we could theoretically achieve human-level reasoning
  relevance_score: 8
  source: llm_enhanced
  text: I think the human brain is running a Turing system. The algorithm you put
    in that Turing machine is very difficult to find. I don't disagree with that,
    but why wouldn't we be able to find that algorithm for neural networks?
  topic: technical
- impact_reason: Reveals a counterintuitive insight that natural language can be more
    effective than code for certain AI tasks, challenging conventional wisdom
  relevance_score: 8
  source: llm_enhanced
  text: I switched from Python to English, which is a much more expressive program.
    You can describe every single RQV2 task in 10 bullet points of plain English,
    most of them in five bullet points.
  topic: technical
- impact_reason: Explains why natural language programming can outperform traditional
    code in AI systems, highlighting expressiveness vs verifiability trade-offs
  relevance_score: 8
  source: llm_enhanced
  text: Python doesn't have these features; it's just not as expressive as natural
    language. Another way to put it is that you have this inductive-transductive trade-off.
  topic: technical
- impact_reason: Identifies a key characteristic of current LLMs - uneven performance
    across domains based on training distribution
  relevance_score: 8
  source: llm_enhanced
  text: These language models are very spiky in certain things where they were trained
    heavily. The general idea is that these networks are very spiky.
  topic: technical
- impact_reason: Provides concrete performance ceiling for current LLM capabilities
    on reasoning tasks, suggesting fundamental limitations that efficiency improvements
    alone cannot overcome
  relevance_score: 8
  source: llm_enhanced
  text: I wouldn't expect anyone to break 40% using the language models we have today.
    But you could probably make my solution twice as efficient, and you wouldn't get
    more than a few percentage points more accurate, is my guess.
  topic: technical
- impact_reason: Fundamental insight about the limitations of current pre-training
    approaches and the need to distinguish between memorized and deducible knowledge
  relevance_score: 8
  source: llm_enhanced
  text: 'There are two types of knowledge: knowledge that is memorized, like the capital
    of New York or the Spanish language, and knowledge that is deduced, like physics,
    special relativity, and general relativity. Pre-training treats all knowledge
    as a knowledge web.'
  topic: technical
- impact_reason: Provides a novel definition of intelligence as compression and deductive
    capability, with concrete example of how this manifests in breakthrough discoveries
  relevance_score: 8
  source: llm_enhanced
  text: My view is that intelligence is compression. You should be able to deduce.
    You can build a knowledge tree based on almost nothing. Einstein was extremely
    intelligent because the hints he needed to come up with special relativity are
    zero.
  topic: strategy
- impact_reason: Proposes a specific technical approach for improving AI generalization
    through structural changes to how knowledge is represented
  relevance_score: 8
  source: llm_enhanced
  text: I think reinforcement learning and reasoning is the process of pruning our
    knowledge network and replacing it with this tree. Until we have weights that
    represent the actual deductive nature of knowledge, we won't get generalization.
  topic: technical
- impact_reason: Identifies a key missing component in current AI systems and suggests
    environmental training as a solution for developing creativity
  relevance_score: 8
  source: llm_enhanced
  text: My hunch is that part of the reason models are not yet great at coming up
    with novel solutions and information is that they don't have the circuitry of
    invention. I think that is a circuit that needs to be developed, and we don't
    have the environments to develop that circuit yet.
  topic: technical
- impact_reason: Outlines a concrete research direction for developing AI systems
    capable of genuine innovation and out-of-distribution reasoning
  relevance_score: 8
  source: llm_enhanced
  text: I want to build environments where the model has never seen something and
    tries to deduce new things that are outside of the distribution. Over time, it
    learns and practices and builds this invention circuit.
  topic: strategy
- impact_reason: Concise definition of intelligence that focuses on learning capability
    rather than accumulated knowledge, relevant for AI development
  relevance_score: 8
  source: llm_enhanced
  text: Intelligence is the speed at which you can build the tree, not how many trees
    you have.
  topic: strategy
- impact_reason: Reveals how current prompting techniques are essentially workarounds
    that mask fundamental understanding limitations rather than solving them
  relevance_score: 8
  source: llm_enhanced
  text: We can put a prompt in there that constrains their generation. Now we can
    make them act as if they understood the tree when they didn't.
  topic: technical
- impact_reason: Proposes that logical reasoning should be the foundational layer
    for building AI systems with deep understanding
  relevance_score: 8
  source: llm_enhanced
  text: Logic is the fundamental block of a tree. Everything comes from logic.
  topic: technical
- impact_reason: Articulates the ultimate goal - AI systems that can truly understand
    and reason about the world to enable genuine innovation and invention
  relevance_score: 8
  source: llm_enhanced
  text: If the representation is a faithful description of what is happening out there
    and we can do this creative reasoning on that understanding, then what's to stop
    us from inventing new things?
  topic: predictions
- impact_reason: Memorable analogy that captures how AI models reflect their training
    data quality without true understanding
  relevance_score: 7
  source: llm_enhanced
  text: A parrot that lives in a courthouse will regurgitate more correct statements
    than a parrot that lives in a madhouse.
  topic: technical
- impact_reason: Strong conviction statement about AGI's transformative potential
    from a leading researcher
  relevance_score: 7
  source: llm_enhanced
  text: I believe that artificial general intelligence will be the most important
    invention of hopefully my lifetime.
  topic: predictions
- impact_reason: Highlights the importance of verifiable outputs in AI systems for
    reliable problem-solving
  relevance_score: 7
  source: llm_enhanced
  text: Python programs are great because they're deterministic, and you can quickly
    check whether or not the Python program works, which is really cheap to verify.
  topic: technical
- impact_reason: Identifies a key limitation in current AI models' ability to solve
    problems correctly on first attempts, motivating iterative approaches
  relevance_score: 7
  source: llm_enhanced
  text: I noticed that the language models struggled on first attempts, even if you
    asked the language model a thousand times to generate Python programs. They were
    always off by small amounts on easy tasks, which I thought they should be able
    to solve.
  topic: technical
- impact_reason: Predicts that evolutionary approaches will become important in AI
    development, suggesting a shift from single-shot to iterative problem-solving
  relevance_score: 7
  source: llm_enhanced
  text: I think evolving solutions is a powerful technique generally. I think it's
    going to play a role in future technologies.
  topic: predictions
- impact_reason: Articulates a fundamental design principle for AI systems balancing
    exploration vs exploitation in solution search
  relevance_score: 7
  source: llm_enhanced
  text: There's a constant trade-off between how deep you go, how many revisions you
    take, and how broad you start out. The problem with going deep and not going broad
    is there are some edge solutions you may never reach.
  topic: technical
- impact_reason: Practical insight about how to work with reasoning-trained models
    differently than traditional language models
  relevance_score: 7
  source: llm_enhanced
  text: You don't need to prompt thinking models to think step by step; they already
    do it.
  topic: technical
- impact_reason: Bold claim about the potential equivalence between artificial and
    biological neural networks for reasoning
  relevance_score: 7
  source: llm_enhanced
  text: I don't think there's a fundamental reason why neural networks can't behave
    like biological neural networks.
  topic: predictions
- impact_reason: Technical insight about computational differences between current
    AI architectures and biological intelligence
  relevance_score: 7
  source: llm_enhanced
  text: His HTM algorithm is computationally stronger than a neural network; it's
    Turing complete. Our brains, even though they are finite, run a Turing complete
    algorithm, which means our brains know how to expand their memory.
  topic: technical
- impact_reason: Reveals massive scale of AI investment and compute infrastructure
    plans that could enable new capabilities
  relevance_score: 7
  source: llm_enhanced
  text: My guess is that Nvidia just put in a hundred billion dollars into OpenAI.
    Sam Altman's plan is to produce a gigawatt of compute a week. I don't think that
    with ever-efficient algorithms is crazy far off.
  topic: business
- impact_reason: Reveals surprising flexibility in transformer architectures for domain-specific
    adaptation
  relevance_score: 7
  source: llm_enhanced
  text: 'This is a curious oddity with transformers: if you start with a ''virgin''
    8 billion transformer, it almost doesn''t matter what it knew about before; you
    could start training it from scratch on the art challenges.'
  topic: technical
- impact_reason: Proposes a technical approach for achieving AI model composability
    through dynamic layer merging
  relevance_score: 7
  source: llm_enhanced
  text: We could do dynamic model merging between different layers, and that would
    be very exciting.
  topic: technical
- impact_reason: Provides practical insight for AI system design - that verification
    capabilities should be prioritized over generation capabilities
  relevance_score: 7
  source: llm_enhanced
  text: I ended up finding it was more important that the checker was stronger than
    the actual instruction creator, which I think is interesting.
  topic: technical
- impact_reason: Predicts how AI solutions will scale with future model improvements,
    suggesting natural language approaches are more future-proof
  relevance_score: 7
  source: llm_enhanced
  text: When Grok 6 or GPT-7 comes out, you can use my V2 solution, and it will win.
    That is not the case for my V1 solution.
  topic: predictions
- impact_reason: Reveals a strategic approach to AI problem-solving - deliberately
    increasing entropy to explore solution spaces more thoroughly
  relevance_score: 7
  source: llm_enhanced
  text: 'I wanted to inject as much entropy as possible, which is partially why my
    prompts are so broad. I could definitely improve my accuracy on a few tasks by
    making the prompts more specific, but I wanted to constantly berate it: more entropy,
    more entropy.'
  topic: technical
- impact_reason: Explains how different prompting styles activate different neural
    pathways in LLMs, providing insight into model behavior
  relevance_score: 7
  source: llm_enhanced
  text: The second you start prompting with code, they go into code mode. There are
    a lot of papers that show that by prompting in a certain direction, it activates
    certain weights that are naturally lower entropy.
  topic: technical
- impact_reason: Provides concrete cost comparisons for different AI approaches, showing
    significant efficiency differences in problem-solving methods
  relevance_score: 7
  source: llm_enhanced
  text: The O3 model from OpenAI was about $200 per task... My latest solution was
    around $30 on V2 and $8 on V1.
  topic: business
- impact_reason: Challenges the assumption that formal programming languages are always
    superior to natural language for problem-solving
  relevance_score: 7
  source: llm_enhanced
  text: If I told you to sell arc with Python programs, you would do a worse job,
    even if you were an expert at Python. So fundamentally, natural language is more
    general and leads to better solutions.
  topic: technical
- impact_reason: Contrasts different optimization approaches and suggests neural evolution
    may find more meaningful representations than SGD
  relevance_score: 7
  source: llm_enhanced
  text: SGD finds the algorithms over here, and neural evolution algorithms find the
    ones over here. It just so happens that the neural evolution algorithms find representations
    that are grounded in the world, that carve the world up by the joints.
  topic: technical
- impact_reason: Explains why natural language interfaces can be more powerful for
    AI systems than constrained programming languages
  relevance_score: 7
  source: llm_enhanced
  text: When you have this natural language description, natural language is more
    expressive, which means there are more degrees of freedom. The beauty of LLMs
    is that there's this huge space that you're traversing around.
  topic: technical
- impact_reason: Identifies a fundamental tension in AI systems between leveraging
    existing knowledge and exploring novel solutions
  relevance_score: 7
  source: llm_enhanced
  text: There's this constant trade-off between reasoning and being constrained by
    our knowledge. When we're being creative and flexible, we want to go in different
    places.
  topic: technical
- impact_reason: Key insight about natural language being more computationally efficient
    for AI reasoning tasks compared to other approaches
  relevance_score: 7
  source: llm_enhanced
  text: I think the fundamental reason why I got higher and could match his efficiency
    while still getting higher is that I was using natural language. Natural language
    is a much more efficient area to play in, at least what I found.
  topic: technical
- impact_reason: Identifies a specific technical approach for making AI systems more
    adaptable and intelligent according to Chollet's definition
  relevance_score: 7
  source: llm_enhanced
  text: I think test-time fine-tuning would be the way to fundamentally make it adaptable.
  topic: technical
- impact_reason: Important realization about the conceptual capabilities already present
    in current language models, affecting research priorities
  relevance_score: 7
  source: llm_enhanced
  text: Language models do operate on the concept level in the hidden layers. That
    was something I slowly came to realize. I think there's a lot of potential in
    JEPA frameworks. I hope people keep pulling on them, but I think most of the benefits
    I thought came from JEPA exist in language models.
  topic: technical
- impact_reason: Proposes a specific experimental methodology for testing and developing
    AI creativity and invention capabilities
  relevance_score: 7
  source: llm_enhanced
  text: If you have the ability to ablate special relativity and all the physics that
    came from special relativity from your pre-training data, that is a gold mine
    of an environment. You can prompt the model, do everything, and really try to
    get it to do special relativity.
  topic: technical
- impact_reason: Provides a technical framework combining RL and knowledge integration
    for building more capable AI systems
  relevance_score: 7
  source: llm_enhanced
  text: 'It''s two things: reinforcement learning to ensure the knowledge tree is
    consistent and making sure the circuitry can pull from its entire corpus of understanding
    of the world and fuel the innovation engine.'
  topic: technical
- impact_reason: Provides a clear framework for evaluating AI understanding that goes
    beyond behavioral tests to mechanistic capabilities
  relevance_score: 7
  source: llm_enhanced
  text: Understanding is a spectrum. On one end, it's memorization, which is zero
    understanding. On the other end, it's the ability to deduce correctly.
  topic: strategy
- impact_reason: Explains a fundamental limitation in current LLM deployment strategies
    and why multiple generations are necessary
  relevance_score: 7
  source: llm_enhanced
  text: Language models famously don't understand the tree very deeply. When language
    models do autonomous generation, the reason we have to do so many generations
    and select the best one is that they don't understand the tree very deeply.
  topic: technical
- impact_reason: Sets a clear goal for next-generation AI systems that could operate
    more reliably without extensive sampling
  relevance_score: 7
  source: llm_enhanced
  text: We need to build models that do understand the tree deeply, and then we can
    trust them to generate autonomously.
  topic: predictions
- impact_reason: Provides specific performance predictions and suggests diminishing
    returns with current approaches, important for research planning
  relevance_score: 7
  source: llm_enhanced
  text: Improvements will be logarithmic. I wouldn't expect anyone to break 40% using
    the language models we have today.
  topic: predictions
- impact_reason: Provides an intuitive analogy distinguishing between intelligence
    (capacity) and knowledge (content), relevant for understanding AI development
  relevance_score: 7
  source: llm_enhanced
  text: You could have a very intelligent child who doesn't know much about the world
    but has the potential to build a tree.
  topic: technical
- impact_reason: Connects AI understanding to fundamental questions about the physical
    basis of cognition and whether AI needs specific hardware architectures
  relevance_score: 7
  source: llm_enhanced
  text: If you abstract it into physics, we're talking about certain types of causal
    graphs. You can argue that certain types of cognition require certain types of
    physical instantiation.
  topic: technical
- impact_reason: Acknowledges the deep philosophical questions about consciousness
    and understanding that may be relevant to building truly intelligent AI
  relevance_score: 7
  source: llm_enhanced
  text: Some components of understanding are phenomenal; they're conscious. When you
    start taking it to this philosophical level, there's almost no end to it.
  topic: safety
- impact_reason: Acknowledges that despite theoretical limitations, current AI systems
    are performing surprisingly well, creating tension between theory and practice
  relevance_score: 7
  source: llm_enhanced
  text: Even though we can make all these arguments, these elements are doing so well.
  topic: technical
- impact_reason: Puts current AI computational limitations in historical perspective,
    suggesting rapid advancement
  relevance_score: 6
  source: llm_enhanced
  text: In 10 years, this is going to be like the Apollo mission computer.
  topic: predictions
- impact_reason: Highlights the current limitations of AI reasoning capabilities even
    on seemingly simple tasks
  relevance_score: 6
  source: llm_enhanced
  text: Most arc tasks for V2, the models don't get close. My solution was the top,
    and it's at 30%.
  topic: technical
- impact_reason: Offers perspective on OpenAI's strategic approach to AI development,
    emphasizing generality over task-specific optimization
  relevance_score: 6
  source: llm_enhanced
  text: I think OpenAI generally wants to do the right thing, and they want their
    solutions to be very general and broad.
  topic: business
- impact_reason: Addresses concerns about data contamination in AI benchmarks and
    establishes what constitutes fair use of training data
  relevance_score: 6
  source: llm_enhanced
  text: They did include the training data in that O3 model, but I think that's fair
    game. I don't think they fine-tuned it; it's just part of the corpus that one
    is pre-training, which to me is fair game.
  topic: safety
- impact_reason: Identifies limitations of traditional programming approaches for
    certain types of AI reasoning tasks
  relevance_score: 6
  source: llm_enhanced
  text: Python programs are brittle. There are many things that are very difficult
    to describe with Python, arc grids in V2 being one of them.
  topic: technical
- impact_reason: Describes a practical technique for improving AI efficiency through
    knowledge transfer and reuse
  relevance_score: 6
  source: llm_enhanced
  text: I had a version that does do library transfer. I would save the traces from
    training and try to pull those in during test time.
  topic: technical
- impact_reason: Important insight about the domain-specific nature of creativity
    that has implications for how we should approach building creative AI systems
  relevance_score: 6
  source: llm_enhanced
  text: I also feel that creativity is very domain-specific. When I hire creative
    professionals, like an editor, they can't edit my show, even if they're really
    good at editing other people's shows. It's because they simply don't know anything
    about machine learning.
  topic: strategy
- impact_reason: Reveals that Reflection AI is actively working on these deep understanding
    problems with significant computational resources
  relevance_score: 6
  source: llm_enhanced
  text: At Reflection, we're building open intelligence models. We're hiring across
    the stack—pre-training, post-training, large language models. We have a lot of
    GPUs.
  topic: business
- impact_reason: References neuroscientific perspectives on cognition that could inform
    AI architecture design, particularly around sensory-motor integration
  relevance_score: 6
  source: llm_enhanced
  text: A lot of neuroscientists are in terminus. They think that all of this happens
    in the brain. We have these sensory-motor circuits, and we've got this master
    algorithm in our neocortex that does all of the things.
  topic: technical
source: Machine Learning Street Talk (MLST)
summary: '# Podcast Summary: Jeremy Berman''s ARC-AGI Breakthrough


  ## Focus Area

  This episode centers on artificial general intelligence (AGI) research, specifically
  discussing Jeremy Berman''s record-breaking 29.4% score on the ARC-AGI-2 benchmark.
  The conversation explores evolutionary programming approaches, reasoning capabilities
  in AI systems, and the fundamental challenges in achieving true machine intelligence.


  ## Key Technical Insights

  • **Evolutionary Algorithm Innovation**: Berman''s breakthrough involved evolving
  natural language descriptions of algorithms rather than explicit Python code, leveraging
  the superior expressiveness of English over programming languages for complex reasoning
  tasks

  • **Reasoning vs. Domain Skills**: The discussion distinguishes between teaching
  AI specific skills versus the "meta-skill" of reasoning - the ability to learn how
  to learn, which represents the core challenge in achieving AGI

  • **RL-Trained Models vs. Traditional LLMs**: Models trained with reinforcement
  learning (like O1) demonstrate built-in revision loops and deeper thinking capabilities,
  reducing the need for artificial prompting strategies


  ## Business/Investment Angle

  • **Compute Infrastructure Scaling**: Discussion of massive investments (Nvidia''s
  $100B into OpenAI, Sam Altman''s gigawatt compute plans) suggesting the industry
  believes computational scale will solve current limitations

  • **Human Data Dependency**: Current AI systems heavily rely on human evaluation
  and fine-tuning, representing both a bottleneck and business opportunity for companies
  like Prolific

  • **Continual Learning Market Gap**: The inability of current models to learn new
  skills without forgetting old ones represents a significant commercial opportunity
  for breakthrough solutions


  ## Notable Companies/People

  • **Jeremy Berman**: Research scientist at Reflection AGI, former Y Combinator CTO
  who pivoted to AGI research

  • **François Chollet**: ARC-AGI creator and neurosymbolic AI advocate, now building
  solutions at his company

  • **Jeff Hawkins**: Author of "A Thousand Brains," influential in hierarchical temporal
  memory approaches

  • **Ryan Greenblatt**: Previous ARC-AGI researcher whose evolutionary programming
  approach inspired Berman''s work


  ## Future Implications

  The conversation suggests the field is moving toward hybrid systems combining neural
  networks with symbolic reasoning capabilities. Key developments expected include:
  composable AI architectures with freezable expert layers, dynamic model merging
  capabilities, and eventually real-time adaptive fine-tuning. The industry appears
  to be approaching an "RL S-curve" followed by a focus on making language models
  truly composable and capable of continual learning without catastrophic forgetting.


  ## Target Audience

  This episode is most valuable for AI researchers, ML engineers, and technical leaders
  working on reasoning systems, program synthesis, or AGI development. The deep technical
  discussion of evolutionary algorithms, symbolic vs. neural approaches, and architectural
  considerations makes it particularly relevant for practitioners building next-generation
  AI systems.


  ---


  ## Comprehensive Analysis


  This podcast episode represents a fascinating deep-dive into one of the most significant
  recent breakthroughs in artificial general intelligence research. Jeremy Berman''s
  achievement of a 29.4% score on the ARC-AGI-2 benchmark marks a substantial leap
  forward in machine reasoning capabilities, and his methodology reveals important
  insights about the future direction of AGI development.


  **The Technical Breakthrough**


  Berman''s approach represents an elegant evolution of previous work by Ryan Greenblatt,
  but with a crucial innovation: instead of evolving Python programs directly, his
  system evolves natural language descriptions of algorithms. This shift acknowledges
  a fundamental truth about human cognition - we typically understand problems in
  natural language before translating them into code. As Berman notes, "You can describe
  every single ARC-AGI task in 10 bullet points of plain English, most of them in
  5 bullet points." This insight led him to leverage the superior expressiveness of
  natural language over programming languages for complex reasoning tasks.


  The evolutionary component of his approach addresses a critical challenge in AI
  problem-solving: the trade-off between breadth and depth of search. Rather than
  trying thousands of shallow attempts or pursuing a single deep revision path, Berman''s
  system finds a "Goldilocks zone" that balances exploration with refinement. Interestingly,
  he discovered that ARC-AGI-2 required more breadth than depth, partly because newer
  RL-trained models like O1 already incorporate sophisticated internal revision loops.


  **The Reasoning Revolution**


  A central theme of the conversation is the distinction between domain-specific skills
  and the meta-skill of reasoning. Berman argues that "reasoning is that meta-skill"
  - the ability to learn how to learn new capabilities. This perspective frames AGI
  not as the accumulation of many specific abilities, but as the development of a
  general learning mechanism that can acquire any skill.


  The discussion reveals how reinforcement learning has fundamentally changed the
  landscape. Pre-O1 models required elaborate prompting strategies to simulate thinking,
  but RL-trained models have internalized these revision loops. This represents a
  shift from "stochastic guessing" to genuine internal reasoning processes, though
  questions remain about whether this constitutes true understanding or sophisticated
  pattern matching.


  **The Symbolic vs. Neural Debate**


  The conversation touches on one of the most fundamental debates in AI: whether neural
  networks alone can achieve human-level reasoning or whether symbolic components
  are necessary. Berman takes an optimistic view, arguing that neural networks have
  the theoretical capacity to represent symbolic systems, while acknowledging current
  limitations. The discussion references François Chollet''s neurosymbolic approach
  and Jerry Fodor''s'
tags:
- artificial-intelligence
- ai-infrastructure
- generative-ai
- investment
- meta
- nvidia
- openai
- google
title: New top score on ARC-AGI-2-pub (29.4%) - Jeremy Berman
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 156
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 31
  prominence: 1.0
  topic: ai infrastructure
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 6
  prominence: 0.6
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-03 14:18:03 UTC -->
