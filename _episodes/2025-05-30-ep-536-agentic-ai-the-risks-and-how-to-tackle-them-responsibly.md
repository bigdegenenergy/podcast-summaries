---
companies:
- category: unknown
  confidence: medium
  context: This is the Everyday AI Show, the everyday podcast where we simplify AI
    and br
  name: Everyday AI Show
  position: 12
- category: unknown
  confidence: medium
  context: o boost your career, business, and everyday life. Responsible AI used to
    be much more straightforward. I don't thi
  name: Responsible AI
  position: 197
- category: unknown
  confidence: medium
  context: e you are too. What's going on, y'all? My name is Jordan Wilson, and welcome
    to Everyday AI. This is your daily l
  name: Jordan Wilson
  position: 1302
- category: unknown
  confidence: medium
  context: ', y''all? My name is Jordan Wilson, and welcome to Everyday AI. This is
    your daily live stream podcast and free'
  name: Everyday AI
  position: 1332
- category: unknown
  confidence: medium
  context: it's something I think about probably every day. And I have so many questions,
    and maybe you do too. And
  name: And I
  position: 2287
- category: unknown
  confidence: medium
  context: eam audience, please help me welcome to the show, Sarah Bird, the Chief
    Product Officer of Responsible AI at M
  name: Sarah Bird
  position: 2465
- category: unknown
  confidence: medium
  context: ease help me welcome to the show, Sarah Bird, the Chief Product Officer
    of Responsible AI at Microsoft. Sarah, thank you
  name: Chief Product Officer
  position: 2481
- category: tech
  confidence: high
  context: d, the Chief Product Officer of Responsible AI at Microsoft. Sarah, thank
    you so much for joining the Everyda
  name: Microsoft
  position: 2524
- category: unknown
  confidence: medium
  context: responsibly, that people can use it responsibly. But I think the big thing
    that's changed is people's aw
  name: But I
  position: 4020
- category: unknown
  confidence: medium
  context: of how important this is and level of engagement. So I feel like before
    generative AI really took off, I
  name: So I
  position: 4133
- category: unknown
  confidence: medium
  context: even just look at what was just announced, right? What Microsoft just announced
    at its Build conference last week,
  name: What Microsoft
  position: 5180
- category: unknown
  confidence: medium
  context: tion. But what does that mean? So whether it's in Copilot Studio, I think
    this is also maybe in the Azure AI Found
  name: Copilot Studio
  position: 7654
- category: unknown
  confidence: medium
  context: Copilot Studio, I think this is also maybe in the Azure AI Foundry, but
    how does that actually work? Multi-agentic o
  name: Azure AI Foundry
  position: 7704
- category: unknown
  confidence: medium
  context: e. One of the areas that we released at Build was Foundry Observability,
    and this is exactly giving you a monitoring syst
  name: Foundry Observability
  position: 9533
- category: tech
  confidence: high
  context: ery podcast. Companies like Adobe, Microsoft, and Nvidia have partnered
    with us because they trust our exp
  name: Nvidia
  position: 17550
- category: unknown
  confidence: medium
  context: ducing harmful content? Can my user jailbreak it? Did I accidentally produce
    copyrighted material? And so
  name: Did I
  position: 18533
- category: unknown
  confidence: medium
  context: system. And people are pretty excited about this. Our Work Trends Index,
    for example, shows that I think 81% of employers
  name: Our Work Trends Index
  position: 18887
- category: unknown
  confidence: medium
  context: what we released at Build, for example, is a new Entra Agent ID so that
    agents can be tracked in your system just
  name: Entra Agent ID
  position: 19927
- category: unknown
  confidence: medium
  context: of the things we released at Build coming out of Microsoft Research is
    a system that is called the Agentic UI, and it
  name: Microsoft Research
  position: 21974
- category: unknown
  confidence: medium
  context: Microsoft Research is a system that is called the Agentic UI, and it is
    a research system for people to play w
  name: Agentic UI
  position: 22024
- category: unknown
  confidence: medium
  context: icrosoft, the categorization that came out in the International Safety
    Report that came out of the AI Action Summit in Paris, a
  name: International Safety Report
  position: 24269
- category: unknown
  confidence: medium
  context: International Safety Report that came out of the AI Action Summit in Paris,
    and it has three categories. So the fir
  name: AI Action Summit
  position: 24318
- category: unknown
  confidence: medium
  context: k at this as we all grapple with what this means. But Sarah, as we wrap
    up the show here, I mean, we've cover
  name: But Sarah
  position: 29079
- category: big_tech
  confidence: high
  context: Sarah Bird is the Chief Product Officer of Responsible AI at Microsoft;
    discussion centered on Microsoft's Build conference announcements, Copilot, Azure
    AI Foundry, and Foundry Observability tools.
  name: Microsoft
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Microsoft's AI assistant product, mentioned in the context of its age and
    integration with agentic AI features.
  name: Copilot
  source: llm_enhanced
- category: ai_infrastructure
  confidence: medium
  context: Mentioned as a platform where multi-agentic orchestration capabilities
    might reside within the Microsoft ecosystem.
  name: Azure AI Foundry
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: A specific monitoring system released by Microsoft (at Build) designed
    for monitoring agents, tracking if they go off-task, and ensuring visibility.
  name: Foundry Observability
  source: llm_enhanced
- category: media/education
  confidence: high
  context: The podcast/show hosting the discussion, which focuses on simplifying AI
    and providing practical advice.
  name: Everyday AI Show
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned as a company that has partnered with the podcast host's organization
    for generative AI education.
  name: Adobe
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Mentioned as a company that has partnered with the podcast host's organization
    for generative AI education.
  name: Nvidia
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: A Microsoft product/service mentioned in the context of providing user
    IDs and access control for securing agents.
  name: Entra
  source: llm_enhanced
- category: big_tech
  confidence: high
  context: A Microsoft product/service mentioned in the context of monitoring systems
    for threats.
  name: Defender
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: Mentioned as a platform where developers can build agents that automatically
    receive an identity for governance.
  name: Foundry
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: The source of the 'Agentic UI' research system released at the Build conference.
  name: Microsoft Research
  source: llm_enhanced
- category: organization
  confidence: high
  context: Mentioned as the source of the International Safety Report which provided
    the three risk categories (malfunctions, misuse, systemic risk) being used by
    Microsoft.
  name: AI Action Summit in Paris
  source: llm_enhanced
date: 2025-05-30 15:00:00 +0000
duration: 32
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: human agency and preparing the workforce for a more agentic future. But
    as we wrap here, what
  text: the future of human agency and preparing the workforce for a more agentic
    future. But as we wrap here, what is maybe the one most important takeaway that
    you have for business leaders when it comes to understanding the risk of agentic
    AI and doing it responsibly? Yeah, I think you have to go into it eyes open.
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://pscrb.fm/rss/p/www.buzzsprout.com/2175779/episodes/17253717-ep-536-agentic-ai-the-risks-and-how-to-tackle-them-responsibly.mp3
processing_date: 2025-10-05 13:29:25 +0000
quotes:
- length: 213
  relevance_score: 6
  text: 'You know, I think that when we switched when generative AI started and we
    were in the era of the chatbot, right, I think that a lot of the focus in responsible
    AI was just: Is this system producing harmful content'
  topics: []
- length: 219
  relevance_score: 5
  text: But maybe specifically when it comes to responsible AI and agentic AI in your
    own internal testing, what has been maybe the biggest surprise or learning that
    you think would be helpful for business leaders to understand
  topics: []
- length: 195
  relevance_score: 5
  text: So whether you're looking for ChatGPT training for thousands or just need
    help building your front-end AI strategy, you can partner with us just like some
    of the biggest companies in the world do
  topics: []
- length: 270
  relevance_score: 4
  text: I don't think it was ever simple necessarily, but with the rate of change
    when it comes to generative AI and everything we've seen from big tech companies
    everywhere with agentic AI and not even that multi-agentic AI, I think it changes
    responsible AI drastically, right
  topics: []
- length: 81
  relevance_score: 4
  text: But how has even just the growth of agents changed what responsible AI even
    means
  topics:
  - growth
- length: 142
  relevance_score: 4
  text: Maybe your company has been tinkering with large language models for a year
    or more but can't really get traction to find ROI on generative AI
  topics: []
- length: 157
  relevance_score: 4
  text: Companies like Adobe, Microsoft, and Nvidia have partnered with us because
    they trust our expertise in educating the masses around generative AI to get ahead
  topics: []
- length: 81
  relevance_score: 3
  text: We're going to be recapping the most important insights from today's conversation
  topics: []
- length: 129
  relevance_score: 3
  text: So live stream audience, please help me welcome to the show, Sarah Bird, the
    Chief Product Officer of Responsible AI at Microsoft
  topics: []
- length: 147
  relevance_score: 3
  text: But before we dive into this topic, could you please just let everyone know
    what you do as the Chief Product Officer of Responsible AI at Microsoft
  topics: []
- length: 257
  relevance_score: 3
  text: So I feel like before generative AI really took off, I was working in this
    space and I would meet with Microsoft's customers and share what we were doing,
    and they're like, it's so great that Microsoft is doing this, but we're really
    early in our AI journey
  topics: []
- length: 88
  relevance_score: 3
  text: It just seems like it's everywhere now within the Microsoft ecosystem and
    within Copilot
  topics: []
- length: 240
  relevance_score: 3
  text: And we've also lost kind of one of our most important mitigations, which is
    having the human just directly in the loop, having oversight, because now you're
    going to have agents working for longer periods of time without a human in the
    loop
  topics: []
- length: 120
  relevance_score: 3
  text: 'Yeah, I think the biggest thing is exactly that: you''re going to have a
    more complex system that you''re trying to govern'
  topics: []
- length: 256
  relevance_score: 3
  text: And so I don't think maybe 12 months ago I expected that I would be spending
    as much time kind of learning about how all of these traditional sort of security
    patterns work today, but that's where I think that's one of the most important
    things with agents
  topics: []
- length: 180
  relevance_score: 3
  text: But as we wrap here, what is maybe the one most important takeaway that you
    have for business leaders when it comes to understanding the risk of agentic AI
    and doing it responsibly
  topics: []
- length: 46
  relevance_score: 3
  text: Yeah, I think you have to go into it eyes open
  topics: []
- impact_reason: 'Highlights the core thesis: the complexity of Responsible AI has
    exploded due to agentic and multi-agentic systems, moving beyond simple chatbot
    governance.'
  relevance_score: 10
  source: llm_enhanced
  text: Responsible AI used to be much more straightforward. I don't think it was
    ever simple necessarily, but with the rate of change when it comes to generative
    AI and everything we've seen from big tech companies everywhere with agentic AI
    and not even that multi-agentic AI, I think it changes responsible AI drastically.
  topic: safety/predictions
- impact_reason: 'Identifies the next frontier of complexity: inter-agent communication
    and autonomous task delegation, which complicates accountability.'
  relevance_score: 10
  source: llm_enhanced
  text: And then when it comes to multi-agentic AI, when agents are talking among
    themselves, divvying tasks up and executing on our behalf, how does that change
    things?
  topic: safety/predictions
- impact_reason: Directly links agent autonomy to increased risk surface area and
    higher potential impact from errors or misuse.
  relevance_score: 10
  source: llm_enhanced
  text: But the challenge is that now if an agent is actually able to go and do tasks
    on your behalf, then there's more that can go wrong because it can actually take
    an action. And that can be a bigger surface area, that can have higher implications
    because of the action it's taking.
  topic: safety
- impact_reason: Identifies the critical loss of direct human oversight as agents
    operate autonomously for extended periods.
  relevance_score: 10
  source: llm_enhanced
  text: And we've also lost kind of one of our most important mitigations, which is
    having the human just directly in the loop, having oversight, because now you're
    going to have agents working for longer periods of time without a human in the
    loop.
  topic: safety/technical
- impact_reason: 'Offers a concrete architectural strategy for governing multi-agent
    systems: componentization, specialized testing, and localized guardrails.'
  relevance_score: 10
  source: llm_enhanced
  text: And so you can make each individual agent really good at the smaller tasks
    that it is doing. And so you can test it specifically to do that task. We can
    put guardrails around it that ensure that it's only doing that task, and then
    you can have them coordinate and work together to complete a bigger picture. But
    every single thing is a component, like an assembly line, doing what it needs
    to do.
  topic: technical/strategy
- impact_reason: 'Provides a clear framework for the changing role of human oversight:
    shifting from ''inner loop'' (real-time checking) to ''outer loop'' (monitoring
    and intervention).'
  relevance_score: 10
  source: llm_enhanced
  text: The way we think about it is in the previous era, we had humans really in
    the inner loop, and it's like you just do a small task, and then the human checks,
    and you just do a small task, and the human checks. And so what happens now is
    we're really moving humans more to the outer loop.
  topic: strategy/safety
- impact_reason: 'Summarizes the strategic shift: Human-in-the-Loop (HITL) is not
    eliminated but reallocated to rigorous pre-deployment testing and post-deployment
    monitoring/administration.'
  relevance_score: 10
  source: llm_enhanced
  text: And so we still have to build these same human-in-the-loop mechanisms; it's
    just where the human goes in the loop is different now, and it's much more in
    the pre-development setting what you actually care about and appropriate testing,
    and in the post-deployment monitoring and administration of the system.
  topic: strategy/safety
- impact_reason: Defines the shift in the Human-in-the-Loop (HITL) paradigm from direct
    execution to pre-development design and post-deployment administration/monitoring.
  relevance_score: 10
  source: llm_enhanced
  text: we still have to build these same human-in-the-loop mechanisms; it's just
    where the human goes in the loop is different now, and it's much more in the pre-development
    setting what you actually care about and appropriate testing, and in the post-deployment
    monitoring and administration of the system.
  topic: strategy/HITL
- impact_reason: Explicitly lists critical, modern AI risks (prompt injection, copyright)
    that must be integrated into standard testing protocols.
  relevance_score: 10
  source: llm_enhanced
  text: is the system vulnerable to new types of prompt injection attacks? Or is the
    system producing copyrighted material? These are all different things that you
    want to test for in your application...
  topic: safety/risks
- impact_reason: Advocates for 'co-developing' testing alongside development (Shift
    Left testing for AI), a crucial strategy for quality assurance in iterative AI
    builds.
  relevance_score: 10
  source: llm_enhanced
  text: we start with what is the system supposed to do, and we build the testing
    right alongside with the development of the system so we don't wait till the final
    last mile and then test and find out we have an issue. We're co-developing...
  topic: strategy/testing
- impact_reason: 'Highlights a major shift in responsible AI focus: moving from novel
    AI risks to applying established enterprise security and governance frameworks
    (like identity management) to agents.'
  relevance_score: 10
  source: llm_enhanced
  text: the first question you're asking is, well, how do we secure and govern agents
    in that same way? And so a lot more focus actually on not just the novel risks
    that we see with AI, but just being able to secure and govern AI like any other
    thing.
  topic: safety/governance
- impact_reason: Describes a concrete technical solution (Agent ID) for integrating
    security and governance directly into the development workflow, ensuring compliance
    by default.
  relevance_score: 10
  source: llm_enhanced
  text: a new Entra Agent ID so that agents can be tracked in your system just like
    anything else and making sure that we're connecting this governance and security
    playing with what the developer is doing so that when I build an agent in Foundry
    or in Copilot, it just has an identity already attached so I've done the right
    thing for my organization...
  topic: technical/governance
- impact_reason: Highlights specific, critical security vulnerabilities (data leakage,
    prompt injection) relevant to deployed AI agents.
  relevance_score: 10
  source: llm_enhanced
  text: It could be that it's leaking sensitive data accidentally, right? And those
    are some of the big ones we see with agents. It's vulnerable to prompt injection
    attacks, right? Those are all types of malfunctions.
  topic: safety
- impact_reason: Introduces 'systemic risk' as a crucial, high-level category beyond
    immediate technical failures or misuse.
  relevance_score: 10
  source: llm_enhanced
  text: The last risk, and then something that is very top of mind for me, is the
    systemic risk with AI.
  topic: safety
- impact_reason: Directly links workforce readiness and upskilling to systemic AI
    risk, emphasizing the societal/organizational challenge.
  relevance_score: 10
  source: llm_enhanced
  text: But that is a different way of working, and so preparing the workforce to
    actually be ready for this new skill set and collaborate with these tools, that's
    some systemic type of risk that we need to go and address.
  topic: strategy
- impact_reason: 'Articulates the core challenge for experienced professionals: how
    their established ''agency'' (critical thinking) translates when collaborating
    with advanced agents.'
  relevance_score: 10
  source: llm_enhanced
  text: This is my agency, this critical thinking. So how do we need to get the workforce
    ready for working hand-in-hand with agentic AI?
  topic: business
- impact_reason: 'Delivers a concise, final piece of advice for leaders: acknowledge
    risks transparently before selecting deployment areas.'
  relevance_score: 10
  source: llm_enhanced
  text: What is maybe the one most important takeaway that you have for business leaders
    when it comes to understanding the risk of agentic AI and doing it responsibly?
    Yeah, I think you have to go into it eyes open. You need to know that there are
    risks and understand the risks and pick use cases
  topic: safety
- impact_reason: Poses a critical governance question for business leaders regarding
    autonomous agents, signaling a shift in required oversight.
  relevance_score: 9
  source: llm_enhanced
  text: But what about now when we talk about agentic AI? How does that change the
    ethics, the governance, the responsibility that we as business leaders need to
    have in order to make this thing work the right way?
  topic: safety/business
- impact_reason: 'Illustrates a massive, positive shift in industry maturity: Responsible
    AI has moved from a ''nice-to-have'' afterthought to a prerequisite for adoption.'
  relevance_score: 9
  source: llm_enhanced
  text: I feel like before generative AI really took off, I was working in this space
    and I would meet with Microsoft's customers and share what we were doing, and
    they're like, it's so great that Microsoft is doing this, but we're really early
    in our AI journey. So we have to get a lot more sophisticated before we even think
    about responsible AI. And now the first thing that people do is ask about responsible
    AI before they even get started with AI, which is excellent.
  topic: business/strategy
- impact_reason: 'Articulates the core value proposition of agentic AI: moving from
    conversational interaction to autonomous task completion.'
  relevance_score: 9
  source: llm_enhanced
  text: I think the thing that's amazing about agents and why I think we're seeing
    so much growth is they really are, I think, a more complete vision of the promise
    of this technology, right? I don't want a system that I just chat with. I want
    a system that's going to go and complete tasks for me so that I don't have to
    think about it.
  topic: predictions/technical
- impact_reason: 'Provides a practical, current definition of multi-agent systems:
    task decomposition for manageable subtasks, which aids in responsible deployment.'
  relevance_score: 9
  source: llm_enhanced
  text: I think what we're seeing in practice right now is something much simpler,
    but still extremely powerful, and I think much easier to deal with from a responsible
    AI point of view, which is that people are really using multi-agent systems to
    break their task into a bunch of subtasks.
  topic: technical/strategy
- impact_reason: Emphasizes the necessity of granular governance—governing components
    rather than just the emergent system behavior.
  relevance_score: 9
  source: llm_enhanced
  text: You do need to break it into these components so that you're still governing
    each individual agent and not just the system as a whole because you still need
    visibility into what's happening in there.
  topic: safety/strategy
- impact_reason: Highlights a specific, practical tool (Foundry Observability) designed
    to solve the governance challenge of tracking agent behavior (task adherence,
    tool selection).
  relevance_score: 9
  source: llm_enhanced
  text: 'One of the areas that we released at Build was Foundry Observability, and
    this is exactly giving you a monitoring system for agents so you can see: did
    the agent go off task? Is it struggling to find the right tool for the job? And
    all that.'
  topic: technical/business
- impact_reason: 'Defines the temporal shift in human-AI interaction: moving from
    immediate feedback loops to long-running, asynchronous tasks.'
  relevance_score: 9
  source: llm_enhanced
  text: I think we've always been trained, at least in the first year of generative
    AI, it's like, okay, I sit, I talk with an AI, I look, I see what it sends back.
    But now it might be many minutes or multiple hours in the very near future where
    these agents are going and doing work.
  topic: predictions/technical
- impact_reason: 'Details the specific metrics required for effective outer-loop monitoring:
    task adherence, intent understanding, and tool selection accuracy.'
  relevance_score: 9
  source: llm_enhanced
  text: Our specific monitoring inside of Foundry Observability, or guardrails that
    look and say, okay, how well is the agent doing at staying on task? How well is
    the agent doing at understanding the user intent? How accurate is it in picking
    the right tool for the job?
  topic: technical/safety
- impact_reason: Highlights the critical need for human oversight and intervention
    mechanisms in autonomous AI systems (agents).
  relevance_score: 9
  source: llm_enhanced
  text: eed to be able to monitor it so if it's going off task, then a human can come
    in and intervene.
  topic: safety/strategy
- impact_reason: Poses the central challenge of testing complex, multi-agent systems,
    signaling a major hurdle for enterprise adoption.
  relevance_score: 9
  source: llm_enhanced
  text: How should organizations be testing, you know, multi-agentic systems? It seems
    like a herculean task.
  topic: strategy/testing
- impact_reason: Emphasizes that testing must evolve to cover novel AI risks (like
    prompt injection or copyright infringement), necessitating specialized testing
    platforms.
  relevance_score: 9
  source: llm_enhanced
  text: what we need to test for are different behaviors and different types of risk.
    So a lot of what we've been building... is building testing systems inside of
    Foundry that people can build on top of and people can use to test for these new
    types of risk.
  topic: safety/testing
- impact_reason: Identifies 'trust' built via rigorous testing as the primary bottleneck
    preventing faster AI adoption/deployment.
  relevance_score: 9
  source: llm_enhanced
  text: the thing that is sort of delaying them into getting an introduction is building
    that trust, and some of that comes through testing.
  topic: business/adoption
- impact_reason: Provides a strong, near-term prediction/statistic on the massive
    scale of agent deployment across the workforce, signaling urgency for governance
    solutions.
  relevance_score: 9
  source: llm_enhanced
  text: I think 81% of employers are looking at deploying agents alongside their workforce
    in the next 18 months, right?
  topic: predictions/business
- impact_reason: Raises the crucial question about the necessary upskilling required
    for human operators monitoring advanced, autonomous AI systems.
  relevance_score: 9
  source: llm_enhanced
  text: does this human-in-the-loop model create a different level of users with higher
    skill levels to understand hallucinations and derailment?
  topic: safety/HITL
- impact_reason: Explains that advanced HITL roles shift from micro-validation to
    macro-statistical decision-making based on aggregated performance data.
  relevance_score: 9
  source: llm_enhanced
  text: If you're in a different point in the loop, you are doing a different job,
    right? ... You're often then looking at aggregates and looking at numbers overall
    and saying, 'Okay, if 99.8% of the time it does the right thing, is that good
    enough for my task?'
  topic: strategy/HITL
- impact_reason: Establishes 'Malfunctions' as the first core category of AI risk,
    covering errors, deviations, and data leaks.
  relevance_score: 9
  source: llm_enhanced
  text: The first is malfunctions, and that is the AI system doing something that
    it's not supposed to be doing. And that could, for example, be producing harmful
    content, or that could be it getting confused and going off task. It could be
    that it's leaking sensitive data accidentally, right?
  topic: safety/risks
- impact_reason: 'Breaks down ''Misuse'' into two distinct sub-categories: unintentional
    misuse (due to lack of understanding) and intentional misuse (malicious actors).'
  relevance_score: 9
  source: llm_enhanced
  text: The next category is misuse, and you can see kind of two types of misuse.
    I might misuse an AI system because I don't understand what it does... And then
    of course, unfortunately, we live in a world where people are also going to understand
    what it does. People are also going to intentionally misuse these systems...
  topic: safety/risks
- impact_reason: Addresses the reality of intentional misuse and the need for traditional
    security measures (guardrails, monitoring) alongside technical fixes.
  relevance_score: 9
  source: llm_enhanced
  text: And then of course, unfortunately, we live in a world where people are also
    going to understand what it does. People are also going to intentionally misuse
    these systems, and there we look at where we can have guardrails and monitoring
    and defenses and traditional security approaches for that.
  topic: safety
- impact_reason: Clearly delineates the different mitigation strategies required for
    technical risks (malfunctions) versus systemic risks (policy/education).
  relevance_score: 9
  source: llm_enhanced
  text: With malfunctions, going much more technical, and then with systemic risk,
    we're looking much more at policy and education and upskilling programs and a
    very different type of work to address those risks.
  topic: strategy
- impact_reason: Identifies motivation and cultural inertia as the primary barrier
    to AI adoption, overriding technical capability.
  relevance_score: 9
  source: llm_enhanced
  text: First, I think it's motivating people to want to do it. I mean, if you've
    been doing your job the same way for 20 years, depending on your personality,
    you might not want to just go randomly pick up new tools.
  topic: business
- impact_reason: 'Defines the necessary psychological trigger for widespread adoption:
    experiencing a tangible, positive job transformation.'
  relevance_score: 9
  source: llm_enhanced
  text: But we need everyone to have that moment where they see how it changes their
    job in a good way, and then that makes you want to try more.
  topic: business
- impact_reason: Emphasizes the current reality of AI utility—it's uneven—and promotes
    peer-to-peer learning to map out successful patterns.
  relevance_score: 9
  source: llm_enhanced
  text: The tools work well for some things; they don't work well for everything.
    So one of the things that we do even within my own team at Microsoft is have learning
    sessions where people share, 'Look, I built an agent that does this, and isn't
    it cool? And it worked really well.' And, 'I've tried this, and I'm struggling
    to get it to work.'
  topic: business
- impact_reason: 'Defines the mission of a Chief Product Officer of Responsible AI:
    risk identification, mitigation, testing, and democratization of solutions.'
  relevance_score: 8
  source: llm_enhanced
  text: At the core, we look at kind of risk we see emerging in new AI systems and
    then figure out how do we actually go address those risks? What does it take to
    test them? What does it take to mitigate them? And then how do we make it easy
    for everyone to do that?
  topic: strategy/business
- impact_reason: Categorizes agents as a novel entity requiring new governance frameworks
    beyond existing user/application security models.
  relevance_score: 8
  source: llm_enhanced
  text: Agents are users and applications and devices and all sorts of things today,
    but we also need to address those new types of risks that agents bring in terms
    of being able to go off task or accidentally leak sensitive data.
  topic: safety/technical
- impact_reason: Suggests that the core governance principle (guardrails per entity)
    remains consistent, even as the system complexity increases.
  relevance_score: 8
  source: llm_enhanced
  text: And so even though they're multi-agent systems and they're combining together,
    I don't think that that has to look different than a single agent working with
    a human. You can have different types of entities in the system, and you want
    to make sure that you're governing each of them and you're having guardrails around
    each of them.
  topic: safety/strategy
- impact_reason: Stresses that outer-loop control requires robust monitoring systems
    capable of detecting deviations that trigger human intervention.
  relevance_score: 8
  source: llm_enhanced
  text: But then you also need to be able to monitor it so if it's going off task,
    then a human can come in and intervene.
  topic: safety
- impact_reason: Describes the automated alerting mechanism necessary for outer-loop
    systems, distinguishing between administrator and end-user alerts.
  relevance_score: 8
  source: llm_enhanced
  text: And if any of those seem to be not performing well, our system in Foundry
    is going to detect that issue and go and alert the human, either the human administrator
    or the human user depending on the application and what makes sense there.
  topic: technical
- impact_reason: 'Provides a foundational, scalable approach to testing complex AI
    systems: component testing followed by system-level testing.'
  relevance_score: 8
  source: llm_enhanced
  text: you test the components, and then you test the system, and you build up a
    testing paradigm.
  topic: technical/testing
- impact_reason: Identifies the Human-AI Interface (HAI) for agent monitoring as an
    underdeveloped area ripe for innovation.
  relevance_score: 8
  source: llm_enhanced
  text: I'd actually love to see a lot more innovation in the human-AI interface and
    how we design it for the world of agents where humans are farther out in the loop.
  topic: technical/interface
- impact_reason: 'Defines the goal of future AI interfaces: acting as a translator
    or bridge between complex AI outputs and human intent/governance needs.'
  relevance_score: 8
  source: llm_enhanced
  text: where we can provide tools that bridge the gap between what the AI system
    is doing and what the user or the administrator wants to specify, then we can
    help make those interfaces actually feel natural, and AI and humans work together.
  topic: strategy/interface
- impact_reason: Provides a strong testimonial on the transformative power of GenAI,
    suggesting it unlocks capabilities previously impossible.
  relevance_score: 8
  source: llm_enhanced
  text: Most of the systems I told you that we were developing to address these different
    risks, those are AI-powered. Those are all things we couldn't have done five years
    ago without generative AI.
  topic: technical
- impact_reason: Advocates for collaborative, shared learning environments over siloed
    experimentation for effective AI integration.
  relevance_score: 8
  source: llm_enhanced
  text: And so having people learn from each other about the patterns of what's working
    and what's not, and really having this be a shared learning journey and not something
    that just everyone is on in their own.
  topic: strategy
- impact_reason: 'Provides a timeline perspective: current adoption requires experimentation,
    while future adoption will rely on standardized best practices.'
  relevance_score: 8
  source: llm_enhanced
  text: As the technology gets more mature, and we have sort of more standard patterns,
    then it will be easier to say, 'This is how you use it; go do that.' But in the
    earlier days right now, it's a lot of also getting people to experiment and find
    the things that are going to work best for them and their job.
  topic: strategy
- impact_reason: Highlights ongoing research into novel interfaces specifically designed
    for agent interaction and monitoring.
  relevance_score: 7
  source: llm_enhanced
  text: one of the things we released at Build coming out of Microsoft Research is
    a system that is called the Agentic UI, and it is a research system for people
    to play with and experiment with different interfaces for users to interact with
    agents basically.
  topic: technical/research
- impact_reason: Introduces the third major risk category from the Paris AI Safety
    Report, suggesting a comprehensive framework for risk assessment.
  relevance_score: 7
  source: llm_enhanced
  text: 'The last risk, and then something that is [implied: harm/societal impact,
    based on context of the Paris report structure].'
  topic: safety/risks
source: Unknown Source
summary: '## Podcast Episode Summary: EP 536: Agentic AI - The risks and how to tackle
  them responsibly


  This episode of the Everyday AI Show features Sarah Bird, Chief Product Officer
  of Responsible AI at Microsoft, discussing the significant shift in responsible
  AI practices necessitated by the rise of **Agentic AI** and **Multi-Agentic Systems**.
  The core narrative revolves around how moving from simple human-chatbot interaction
  to autonomous, task-executing agents drastically increases the surface area for
  risk, demanding new governance, testing, and monitoring paradigms.


  ---


  **1. Focus Area:**

  The discussion centers on **Responsible AI (RAI)** governance, security, and testing
  specifically tailored for **Agentic AI** and **Multi-Agentic Orchestration** systems,
  contrasting these new challenges with the more straightforward RAI concerns of earlier
  generative AI models.


  **2. Key Technical Insights:**

  *   **Shift in Human Oversight:** The paradigm is moving from humans being "in the
  inner loop" (checking every small step) to being "in the outer loop" (monitoring
  long-running tasks and intervening when alerts trigger).

  *   **Component-Based Governance:** Multi-agent systems, often used for task decomposition
  (assembly line style), must be governed at the **individual agent level** rather
  than just the system boundary, requiring granular visibility.

  *   **Agent Identity and Security:** Agents must be treated as new entities requiring
  robust security protocols, similar to users or devices. Microsoft is addressing
  this with tools like **Entra Agent ID** for tracking and access control.


  **3. Business/Investment Angle:**

  *   **High Adoption Rate:** Agent deployment is imminent, with 81% of employers
  looking to deploy agents alongside their workforce in the next 18 months, signaling
  a major operational shift.

  *   **Testing as a Prerequisite for Trust:** Organizations are often delaying deployment
  because they underestimate the necessary testing rigor, highlighting an immediate
  need for mature testing frameworks to build organizational trust.

  *   **Focus on Traditional Security:** A significant portion of the new RAI effort
  involves applying established security patterns (like access control and threat
  monitoring) to agents, indicating that securing these entities is as crucial as
  addressing novel AI risks.


  **4. Notable Companies/People:**

  *   **Sarah Bird (Chief Product Officer of Responsible AI, Microsoft):** The expert
  guest detailing Microsoft''s approach to building tools and frameworks for governing
  agentic systems.

  *   **Microsoft:** Mentioned for recent announcements at Build, including **Foundry
  Observability** (for monitoring agent performance and task adherence) and **Entra
  Agent ID**.

  *   **Microsoft Research:** Mentioned for developing the **Agentic UI**, a research
  system exploring optimal human-agent interaction patterns.


  **5. Future Implications:**

  The industry is moving toward complex, autonomous workflows where agents operate
  for extended durations without direct human input. This necessitates innovation
  in **human-AI interfaces (Agentic UI)** to effectively communicate aggregate performance
  data and intervention points to humans whose skill sets must adapt to monitoring
  performance metrics rather than individual outputs.


  **6. Target Audience:**

  **AI/ML Professionals, Product Managers, Security/Governance Officers, and Business
  Leaders** involved in deploying or scaling AI solutions, particularly those moving
  beyond simple chatbots into autonomous workflow automation.


  ---


  ### Comprehensive Summary


  The podcast episode addresses the critical evolution of Responsible AI (RAI) as
  the industry transitions from basic conversational AI to sophisticated **Agentic
  AI** and **Multi-Agentic Systems**. Host Jordan Wilson opens by noting that the
  complexity of agents—which can take actions on behalf of users for extended periods—fundamentally
  changes governance requirements compared to single-turn interactions.


  Sarah Bird, Microsoft’s CPO of Responsible AI, confirms this shift, noting that
  while awareness of RAI has dramatically increased, the technical challenge has grown
  because agents introduce a larger "surface area" for potential failure or misuse.
  When agents execute tasks autonomously, the traditional mitigation of having the
  human "in the loop" for every step is lost, pushing humans to the "outer loop" for
  monitoring and intervention.


  Bird clarifies that multi-agent orchestration, in practice, often involves **task
  decomposition**, where one large goal is broken into smaller, manageable subtasks
  assigned to specialized agents. This componentized approach is beneficial for RAI
  because each agent can be individually tested and governed. However, this complexity
  requires robust **observability**. Microsoft’s **Foundry Observability** tool is
  highlighted as essential for monitoring individual agent behavior, ensuring they
  stay on task, use the correct tools, and adhere to user intent, which is crucial
  for debugging and maintaining security practices like least privilege access across
  the system.


  A major theme is the need to secure agents as **first-class entities** within organizational
  IT infrastructure. Bird notes that the focus has broadened from novel AI risks (like
  harmful content) to ensuring agents are governed using traditional security frameworks.
  The introduction of **Entra Agent ID** exemplifies this, ensuring agents are tracked
  and controlled just like human users or devices.


  The discussion also touches on the evolving role of the human supervisor. As agents
  work longer, humans must shift from micro-management to interpreting aggregate data
  (e.g., "Is the system performing correctly 99.8% of the time?"). This requires new
  interfaces, like the research-based **Agentic UI**, designed to surface meaningful
  intervention points based on performance metrics rather than raw output review.


  Finally, Bird categorizes risks into three main areas, referencing the International
  Safety Report framework: **Malfunctions** (e.g., going off-task, data leakage, prompt
  injection), and **Misuse** (which includes both user misunderstanding and malicious
  intent). The conversation underscores that building trust in these powerful, autonomous
  systems hinges on rigorous, co-developed testing that starts early in the development
  cycle, not as a last'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- microsoft
- nvidia
title: 'EP 536: Agentic AI - The risks and how to tackle them responsibly'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 95
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 12
  prominence: 1.0
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 1
  prominence: 0.1
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-05 13:29:25 UTC -->
