---
companies:
- category: unknown
  confidence: medium
  context: llo, Protocols, Packets, and Programs. Welcome to Scary October. You'll
    hear supply chains rattling. You'll see L
  name: Scary October
  position: 52
- category: unknown
  confidence: medium
  context: e on your own. Which means this week we talk with Nico Weisman about creating
    LLMs to be bug bounty hunters and
  name: Nico Weisman
  position: 295
- category: unknown
  confidence: medium
  context: care about. Grab a flashlight and stay tuned for Application Security Weekly.
    For the latest app-sec news, it's time for Appli
  name: Application Security Weekly
  position: 445
- category: unknown
  confidence: medium
  context: essary packages. Just what your app needs to run. Drop Minimus images into
    existing deployments to dramatically
  name: Drop Minimus
  position: 748
- category: unknown
  confidence: medium
  context: ats don't wait, and neither should your defenses. With ThreatLocker, you're
    not just reacting to threats, you're stop
  name: With ThreatLocker
  position: 1059
- category: unknown
  confidence: medium
  context: ode 351, recorded October 6, 2025. I'm your host, Mike Schemm, and I'm
    here with one announcement, but no co-ho
  name: Mike Schemm
  position: 1629
- category: unknown
  confidence: medium
  context: s, as you might remember for October, to not miss InvoSec World 2025. It's
    October 27th to 29th at Disney's Coron
  name: InvoSec World
  position: 1772
- category: unknown
  confidence: medium
  context: World 2025. It's October 27th to 29th at Disney's Coronado Springs Resort.
    It'll be a ton of cybersecurity pros, workshops
  name: Coronado Springs Resort
  position: 1830
- category: unknown
  confidence: medium
  context: nd presented at conferences, at the very least of Black Hat, Paxek, Siskan,
    and Cocoa Party, and many, many m
  name: Black Hat
  position: 2461
- category: unknown
  confidence: medium
  context: t the very least of Black Hat, Paxek, Siskan, and Cocoa Party, and many,
    many more. And he's taking the time to
  name: Cocoa Party
  position: 2491
- category: unknown
  confidence: medium
  context: very specifically, one of the things that brought Crossbow Expo onto my
    radar was the LLM Bug Bounty hunting that
  name: Crossbow Expo
  position: 2813
- category: unknown
  confidence: medium
  context: that brought Crossbow Expo onto my radar was the LLM Bug Bounty hunting
    that your company has been doing. But of
  name: LLM Bug Bounty
  position: 2849
- category: unknown
  confidence: medium
  context: d to do away with, but to get some LLM help here. So I started very young.
    I'm assuming everyone in a lo
  name: So I
  position: 3274
- category: unknown
  confidence: medium
  context: hobby, like playing security in my early years in Buenos Aires, Argentina.
    And as many people that started a lon
  name: Buenos Aires
  position: 3408
- category: tech
  confidence: high
  context: t was like, you need to convince why? You need to replicate what an attacker
    will do in your environment and
  name: Replicate
  position: 3883
- category: unknown
  confidence: medium
  context: modern days for like AIs performing pen testing. Now I feel like I'm having
    these conversations that I u
  name: Now I
  position: 4065
- category: unknown
  confidence: medium
  context: understand and react to that behavior. So, yeah. And I think one of the
    things you hit on there too was
  name: And I
  position: 4754
- category: unknown
  confidence: medium
  context: w that doesn't work. And we've seen, for example, Daniel Stenberg of the
    curl project, he's been very vocal on the
  name: Daniel Stenberg
  position: 16387
- category: unknown
  confidence: medium
  context: dea, the answer prompt morphizes it a little bit. The LLM just thinking,
    oh, what if there was a CVE? Let m
  name: The LLM
  position: 20969
- category: unknown
  confidence: medium
  context: ot real, continue through the process. Excellent. Generative AI is powerful,
    but it comes with real risks, from p
  name: Generative AI
  position: 21430
- category: unknown
  confidence: medium
  context: njection to data leakage, misuse, and agentic AI. The OWASP Gen AI Security
    Project unites a global community of expert practitioners
  name: The OWASP Gen AI Security Project
  position: 21550
- category: unknown
  confidence: medium
  context: ilt practical playbooks, adoption guides, and the OWASP Top 10 for LLM
    resources any team can apply right now
  name: OWASP Top
  position: 21743
- category: unknown
  confidence: medium
  context: ll. And partially too, we just saw this summer at DEF CON, the AI Cyber
    Challenge, and about half of the ap
  name: DEF CON
  position: 22256
- category: unknown
  confidence: medium
  context: ally too, we just saw this summer at DEF CON, the AI Cyber Challenge, and
    about half of the approaches were an LLM on
  name: AI Cyber Challenge
  position: 22269
- category: tech
  confidence: high
  context: al information. And so we are seeing from the new OpenAI, like the 5.0,
    have seen this really good thinkin
  name: Openai
  position: 25573
- category: unknown
  confidence: medium
  context: wo months from now, there'll be a new, fancy one. But I guess I will just
    so I don't completely ignore th
  name: But I
  position: 41942
- category: tech
  confidence: high
  context: or families of models, like OpenAI and Gemini, or Anthropic and Gemini,
    or Gemini or OpenAI and Anthropic. Wh
  name: Anthropic
  position: 44510
- category: ai_application
  confidence: high
  context: The company developing and deploying an LLM specifically designed to act
    as a bug bounty hunter, climbing leaderboards by finding vulnerabilities like
    XSS and SQL injection.
  name: Crossbow Expo (X-Bow)
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: A company where the interviewee (Nico Weisman) previously held a leadership
    role, associated with penetration testing.
  name: Immunity
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: The bug bounty platform used by Crossbow Expo to test and validate their
    LLM's capabilities against real production environments.
  name: HackerOne
  source: llm_enhanced
- category: security_tooling
  confidence: medium
  context: A company providing minimal container images to reduce the attack surface,
    mentioned in an advertisement segment.
  name: Minimus
  source: llm_enhanced
- category: security_tooling
  confidence: low
  context: A company offering zero-trust endpoint protection, mentioned in an advertisement
    segment.
  name: ThreatLocker
  source: llm_enhanced
- category: software_security
  confidence: low
  context: A company where Nico Weisman previously held a leadership role.
  name: GitHub
  source: llm_enhanced
- category: software_security
  confidence: low
  context: A company where Nico Weisman previously held a leadership role.
  name: Semmel
  source: llm_enhanced
- category: software_security
  confidence: low
  context: A company where Nico Weisman previously held a leadership role.
  name: Sixterra
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: Mentioned in the context of newer models (like the hypothetical ChatGPT-5
    or 5.0) being better at 'cheating' validators and having 'really good thinking
    models'.
  name: OpenAI
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: An organization uniting experts to tackle generative AI security challenges,
    producing playbooks and the OWASP Top 10 for LLM.
  name: OWASP Gen AI Security Project
  source: llm_enhanced
- category: ai_research
  confidence: high
  context: Mentioned via the 'OWASP Top 10 for LLM' resource, indicating involvement
    in security standards relevant to AI.
  name: OWASP
  source: llm_enhanced
- category: ai_application (Security)
  confidence: high
  context: The AI penetration testing product/system developed by the speaker's current
    company, which uses LLMs as its 'brain' or coordinator.
  name: X-Bow
  source: llm_enhanced
- category: ai_infrastructure/model_provider
  confidence: high
  context: Mentioned as a family of AI models (likely Google's) used in their 'alloy
    models' approach.
  name: Gemini
  source: llm_enhanced
- category: ai_infrastructure/model_provider
  confidence: high
  context: Mentioned as a family of AI models used in their 'alloy models' approach.
  name: Anthropic
  source: llm_enhanced
- category: ai_infrastructure/model_provider
  confidence: high
  context: Mentioned as a model that, at one point, was the best for offensive security
    testing.
  name: Grok
  source: llm_enhanced
- category: ai_infrastructure/model_provider
  confidence: high
  context: Mentioned as a specific, newer model release that provided amazing results.
  name: ChatGPT-5
  source: llm_enhanced
- category: ai_infrastructure/model_provider
  confidence: medium
  context: The entity/company that released the Grok model, implied by the context
    ('we saw the company, we never even imagined that X would come out with a model').
  name: X
  source: llm_enhanced
- category: Organization (General/Partner)
  confidence: medium
  context: An external entity hired to help build and validate their security benchmarks.
  name: (Third Party)
  source: llm_enhanced
- category: ai_research/community
  confidence: medium
  context: Entities whose open-source projects were used to help build vulnerability-focused
    benchmarks.
  name: (Open-Source Projects)
  source: llm_enhanced
date: 2025-10-07 09:00:00 +0000
duration: 54
has_transcript: false
insights:
- actionable: false
  confidence: medium
  extracted: penetration testing
  text: 'the future of penetration testing is going to look like: use LLM penetration
    testing tools to basically cover all the bases, the annoying part, and then take
    your human expertise and go deep range into finding the more fun bugs, to be quiet.'
  type: prediction
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/7c13abd38ab548aa9fad61188902b14a/
processing_date: 2025-10-08 02:45:42 +0000
quotes:
- length: 197
  relevance_score: 5
  text: I'm glad you have to hear in person as a human, because very specifically,
    one of the things that brought Crossbow Expo onto my radar was the LLM Bug Bounty
    hunting that your company has been doing
  topics: []
- length: 97
  relevance_score: 5
  text: But this is why you have to always build a really good set of benchmarks for
    anything LLM related
  topics: []
- length: 200
  relevance_score: 4
  text: We have all the actions that the LLM took, that includes the reasoning of
    why X-Bow, the pen tester, is performing an action, what is the action that is
    performed, and what is the output of the action
  topics: []
- length: 155
  relevance_score: 4
  text: What we did that was very important, and definitely encourage anyone that's
    building anything on LLMs, is you need to build a very solid base of benchmarks
  topics: []
- length: 182
  relevance_score: 3
  text: I remember in my first years, when I was actually working doing pen testing,
    you have to actually convince the customer that a pen test was required and why
    pen testing was important
  topics: []
- length: 94
  relevance_score: 3
  text: You have to also figure out how you are ready for a future where AIs will
    be doing pen testing
  topics: []
- length: 88
  relevance_score: 3
  text: It feels like 20 years ago when you have to convince people about, no, hackers
    are there
  topics: []
- length: 89
  relevance_score: 3
  text: Then you have to build these design partners, but not every company is equal
    to the other
  topics: []
- length: 40
  relevance_score: 3
  text: So you have to kind of optimize that ROI
  topics: []
- length: 80
  relevance_score: 3
  text: But the reality is, after a couple of demos, I was like, this CVE is interesting
  topics: []
- length: 281
  relevance_score: 3
  text: You give them developer documentation, even could be a PDF with information,
    and it will read that document and better understand the workflow of the tool
    itself, so you can see, first you have to use this API to log in, then get this
    stuff, and then go and reach out this endpoint
  topics: []
- length: 139
  relevance_score: 3
  text: We just have to say that at Immunity, if you don't find a bug, you have to
    actually write a report, because that's when you have to justify
  topics: []
- length: 88
  relevance_score: 3
  text: If you don't find anything, you have to now explain all you have done and
    how you did it
  topics: []
- length: 122
  relevance_score: 3
  text: So, what we found is that for every action that you do, you flip a coin and
    decide which of the two models you want to use
  topics: []
- length: 85
  relevance_score: 3
  text: So, you have to constantly be able to benchmark that information and build
    up on that
  topics: []
- length: 71
  relevance_score: 3
  text: And so, if prompting better finds more bugs, that's what you have to do
  topics: []
- length: 161
  relevance_score: 3
  text: And then later in the future, you can move to a different type of prompting
    model, but you have to constantly be able to benchmark and work around those problems
  topics: []
- impact_reason: 'This is the core thesis of the episode: using LLMs for sophisticated,
    human-relevant vulnerability discovery, moving beyond simple scanning.'
  relevance_score: 10
  source: llm_enhanced
  text: This week we talk with Nico Weisman about creating LLMs to be bug bounty hunters
    and to find the kinds of vulns that humans care about.
  topic: AI Technology Trends
- impact_reason: Provides a specific technical insight into mitigating a major LLM
    weakness (hallucination/false positives) using a secondary verification mechanism
    ('validators').
  relevance_score: 10
  source: llm_enhanced
  text: The product is very focused on having very low false positives. And we do
    that with a thing called validators, which is going to think of it as a second
    pair of eyes. When the LLM finds a vulnerability, we have this second profile
    that looks at the finding, I would say, yes, this is a real vulnerability or not.
  topic: Technical Insights
- impact_reason: 'A critical technical confirmation: the LLM autonomously handled
    complex state management, workflow navigation, and business logic interaction,
    demonstrating advanced capability beyond simple request fuzzing.'
  relevance_score: 10
  source: llm_enhanced
  text: How did you get the LLM to interact with the site very well? Because there
    absolutely is just an art to finding links, setting up state, understanding workflows,
    business logic. Was that also part of the LLM, or was this other just humans behind
    the scene helping it along? How did that look? No, absolutely. There was no human
    helping in the process, so it was all the LLM.
  topic: Technical Breakthroughs
- impact_reason: Positions LLMs as a revolutionary tool for automating the discovery
    phase (crawling and state management) in security testing, traditionally done
    manually with tools like Burp Suite.
  relevance_score: 10
  source: llm_enhanced
  text: LLMs are almost like full out of heaven for us, because suddenly you have
    these models that can interact with a real browser, and you can basically have
    what you used to do with Burp, which is browsing the website around and seeing
    the proxy getting slowly collecting all this data.
  topic: technical/breakthroughs
- impact_reason: 'A counter-intuitive and highly impactful strategic view: reframing
    LLM hallucination from a bug to a feature when paired with robust validation mechanisms.'
  relevance_score: 10
  source: llm_enhanced
  text: We consider hallucination almost like a feature of the product. And I think
    that in general, if you are able to manage the context around the hallucination,
    if you have a very good success criteria, then hallucination plays on your side.
  topic: strategy/technical
- impact_reason: A powerful anecdote demonstrating that LLM 'hallucination' (in this
    case, referencing a non-existent CVE) can still lead to the discovery of a real,
    previously unknown vulnerability.
  relevance_score: 10
  source: llm_enhanced
  text: And there's an interesting case of a vulnerability that we found that basically
    was an XXE, and it was on a product from Akamai. We posted a blog post about it...
    I went online and walked the customer through what exactly X-Bow does... I was
    like, this CVE is interesting. I'm going to go and search for that. It didn't
    exist at all. There's no CVE at all. But the hallucination was the initial step
    to discovering that on that specific endpoint, there is an XXE.
  topic: technical/breakthroughs
- impact_reason: 'Describes an ongoing adversarial arms race: as LLMs become smarter,
    they become better at bypassing security validators, requiring constant updates
    to the validation logic.'
  relevance_score: 10
  source: llm_enhanced
  text: Every new model that is better than the other one, like ChatGPT-5 or whatever,
    is also better at cheating. So we have this validator that basically is a second
    profile... The better the elements are, they will come out with new tricks on
    how to make the validator tell that the vulnerability is real when it's not.
  topic: safety/technical
- impact_reason: 'Defines the core strength of LLMs in security testing: their ability
    to execute complex, multi-step, iterative attack chains that mimic human reasoning
    (Observe, Refine, Attack, Repeat).'
  relevance_score: 10
  source: llm_enhanced
  text: What we find with the LLM is that no matter what type of vulnerability class...
    LLMs are really good at following similar paths like a human. That is, they will
    gather information, they will try something, they will refine that attack that
    it made, they will reflect on it, and then they'll repeat it. We've constantly
    seen that you can see the flow of the attack chain.
  topic: technical/breakthroughs
- impact_reason: Provides a strategic breakdown of penetration testing effort, suggesting
    LLMs are ideal for automating the predictable 80% (known vulnerabilities).
  relevance_score: 10
  source: llm_enhanced
  text: there's 80% of penetration test is going for the OWASP Top 10 and finding
    all these known vulnerabilities out there, right? And that last 20% of the penetration
    test is where you have the time to be a little more creative and you have a good
    knowledge of the application.
  topic: strategy/business advice
- impact_reason: 'A clear prediction on the future division of labor between AI and
    human pentesters: AI handles the routine, humans handle the novel/creative.'
  relevance_score: 10
  source: llm_enhanced
  text: 'That is exactly what I think the future of penetration testing is going to
    look like: use LLM penetration testing tools to basically cover all the bases,
    the annoying part, and then take your human expertise and go deep range into finding
    the more fun bugs, to be quiet.'
  topic: predictions/strategy
- impact_reason: 'Describes a key architectural strategy for mitigating LLM non-determinism:
    breaking down complex tasks into smaller, manageable, repeatable ''unit tests''
    managed by a coordinator.'
  relevance_score: 10
  source: llm_enhanced
  text: What we do in terms of non-determinism is try to make the problems smaller.
    So rather than just telling this is an application, go and find bugs, we try to
    transform the attack, especially attack chains, into small unit tests.
  topic: technical/mitigation
- impact_reason: Details the superior auditability and data capture capabilities of
    AI-driven pen testing (X-Bow) compared to human testing, providing a full trace
    of reasoning and actions.
  relevance_score: 10
  source: llm_enhanced
  text: The good thing about AI penetration testing is that it's all built by a computer.
    So basically, what we do is we collect everything that we do over the penetration
    test. We have every network packet that was sent and received from the attacker.
    We have all the actions that the LLM took, that includes the reasoning of why
    X-Bow, the pen tester, is performing an action, what is the action that is performed,
    and what is the output of the action.
  topic: technical/AI product insight
- impact_reason: 'Highlights an unexpected, high-value secondary benefit of AI scanning:
    automated asset discovery and mapping, which is valuable even without finding
    vulnerabilities.'
  relevance_score: 10
  source: llm_enhanced
  text: getting an inventory of the API endpoints and getting a heat map or a coverage
    map of just what was tested and what wasn't just feels very helpful to then make
    some human-based decisions on where to invest, where to go and look at the code
    in more detail
  topic: business/unexpected value
- impact_reason: A powerful anecdote showing AI security tools solving fundamental
    data management problems (API inventory) that human processes often neglect.
  relevance_score: 10
  source: llm_enhanced
  text: He was like, can you actually create an OpenAPI spec for me after all the
    findings you got? I was like, why are you asking that? Is because I don't have
    that data. The engineer had not put a good inventory of all our endpoints, and
    suddenly X-Bow can help me with that.
  topic: business/AI adoption insight
- impact_reason: 'Crucial architectural advice: prioritize model agnosticism over
    deep integration with any single LLM family to future-proof the product against
    rapid model evolution.'
  relevance_score: 10
  source: llm_enhanced
  text: we were smart enough, and honestly, we have a really amazing Head of AI at
    Expo, to build the product very AI-agnostic.
  topic: strategy/technical architecture
- impact_reason: 'Actionable advice for building LLM products: establish robust, customized,
    and diverse benchmarks (internal and external) to track performance across evolving
    models.'
  relevance_score: 10
  source: llm_enhanced
  text: you need to build a very solid base of benchmarks. That could be built by
    you. What we did is we built our own benchmark, actually asked a third party to
    help us with that, and we actually went and found one of these open-source projects
    to actually help us also build some other benchmarks that are focused on real
    vulnerabilities.
  topic: technical/best practice
- impact_reason: Introduces the concept of 'alloy models' (model ensembling/fusion)
    as a technique to achieve superior performance by combining different LLM families.
  relevance_score: 10
  source: llm_enhanced
  text: we even have this amazing thing called alloy models, which is still true nowadays,
    is that when you combine two families of models, it works better than just using
    one family.
  topic: technical/model architecture
- impact_reason: 'Provides a specific, practical implementation detail for model ensembling:
    randomized selection between distinct model families during sequential decision-making
    steps (chain of thought) yields better results.'
  relevance_score: 10
  source: llm_enhanced
  text: What we found is that for every action that you do, you flip a coin and decide
    which of the two models you want to use. And the two models have to be two different
    types or families of models, like OpenAI and Gemini, or Anthropic and Gemini,
    or Gemini or OpenAI and Anthropic. What happens is you get much better results,
    you get more vulnerabilities when you are basically using two models at the same
    time.
  topic: technical/model architecture
- impact_reason: Directly links LLM capabilities (coding) with the immediate security
    threat landscape (CVEs), setting the stage for the discussion on AI in security.
  relevance_score: 9
  source: llm_enhanced
  text: You'll see LLMs coding. You'll feel the chilling presence of CVEs.
  topic: Predictions
- impact_reason: Highlights the paradigm shift in security testing driven by AI speed,
    drawing a parallel to the initial struggle to convince clients about the necessity
    of human pen testing 20 years prior.
  relevance_score: 9
  source: llm_enhanced
  text: I feel like I'm having these conversations that I used to have 20 years ago
    with some of our new customers and telling them, yeah, pen testing is changing
    the game a little bit. Now, there's a speed that is different from a human speed.
  topic: Predictions/Strategy
- impact_reason: 'A direct strategic warning for organizations: readiness for AI-driven
    adversarial testing is now a necessity, not an option.'
  relevance_score: 9
  source: llm_enhanced
  text: You have to also figure out how you are ready for a future where AIs will
    be doing pen testing.
  topic: Strategy
- impact_reason: 'Shifts the focus from pure technical achievement to the business
    reality of bug bounty programs: resource management and ROI optimization are critical,
    especially at scale.'
  relevance_score: 9
  source: llm_enhanced
  text: Bug bounty is not just about finding bugs. Bug bounty is a lot as managing
    the ROI, managing the return on investment.
  topic: Business Advice
- impact_reason: 'Reveals a crucial strategic optimization technique for large-scale
    automated testing: intelligent surface area mapping to maximize efficiency and
    ROI, avoiding redundant testing.'
  relevance_score: 9
  source: llm_enhanced
  text: We need to suddenly fingerprint the whole attack surface of all these bug
    bounty programs, finding the areas that make more sense for us to run a test on.
  topic: Strategy/Business Advice
- impact_reason: 'Highlights the core challenge in applying LLMs to security testing:
    mastering the ''art'' of stateful interaction, workflow understanding, and business
    logic traversal, which traditionally requires human expertise.'
  relevance_score: 9
  source: llm_enhanced
  text: I was impressed to see an LLM climb up a bug bounty leaderboard to find vulns.
    But what I thought more was, as part of my behind-the-scenes kind of questions,
    is how did you get the LLM to interact with the site very well? Because there
    absolutely is just an art to finding links, setting up state, understanding workflows,
    business logic.
  topic: technical/business
- impact_reason: A definitive statement confirming full autonomy of the LLM in a complex
    task (bug bounty participation), which is a significant milestone for agentic
    AI in security.
  relevance_score: 9
  source: llm_enhanced
  text: No, absolutely. There was no human helping in the process, so it was all the
    LLM.
  topic: technical/predictions
- impact_reason: 'Details the mechanism for turning hallucination into discovery:
    generating a hypothetical endpoint and immediately validating its existence via
    network response (e.g., checking for a 200 OK vs. 404).'
  relevance_score: 9
  source: llm_enhanced
  text: So, for example, you were mentioning discovery and how it will hallucinate
    endpoints that are not even there. If you are able to have a way where you can
    actually say, okay, you came up with an endpoint that you believe is real, and
    you go and connect to that and see the response of that endpoint, suddenly that
    hallucination is basically discovering you things.
  topic: technical/breakthroughs
- impact_reason: Reinforces the critical importance of a strong, independent validation
    layer (the 'success criteria') to filter out noise and confirm LLM-generated hypotheses.
  relevance_score: 9
  source: llm_enhanced
  text: The true here was that since at the end of the attack, we were able to always
    validate that the XXE is real or not, then whatever hallucination the product
    came up with, the LLM came up with, we were able to properly confirm that it is
    real or not.
  topic: safety/strategy
- impact_reason: A profound insight suggesting that the emergent ability of advanced
    LLMs to 'game the system' (i.e., adversarial thinking) is directly correlated
    with their effectiveness in finding novel exploits.
  relevance_score: 9
  source: llm_enhanced
  text: And I believe that that cleverness they have about cheating those validators
    is what makes it better at actually finding vulnerabilities.
  topic: predictions/strategy
- impact_reason: 'Identifies the current frontier and limitation: LLMs excel at pattern-based
    vulnerabilities (like XXE) but struggle more with context-dependent issues like
    Business Logic Flaws and IDORs, requiring deeper application context.'
  relevance_score: 9
  source: llm_enhanced
  text: The other vulnerabilities are quite more interesting, and those are like the
    business logic, the IDOR, the authorization issues. Then you need to have better
    context on them. I think all LLMs are the right tool to do that, but they need
    to have a really good understanding of the application context, and that sometimes
    is hard, and that sometimes gets a series of false positives in there.
  topic: limitations/technical
- impact_reason: This describes the core operational loop of the AI security tool
    (X-Bow), highlighting iterative, autonomous attack capabilities, which is a key
    trend in AI-driven security testing.
  relevance_score: 9
  source: llm_enhanced
  text: there's this almost like an auto loop of observing, refining, attacking, and
    so on.
  topic: technical/AI trends
- impact_reason: Asserts that LLMs offer a depth advantage in security testing due
    to their ability to self-correct and iterate during an attack, surpassing traditional
    tools.
  relevance_score: 9
  source: llm_enhanced
  text: I believe there's a main difference because X-Bow can go deeper than anything
    else, or LLMs in general can go deeper than that because they can do that auto
    loop when they're attacking.
  topic: AI trends/limitations
- impact_reason: 'Pinpoints the primary current limitation of LLMs in security: the
    difficulty in achieving deep, accurate application context, leading to false positives.'
  relevance_score: 9
  source: llm_enhanced
  text: I think all LLMs are the right tool to do that, but they need to have a really
    good understanding of the application context, and that sometimes is hard, and
    that sometimes gets a series of false positives in there.
  topic: limitations/technical
- impact_reason: 'Defines the remaining human advantage in security testing: deep,
    specialized knowledge leading to application-specific, novel vulnerability discovery.'
  relevance_score: 9
  source: llm_enhanced
  text: what humans are better at is finding more application-specific vulnerabilities,
    the ones where we call good knowledge of the applications, like going deeper into
    understanding the interconnectedness of the application with other tools and so
    on.
  topic: human role/strategy
- impact_reason: Quantifies the direct benefit of providing source code (gray-box
    testing) to an LLM security tool, showing a significant performance uplift.
  relevance_score: 9
  source: llm_enhanced
  text: We actually measure that. If you provide source code, we find 30% more vulnerabilities.
  topic: technical/deployment
- impact_reason: Draws a direct parallel between human penetration testing best practices
    (requiring source code) and the optimal way to utilize LLM-based testing tools.
  relevance_score: 9
  source: llm_enhanced
  text: It's very similar to a human. In my old days at Immunity, when I was running
    professional services, I would talk to clients and say, give me the source code,
    and you will get more out of the test, to be quite honest, because you can map
    the attack surface much better and go much deeper than you can go with just black
    box testing.
  topic: strategy/technical
- impact_reason: 'Actionable advice for anyone building or deploying LLM solutions:
    robust benchmarking is essential due to model variability.'
  relevance_score: 9
  source: llm_enhanced
  text: But this is why you have to always build a really good set of benchmarks for
    anything LLM related.
  topic: business advice/strategy
- impact_reason: 'Highlights a major advantage of AI testing: perfect, comprehensive
    logging of every action taken, which is difficult to achieve consistently with
    human testers.'
  relevance_score: 9
  source: llm_enhanced
  text: So the good thing about AI penetration testing is that it's all built by a
    computer. So basically, what we do is we collect everything that we do over the
    penetration test. We have every network packet that was sent and received from
    the attacker.
  topic: technical/deployment
- impact_reason: Describes a key deliverable—a coverage map—that quantifies the testing
    scope across endpoints and vulnerability classes, offering transparent metrics
    beyond just vulnerability counts.
  relevance_score: 9
  source: llm_enhanced
  text: We also built this coverage map where we can tell, this is all the things
    that or this is all the endpoints that X-Bow found, and these are for each individual
    vulnerability class, this is how much we were able to cover and so on.
  topic: technical/product feature
- impact_reason: A strong assertion on the fundamental, unsolved nature of prompt
    injection due to the inherent ambiguity of natural language inputs.
  relevance_score: 9
  source: llm_enhanced
  text: Prompt injection is one of those things that is unsolved, I want to assert,
    because as you mentioned earlier, LLMs are essentially natural language, and natural
    language can appear everywhere, anywhere.
  topic: safety/limitations
- impact_reason: 'Reveals the necessary adversarial testing loop: internal security
    teams must actively try to hack their own AI products to ensure robustness against
    prompt injection.'
  relevance_score: 9
  source: llm_enhanced
  text: No, we have not [hit a website with a prompt injection telling the scanner
    to stop]. However, as you can imagine, our security team are all security researchers,
    and they always build that themselves. They run, build their own website that
    has all kinds of interesting prompt injections, and we run the tool against it
    and test it and find bugs and solve and fix them and so on.
  topic: safety/testing methodology
- impact_reason: Demonstrates the extreme volatility and unpredictability of LLM performance
    in specialized tasks, emphasizing why model agnosticism and constant benchmarking
    are essential.
  relevance_score: 9
  source: llm_enhanced
  text: As new models were coming, suddenly two months ago, the best model for offensive
    security was Grok, again, you never expect that, but it was suddenly a really
    amazing, and when we saw the company, we never even imagined that X would come
    out with a model that would be better at that time.
  topic: AI technology trends/volatility
- impact_reason: 'Articulates the core tension in prompt engineering for dynamic systems:
    balancing the need for model-specific optimization (better current results) against
    the need for long-term portability (generic prompts).'
  relevance_score: 9
  source: llm_enhanced
  text: The other thing that is kind of a struggle we're building what we don't love
    is deciding for the future. If you keep the prompts more generic, it's going to
    be better because you can switch between different models, but the problem you
    have is now. And so, if prompting better finds more bugs, that's what you have
    to do.
  topic: technical/prompt engineering strategy
- impact_reason: 'Illustrates a key business advantage of using LLMs in bug bounty
    hunting: rapid, large-scale testing against real-world production environments
    without lengthy procurement cycles.'
  relevance_score: 8
  source: llm_enhanced
  text: Suddenly you have all these production environments that you can just go and
    play and test your products against it and improve your product with real production
    environments.
  topic: Business Advice
- impact_reason: Describes the iterative, self-improving loop of using bug bounty
    feedback to refine the LLM's offensive capabilities, leading to higher quality
    results.
  relevance_score: 8
  source: llm_enhanced
  text: We start finding corner cases and improve them. And then we'll find more true
    positives, less false positives, and so on. And we slowly start expanding the
    vulnerability classes that we find.
  topic: Technical Insights/Product Building
- impact_reason: Suggests that the LLM's success distribution across vulnerability
    types (including rare ones like XXE) closely mirrors the findings profile of an
    experienced human penetration tester.
  relevance_score: 8
  source: llm_enhanced
  text: You will not easily find an XXE on a normal penetration test, right? And if
    you look at the distribution of points with X-Bow, it reflects so much for you
    doing a penetration test.
  topic: Technical Insights
- impact_reason: Provides a direct comparison between automated vulnerability scanning
    (X-Bow) results and traditional penetration testing outcomes, suggesting LLM-driven
    tools can mimic human testing distribution.
  relevance_score: 8
  source: llm_enhanced
  text: And if you look at the distribution of points with X-Bow, it reflects so much
    for you doing a penetration test. So, you get like a good number, like 40% of
    your low-hanging fruits first, and then you get like maybe the other part, which
    you like medium to high and critical, and so on.
  topic: strategy/predictions
- impact_reason: Describes a novel approach where LLM creativity (even generating
    non-existent endpoints) is used to expand the attack surface before targeted exploitation
    begins.
  relevance_score: 8
  source: llm_enhanced
  text: We used that to discover things that we know were not there, and this is what
    we have in the product. And then now that you have a good attack surface, now
    you can go and forage individual endpoints, go and attack them.
  topic: strategy/technical
- impact_reason: Acknowledges the significant industry concern regarding LLM hallucination
    leading to false positives in security reporting, referencing external criticism.
  relevance_score: 8
  source: llm_enhanced
  text: I can also imagine an LLM hallucinating, here is an endpoint that doesn't
    exist, here is a flaw that doesn't work. And we've seen, for example, Daniel Stenberg
    of the curl project, he's been very vocal on the other side of the coin, in the
    sense of here is just some LLM slop that was reported as not a real bug.
  topic: safety/limitations
- impact_reason: Provides a concise summary of the major security risks associated
    with GenAI and points to the community effort (OWASP) addressing them.
  relevance_score: 8
  source: llm_enhanced
  text: Generative AI is powerful, but it comes with real risks, from prompt injection
    to data leakage, misuse, and agentic AI. The OWASP Gen AI Security Project unites
    a global community... to secure generative AI.
  topic: safety/ethics
- impact_reason: 'Identifies the current frontier for advanced security testing: business
    logic flaws, which require deep contextual understanding beyond simple pattern
    matching.'
  relevance_score: 8
  source: llm_enhanced
  text: But the other vulnerabilities are quite more interesting, and those are like
    the business logic, the IDOR, the authorization issues. Then you need to have
    better context on them.
  topic: safety/vulnerability classes
- impact_reason: Highlights a significant improvement in newer models (like GPT-5.0
    mentioned) regarding self-validation using context, reducing the need for external,
    coded validation steps.
  relevance_score: 8
  source: llm_enhanced
  text: The good thing is that the new models are much better at validating findings
    by just using LLM contextual information.
  topic: AI breakthroughs/technical
- impact_reason: Strong business advice cautioning against rewarding low-effort, scanner-based
    findings in bug bounty programs, suggesting inefficiency.
  relevance_score: 8
  source: llm_enhanced
  text: if you're setting up a bug bounty program and you're just giving out awards
    because bug bounty researchers ran a scanner and essentially rewrote with scanner
    output, you're subsidizing people to run this scanner for you.
  topic: business advice
- impact_reason: 'Highlights the unique advantage of LLMs: their ability to ingest
    and act upon unstructured documentation (like Swagger specs or PDFs) in natural
    language, mimicking human documentation review.'
  relevance_score: 8
  source: llm_enhanced
  text: If you provide that to the LLM, and this is the beauty of them, they can read
    natural language and act upon it, you can actually improve the results of your
    penetration test like you would do with a pen tester.
  topic: AI capabilities/technical
- impact_reason: Discusses the current instability and trial-and-error nature of high-level
    prompting for complex, multi-step security tests, indicating prompting is still
    immature for strategic direction.
  relevance_score: 8
  source: llm_enhanced
  text: We were not yet able to prompt the general direction of the pen test without
    having repercussions to the results. If you do it at the attack vector level,
    you definitely sometimes get better results, sometimes you don't.
  topic: technical/limitations
- impact_reason: Explicitly names and explains the challenge of non-determinism in
    LLMs, which affects testing consistency.
  relevance_score: 8
  source: llm_enhanced
  text: Part of that too is that aspect, especially of the non-deterministic one of
    my favorite words to bring up over time about LLMs, in a sense of, well, maybe
    it just didn't test for this XSS with the right payload this time, but the next
    time it did.
  topic: limitations/technical
- impact_reason: 'Offers insight into the emergent behavior of the AI agent: after
    exhausting initial, predictable paths, it shifts to more novel, creative problem-solving.'
  relevance_score: 8
  source: llm_enhanced
  text: The interesting part happens after the first 20 steps, and one to go a little
    farther than that, and then it runs out of ideas, and then it comes out with the
    more security ideas that will end up finding new vulnerabilities.
  topic: AI behavior/technical
- impact_reason: Illustrates a non-deterministic behavior in AI security testing where
    initial exploration leads to deeper, more novel vulnerability discovery, suggesting
    a benefit to letting the AI explore beyond obvious paths.
  relevance_score: 8
  source: llm_enhanced
  text: Interesting part happens after the first 20 steps, and one to go a little
    farther than that, and then it runs out of ideas, and then it comes out with the
    more security ideas that will end up finding new vulnerabilities.
  topic: technical/AI behavior
- impact_reason: Emphasizes the crucial need for human-centric design in AI tools,
    focusing on how AI outputs should augment, not just replace, human workflows.
  relevance_score: 8
  source: llm_enhanced
  text: helping us understand what is really useful for humans to consume after a
    penetration test, what will help you as a pen tester after X-Bow has done the
    penetration test.
  topic: strategy/human-AI collaboration
- impact_reason: 'Highlights the inherent, perpetual security challenge in building
    LLM-based products: the internal incentive for security researchers to find flaws
    in the core mechanism.'
  relevance_score: 8
  source: llm_enhanced
  text: That's never going to stop because there's an internal incentive to hack the
    product itself.
  topic: safety/ongoing challenge
- impact_reason: 'Offers a realistic assessment of vulnerability distribution, mirroring
    traditional pentesting results: easy wins provide volume, high-impact finds are
    rare.'
  relevance_score: 7
  source: llm_enhanced
  text: The low-hanging fruits are the ones that you get more quantity, and the more
    interesting, more impactful vulnerabilities you get more, and it's a small bunch.
  topic: Predictions/Technical
- impact_reason: Contrasts older, scripted validation methods with the new context-aware
    validation capabilities of modern LLMs.
  relevance_score: 7
  source: llm_enhanced
  text: Whereas in the past, most of our validation was more like coded, like go and
    navigate to the website, see the JavaScript, read there or not.
  topic: technical/comparison
- impact_reason: Provides a crucial counterpoint to the LLM non-determinism critique
    by comparing it to the inherent variability in human expert teams.
  relevance_score: 7
  source: llm_enhanced
  text: In many ways, humans are non-deterministic in a certain way. When we're doing
    a penetration test, you have five different pen testers, and you give them an
    application, you're going to see five different tests.
  topic: comparison/strategy
- impact_reason: Relates to the business challenge of delivering value when security
    testing yields no critical findings, contrasting the burden of proof for humans
    vs. AI.
  relevance_score: 7
  source: llm_enhanced
  text: If you don't find a bug, you have to actually write a report, because that's
    when you have to justify. Otherwise, you should show really three or four really
    good vulnerabilities, and the customer will be happy.
  topic: business advice/strategy
- impact_reason: Reinforces the importance of vendor responsiveness to real-world
    user needs rather than just pushing technology for technology's sake.
  relevance_score: 7
  source: llm_enhanced
  text: this is the useful feedback loop, and we're looking for LLMs assisting with
    appsec.
  topic: strategy/customer focus
source: Unknown Source
summary: '## Podcast Summary: Finding Large Bounties with Large Language Models -
  Nico Waisman - ASW #351


  This episode of Application Security Weekly features Nico Waisman, CISO at Expo
  (Crossbow), discussing his company''s groundbreaking work in using Large Language
  Models (LLMs) to automate and scale bug bounty hunting, achieving top rankings on
  major platforms.


  ### 1. Focus Area

  The primary focus is the application of **Large Language Models (LLMs) in offensive
  security, specifically for automated penetration testing and bug bounty hunting**.
  The discussion centers on how LLMs can mimic human-like reasoning, discovery, and
  iterative attack refinement, contrasting this with traditional DAST tools and fuzzing
  techniques.


  ### 2. Key Technical Insights

  *   **LLM as an Iterative Attacker:** LLMs excel because they can follow a human-like
  attack flow: gather information, attempt an exploit, reflect on the error/response,
  refine the strategy, and repeat (an "auto-loop" of observing, refining, and attacking).

  *   **Managing Hallucination as a Feature:** Instead of viewing LLM hallucination
  as a flaw, the team treats it as a discovery mechanism. Hallucinated endpoints or
  non-existent CVEs often lead to the LLM testing novel paths, sometimes revealing
  real, undiscovered vulnerabilities (like the XXE example where a non-existent CVE
  pointed to a real, old flaw).

  *   **Validator Layer for Accuracy:** To combat false positives inherent in LLM
  output, a "validator" profile acts as a second pair of eyes. This validator uses
  a headless browser to actively test the alleged vulnerability (e.g., checking if
  JavaScript renders for XSS) to confirm its reality before reporting.


  ### 3. Business/Investment Angle

  *   **Efficiency in Bug Bounty ROI:** LLM-driven hunting allows for massive scaling
  across millions of hosts in bug bounty programs, optimizing the Return on Investment
  (ROI) by intelligently fingerprinting the attack surface and focusing effort where
  it matters, bypassing redundant testing of mirrored staging environments.

  *   **Product Validation in Production:** Participating in bug bounties provided
  Expo with a unique, real-world testing ground for their offensive security product
  (X-Bow) against live production environments, accelerating product improvement far
  faster than traditional design partner cycles.

  *   **Shifting Pen Testing Paradigm:** The conversation echoes the difficulty 20
  years ago in convincing clients about the need for pen testing; today, the challenge
  is convincing them they need to prepare for a future where AI performs testing at
  machine speed.


  ### 4. Notable Companies/People

  *   **Nico Waisman (Expo/Crossbow):** The central figure, detailing his company''s
  success in ranking #1 in US bug bounties in 2025 using their LLM-based tool, X-Bow.

  *   **Immunity, GitHub, Lyft:** Mentioned as previous organizations where Waisman
  held leadership roles, highlighting his deep background in hands-on penetration
  testing.

  *   **Daniel Stenberg (curl project):** Referenced as a vocal critic of low-quality,
  LLM-generated security reports ("LLM slop"), providing context for the importance
  of the validator layer.


  ### 5. Future Implications

  The industry is moving toward **AI-driven, high-speed, and deep security testing**.
  The future involves LLMs not just finding low-hanging fruit (like XSS) but demonstrating
  the capability to chain complex attacks, discover obscure logic flaws, and iterate
  on exploits for high-impact vulnerabilities (like SSRF or SQLi) with a depth that
  surpasses traditional automated scanners. The arms race will shift to LLMs developing
  new "tricks" to bypass validation mechanisms.


  ### 6. Target Audience

  This episode is highly valuable for **Application Security Professionals, Security
  Engineers, Product Managers in Security Tech, and Cybersecurity Investors**. It
  provides concrete examples of advanced LLM implementation in a high-stakes environment
  (bug bounties) and discusses the technical challenges of validation and discovery
  at scale.'
tags:
- artificial-intelligence
- generative-ai
- ai-infrastructure
- investment
- openai
- anthropic
title: 'Finding Large Bounties with Large Language Models - Nico Waisman - ASW #351'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 108
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 7
  prominence: 0.7
  topic: generative ai
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 2
  prominence: 0.2
  topic: ai infrastructure
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 1
  prominence: 0.1
  topic: investment
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-08 02:45:42 UTC -->
