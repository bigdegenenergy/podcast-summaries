---
companies:
- category: unknown
  confidence: medium
  context: The AWS for software companies podcast, episode 156, LLM
  name: The AWS
  position: 0
- category: tech
  confidence: high
  context: igrations to one cloud, Coveo's strategic move to Amazon Bedrock. [♪ music
    playing in background...] Hi ev
  name: Amazon
  position: 108
- category: unknown
  confidence: medium
  context: igrations to one cloud, Coveo's strategic move to Amazon Bedrock. [♪ music
    playing in background...] Hi everyone,
  name: Amazon Bedrock
  position: 108
- category: unknown
  confidence: medium
  context: he cloud, overcoming obstacles, and the role that Amazon Web Services play
    in their success. Today, learn how Coveo aut
  name: Amazon Web Services
  position: 355
- category: unknown
  confidence: medium
  context: so excited to be here. I'm excited because it is Gen AI, and more important,
    it is successfully for one o
  name: Gen AI
  position: 630
- category: unknown
  confidence: medium
  context: e important, it is successfully for one of my top Canadian ISV customers,
    Coveo. My name is Janik Kongvijiam, AW
  name: Canadian ISV
  position: 695
- category: unknown
  confidence: medium
  context: my top Canadian ISV customers, Coveo. My name is Janik Kongvijiam, AWS
    solution architect, working with ISV. And to
  name: Janik Kongvijiam
  position: 737
- category: unknown
  confidence: medium
  context: ating together in Gen AI using Amazon Bedrock and Amazon Titan. I have
    a special guest here from Coveo, Sebastia
  name: Amazon Titan
  position: 926
- category: unknown
  confidence: medium
  context: on Titan. I have a special guest here from Coveo, Sebastian Kulio. Introduce
    yourself. Hi Sebastian, I'm VP of AI S
  name: Sebastian Kulio
  position: 980
- category: unknown
  confidence: medium
  context: from Coveo, Sebastian Kulio. Introduce yourself. Hi Sebastian, I'm VP of
    AI Strategy at Coveo. I've been buildi
  name: Hi Sebastian
  position: 1017
- category: unknown
  confidence: medium
  context: ulio. Introduce yourself. Hi Sebastian, I'm VP of AI Strategy at Coveo.
    I've been building for the last 12 year
  name: AI Strategy
  position: 1041
- category: unknown
  confidence: medium
  context: going to talk with Janik about our success using Bedrock Titan models for
    our product. Thank you, Sebastian. Bef
  name: Bedrock Titan
  position: 1204
- category: tech
  confidence: high
  context: your data sources, being SharePoint, Salesforce, Google Drive, NameIt,
    all companies have hundreds of dat
  name: Google
  position: 2813
- category: unknown
  confidence: medium
  context: your data sources, being SharePoint, Salesforce, Google Drive, NameIt,
    all companies have hundreds of data sour
  name: Google Drive
  position: 2813
- category: unknown
  confidence: medium
  context: want to focus on is one of our products, which is Coveo Relevance Generative
    Answering. What is this product? It's our fully managed RAG
  name: Coveo Relevance Generative Answering
  position: 5324
- category: unknown
  confidence: medium
  context: o. This is huge for our customers. As an example, SAP Concur is saving
    8 million euros a year with deploying g
  name: SAP Concur
  position: 7115
- category: unknown
  confidence: medium
  context: mation. Then we were sending the prompt to GPT on Azure Cloud and streaming
    back the answer. Today, we're going
  name: Azure Cloud
  position: 8401
- category: unknown
  confidence: medium
  context: '''re going to talk how we were able to migrate our GPT LLM model that
    we''re using on Azure Cloud to Bedrock.'
  name: GPT LLM
  position: 8503
- category: unknown
  confidence: medium
  context: ebastian. So how Bedrock helps for this use case? So Bedrock is AWS's fully
    managed service for generative AI.
  name: So Bedrock
  position: 8755
- category: unknown
  confidence: medium
  context: u have in slide shows the best model helps a lot. So AWS has a selection
    of fully managed leading foundati
  name: So AWS
  position: 9033
- category: unknown
  confidence: medium
  context: e, especially Coveo went with Amazon Titan model, Amazon Titan Lite. Sebastian
    will elaborate more for the use case.
  name: Amazon Titan Lite
  position: 9165
- category: unknown
  confidence: medium
  context: . Sebastian will elaborate more for the use case. So Sebastian, could you
    dive into the architecture, target arc
  name: So Sebastian
  position: 9232
- category: unknown
  confidence: medium
  context: del on Azure Cloud. Now we migrated this model to Titan Lite on Bedrock.
    This required two main steps for the
  name: Titan Lite
  position: 9570
- category: unknown
  confidence: medium
  context: t behavior. So what we had to do to make it work. As I said, we had to
    do three migrations in the last y
  name: As I
  position: 12480
- category: unknown
  confidence: medium
  context: fine specific behaviors and be able to test them. If I give you an example
    of a behavior for us, it's, f
  name: If I
  position: 13280
- category: unknown
  confidence: medium
  context: ns with agentic platforms. So we have plugins for Amazon Q. We have action
    groups for Bedrock. We have an MC
  name: Amazon Q
  position: 18715
- category: unknown
  confidence: medium
  context: ions with other platforms outside of AWS, such as Agent Force, Microsoft
    Co-Pilot. So within the same retrieval
  name: Agent Force
  position: 18849
- category: tech
  confidence: high
  context: er platforms outside of AWS, such as Agent Force, Microsoft Co-Pilot. So
    within the same retrieval platform,
  name: Microsoft
  position: 18862
- category: unknown
  confidence: medium
  context: er platforms outside of AWS, such as Agent Force, Microsoft Co-Pilot. So
    within the same retrieval platform, you
  name: Microsoft Co
  position: 18862
- category: ai_application
  confidence: high
  context: The primary subject of the podcast; an ISV customer of AWS that provides
    an AI relevance platform for search, generative, and agentic experiences, focusing
    on RAG solutions (CRGA).
  name: Coveo
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: The host platform and provider of the services discussed (Bedrock, Titan).
  name: Amazon Web Services (AWS)
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: AWS's fully managed service for generative AI, used by Coveo for hosting
    foundation models.
  name: Amazon Bedrock
  source: llm_enhanced
- category: ai_model_provider
  confidence: high
  context: The family of foundation models offered by AWS, specifically mentioning
    'Amazon Titan Lite' which Coveo migrated to.
  name: Amazon Titan
  source: llm_enhanced
- category: ai_model_provider
  confidence: medium
  context: A general reference to OpenAI's models (likely GPT-4 or similar) that Coveo
    was previously using on Azure Cloud.
  name: GPT
  source: llm_enhanced
- category: ai_model_provider
  confidence: high
  context: A specific model mentioned in the comparison results against Titan Lite.
  name: GPT-4o mini
  source: llm_enhanced
- category: ai_model_provider
  confidence: high
  context: A foundation model family (likely Anthropic's Claude) that Coveo previously
    used or compared against, noted for being less verbose than Titan Lite by default.
  name: Claude
  source: llm_enhanced
- category: ai_service_partner
  confidence: high
  context: Mentioned as one of the best partners that helped Coveo set up the infrastructure
    on Bedrock (likely a consulting or infrastructure firm specializing in AWS/AI).
  name: Talent
  source: llm_enhanced
- category: ai_customer
  confidence: high
  context: A customer of Coveo mentioned as achieving significant cost savings (€8
    million/year) by deploying Coveo's generative answering solution.
  name: SAP Concur
  source: llm_enhanced
- category: ai_customer
  confidence: high
  context: A customer of Coveo mentioned for deploying their generative answering
    solution in production in days.
  name: Zero
  source: llm_enhanced
- category: data_source
  confidence: low
  context: Mentioned as a data source Coveo indexes (Microsoft product, but context
    is about Coveo's indexing capability).
  name: SharePoint
  source: llm_enhanced
- category: data_source
  confidence: low
  context: Mentioned as a data source Coveo indexes.
  name: Salesforce
  source: llm_enhanced
- category: data_source
  confidence: low
  context: Mentioned as a data source Coveo indexes.
  name: Google Drive
  source: llm_enhanced
- category: data_source
  confidence: low
  context: Mentioned vaguely as one of the hundreds of data sources Coveo indexes
    (likely a placeholder or a specific customer system).
  name: NameIt
  source: llm_enhanced
- category: ai_model
  confidence: high
  context: An LLM model running on Amazon Bedrock, used for comparison and migration.
  name: Titan Lite
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Amazon's managed service for building and scaling generative AI applications,
    hosting models like Titan Lite.
  name: Bedrock
  source: llm_enhanced
- category: ai_infrastructure
  confidence: high
  context: Amazon Web Services, the cloud provider facilitating the migration and
    hosting the Bedrock service.
  name: AWS
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: An AWS AI assistant service for which Coveo provides plugins.
  name: Amazon Q
  source: llm_enhanced
- category: ai_application
  confidence: medium
  context: An external platform (likely an agentic framework) with which Coveo integrates.
  name: Agent Force
  source: llm_enhanced
- category: ai_application
  confidence: high
  context: A Microsoft AI assistant product with which Coveo integrates.
  name: Microsoft Co-Pilot
  source: llm_enhanced
date: 2025-10-09 07:00:00 +0000
duration: 25
has_transcript: false
layout: episode
llm_enhanced: true
original_url: https://audio.listennotes.com/e/p/88bfe88c1cb74796950941922e84495e/
processing_date: 2025-10-09 14:10:30 +0000
quotes:
- length: 189
  relevance_score: 6
  text: The advantage of having a fully managed RAG solution is that Coveo manages
    all the complexity of working with LLMs, including prompt engineering, including
    maintaining the prompts over time
  topics: []
- length: 142
  relevance_score: 5
  text: If here we have people that have worked with LLMs, you know that if you're
    migrating from one version to another, you have to redo your prompt
  topics: []
- length: 244
  relevance_score: 4
  text: So we can index all the data sources within an organization, we manage everything
    to be able to index a document, including generating embeddings, including the
    vector database, and we can provide the relevance afterwards for all your use
    cases
  topics: []
- length: 122
  relevance_score: 3
  text: The AWS for software companies podcast, episode 156, LLM migrations to one
    cloud, Coveo's strategic move to Amazon Bedrock
  topics: []
- length: 149
  relevance_score: 3
  text: So AWS has a selection of fully managed leading foundation models for this
    use case, especially Coveo went with Amazon Titan model, Amazon Titan Lite
  topics: []
- impact_reason: This powerfully frames LLM migration not as a simple software update
    but as a fundamental change in system intelligence ('mind transplant'), highlighting
    the complexity of prompt engineering and quality assurance during model swaps.
  relevance_score: 10
  source: llm_enhanced
  text: Coveo automated LLM migration like a mind transplant, building frameworks
    to optimize prompts and maintain quality across model changes.
  topic: technical/strategy
- impact_reason: Crucially explains how enterprise AI/RAG solutions must handle complex
    security permissions (access rights) alongside content indexing, which is vital
    for secure enterprise deployment.
  relevance_score: 10
  source: llm_enhanced
  text: Coveo can index all your content within the enterprise in all your data sources,
    being SharePoint, Salesforce, Google Drive, NameIt, all companies have hundreds
    of data sources. Coveo can index the content within these data sources, and at
    the same time, we not only index the content, we index the access rights.
  topic: technical/safety
- impact_reason: Emphasizes the critical importance of prompt optimization for grounding
    answers and preventing unwanted responses (hallucinations or answering outside
    scope).
  relevance_score: 10
  source: llm_enhanced
  text: We have worked really hard to optimize the prompts for question answering,
    so that you always get the accurate answer for a specific question. And even more
    importantly, the system will not answer if the question shouldn't be answered,
    which is really important for our customers.
  topic: safety/technical
- impact_reason: Provides a detailed, high-level overview of a production-grade RAG
    pipeline, emphasizing the crucial step of context extraction/passage selection
    to prevent LLM confusion.
  relevance_score: 10
  source: llm_enhanced
  text: Previously, most of our services were on AWS. So when a query comes in, we
    have the search API, then we apply multiple ranking, machine learning algorithms
    to optimize the ranking. We use our hybrid index to find the best documents respecting
    the security. Then we go deep into the top documents to extract the passages from
    these documents so that we can generate the prompt with a really precise context
    that will not get the LLM confused with too much information.
  topic: technical
- impact_reason: 'This is a core technical insight: prompts are model-specific and
    non-transferable, debunking the myth of simple ''drop-in'' LLM replacements.'
  relevance_score: 10
  source: llm_enhanced
  text: The hard part is making the system work as you want with a new LLM. Because
    migrating from one LLM to another is very complex. If here we have people that
    have worked with LLMs, you know that if you're migrating from one version to another,
    you have to redo your prompt. It's not true that you can use the same prompt from
    one LLM and use it for the second one.
  topic: technical
- impact_reason: 'Explains the root cause of prompt incompatibility: differences in
    underlying training data leading to divergent interpretations of instructions.'
  relevance_score: 10
  source: llm_enhanced
  text: Why do we see this change of behavior is because these models are trained
    on different datasets? So the inner knowledge of the model is always different.
    The model will interpret the system message differently. So you will need to talk
    to it differently.
  topic: technical
- impact_reason: A stark warning about the rapid pace of LLM version deprecation,
    forcing enterprises to build robust, automated migration pipelines.
  relevance_score: 10
  source: llm_enhanced
  text: Currently, the model suppliers are asking us to migrate every six months.
    So they are deprecating really fast the model's version.
  topic: strategy/predictions
- impact_reason: 'The key takeaway for building resilient AI products: investing heavily
    in automated evaluation frameworks is mandatory for managing continuous model
    updates.'
  relevance_score: 10
  source: llm_enhanced
  text: We worked really hard on our evaluation framework to automate a big part of
    the evaluation and the migration. So for us, it's really important that we can
    keep the same performance over time and improve it.
  topic: technical/strategy
- impact_reason: 'Provides a foundational rule for RAG evaluation: strict adherence
    to provided context, which directly combats hallucination.'
  relevance_score: 10
  source: llm_enhanced
  text: If I give you an example of a behavior for us, it's, for example, if the context
    contains the information, we want the LLM to answer. This is an easy one. This
    is the obvious one. The opposite needs to be true. So if the [context does not
    contain the information, the model should not answer].
  topic: safety/technical
- impact_reason: Highlights a critical limitation/tendency of current LLMs (over-eagerness
    to respond) and the difficulty in enforcing grounding and refusal, which is vital
    for enterprise use cases.
  relevance_score: 10
  source: llm_enhanced
  text: If the context does not contain the answer, the LLM should not answer. And
    this is actually harder to do because the LLMs, they want to talk. They always
    want to say something. So it's really harder to make them not answer when they
    should not.
  topic: safety/technical
- impact_reason: Directly addresses a critical compliance and regulatory hurdle for
    deploying enterprise Gen AI solutions globally.
  relevance_score: 9
  source: llm_enhanced
  text: The second challenge is data residency, compliancy. The requirement here is
    Coveo customers need to run Gen AI applications in specific regions.
  topic: safety/business
- impact_reason: Identifies the 'model access' problem as a key driver for adopting
    managed services like Bedrock, emphasizing the need for flexibility across foundation
    models.
  relevance_score: 9
  source: llm_enhanced
  text: The third one, and we are seeing this often, is about the need for seamless
    access to leading Gen AI foundation models. This is where Amazon Bedrock shines.
  topic: business/strategy
- impact_reason: Reinforces that security context must be strictly maintained across
    both traditional search and new generative agentic workflows.
  relevance_score: 9
  source: llm_enhanced
  text: This allows us to do secured search, meaning that if a user is searching for
    something, if he doesn't have access to a document, he will not see it. And this
    is also true for agentic AI.
  topic: safety
- impact_reason: Provides a concrete, high-impact business metric (20-30% reduction
    in support cases) achieved via generative AI/RAG implementation.
  relevance_score: 9
  source: llm_enhanced
  text: We've seen tremendous results with this generative answering application.
    So we have customers, on average, I would say that our customers are seeing 20
    to 30% reduction in case creation when they are deploying Coveo.
  topic: business
- impact_reason: 'Defines the value proposition of a managed RAG layer: abstracting
    away the constant, complex work of prompt engineering and maintenance.'
  relevance_score: 9
  source: llm_enhanced
  text: The advantage of having a fully managed RAG solution is that Coveo manages
    all the complexity of working with LLMs, including prompt engineering, including
    maintaining the prompts over time.
  topic: business/technical
- impact_reason: Provides a staggering, concrete metric of high-volume enterprise
    LLM consumption, underscoring the need for scalable infrastructure and cost optimization.
  relevance_score: 9
  source: llm_enhanced
  text: We are consuming more than 70 billion tokens a month.
  topic: business/technical
- impact_reason: Highlights that context utilization (crucial for RAG accuracy) varies
    significantly between models, requiring specific prompt tuning for context adherence.
  relevance_score: 9
  source: llm_enhanced
  text: We've seen that the context and link changes between models. So they don't
    necessarily use the context we're providing the same way. We need different instructions
    to make sure that they're using the context we're providing.
  topic: technical
- impact_reason: Reiterates the central metaphor, emphasizing the deep, behavioral
    change inherent in LLM replacement.
  relevance_score: 9
  source: llm_enhanced
  text: When you're migrating an LLM, the thing I like to say is that it's really
    not a software upgrade. It's really more like a mind transplant. It's like you're
    changing the mind of your system.
  topic: strategy
- impact_reason: 'Highlights the core difficulty in evaluating generative AI: defining
    measurable, deterministic success criteria for non-deterministic outputs.'
  relevance_score: 9
  source: llm_enhanced
  text: The first thing we need to do is to define the expected behaviors. What do
    you want the system to do? And this is especially difficult with generative experiences
    and even more with agentic experiences because the output is non-deterministic.
  topic: technical/safety
- impact_reason: Indicates the rapid pace of LLM deprecation/updates (every 4-6 months)
    and emphasizes the necessity of robust, automated evaluation frameworks to manage
    this churn.
  relevance_score: 9
  source: llm_enhanced
  text: we had to do three migrations in the last year alone. So we worked really
    hard on our evaluation framework to automate a big part of the evaluation and
    the migration.
  topic: business/strategy
- impact_reason: A strong, actionable recommendation for the entire AI development
    community regarding necessary infrastructure for production LLM systems.
  relevance_score: 9
  source: llm_enhanced
  text: This automated evaluation is really important. This is something I would say
    everyone that works with LLM or agents should do.
  topic: strategy/business
- impact_reason: Connects rigorous evaluation directly to automated prompt engineering,
    showing a path to continuous, data-driven performance improvement.
  relevance_score: 9
  source: llm_enhanced
  text: This automation of the evaluation of the behavior allowed us to automate the
    prompt optimization. So we're able with this system to automatically optimize
    the prompt for a specific use case.
  topic: technical/business
- impact_reason: Provides concrete comparative results showing that model switching
    (from Claude to Titan Lite) can significantly improve the crucial 'refusal' or
    grounding metric (negative accuracy), especially important for regulated industries.
  relevance_score: 9
  source: llm_enhanced
  text: with Titan Lite, we slightly increase the positive accuracy, meaning that
    the model knows more when to answer. But more importantly, we really increase
    the negative accuracy. So the model is better in deciding when not to answer.
  topic: technical/business
- impact_reason: A strong testimonial for vendor diversification (moving to Bedrock)
    achieving performance parity/improvement while simultaneously reducing cost and
    infrastructure complexity.
  relevance_score: 9
  source: llm_enhanced
  text: All in all, we were able to have an answer rate that is expected to remain
    stable over all our customers. But the quality of answers were increased based
    on the other metrics I just talked about. So all in all, a really successful project.
    We were able to migrate to Amazon Bedrock. The performances meet or exceed our
    current performances with Claude. And we got all of that with a simpler infrastructure
    that reduced our costs.
  topic: business/strategy
- impact_reason: Highlights a major real-world challenge for large ISVs operating
    in a multi-cloud environment, specifically concerning security and governance.
  relevance_score: 8
  source: llm_enhanced
  text: The first one was managing multi-cloud governance complexity.
  topic: business/strategy
- impact_reason: Offers a specific, quantifiable success story demonstrating the massive
    ROI potential of enterprise RAG solutions.
  relevance_score: 8
  source: llm_enhanced
  text: SAP Concur is saving 8 million euros a year with deploying generative answering.
  topic: business
- impact_reason: Addresses the crucial need for output consistency (tone, style, verbosity)
    in enterprise applications, which is often overlooked when focusing only on factual
    accuracy.
  relevance_score: 8
  source: llm_enhanced
  text: The formatting of the answer will vary. So some of them are more verbose.
    Some of the tone may change, the style may change. But this is really annoying
    because for our customers, they don't want their system to change each time we
    need to migrate.
  topic: safety/business
- impact_reason: Illustrates the extreme sensitivity and fragility of prompt engineering,
    reinforcing the need to move beyond manual iteration toward automated optimization
    loops.
  relevance_score: 8
  source: llm_enhanced
  text: you just change a comma, you just change one letter and out of a sudden you
    have a different experience. So it's really hard to come up with the best prompt.
    So having an automated system is really important.
  topic: technical
- impact_reason: Shifts the focus of AI quality from purely functional accuracy to
    brand risk management, making grounding and refusal a universal business requirement,
    not just a regulatory one.
  relevance_score: 8
  source: llm_enhanced
  text: We have many customers in regulated industries, but actually all our customers
    care about their brand and they don't want bad exposure. So for them, it's really
    important.
  topic: business/safety
- impact_reason: Highlights the massive acceleration in deployment time for RAG/grounded
    generation solutions when using managed platforms, contrasting hours vs. months.
  relevance_score: 8
  source: llm_enhanced
  text: You can deploy that in hours instead of months. You will have good answers.
    You will have answers that are accurate. And this is really the out-of-the-box
    and the really easy-to-use solution.
  topic: business/strategy
- impact_reason: Explains the modularity of modern AI stacks, where retrieval (RAG)
    can be decoupled and used to ground proprietary or fine-tuned models, rather than
    being locked into a single vendor's RAG solution.
  relevance_score: 8
  source: llm_enhanced
  text: We have the passage retrieval API when you can have access directly to the
    best passages within your documents to answer any question. This can then be reused
    by your own LLM that maybe you have fine-tuned on your data, for example.
  topic: technical/strategy
- impact_reason: 'Summarizes the key operational benefits of cloud consolidation for
    AI workloads: simplified governance, reduced latency, and easier compliance management.'
  relevance_score: 8
  source: llm_enhanced
  text: Right now, we only have one cloud to manage, one set of services. We have
    reduced the latency between the answers because the calls are closer to one another.
    We are now able to offer regional data compliance.
  topic: business/strategy
- impact_reason: Indicates that automated prompt optimization, driven by evaluation
    frameworks, is a universally applicable and highly effective technique across
    all LLM-powered product lines.
  relevance_score: 8
  source: llm_enhanced
  text: We're using this same process for all our products using LLMs. Because each
    time we're applying the prompt optimization, the results get way better.
  topic: business/technical
- impact_reason: Defines Coveo's core business and positions them as pioneers in relevance
    AI, spanning traditional search to modern generative/agentic applications.
  relevance_score: 7
  source: llm_enhanced
  text: Coveo, we're the AI relevance company. We've been building for the last two
    decades the AI relevance platform that can power all searches, generative, or
    agentic experiences across an enterprise.
  topic: business
- impact_reason: Highlights the speed of deployment for managed RAG solutions, contrasting
    it with traditional, slower enterprise IT projects.
  relevance_score: 7
  source: llm_enhanced
  text: Zero was able to have in production their generative answering solution in
    days. So this solution can be deployed in days, not months.
  topic: business
- impact_reason: Clearly outlines the 'before' state (using GPT on Azure) and the
    'after' state (migrating to Titan on Bedrock), setting the stage for the migration
    discussion.
  relevance_score: 7
  source: llm_enhanced
  text: We were sending the prompt to GPT on Azure Cloud and streaming back the answer.
    Today, we're going to talk how we were able to migrate our GPT LLM model that
    we're using on Azure Cloud to Bedrock.
  topic: strategy
- impact_reason: A concise definition of Amazon Bedrock's value proposition for enterprise
    Gen AI adoption.
  relevance_score: 7
  source: llm_enhanced
  text: Bedrock is AWS's fully managed service for generative AI. So it has everything
    you need to prototype, build, deploy, and in production scale, generative applications.
  topic: technical
- impact_reason: 'Delineates a two-tiered strategy for AI adoption: managed solutions
    for quick Q&A vs. API/integration layers for building complex, custom agentic
    workflows.'
  relevance_score: 7
  source: llm_enhanced
  text: if you want to go deeper and develop AI agents, AI assistants, and have the
    full control on the experience and be able to do more than just question answering,
    then you can use our set of APIs and integrations.
  topic: strategy
- impact_reason: Demonstrates the necessity of platform agnosticism and broad integration
    capabilities for retrieval platforms to support the fragmented landscape of enterprise
    AI agents (Q, CoPilot, Bedrock).
  relevance_score: 7
  source: llm_enhanced
  text: We have plugins for Amazon Q. We have action groups for Bedrock. We have an
    MCP server. We have integrations with other platforms outside of AWS, such as
    Agent Force, Microsoft Co-Pilot.
  topic: business/strategy
- impact_reason: 'Provides a concrete, foundational example of a testable behavior
    for RAG systems: positive grounding.'
  relevance_score: 7
  source: llm_enhanced
  text: We need to be able to define specific behaviors and be able to test them.
    If I give you an example of a behavior for us, it's, for example, if the context
    contains the information, we want the LLM to answer.
  topic: technical
- impact_reason: Defines the requirement for semantic consistency over exact literal
    consistency in generative outputs, a key nuance for enterprise quality control.
  relevance_score: 7
  source: llm_enhanced
  text: if we're asking the same question over and over again, we want the same answer.
    It will not always be exactly the same. We want the same meaning for the answer.
  topic: technical/safety
source: Unknown Source
summary: '## Comprehensive Summary: Ep156: LLM Migrations to One Cloud: Coveo''s Strategic
  Move to Amazon Bedrock


  This podcast episode details Coveo''s strategic migration of its core Generative
  AI (Gen AI) functionality, specifically its Relevance Generative Answering (CRGA)
  RAG solution, from a multi-cloud setup (utilizing GPT models on Azure) entirely
  to **Amazon Bedrock**, leveraging the **Amazon Titan Lite** model. The discussion
  focuses heavily on the technical complexity of LLM model swapping and the robust
  framework Coveo built to manage this process.


  ### 1. Focus Area

  The primary focus is the **successful migration of a production-grade, enterprise
  RAG application (Coveo Relevance Generative Answering) to a single cloud provider
  (AWS) using Amazon Bedrock**. Key sub-topics include multi-cloud governance simplification,
  data residency compliance, LLM prompt engineering challenges across different models,
  and the development of automated evaluation frameworks for maintaining quality during
  model swaps.


  ### 2. Key Technical Insights

  *   **LLM Migration as a "Mind Transplant":** Migrating between foundation models
  (even minor version upgrades) is not a simple software upgrade but requires extensive
  re-engineering of prompts because different models interpret system messages and
  context differently due to varying training data.

  *   **Automated Evaluation Framework Necessity:** Coveo developed a comprehensive
  framework to define and automatically test over 20 expected behaviors (e.g., answering
  only when context is present, respecting language, maintaining consistent meaning)
  to ensure quality parity or improvement post-migration.

  *   **Automated Prompt Optimization:** The evaluation framework feeds into an automated
  process where prompt candidates are generated, evaluated against metrics, and refined
  iteratively by an LLM until performance plateaus, ensuring optimal results for the
  new model (Titan Lite).


  ### 3. Business/Investment Angle

  *   **Simplifying Governance and Compliance:** Moving to a single cloud (AWS) resolves
  Coveo’s multi-cloud governance complexity, security rule fragmentation, and simplifies
  meeting critical customer requirements for **data residency and regional compliance**.

  *   **Cost and Performance Optimization:** The migration to Titan Lite on Bedrock
  resulted in reduced infrastructure complexity and **lower costs** while meeting
  or exceeding the performance benchmarks previously achieved with proprietary models
  like GPT-4o mini.

  *   **Rapid Deployment for Customers:** Coveo’s managed RAG solution allows enterprise
  customers (like SAP Concur, saving €8M annually) to deploy generative answering
  solutions in **days, not months**, providing immediate, high-confidence results
  that reduce case creation by 20-30%.


  ### 4. Notable Companies/People

  *   **Coveo:** The AI relevance company, providing enterprise search, personalization,
  and generative answering solutions built on a unified indexing platform that respects
  access rights.

  *   **Sebastian Kulio (VP of AI Strategy, Coveo):** The key speaker detailing the
  technical challenges, the "mind transplant" analogy, and the success of the migration.

  *   **Janik Kongvijiam (AWS Solution Architect):** Host guiding the discussion,
  emphasizing the role of AWS services.

  *   **Amazon Bedrock:** AWS''s fully managed service for building generative AI
  applications, used here to host the Amazon Titan Lite model.

  *   **Amazon Titan Lite:** The specific foundation model Coveo selected for the
  migration on Bedrock.

  *   **ELB with Talent:** Mentioned as a key partner assisting with the Bedrock infrastructure
  setup and load testing (handling over 70 billion tokens monthly).


  ### 5. Future Implications

  The industry trend is moving toward **cloud consolidation for Gen AI workloads**
  to simplify governance and compliance. Furthermore, the necessity of sophisticated,
  **automated evaluation and prompt engineering frameworks** will become standard
  practice for any ISV relying on external foundation models, especially given the
  rapid deprecation cycles imposed by model providers. Coveo is positioning its platform
  to power both grounded generative answers and complex AI agents via unified retrieval
  APIs integrated across various platforms (including Amazon Q and Microsoft Co-Pilot).


  ### 6. Target Audience

  This episode is highly valuable for **AI/ML Engineers, CTOs, Cloud Architects, and
  Product Leaders at ISVs** who are currently managing multi-cloud AI deployments,
  struggling with prompt stability across model updates, or seeking to leverage managed
  services like Amazon Bedrock for enterprise-grade RAG solutions.'
tags:
- artificial-intelligence
- generative-ai
- investment
- ai-infrastructure
- google
- microsoft
title: 'Ep156: LLM Migrations to One Cloud: Coveo''s Strategic Move to Amazon Bedrock'
topics:
- keywords:
  - ai
  - machine learning
  - deep learning
  - neural networks
  - llm
  - large language model
  mentions: 62
  prominence: 1.0
  topic: artificial intelligence
- keywords:
  - generative ai
  - genai
  - chatgpt
  - gpt
  - claude
  - text generation
  - image generation
  mentions: 10
  prominence: 1.0
  topic: generative ai
- keywords:
  - investment
  - funding
  - valuation
  - ipo
  - acquisition
  mentions: 7
  prominence: 0.7
  topic: investment
- keywords:
  - gpu
  - tensor
  - training
  - inference
  - model deployment
  - vector database
  mentions: 2
  prominence: 0.2
  topic: ai infrastructure
---

<!-- Episode automatically generated from analysis data -->
<!-- Processing completed: 2025-10-09 14:10:30 UTC -->
